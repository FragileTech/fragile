{
  "chapter_index": 7,
  "section_id": "## 7. Algorithmic Construction from Swarm Trajectories",
  "directive_count": 6,
  "hints": [
    {
      "directive_type": "algorithm",
      "label": "alg-action-estimation",
      "title": "Discrete-Time Action Estimation from Swarm Data",
      "start_line": 2117,
      "end_line": 2205,
      "header_lines": [
        2118
      ],
      "content_start": 2120,
      "content_end": 2204,
      "content": "2120: :label: alg-action-estimation\n2121: \n2122: **Context:** For Langevin dynamics, sample paths have finite quadratic variation but are not differentiable (H\u00f6lder-$\\alpha$ for $\\alpha < 1/2$). Therefore, **we cannot compute $\\dot{v}(t)$ pointwise**. Instead, we use the **discrete-time log-likelihood** of the path, which is well-defined and approximates the Onsager-Machlup action as $\\Delta t \\to 0$.\n2123: \n2124: **Input:**\n2125: - Swarm trajectories $\\{(x_i(t_k), v_i(t_k))\\}_{i=1}^N$ at discrete times $t_k = k \\Delta t$ for $k = 0, 1, \\ldots, M$\n2126: - Parameters: $\\gamma, \\sigma_v, U_{\\text{eff}}(x)$ (or gradient $\\nabla U_{\\text{eff}}$)\n2127: - Time step $\\Delta t$\n2128: \n2129: **Output:**\n2130: - Estimated discrete action $\\hat{S}_{\\Delta t,i}$ for each trajectory\n2131: - Path measure statistics: $\\langle S_{\\Delta t} \\rangle$, $\\text{Var}(S_{\\Delta t})$\n2132: - Most probable path (minimum action)\n2133: \n2134: **Theoretical Foundation:**\n2135: \n2136: The kinetic operator generates velocities via the Ornstein-Uhlenbeck process (consistent with {prf:ref}`prop-kinetic-operator-path-measure`):\n2137: \n2138: $$\n2139: dv_t = \\left(-\\nabla U_{\\text{eff}}(x_t) - \\gamma v_t\\right) dt + \\sqrt{2\\gamma T} \\, dW_t\n2140: $$\n2141: \n2142: where $T = \\sigma_v^2/\\gamma$ is the effective temperature, so $\\sqrt{2\\gamma T} = \\sqrt{2}\\sigma_v$.\n2143: \n2144: The discrete-time transition density (Euler-Maruyama approximation) is Gaussian:\n2145: \n2146: $$\n2147: v_{k+1} \\mid v_k, x_k \\sim \\mathcal{N}\\left(v_k + F_k \\Delta t, \\, 2\\sigma_v^2 \\Delta t \\cdot I\\right)\n2148: $$\n2149: \n2150: where $F_k = -\\nabla U_{\\text{eff}}(x_k) - \\gamma v_k$ is the deterministic drift.\n2151: \n2152: The **negative log-likelihood** of the discrete path $(v_0, v_1, \\ldots, v_M)$ is:\n2153: \n2154: $$\n2155: -\\log P[v_0, \\ldots, v_M] = \\sum_{k=0}^{M-1} \\frac{\\|v_{k+1} - v_k - F_k \\Delta t\\|^2}{4\\sigma_v^2 \\Delta t} + \\text{const}\n2156: $$\n2157: \n2158: This defines the **discrete Onsager-Machlup action**:\n2159: \n2160: $$\n2161: S_{\\Delta t}[v] = \\frac{\\Delta t}{4\\sigma_v^2} \\sum_{k=0}^{M-1} \\left\\|\\frac{v_{k+1} - v_k}{\\Delta t} + \\nabla U_{\\text{eff}}(x_k) + \\gamma v_k\\right\\|^2\n2162: $$\n2163: \n2164: As $\\Delta t \\to 0$, $S_{\\Delta t} \\to S_{\\text{OM}}$ (Freidlin-Wentzell approximation).\n2165: \n2166: **Algorithm Steps:**\n2167: \n2168: 1. **Trajectory Validation:**\n2169:    - Check viability: discard trajectories exiting $\\mathcal{X}_{\\text{valid}}$\n2170:    - Ensure $M \\geq 2$ (at least two time steps)\n2171: \n2172: 2. **Compute Discrete Drift:**\n2173: \n2174: For each walker $i$ and time step $k = 0, \\ldots, M-1$:\n2175: \n2176: $$\n2177: F_{i,k} = -\\nabla U_{\\text{eff}}(x_i(t_k)) - \\gamma v_i(t_k)\n2178: $$\n2179: \n2180: 3. **Evaluate Discrete Action:**\n2181: \n2182: For each walker $i$:\n2183: \n2184: $$\n2185: \\hat{S}_{\\Delta t,i} = \\frac{\\Delta t}{4\\sigma_v^2} \\sum_{k=0}^{M-1} \\left\\|\\frac{v_i(t_{k+1}) - v_i(t_k)}{\\Delta t} - F_{i,k}\\right\\|^2\n2186: $$\n2187: \n2188: Equivalently, using $T = \\sigma_v^2/\\gamma$ and $4\\sigma_v^2 = 4\\gamma T$:\n2189: \n2190: $$\n2191: \\hat{S}_{\\Delta t,i} = \\frac{1}{4\\gamma T} \\sum_{k=0}^{M-1} \\Delta t \\left\\|\\frac{v_i(t_{k+1}) - v_i(t_k)}{\\Delta t} + \\nabla U_{\\text{eff}}(x_i(t_k)) + \\gamma v_i(t_k)\\right\\|^2\n2192: $$\n2193: \n2194: 4. **Compute Path Statistics:**\n2195:    - Mean action: $\\langle S_{\\Delta t} \\rangle = \\frac{1}{N}\\sum_{i=1}^N \\hat{S}_{\\Delta t,i}$\n2196:    - Variance: $\\text{Var}(S_{\\Delta t}) = \\frac{1}{N}\\sum_{i} (\\hat{S}_{\\Delta t,i} - \\langle S_{\\Delta t} \\rangle)^2$\n2197:    - Path probability weights: $w_i = \\exp(-\\hat{S}_{\\Delta t,i})$ (normalized)\n2198: \n2199: 5. **Identify Most Probable Path:**\n2200: \n2201: $$\n2202: i^* = \\arg\\min_i \\hat{S}_{\\Delta t,i}\n2203: $$\n2204: ",
      "metadata": {
        "label": "alg-action-estimation"
      },
      "section": "## 7. Algorithmic Construction from Swarm Trajectories",
      "raw_directive": "2117: We now develop algorithms to compute the Onsager-Machlup action from **empirical swarm trajectories**.\n2118: \n2119: :::{prf:algorithm} Discrete-Time Action Estimation from Swarm Data\n2120: :label: alg-action-estimation\n2121: \n2122: **Context:** For Langevin dynamics, sample paths have finite quadratic variation but are not differentiable (H\u00f6lder-$\\alpha$ for $\\alpha < 1/2$). Therefore, **we cannot compute $\\dot{v}(t)$ pointwise**. Instead, we use the **discrete-time log-likelihood** of the path, which is well-defined and approximates the Onsager-Machlup action as $\\Delta t \\to 0$.\n2123: \n2124: **Input:**\n2125: - Swarm trajectories $\\{(x_i(t_k), v_i(t_k))\\}_{i=1}^N$ at discrete times $t_k = k \\Delta t$ for $k = 0, 1, \\ldots, M$\n2126: - Parameters: $\\gamma, \\sigma_v, U_{\\text{eff}}(x)$ (or gradient $\\nabla U_{\\text{eff}}$)\n2127: - Time step $\\Delta t$\n2128: \n2129: **Output:**\n2130: - Estimated discrete action $\\hat{S}_{\\Delta t,i}$ for each trajectory\n2131: - Path measure statistics: $\\langle S_{\\Delta t} \\rangle$, $\\text{Var}(S_{\\Delta t})$\n2132: - Most probable path (minimum action)\n2133: \n2134: **Theoretical Foundation:**\n2135: \n2136: The kinetic operator generates velocities via the Ornstein-Uhlenbeck process (consistent with {prf:ref}`prop-kinetic-operator-path-measure`):\n2137: \n2138: $$\n2139: dv_t = \\left(-\\nabla U_{\\text{eff}}(x_t) - \\gamma v_t\\right) dt + \\sqrt{2\\gamma T} \\, dW_t\n2140: $$\n2141: \n2142: where $T = \\sigma_v^2/\\gamma$ is the effective temperature, so $\\sqrt{2\\gamma T} = \\sqrt{2}\\sigma_v$.\n2143: \n2144: The discrete-time transition density (Euler-Maruyama approximation) is Gaussian:\n2145: \n2146: $$\n2147: v_{k+1} \\mid v_k, x_k \\sim \\mathcal{N}\\left(v_k + F_k \\Delta t, \\, 2\\sigma_v^2 \\Delta t \\cdot I\\right)\n2148: $$\n2149: \n2150: where $F_k = -\\nabla U_{\\text{eff}}(x_k) - \\gamma v_k$ is the deterministic drift.\n2151: \n2152: The **negative log-likelihood** of the discrete path $(v_0, v_1, \\ldots, v_M)$ is:\n2153: \n2154: $$\n2155: -\\log P[v_0, \\ldots, v_M] = \\sum_{k=0}^{M-1} \\frac{\\|v_{k+1} - v_k - F_k \\Delta t\\|^2}{4\\sigma_v^2 \\Delta t} + \\text{const}\n2156: $$\n2157: \n2158: This defines the **discrete Onsager-Machlup action**:\n2159: \n2160: $$\n2161: S_{\\Delta t}[v] = \\frac{\\Delta t}{4\\sigma_v^2} \\sum_{k=0}^{M-1} \\left\\|\\frac{v_{k+1} - v_k}{\\Delta t} + \\nabla U_{\\text{eff}}(x_k) + \\gamma v_k\\right\\|^2\n2162: $$\n2163: \n2164: As $\\Delta t \\to 0$, $S_{\\Delta t} \\to S_{\\text{OM}}$ (Freidlin-Wentzell approximation).\n2165: \n2166: **Algorithm Steps:**\n2167: \n2168: 1. **Trajectory Validation:**\n2169:    - Check viability: discard trajectories exiting $\\mathcal{X}_{\\text{valid}}$\n2170:    - Ensure $M \\geq 2$ (at least two time steps)\n2171: \n2172: 2. **Compute Discrete Drift:**\n2173: \n2174: For each walker $i$ and time step $k = 0, \\ldots, M-1$:\n2175: \n2176: $$\n2177: F_{i,k} = -\\nabla U_{\\text{eff}}(x_i(t_k)) - \\gamma v_i(t_k)\n2178: $$\n2179: \n2180: 3. **Evaluate Discrete Action:**\n2181: \n2182: For each walker $i$:\n2183: \n2184: $$\n2185: \\hat{S}_{\\Delta t,i} = \\frac{\\Delta t}{4\\sigma_v^2} \\sum_{k=0}^{M-1} \\left\\|\\frac{v_i(t_{k+1}) - v_i(t_k)}{\\Delta t} - F_{i,k}\\right\\|^2\n2186: $$\n2187: \n2188: Equivalently, using $T = \\sigma_v^2/\\gamma$ and $4\\sigma_v^2 = 4\\gamma T$:\n2189: \n2190: $$\n2191: \\hat{S}_{\\Delta t,i} = \\frac{1}{4\\gamma T} \\sum_{k=0}^{M-1} \\Delta t \\left\\|\\frac{v_i(t_{k+1}) - v_i(t_k)}{\\Delta t} + \\nabla U_{\\text{eff}}(x_i(t_k)) + \\gamma v_i(t_k)\\right\\|^2\n2192: $$\n2193: \n2194: 4. **Compute Path Statistics:**\n2195:    - Mean action: $\\langle S_{\\Delta t} \\rangle = \\frac{1}{N}\\sum_{i=1}^N \\hat{S}_{\\Delta t,i}$\n2196:    - Variance: $\\text{Var}(S_{\\Delta t}) = \\frac{1}{N}\\sum_{i} (\\hat{S}_{\\Delta t,i} - \\langle S_{\\Delta t} \\rangle)^2$\n2197:    - Path probability weights: $w_i = \\exp(-\\hat{S}_{\\Delta t,i})$ (normalized)\n2198: \n2199: 5. **Identify Most Probable Path:**\n2200: \n2201: $$\n2202: i^* = \\arg\\min_i \\hat{S}_{\\Delta t,i}\n2203: $$\n2204: \n2205: :::{warning}"
    },
    {
      "directive_type": "theorem",
      "label": "thm-action-estimator-convergence",
      "title": "Convergence of Discrete Action Estimator",
      "start_line": 2210,
      "end_line": 2288,
      "header_lines": [
        2211
      ],
      "content_start": 2213,
      "content_end": 2287,
      "content": "2213: :label: thm-action-estimator-convergence\n2214: \n2215: Let $\\{v(t_k)\\}_{k=0}^M$ be a discrete-time sample from the Langevin kinetic operator with time step $\\Delta t$, and let $\\hat{S}_{\\Delta t}$ be the discrete action from {prf:ref}`alg-action-estimation`.\n2216: \n2217: **Weak Convergence:**\n2218: \n2219: As $\\Delta t \\to 0$ (with $M \\Delta t = T$ fixed), the discrete action converges to the Onsager-Machlup action:\n2220: \n2221: $$\n2222: \\hat{S}_{\\Delta t} \\xrightarrow{d} S_{\\text{OM}}\n2223: $$\n2224: \n2225: in distribution, with:\n2226: \n2227: $$\n2228: \\mathbb{E}[|\\hat{S}_{\\Delta t} - S_{\\text{OM}}|] \\leq C \\Delta t\n2229: $$\n2230: \n2231: where $C$ depends on $T$, $\\|\\nabla^2 U_{\\text{eff}}\\|_{\\infty}$, $\\gamma$, and $\\sigma_v$.\n2232: \n2233: **Proof Sketch:**\n2234: \n2235: The discrete action is:\n2236: \n2237: $$\n2238: \\hat{S}_{\\Delta t} = \\frac{1}{4\\sigma_v^2} \\sum_{k=0}^{M-1} \\frac{\\|v_{k+1} - v_k - F_k \\Delta t\\|^2}{\\Delta t}\n2239: $$\n2240: \n2241: By the Euler-Maruyama approximation theory (Kloeden & Platen, Theorem 10.2.2), the discrete-time Langevin scheme has **strong convergence** of order $\\Delta t^{1/2}$:\n2242: \n2243: $$\n2244: \\mathbb{E}\\left[\\sup_{0 \\leq t \\leq T} \\|v^{\\Delta t}(t) - v(t)\\|\\right] \\leq C_1 \\Delta t^{1/2}\n2245: $$\n2246: \n2247: where $v^{\\Delta t}(t)$ is the piecewise-linear interpolation of the discrete samples.\n2248: \n2249: The discrete action can be rewritten as:\n2250: \n2251: $$\n2252: \\hat{S}_{\\Delta t} = \\frac{1}{4\\sigma_v^2} \\sum_{k=0}^{M-1} \\Delta t \\left\\|\\frac{v_{k+1} - v_k}{\\Delta t} - F_k\\right\\|^2\n2253: $$\n2254: \n2255: This is a **Riemann sum** approximating the (generalized) integral defining $S_{\\text{OM}}$. However, since $(v_{k+1} - v_k)/\\Delta t$ does **not** converge pointwise to $\\dot{v}(t_k)$ (paths are not differentiable), we must interpret the convergence in the sense of distributions or weak convergence.\n2256: \n2257: By the Girsanov change-of-measure formula (Section 1.3), the discrete negative log-likelihood $-\\log P[v_0, \\ldots, v_M]$ equals $\\hat{S}_{\\Delta t}$ plus a constant. The continuous-time limit of this log-likelihood is precisely the Onsager-Machlup action $S_{\\text{OM}}$ (Freidlin & Wentzell, 2012, Theorem 3.1).\n2258: \n2259: The $O(\\Delta t)$ error bound comes from the weak approximation error of the Euler-Maruyama scheme (Kloeden & Platen, Theorem 14.5.1), which states:\n2260: \n2261: $$\n2262: |\\mathbb{E}[f(v^{\\Delta t}_T)] - \\mathbb{E}[f(v_T)]| \\leq C_f \\Delta t\n2263: $$\n2264: \n2265: for sufficiently smooth test functions $f$. Applying this to the action functional yields the stated bound. \u220e\n2266: \n2267: **Statistical Error (Finite Walkers):**\n2268: \n2269: For $N$ independent walkers:\n2270: \n2271: $$\n2272: \\mathbb{E}\\left[|\\langle \\hat{S}_{\\Delta t} \\rangle_N - \\mathbb{E}[\\hat{S}_{\\Delta t}]|\\right] \\leq \\frac{\\text{Std}(S_{\\Delta t})}{\\sqrt{N}}\n2273: $$\n2274: \n2275: by the central limit theorem.\n2276: \n2277: **Combined Error:**\n2278: \n2279: $$\n2280: \\text{Total Error} = O(\\Delta t) + O(N^{-1/2})\n2281: $$\n2282: \n2283: **Convergence Rate:**\n2284: \n2285: To achieve error $\\epsilon$, require:\n2286: - $\\Delta t \\sim O(\\epsilon)$ (weak convergence rate)\n2287: - $N \\sim O(\\epsilon^{-2})$ (Monte Carlo rate)",
      "metadata": {
        "label": "thm-action-estimator-convergence"
      },
      "section": "## 7. Algorithmic Construction from Swarm Trajectories",
      "raw_directive": "2210: ### 7.2. Error Analysis\n2211: \n2212: :::{prf:theorem} Convergence of Discrete Action Estimator\n2213: :label: thm-action-estimator-convergence\n2214: \n2215: Let $\\{v(t_k)\\}_{k=0}^M$ be a discrete-time sample from the Langevin kinetic operator with time step $\\Delta t$, and let $\\hat{S}_{\\Delta t}$ be the discrete action from {prf:ref}`alg-action-estimation`.\n2216: \n2217: **Weak Convergence:**\n2218: \n2219: As $\\Delta t \\to 0$ (with $M \\Delta t = T$ fixed), the discrete action converges to the Onsager-Machlup action:\n2220: \n2221: $$\n2222: \\hat{S}_{\\Delta t} \\xrightarrow{d} S_{\\text{OM}}\n2223: $$\n2224: \n2225: in distribution, with:\n2226: \n2227: $$\n2228: \\mathbb{E}[|\\hat{S}_{\\Delta t} - S_{\\text{OM}}|] \\leq C \\Delta t\n2229: $$\n2230: \n2231: where $C$ depends on $T$, $\\|\\nabla^2 U_{\\text{eff}}\\|_{\\infty}$, $\\gamma$, and $\\sigma_v$.\n2232: \n2233: **Proof Sketch:**\n2234: \n2235: The discrete action is:\n2236: \n2237: $$\n2238: \\hat{S}_{\\Delta t} = \\frac{1}{4\\sigma_v^2} \\sum_{k=0}^{M-1} \\frac{\\|v_{k+1} - v_k - F_k \\Delta t\\|^2}{\\Delta t}\n2239: $$\n2240: \n2241: By the Euler-Maruyama approximation theory (Kloeden & Platen, Theorem 10.2.2), the discrete-time Langevin scheme has **strong convergence** of order $\\Delta t^{1/2}$:\n2242: \n2243: $$\n2244: \\mathbb{E}\\left[\\sup_{0 \\leq t \\leq T} \\|v^{\\Delta t}(t) - v(t)\\|\\right] \\leq C_1 \\Delta t^{1/2}\n2245: $$\n2246: \n2247: where $v^{\\Delta t}(t)$ is the piecewise-linear interpolation of the discrete samples.\n2248: \n2249: The discrete action can be rewritten as:\n2250: \n2251: $$\n2252: \\hat{S}_{\\Delta t} = \\frac{1}{4\\sigma_v^2} \\sum_{k=0}^{M-1} \\Delta t \\left\\|\\frac{v_{k+1} - v_k}{\\Delta t} - F_k\\right\\|^2\n2253: $$\n2254: \n2255: This is a **Riemann sum** approximating the (generalized) integral defining $S_{\\text{OM}}$. However, since $(v_{k+1} - v_k)/\\Delta t$ does **not** converge pointwise to $\\dot{v}(t_k)$ (paths are not differentiable), we must interpret the convergence in the sense of distributions or weak convergence.\n2256: \n2257: By the Girsanov change-of-measure formula (Section 1.3), the discrete negative log-likelihood $-\\log P[v_0, \\ldots, v_M]$ equals $\\hat{S}_{\\Delta t}$ plus a constant. The continuous-time limit of this log-likelihood is precisely the Onsager-Machlup action $S_{\\text{OM}}$ (Freidlin & Wentzell, 2012, Theorem 3.1).\n2258: \n2259: The $O(\\Delta t)$ error bound comes from the weak approximation error of the Euler-Maruyama scheme (Kloeden & Platen, Theorem 14.5.1), which states:\n2260: \n2261: $$\n2262: |\\mathbb{E}[f(v^{\\Delta t}_T)] - \\mathbb{E}[f(v_T)]| \\leq C_f \\Delta t\n2263: $$\n2264: \n2265: for sufficiently smooth test functions $f$. Applying this to the action functional yields the stated bound. \u220e\n2266: \n2267: **Statistical Error (Finite Walkers):**\n2268: \n2269: For $N$ independent walkers:\n2270: \n2271: $$\n2272: \\mathbb{E}\\left[|\\langle \\hat{S}_{\\Delta t} \\rangle_N - \\mathbb{E}[\\hat{S}_{\\Delta t}]|\\right] \\leq \\frac{\\text{Std}(S_{\\Delta t})}{\\sqrt{N}}\n2273: $$\n2274: \n2275: by the central limit theorem.\n2276: \n2277: **Combined Error:**\n2278: \n2279: $$\n2280: \\text{Total Error} = O(\\Delta t) + O(N^{-1/2})\n2281: $$\n2282: \n2283: **Convergence Rate:**\n2284: \n2285: To achieve error $\\epsilon$, require:\n2286: - $\\Delta t \\sim O(\\epsilon)$ (weak convergence rate)\n2287: - $N \\sim O(\\epsilon^{-2})$ (Monte Carlo rate)\n2288: "
    },
    {
      "directive_type": "remark",
      "label": "unlabeled-remark-178",
      "title": "Practical Considerations",
      "start_line": 2290,
      "end_line": 2294,
      "header_lines": [],
      "content_start": 2291,
      "content_end": 2293,
      "content": "2291: \n2292: :::{prf:remark} Practical Considerations\n2293: - **Curvature estimation**: Computing $\\nabla U_{\\text{eff}}(x)$ requires either analytical gradient or numerical differentiation (adds $O(\\Delta x)$ error)",
      "metadata": {},
      "section": "## 7. Algorithmic Construction from Swarm Trajectories",
      "raw_directive": "2290: :::\n2291: \n2292: :::{prf:remark} Practical Considerations\n2293: - **Curvature estimation**: Computing $\\nabla U_{\\text{eff}}(x)$ requires either analytical gradient or numerical differentiation (adds $O(\\Delta x)$ error)\n2294: - **Boundary issues**: Near $\\partial \\mathcal{X}_{\\text{valid}}$, paths may be curtailed; use importance reweighting ({prf:ref}`alg-importance-reweighting-paths`)"
    },
    {
      "directive_type": "algorithm",
      "label": "alg-importance-reweighting-paths",
      "title": "Importance Reweighting for Path Space",
      "start_line": 2300,
      "end_line": 2349,
      "header_lines": [
        2301
      ],
      "content_start": 2303,
      "content_end": 2348,
      "content": "2303: :label: alg-importance-reweighting-paths\n2304: \n2305: **Goal:** Estimate $\\langle \\mathcal{O} \\rangle_{\\text{target}}$ for a functional $\\mathcal{O}: \\mathcal{P} \\to \\mathbb{R}$ under target measure $\\mathbb{P}_{\\text{target}}$, using samples from biased measure $\\mathbb{P}_{\\text{swarm}}$.\n2306: \n2307: **Input:**\n2308: - $N$ paths $\\{\\gamma_i\\}_{i=1}^N \\sim \\mathbb{P}_{\\text{swarm}}$\n2309: - Target action $S_{\\text{target}}[\\gamma]$ and swarm action $S_{\\text{swarm}}[\\gamma]$\n2310: \n2311: **Output:**\n2312: - Unbiased estimate $\\hat{\\mathcal{O}}$\n2313: \n2314: **Steps:**\n2315: \n2316: 1. **Compute Actions:**\n2317: \n2318: \n2319: $$\n2320: S_{\\text{target},i} = S_{\\text{target}}[\\gamma_i], \\quad S_{\\text{swarm},i} = S_{\\text{swarm}}[\\gamma_i]\n2321: $$\n2322: \n2323: 2. **Compute Importance Weights:**\n2324: \n2325: \n2326: $$\n2327: w_i = \\frac{e^{-S_{\\text{target},i}/T}}{e^{-S_{\\text{swarm},i}/T}} = e^{-(S_{\\text{target},i} - S_{\\text{swarm},i})/T}\n2328: $$\n2329: \n2330: 3. **Normalize Weights:**\n2331: \n2332: \n2333: $$\n2334: \\tilde{w}_i = \\frac{w_i}{\\sum_{j=1}^N w_j}\n2335: $$\n2336: \n2337: 4. **Reweighted Estimate:**\n2338: \n2339: \n2340: $$\n2341: \\hat{\\mathcal{O}} = \\sum_{i=1}^N \\tilde{w}_i \\mathcal{O}[\\gamma_i]\n2342: $$\n2343: \n2344: **Error Bound:**\n2345: \n2346: $$\n2347: \\mathbb{E}[|\\hat{\\mathcal{O}} - \\langle \\mathcal{O} \\rangle|] \\leq \\frac{\\text{ESS}^{-1/2}}{\\sqrt{N}} \\|\\mathcal{O}\\|_{\\infty}\n2348: $$",
      "metadata": {
        "label": "alg-importance-reweighting-paths"
      },
      "section": "## 7. Algorithmic Construction from Swarm Trajectories",
      "raw_directive": "2300: When the swarm is not at QSD (transient regime), paths are biased.\n2301: \n2302: :::{prf:algorithm} Importance Reweighting for Path Space\n2303: :label: alg-importance-reweighting-paths\n2304: \n2305: **Goal:** Estimate $\\langle \\mathcal{O} \\rangle_{\\text{target}}$ for a functional $\\mathcal{O}: \\mathcal{P} \\to \\mathbb{R}$ under target measure $\\mathbb{P}_{\\text{target}}$, using samples from biased measure $\\mathbb{P}_{\\text{swarm}}$.\n2306: \n2307: **Input:**\n2308: - $N$ paths $\\{\\gamma_i\\}_{i=1}^N \\sim \\mathbb{P}_{\\text{swarm}}$\n2309: - Target action $S_{\\text{target}}[\\gamma]$ and swarm action $S_{\\text{swarm}}[\\gamma]$\n2310: \n2311: **Output:**\n2312: - Unbiased estimate $\\hat{\\mathcal{O}}$\n2313: \n2314: **Steps:**\n2315: \n2316: 1. **Compute Actions:**\n2317: \n2318: \n2319: $$\n2320: S_{\\text{target},i} = S_{\\text{target}}[\\gamma_i], \\quad S_{\\text{swarm},i} = S_{\\text{swarm}}[\\gamma_i]\n2321: $$\n2322: \n2323: 2. **Compute Importance Weights:**\n2324: \n2325: \n2326: $$\n2327: w_i = \\frac{e^{-S_{\\text{target},i}/T}}{e^{-S_{\\text{swarm},i}/T}} = e^{-(S_{\\text{target},i} - S_{\\text{swarm},i})/T}\n2328: $$\n2329: \n2330: 3. **Normalize Weights:**\n2331: \n2332: \n2333: $$\n2334: \\tilde{w}_i = \\frac{w_i}{\\sum_{j=1}^N w_j}\n2335: $$\n2336: \n2337: 4. **Reweighted Estimate:**\n2338: \n2339: \n2340: $$\n2341: \\hat{\\mathcal{O}} = \\sum_{i=1}^N \\tilde{w}_i \\mathcal{O}[\\gamma_i]\n2342: $$\n2343: \n2344: **Error Bound:**\n2345: \n2346: $$\n2347: \\mathbb{E}[|\\hat{\\mathcal{O}} - \\langle \\mathcal{O} \\rangle|] \\leq \\frac{\\text{ESS}^{-1/2}}{\\sqrt{N}} \\|\\mathcal{O}\\|_{\\infty}\n2348: $$\n2349: "
    },
    {
      "directive_type": "algorithm",
      "label": "alg-transition-path-sampling",
      "title": "Transition Path Sampling",
      "start_line": 2355,
      "end_line": 2391,
      "header_lines": [
        2356
      ],
      "content_start": 2358,
      "content_end": 2390,
      "content": "2358: :label: alg-transition-path-sampling\n2359: \n2360: **Goal:** Sample paths connecting two regions $A, B \\subset \\mathcal{M}$ (e.g., reactant and product states).\n2361: \n2362: **Method:** Markov chain Monte Carlo on path space.\n2363: \n2364: **Steps:**\n2365: \n2366: 1. **Initialize:** Start with a known transition path $\\gamma_0: A \\to B$ (e.g., instanton or deterministic trajectory)\n2367: \n2368: 2. **Shooting Move:**\n2369:    - Select random time $t^* \\in (0, T)$\n2370:    - Perturb velocity: $v^* \\to v^* + \\delta v$ where $\\delta v \\sim \\mathcal{N}(0, \\epsilon^2 I)$\n2371:    - Integrate forward and backward to generate new path $\\gamma'$\n2372: \n2373: 3. **Acceptance:**\n2374:    - Check if $\\gamma'$ connects $A \\to B$ (both endpoints in correct regions)\n2375:    - Compute acceptance probability (Metropolis-Hastings):\n2376: \n2377: \n2378: $$\n2379: \\alpha = \\min\\left(1, e^{-[S_{\\text{OM}}[\\gamma'] - S_{\\text{OM}}[\\gamma]]/T}\\right)\n2380: $$\n2381: \n2382:    - Accept with probability $\\alpha$\n2383: \n2384: 4. **Iterate:** Repeat Steps 2-3 for $M$ iterations\n2385: \n2386: **Output:** Ensemble of $M$ transition paths $\\{\\gamma_1, \\ldots, \\gamma_M\\}$\n2387: \n2388: **Efficiency:**\n2389: \n2390: Acceptance rate depends on $\\epsilon$ (perturbation size). Optimal $\\epsilon \\sim \\sqrt{T \\Delta t}$ gives $\\sim 30\\%$ acceptance.",
      "metadata": {
        "label": "alg-transition-path-sampling"
      },
      "section": "## 7. Algorithmic Construction from Swarm Trajectories",
      "raw_directive": "2355: For rare events, direct sampling is inefficient. We use **transition path sampling**.\n2356: \n2357: :::{prf:algorithm} Transition Path Sampling\n2358: :label: alg-transition-path-sampling\n2359: \n2360: **Goal:** Sample paths connecting two regions $A, B \\subset \\mathcal{M}$ (e.g., reactant and product states).\n2361: \n2362: **Method:** Markov chain Monte Carlo on path space.\n2363: \n2364: **Steps:**\n2365: \n2366: 1. **Initialize:** Start with a known transition path $\\gamma_0: A \\to B$ (e.g., instanton or deterministic trajectory)\n2367: \n2368: 2. **Shooting Move:**\n2369:    - Select random time $t^* \\in (0, T)$\n2370:    - Perturb velocity: $v^* \\to v^* + \\delta v$ where $\\delta v \\sim \\mathcal{N}(0, \\epsilon^2 I)$\n2371:    - Integrate forward and backward to generate new path $\\gamma'$\n2372: \n2373: 3. **Acceptance:**\n2374:    - Check if $\\gamma'$ connects $A \\to B$ (both endpoints in correct regions)\n2375:    - Compute acceptance probability (Metropolis-Hastings):\n2376: \n2377: \n2378: $$\n2379: \\alpha = \\min\\left(1, e^{-[S_{\\text{OM}}[\\gamma'] - S_{\\text{OM}}[\\gamma]]/T}\\right)\n2380: $$\n2381: \n2382:    - Accept with probability $\\alpha$\n2383: \n2384: 4. **Iterate:** Repeat Steps 2-3 for $M$ iterations\n2385: \n2386: **Output:** Ensemble of $M$ transition paths $\\{\\gamma_1, \\ldots, \\gamma_M\\}$\n2387: \n2388: **Efficiency:**\n2389: \n2390: Acceptance rate depends on $\\epsilon$ (perturbation size). Optimal $\\epsilon \\sim \\sqrt{T \\Delta t}$ gives $\\sim 30\\%$ acceptance.\n2391: "
    },
    {
      "directive_type": "proposition",
      "label": "prop-path-space-complexity",
      "title": "Complexity of Path Space Algorithms",
      "start_line": 2395,
      "end_line": 2416,
      "header_lines": [
        2396
      ],
      "content_start": 2398,
      "content_end": 2415,
      "content": "2398: :label: prop-path-space-complexity\n2399: \n2400: **Action Estimation ({prf:ref}`alg-action-estimation`):**\n2401: - Per-path cost: $O(M)$ where $M$ is number of time samples\n2402: - Total cost for $N$ walkers: $O(NM)$\n2403: - Gradient evaluation: $+O(NM d_{\\text{space}})$\n2404: \n2405: **Importance Reweighting ({prf:ref}`alg-importance-reweighting-paths`):**\n2406: - Weight computation: $O(N)$\n2407: - Negligible compared to action evaluation\n2408: \n2409: **Transition Path Sampling ({prf:ref}`alg-transition-path-sampling`):**\n2410: - Per-iteration cost: $O(M d)$ (integrating trajectory)\n2411: - Equilibration: $\\sim 10^3$ iterations\n2412: - Production: $\\sim 10^4 - 10^5$ paths for statistics\n2413: - Total: $O(10^5 M d)$\n2414: \n2415: **Scalability:**",
      "metadata": {
        "label": "prop-path-space-complexity"
      },
      "section": "## 7. Algorithmic Construction from Swarm Trajectories",
      "raw_directive": "2395: ### 7.5. Computational Complexity\n2396: \n2397: :::{prf:proposition} Complexity of Path Space Algorithms\n2398: :label: prop-path-space-complexity\n2399: \n2400: **Action Estimation ({prf:ref}`alg-action-estimation`):**\n2401: - Per-path cost: $O(M)$ where $M$ is number of time samples\n2402: - Total cost for $N$ walkers: $O(NM)$\n2403: - Gradient evaluation: $+O(NM d_{\\text{space}})$\n2404: \n2405: **Importance Reweighting ({prf:ref}`alg-importance-reweighting-paths`):**\n2406: - Weight computation: $O(N)$\n2407: - Negligible compared to action evaluation\n2408: \n2409: **Transition Path Sampling ({prf:ref}`alg-transition-path-sampling`):**\n2410: - Per-iteration cost: $O(M d)$ (integrating trajectory)\n2411: - Equilibration: $\\sim 10^3$ iterations\n2412: - Production: $\\sim 10^4 - 10^5$ paths for statistics\n2413: - Total: $O(10^5 M d)$\n2414: \n2415: **Scalability:**\n2416: - Embarrassingly parallel over walkers (linear speedup with cores)"
    }
  ],
  "validation": {
    "ok": true,
    "errors": []
  }
}