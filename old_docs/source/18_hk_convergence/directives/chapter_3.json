{
  "chapter_index": 3,
  "section_id": "## 3. Lemma B: Exponential Contraction of Structural Variance",
  "directive_count": 2,
  "hints": [
    {
      "directive_type": "lemma",
      "label": "lem-structural-variance-contraction",
      "title": "Exponential Contraction of Structural Variance",
      "start_line": 804,
      "end_line": 821,
      "header_lines": [
        805
      ],
      "content_start": 807,
      "content_end": 820,
      "content": "807: :label: lem-structural-variance-contraction\n808: \n809: Let $\\mu_t$ be the law of the empirical measure at time $t$ and let $\\pi_{\\text{QSD}}$ be the quasi-stationary distribution. Assume $\\pi_{\\text{QSD}}$ is log-concave (Axiom {prf:ref}`ax-qsd-log-concave`).\n810: \n811: Then for any initial measure $\\mu_0$, the structural variance $V_{\\text{struct}}(\\mu_t, \\pi_{\\text{QSD}})$ contracts exponentially to zero:\n812: \n813: $$\n814: V_{\\text{struct}}(\\mu_t, \\pi_{\\text{QSD}}) \\leq \\frac{2}{\\kappa_{\\text{conf}}} \\cdot D_{\\text{KL}}(\\mu_0 \\| \\pi_{\\text{QSD}}) \\cdot e^{-\\lambda t}\n815: $$\n816: \n817: where:\n818: - $\\lambda > 0$ is the exponential convergence rate from the LSI (Theorem {prf:ref}`thm-main-kl-convergence` in [10_kl_convergence.md](../10_kl_convergence/10_kl_convergence.md))\n819: - $\\kappa_{\\text{conf}} > 0$ is the strong convexity constant of the confining potential\n820: - $D_{\\text{KL}}(\\mu_0 \\| \\pi_{\\text{QSD}})$ is the initial KL-divergence from equilibrium",
      "metadata": {
        "label": "lem-structural-variance-contraction"
      },
      "section": "## 3. Lemma B: Exponential Contraction of Structural Variance",
      "raw_directive": "804: 3. Variance decomposition of Wasserstein distance\n805: \n806: :::{prf:lemma} Exponential Contraction of Structural Variance\n807: :label: lem-structural-variance-contraction\n808: \n809: Let $\\mu_t$ be the law of the empirical measure at time $t$ and let $\\pi_{\\text{QSD}}$ be the quasi-stationary distribution. Assume $\\pi_{\\text{QSD}}$ is log-concave (Axiom {prf:ref}`ax-qsd-log-concave`).\n810: \n811: Then for any initial measure $\\mu_0$, the structural variance $V_{\\text{struct}}(\\mu_t, \\pi_{\\text{QSD}})$ contracts exponentially to zero:\n812: \n813: $$\n814: V_{\\text{struct}}(\\mu_t, \\pi_{\\text{QSD}}) \\leq \\frac{2}{\\kappa_{\\text{conf}}} \\cdot D_{\\text{KL}}(\\mu_0 \\| \\pi_{\\text{QSD}}) \\cdot e^{-\\lambda t}\n815: $$\n816: \n817: where:\n818: - $\\lambda > 0$ is the exponential convergence rate from the LSI (Theorem {prf:ref}`thm-main-kl-convergence` in [10_kl_convergence.md](../10_kl_convergence/10_kl_convergence.md))\n819: - $\\kappa_{\\text{conf}} > 0$ is the strong convexity constant of the confining potential\n820: - $D_{\\text{KL}}(\\mu_0 \\| \\pi_{\\text{QSD}})$ is the initial KL-divergence from equilibrium\n821: "
    },
    {
      "directive_type": "proof",
      "label": "prop-wasserstein-variance-decomposition",
      "title": null,
      "start_line": 825,
      "end_line": 908,
      "header_lines": [
        892
      ],
      "content_start": 827,
      "content_end": 907,
      "content": "827: :::{prf:proof}\n828: \n829: The proof uses a three-step argument connecting KL-divergence to Wasserstein distance to structural variance, leveraging the LSI results from [10_kl_convergence/](../10_kl_convergence/).\n830: \n831: **Step 1: Exponential Contraction of KL-Divergence**\n832: \n833: From the Logarithmic Sobolev Inequality established in [10_kl_convergence.md](../10_kl_convergence/10_kl_convergence.md), Theorem {prf:ref}`thm-main-kl-convergence`, the Euclidean Gas satisfies exponential KL-convergence:\n834: \n835: $$\n836: D_{\\text{KL}}(\\mu_t \\| \\pi_{\\text{QSD}}) \\leq e^{-\\lambda t} \\cdot D_{\\text{KL}}(\\mu_0 \\| \\pi_{\\text{QSD}})\n837: $$\n838: \n839: where $\\lambda = 1/C_{\\text{LSI}} > 0$ is the exponential rate determined by the LSI constant.\n840: \n841: **Explicit LSI constant:** From [10_kl_convergence.md](../10_kl_convergence/10_kl_convergence.md) line 56:\n842: \n843: $$\n844: C_{\\text{LSI}} = O\\left(\\frac{1}{\\gamma \\kappa_{\\text{conf}} \\kappa_W \\delta^2}\\right)\n845: $$\n846: \n847: Therefore, the exponential rate is:\n848: \n849: $$\n850: \\lambda = \\frac{1}{C_{\\text{LSI}}} = O(\\gamma \\kappa_{\\text{conf}} \\kappa_W \\delta^2)\n851: $$\n852: \n853: where:\n854: - $\\gamma > 0$ is the friction coefficient (kinetic operator parameter)\n855: - $\\kappa_{\\text{conf}} > 0$ is the strong convexity constant of the confining potential\n856: - $\\kappa_W > 0$ is the Wasserstein contraction rate of the cloning operator (see Theorem 8.1.1 in [03_cloning.md](../03_cloning.md))\n857: - $\\delta^2 > 0$ is the cloning noise variance\n858: \n859: **Step 2: Bounding Wasserstein by KL-Divergence (Reverse Talagrand)**\n860: \n861: By the reverse Talagrand inequality (Villani 2009, Theorem 22.17), for log-concave $\\pi_{\\text{QSD}}$:\n862: \n863: $$\n864: W_2^2(\\mu, \\pi_{\\text{QSD}}) \\leq \\frac{2}{\\lambda_{\\min}(\\text{Hess} \\log \\pi_{\\text{QSD}})} \\cdot D_{\\text{KL}}(\\mu \\| \\pi_{\\text{QSD}})\n865: $$\n866: \n867: where $\\lambda_{\\min} \\geq \\kappa_{\\text{conf}}$ is the minimum eigenvalue of the Hessian of $\\log \\pi_{\\text{QSD}}$, which bounds the strong convexity constant.\n868: \n869: **Reference:** This inequality is proven in [10_kl_convergence.md](../10_kl_convergence/10_kl_convergence.md), line 857.\n870: \n871: Defining:\n872: \n873: $$\n874: C_T := \\frac{2}{\\kappa_{\\text{conf}}}\n875: $$\n876: \n877: we have:\n878: \n879: $$\n880: W_2^2(\\mu_t, \\pi_{\\text{QSD}}) \\leq C_T \\cdot D_{\\text{KL}}(\\mu_t \\| \\pi_{\\text{QSD}})\n881: $$\n882: \n883: Combining with Step 1:\n884: \n885: $$\n886: W_2^2(\\mu_t, \\pi_{\\text{QSD}}) \\leq C_T \\cdot e^{-\\lambda t} \\cdot D_{\\text{KL}}(\\mu_0 \\| \\pi_{\\text{QSD}})\n887: $$\n888: \n889: **Step 3: Bounding Structural Variance by Wasserstein (Variance Decomposition)**\n890: \n891: The key result is the **variance decomposition** of the squared Wasserstein distance:\n892: \n893: :::{prf:proposition} Wasserstein Variance Decomposition\n894: :label: prop-wasserstein-variance-decomposition\n895: \n896: For any two probability measures $\\mu$ and $\\pi$ on $\\mathbb{R}^d$ with finite second moments:\n897: \n898: $$\n899: W_2^2(\\mu, \\pi) = W_2^2(\\tilde{\\mu}, \\tilde{\\pi}) + \\|m_\\mu - m_\\pi\\|^2\n900: $$\n901: \n902: where:\n903: - $\\tilde{\\mu}$ is the centered version of $\\mu$ (mean translated to origin)\n904: - $\\tilde{\\pi}$ is the centered version of $\\pi$ (mean translated to origin)\n905: - $m_\\mu := \\mathbb{E}_{X \\sim \\mu}[X]$ is the mean of $\\mu$\n906: - $m_\\pi := \\mathbb{E}_{Y \\sim \\pi}[Y]$ is the mean of $\\pi$\n907: ",
      "metadata": {
        "label": "prop-wasserstein-variance-decomposition"
      },
      "section": "## 3. Lemma B: Exponential Contraction of Structural Variance",
      "raw_directive": "825: ### Proof of Lemma B\n826: \n827: :::{prf:proof}\n828: \n829: The proof uses a three-step argument connecting KL-divergence to Wasserstein distance to structural variance, leveraging the LSI results from [10_kl_convergence/](../10_kl_convergence/).\n830: \n831: **Step 1: Exponential Contraction of KL-Divergence**\n832: \n833: From the Logarithmic Sobolev Inequality established in [10_kl_convergence.md](../10_kl_convergence/10_kl_convergence.md), Theorem {prf:ref}`thm-main-kl-convergence`, the Euclidean Gas satisfies exponential KL-convergence:\n834: \n835: $$\n836: D_{\\text{KL}}(\\mu_t \\| \\pi_{\\text{QSD}}) \\leq e^{-\\lambda t} \\cdot D_{\\text{KL}}(\\mu_0 \\| \\pi_{\\text{QSD}})\n837: $$\n838: \n839: where $\\lambda = 1/C_{\\text{LSI}} > 0$ is the exponential rate determined by the LSI constant.\n840: \n841: **Explicit LSI constant:** From [10_kl_convergence.md](../10_kl_convergence/10_kl_convergence.md) line 56:\n842: \n843: $$\n844: C_{\\text{LSI}} = O\\left(\\frac{1}{\\gamma \\kappa_{\\text{conf}} \\kappa_W \\delta^2}\\right)\n845: $$\n846: \n847: Therefore, the exponential rate is:\n848: \n849: $$\n850: \\lambda = \\frac{1}{C_{\\text{LSI}}} = O(\\gamma \\kappa_{\\text{conf}} \\kappa_W \\delta^2)\n851: $$\n852: \n853: where:\n854: - $\\gamma > 0$ is the friction coefficient (kinetic operator parameter)\n855: - $\\kappa_{\\text{conf}} > 0$ is the strong convexity constant of the confining potential\n856: - $\\kappa_W > 0$ is the Wasserstein contraction rate of the cloning operator (see Theorem 8.1.1 in [03_cloning.md](../03_cloning.md))\n857: - $\\delta^2 > 0$ is the cloning noise variance\n858: \n859: **Step 2: Bounding Wasserstein by KL-Divergence (Reverse Talagrand)**\n860: \n861: By the reverse Talagrand inequality (Villani 2009, Theorem 22.17), for log-concave $\\pi_{\\text{QSD}}$:\n862: \n863: $$\n864: W_2^2(\\mu, \\pi_{\\text{QSD}}) \\leq \\frac{2}{\\lambda_{\\min}(\\text{Hess} \\log \\pi_{\\text{QSD}})} \\cdot D_{\\text{KL}}(\\mu \\| \\pi_{\\text{QSD}})\n865: $$\n866: \n867: where $\\lambda_{\\min} \\geq \\kappa_{\\text{conf}}$ is the minimum eigenvalue of the Hessian of $\\log \\pi_{\\text{QSD}}$, which bounds the strong convexity constant.\n868: \n869: **Reference:** This inequality is proven in [10_kl_convergence.md](../10_kl_convergence/10_kl_convergence.md), line 857.\n870: \n871: Defining:\n872: \n873: $$\n874: C_T := \\frac{2}{\\kappa_{\\text{conf}}}\n875: $$\n876: \n877: we have:\n878: \n879: $$\n880: W_2^2(\\mu_t, \\pi_{\\text{QSD}}) \\leq C_T \\cdot D_{\\text{KL}}(\\mu_t \\| \\pi_{\\text{QSD}})\n881: $$\n882: \n883: Combining with Step 1:\n884: \n885: $$\n886: W_2^2(\\mu_t, \\pi_{\\text{QSD}}) \\leq C_T \\cdot e^{-\\lambda t} \\cdot D_{\\text{KL}}(\\mu_0 \\| \\pi_{\\text{QSD}})\n887: $$\n888: \n889: **Step 3: Bounding Structural Variance by Wasserstein (Variance Decomposition)**\n890: \n891: The key result is the **variance decomposition** of the squared Wasserstein distance:\n892: \n893: :::{prf:proposition} Wasserstein Variance Decomposition\n894: :label: prop-wasserstein-variance-decomposition\n895: \n896: For any two probability measures $\\mu$ and $\\pi$ on $\\mathbb{R}^d$ with finite second moments:\n897: \n898: $$\n899: W_2^2(\\mu, \\pi) = W_2^2(\\tilde{\\mu}, \\tilde{\\pi}) + \\|m_\\mu - m_\\pi\\|^2\n900: $$\n901: \n902: where:\n903: - $\\tilde{\\mu}$ is the centered version of $\\mu$ (mean translated to origin)\n904: - $\\tilde{\\pi}$ is the centered version of $\\pi$ (mean translated to origin)\n905: - $m_\\mu := \\mathbb{E}_{X \\sim \\mu}[X]$ is the mean of $\\mu$\n906: - $m_\\pi := \\mathbb{E}_{Y \\sim \\pi}[Y]$ is the mean of $\\pi$\n907: \n908: **Reference:** This is a standard result in optimal transport theory. For a proof, see Villani (2009), *Optimal Transport: Old and New*, Theorem 7.17, or Peyr\u00e9 & Cuturi (2019), *Computational Optimal Transport*, Section 2.3."
    }
  ],
  "validation": {
    "ok": true,
    "errors": []
  }
}