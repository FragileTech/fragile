{
  "chapter_index": 9,
  "section_id": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
  "directive_count": 13,
  "hints": [
    {
      "directive_type": "definition",
      "label": "def-fitness-algorithmic",
      "title": "Fitness Potential Construction (Algorithmic Specification)",
      "start_line": 2595,
      "end_line": 2650,
      "header_lines": [
        2596
      ],
      "content_start": 2598,
      "content_end": 2649,
      "content": "2598: :label: def-fitness-algorithmic\n2599: \n2600: For a swarm state $S = \\{(x_i, v_i, s_i)\\}_{i=1}^N$ with alive walkers $A_k = \\{i : s_i = \\text{alive}\\}$, the fitness potential at position $x \\in \\mathcal{X}$ is constructed through the following pipeline:\n2601: \n2602: **Step 1: Measurement Function.** Given a measurement function $d: \\mathcal{X} \\to \\mathbb{R}$ (e.g., reward, diversity score), evaluate:\n2603: \n2604: $$\n2605: d_i = d(x_i) \\quad \\text{for all } i \\in A_k\n2606: $$\n2607: \n2608: **Step 2: Localization Weights.** For localization scale $\\rho > 0$ and localization kernel $K_\\rho(x, x') = \\frac{1}{Z_K(x, \\rho)} \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\rho^2}\\right)$, compute normalized weights:\n2609: \n2610: $$\n2611: w_{ij}(\\rho) = \\frac{K_\\rho(x, x_j)}{\\sum_{\\ell \\in A_k} K_\\rho(x, x_\\ell)}\n2612: $$\n2613: \n2614: where the normalization ensures $\\sum_{j \\in A_k} w_{ij}(\\rho) = 1$.\n2615: \n2616: **Step 3: Localized Moments.** Compute the \u03c1-localized mean and variance at position $x$:\n2617: \n2618: $$\n2619: \\mu_\\rho[f_k, d, x] = \\sum_{j \\in A_k} w_{ij}(\\rho) \\, d_j\n2620: $$\n2621: \n2622: $$\n2623: \\sigma^2_\\rho[f_k, d, x] = \\sum_{j \\in A_k} w_{ij}(\\rho) \\, (d_j - \\mu_\\rho[f_k, d, x])^2\n2624: $$\n2625: \n2626: **Step 4: Regularized Standard Deviation.** Apply numerical regularization using a C\u00b9-smooth patching function:\n2627: \n2628: $$\n2629: \\sigma'_\\rho[f_k, d, x] = \\sigma\\'_{\\text{reg}}\\left(\\sqrt{\\sigma^2_\\rho[f_k, d, x]}\\right)\n2630: $$\n2631: \n2632: where $\\sigma\\'_{\\text{reg}}: [0, \\infty) \\to [\\kappa_{\\text{var,min}}, \\infty)$ is a C\u00b9-smooth function (see Definition {prf:ref}`def-unified-z-score` in `07_adaptative_gas.md`) that:\n2633: - Equals $\\kappa_{\\text{var,min}}$ for $\\sigma \\le \\kappa_{\\text{var,min}} - \\delta$\n2634: - Smoothly transitions through a polynomial patch in $[\\kappa_{\\text{var,min}} - \\delta, \\kappa_{\\text{var,min}} + \\delta]$\n2635: - Equals the identity $\\sigma$ for $\\sigma \\ge \\kappa_{\\text{var,min}} + \\delta$\n2636: \n2637: This ensures $V_{\\text{fit}}$ is C\u00b2 everywhere, as required for the Hessian to be well-defined.\n2638: \n2639: **Step 5: Localized Z-Score.** Compute the standardized measurement:\n2640: \n2641: $$\n2642: Z_\\rho[f_k, d, x] = \\frac{d(x) - \\mu_\\rho[f_k, d, x]}{\\sigma'_\\rho[f_k, d, x]}\n2643: $$\n2644: \n2645: **Step 6: Rescale to Bounded Potential.** Apply a smooth, monotone, bounded rescale function $g_A: \\mathbb{R} \\to [0, A]$ (e.g., $g_A(z) = \\frac{A}{1 + e^{-z}}$):\n2646: \n2647: $$\n2648: V_{\\text{fit}}[f_k, \\rho](x) = g_A\\left(Z_\\rho[f_k, d, x]\\right)\n2649: $$",
      "metadata": {
        "label": "def-fitness-algorithmic"
      },
      "section": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "2595: We begin with the algorithmic construction of the fitness potential.\n2596: \n2597: :::{prf:definition} Fitness Potential Construction (Algorithmic Specification)\n2598: :label: def-fitness-algorithmic\n2599: \n2600: For a swarm state $S = \\{(x_i, v_i, s_i)\\}_{i=1}^N$ with alive walkers $A_k = \\{i : s_i = \\text{alive}\\}$, the fitness potential at position $x \\in \\mathcal{X}$ is constructed through the following pipeline:\n2601: \n2602: **Step 1: Measurement Function.** Given a measurement function $d: \\mathcal{X} \\to \\mathbb{R}$ (e.g., reward, diversity score), evaluate:\n2603: \n2604: $$\n2605: d_i = d(x_i) \\quad \\text{for all } i \\in A_k\n2606: $$\n2607: \n2608: **Step 2: Localization Weights.** For localization scale $\\rho > 0$ and localization kernel $K_\\rho(x, x') = \\frac{1}{Z_K(x, \\rho)} \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\rho^2}\\right)$, compute normalized weights:\n2609: \n2610: $$\n2611: w_{ij}(\\rho) = \\frac{K_\\rho(x, x_j)}{\\sum_{\\ell \\in A_k} K_\\rho(x, x_\\ell)}\n2612: $$\n2613: \n2614: where the normalization ensures $\\sum_{j \\in A_k} w_{ij}(\\rho) = 1$.\n2615: \n2616: **Step 3: Localized Moments.** Compute the \u03c1-localized mean and variance at position $x$:\n2617: \n2618: $$\n2619: \\mu_\\rho[f_k, d, x] = \\sum_{j \\in A_k} w_{ij}(\\rho) \\, d_j\n2620: $$\n2621: \n2622: $$\n2623: \\sigma^2_\\rho[f_k, d, x] = \\sum_{j \\in A_k} w_{ij}(\\rho) \\, (d_j - \\mu_\\rho[f_k, d, x])^2\n2624: $$\n2625: \n2626: **Step 4: Regularized Standard Deviation.** Apply numerical regularization using a C\u00b9-smooth patching function:\n2627: \n2628: $$\n2629: \\sigma'_\\rho[f_k, d, x] = \\sigma\\'_{\\text{reg}}\\left(\\sqrt{\\sigma^2_\\rho[f_k, d, x]}\\right)\n2630: $$\n2631: \n2632: where $\\sigma\\'_{\\text{reg}}: [0, \\infty) \\to [\\kappa_{\\text{var,min}}, \\infty)$ is a C\u00b9-smooth function (see Definition {prf:ref}`def-unified-z-score` in `07_adaptative_gas.md`) that:\n2633: - Equals $\\kappa_{\\text{var,min}}$ for $\\sigma \\le \\kappa_{\\text{var,min}} - \\delta$\n2634: - Smoothly transitions through a polynomial patch in $[\\kappa_{\\text{var,min}} - \\delta, \\kappa_{\\text{var,min}} + \\delta]$\n2635: - Equals the identity $\\sigma$ for $\\sigma \\ge \\kappa_{\\text{var,min}} + \\delta$\n2636: \n2637: This ensures $V_{\\text{fit}}$ is C\u00b2 everywhere, as required for the Hessian to be well-defined.\n2638: \n2639: **Step 5: Localized Z-Score.** Compute the standardized measurement:\n2640: \n2641: $$\n2642: Z_\\rho[f_k, d, x] = \\frac{d(x) - \\mu_\\rho[f_k, d, x]}{\\sigma'_\\rho[f_k, d, x]}\n2643: $$\n2644: \n2645: **Step 6: Rescale to Bounded Potential.** Apply a smooth, monotone, bounded rescale function $g_A: \\mathbb{R} \\to [0, A]$ (e.g., $g_A(z) = \\frac{A}{1 + e^{-z}}$):\n2646: \n2647: $$\n2648: V_{\\text{fit}}[f_k, \\rho](x) = g_A\\left(Z_\\rho[f_k, d, x]\\right)\n2649: $$\n2650: "
    },
    {
      "directive_type": "remark",
      "label": "unlabeled-remark-75",
      "title": "Algorithmic Meaning of the Fitness Potential",
      "start_line": 2652,
      "end_line": 2660,
      "header_lines": [
        2653
      ],
      "content_start": 2655,
      "content_end": 2659,
      "content": "2655: :class: note\n2656: \n2657: The fitness potential $V_{\\text{fit}}[f_k, \\rho](x)$ encodes **relative performance in a local neighborhood**:\n2658: - High values indicate positions where the measurement $d(x)$ is **above the local mean** (good regions)\n2659: - Low values indicate positions **below the local mean** (poor regions)",
      "metadata": {
        "class": "note"
      },
      "section": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "2652: :::\n2653: \n2654: :::{prf:remark} Algorithmic Meaning of the Fitness Potential\n2655: :class: note\n2656: \n2657: The fitness potential $V_{\\text{fit}}[f_k, \\rho](x)$ encodes **relative performance in a local neighborhood**:\n2658: - High values indicate positions where the measurement $d(x)$ is **above the local mean** (good regions)\n2659: - Low values indicate positions **below the local mean** (poor regions)\n2660: - The Z-score normalization makes this **scale-invariant**: fitness depends on relative position, not absolute measurement values"
    },
    {
      "directive_type": "theorem",
      "label": "thm-explicit-hessian",
      "title": "Explicit Hessian Formula",
      "start_line": 2666,
      "end_line": 2687,
      "header_lines": [
        2667
      ],
      "content_start": 2669,
      "content_end": 2686,
      "content": "2669: :label: thm-explicit-hessian\n2670: \n2671: The Hessian of the fitness potential with respect to position $x \\in \\mathcal{X}$ is:\n2672: \n2673: $$\n2674: H(x, S) = \\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x) = g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z + g'_A(Z) \\, \\nabla^2_x Z\n2675: $$\n2676: \n2677: where:\n2678: - $Z = Z_\\rho[f_k, d, x]$ is the localized Z-score\n2679: - $g'_A(Z)$ and $g''_A(Z)$ are the first and second derivatives of the rescale function\n2680: - $\\nabla_x Z$ and $\\nabla^2_x Z$ are the gradient and Hessian of the Z-score with respect to $x$\n2681: \n2682: **Expanded Form:**\n2683: \n2684: $$\n2685: H(x, S) = \\frac{g''_A(Z)}{\\sigma'^2_\\rho} \\nabla_x d \\otimes \\nabla_x d + \\frac{g'_A(Z)}{\\sigma'_\\rho} \\nabla^2_x d + \\text{(moment correction terms)}\n2686: $$",
      "metadata": {
        "label": "thm-explicit-hessian"
      },
      "section": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "2666: The metric emerges from the **curvature** of the fitness landscape, encoded in the Hessian. We now derive the Hessian explicitly using the chain rule.\n2667: \n2668: :::{prf:theorem} Explicit Hessian Formula\n2669: :label: thm-explicit-hessian\n2670: \n2671: The Hessian of the fitness potential with respect to position $x \\in \\mathcal{X}$ is:\n2672: \n2673: $$\n2674: H(x, S) = \\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x) = g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z + g'_A(Z) \\, \\nabla^2_x Z\n2675: $$\n2676: \n2677: where:\n2678: - $Z = Z_\\rho[f_k, d, x]$ is the localized Z-score\n2679: - $g'_A(Z)$ and $g''_A(Z)$ are the first and second derivatives of the rescale function\n2680: - $\\nabla_x Z$ and $\\nabla^2_x Z$ are the gradient and Hessian of the Z-score with respect to $x$\n2681: \n2682: **Expanded Form:**\n2683: \n2684: $$\n2685: H(x, S) = \\frac{g''_A(Z)}{\\sigma'^2_\\rho} \\nabla_x d \\otimes \\nabla_x d + \\frac{g'_A(Z)}{\\sigma'_\\rho} \\nabla^2_x d + \\text{(moment correction terms)}\n2686: $$\n2687: "
    },
    {
      "directive_type": "proof",
      "label": "unlabeled-proof-112",
      "title": null,
      "start_line": 2689,
      "end_line": 2815,
      "header_lines": [],
      "content_start": 2690,
      "content_end": 2814,
      "content": "2690: \n2691: :::{prf:proof}\n2692: We compute the Hessian by applying the chain rule twice. We first establish a technical lemma for the gradient of the localization weights.\n2693: \n2694: **Lemma: Gradient of Normalized Localization Weights.**\n2695: \n2696: For the normalized Gaussian weights:\n2697: \n2698: $$\n2699: w_{ij}(\\rho) = \\frac{K_\\rho(x, x_j)}{\\sum_{\\ell \\in A_k} K_\\rho(x, x_\\ell)} = \\frac{\\exp\\left(-\\frac{\\|x - x_j\\|^2}{2\\rho^2}\\right)}{\\sum_{\\ell \\in A_k} \\exp\\left(-\\frac{\\|x - x_\\ell\\|^2}{2\\rho^2}\\right)}\n2700: $$\n2701: \n2702: **Gradient derivation:** Using the quotient rule:\n2703: \n2704: $$\n2705: \\nabla_x w_{ij} = \\frac{\\nabla_x K_\\rho(x, x_j) \\cdot \\sum_\\ell K_\\rho(x, x_\\ell) - K_\\rho(x, x_j) \\cdot \\sum_\\ell \\nabla_x K_\\rho(x, x_\\ell)}{\\left(\\sum_\\ell K_\\rho(x, x_\\ell)\\right)^2}\n2706: $$\n2707: \n2708: For the Gaussian kernel, $\\nabla_x K_\\rho(x, x_j) = -\\frac{1}{\\rho^2} K_\\rho(x, x_j) (x - x_j)$. Therefore:\n2709: \n2710: $$\n2711: \\nabla_x w_{ij} = \\frac{-\\frac{1}{\\rho^2} K_\\rho(x, x_j) (x - x_j) \\sum_\\ell K_\\rho(x, x_\\ell) - K_\\rho(x, x_j) \\sum_\\ell \\left(-\\frac{1}{\\rho^2} K_\\rho(x, x_\\ell) (x - x_\\ell)\\right)}{\\left(\\sum_\\ell K_\\rho(x, x_\\ell)\\right)^2}\n2712: $$\n2713: \n2714: Simplifying using $w_{ij} = K_\\rho(x, x_j) / \\sum_\\ell K_\\rho(x, x_\\ell)$:\n2715: \n2716: $$\n2717: \\nabla_x w_{ij} = -\\frac{w_{ij}}{\\rho^2} (x - x_j) + \\frac{w_{ij}}{\\rho^2} \\sum_{\\ell \\in A_k} w_{i\\ell} (x - x_\\ell)\n2718: $$\n2719: \n2720: $$\n2721: = \\frac{1}{\\rho^2} w_{ij} \\left( \\sum_{\\ell \\in A_k} w_{i\\ell} (x - x_\\ell) - (x - x_j) \\right)\n2722: $$\n2723: \n2724: $$\n2725: = \\frac{1}{\\rho^2} \\left( w_{ij} \\sum_{\\ell \\in A_k} w_{i\\ell} (x - x_\\ell) - w_{ij} (x - x_j) \\right)\n2726: $$\n2727: \n2728: which can be rewritten as the formula used in the main proof. $\\square$\n2729: \n2730: **Step 1: First Derivative (Gradient).**\n2731: \n2732: By the chain rule for $V_{\\text{fit}}(x) = g_A(Z_\\rho(x))$:\n2733: \n2734: $$\n2735: \\nabla_x V_{\\text{fit}} = g'_A(Z) \\, \\nabla_x Z_\\rho\n2736: $$\n2737: \n2738: For the Z-score $Z_\\rho = \\frac{d(x) - \\mu_\\rho}{\\sigma'_\\rho}$, we have:\n2739: \n2740: $$\n2741: \\nabla_x Z_\\rho = \\frac{1}{\\sigma'_\\rho} \\left( \\nabla_x d - \\nabla_x \\mu_\\rho \\right) - \\frac{d(x) - \\mu_\\rho}{\\sigma'^2_\\rho} \\nabla_x \\sigma'_\\rho\n2742: $$\n2743: \n2744: **Moment Gradients.** The localized mean depends on $x$ through the weights:\n2745: \n2746: $$\n2747: \\mu_\\rho = \\sum_{j \\in A_k} w_{ij}(\\rho) d_j\n2748: $$\n2749: \n2750: $$\n2751: \\nabla_x \\mu_\\rho = \\sum_{j \\in A_k} (\\nabla_x w_{ij}) d_j\n2752: $$\n2753: \n2754: For the normalized Gaussian weights $w_{ij} = K_\\rho(x, x_j) / \\sum_\\ell K_\\rho(x, x_\\ell)$:\n2755: \n2756: $$\n2757: \\nabla_x w_{ij} = \\frac{1}{\\rho^2} \\left( w_{ij} (x - x_j) - \\sum_{\\ell \\in A_k} w_{i\\ell} w_{ij} (x - x_\\ell) \\right)\n2758: $$\n2759: \n2760: **Critical Telescoping Property:** The normalization constraint $\\sum_j w_{ij} = 1$ implies:\n2761: \n2762: $$\n2763: \\sum_{j \\in A_k} \\nabla_x w_{ij} = 0\n2764: $$\n2765: \n2766: This telescoping property ensures that the gradient of the mean is bounded **independently of $k$** (and thus $N$).\n2767: \n2768: **Step 2: Second Derivative (Hessian).**\n2769: \n2770: Taking another derivative of $\\nabla_x V_{\\text{fit}} = g'_A(Z) \\nabla_x Z$:\n2771: \n2772: $$\n2773: \\nabla^2_x V_{\\text{fit}} = g''_A(Z) \\, (\\nabla_x Z) \\otimes (\\nabla_x Z) + g'_A(Z) \\, \\nabla^2_x Z\n2774: $$\n2775: \n2776: **Term 1 (Outer Product):** The first term is a rank-1 matrix:\n2777: \n2778: $$\n2779: \\left[g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z\\right]_{ab} = g''_A(Z) \\, \\frac{\\partial Z}{\\partial x_a} \\frac{\\partial Z}{\\partial x_b}\n2780: $$\n2781: \n2782: For the Z-score, to leading order (ignoring moment correction terms):\n2783: \n2784: $$\n2785: \\nabla_x Z \\approx \\frac{1}{\\sigma'_\\rho} \\nabla_x d\n2786: $$\n2787: \n2788: Thus:\n2789: \n2790: $$\n2791: g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z \\approx \\frac{g''_A(Z)}{\\sigma'^2_\\rho} \\, \\nabla_x d \\otimes \\nabla_x d\n2792: $$\n2793: \n2794: **Term 2 (Hessian of Z-Score):** The second term involves:\n2795: \n2796: $$\n2797: \\nabla^2_x Z = \\frac{1}{\\sigma'_\\rho} (\\nabla^2_x d - \\nabla^2_x \\mu_\\rho) + \\text{(variance correction terms)}\n2798: $$\n2799: \n2800: The moment Hessian $\\nabla^2_x \\mu_\\rho$ involves second derivatives of the weights $w_{ij}$. By the same telescoping argument, these terms remain bounded.\n2801: \n2802: **Step 3: Explicit Bound.**\n2803: \n2804: Combining both terms and using the bounds:\n2805: - $|g'_A(Z)| \\le g'_{\\max}$, $|g''_A(Z)| \\le g''_{\\max}$ (smoothness of rescale function)\n2806: - $\\|\\nabla_x d\\|_\\infty \\le d'_{\\max}$, $\\|\\nabla^2_x d\\|_\\infty \\le d''_{\\max}$ (regularity of measurement)\n2807: - $\\sigma'_\\rho \\ge \\kappa_{\\text{var,min}}$ (regularization floor)\n2808: - $\\|\\nabla_x \\mu_\\rho\\|, \\|\\nabla^2_x \\mu_\\rho\\| = O(1/\\rho)$ (localization scale dependence)\n2809: \n2810: We obtain the **N-uniform bound**:\n2811: \n2812: $$\n2813: \\|H(x, S)\\| \\le H_{\\max}(\\rho) = \\frac{g''_{\\max} (d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} + \\frac{g'_{\\max} d''_{\\max}}{\\kappa_{\\text{var,min}}} + O(1/\\rho)\n2814: $$",
      "metadata": {},
      "section": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "2689: :::\n2690: \n2691: :::{prf:proof}\n2692: We compute the Hessian by applying the chain rule twice. We first establish a technical lemma for the gradient of the localization weights.\n2693: \n2694: **Lemma: Gradient of Normalized Localization Weights.**\n2695: \n2696: For the normalized Gaussian weights:\n2697: \n2698: $$\n2699: w_{ij}(\\rho) = \\frac{K_\\rho(x, x_j)}{\\sum_{\\ell \\in A_k} K_\\rho(x, x_\\ell)} = \\frac{\\exp\\left(-\\frac{\\|x - x_j\\|^2}{2\\rho^2}\\right)}{\\sum_{\\ell \\in A_k} \\exp\\left(-\\frac{\\|x - x_\\ell\\|^2}{2\\rho^2}\\right)}\n2700: $$\n2701: \n2702: **Gradient derivation:** Using the quotient rule:\n2703: \n2704: $$\n2705: \\nabla_x w_{ij} = \\frac{\\nabla_x K_\\rho(x, x_j) \\cdot \\sum_\\ell K_\\rho(x, x_\\ell) - K_\\rho(x, x_j) \\cdot \\sum_\\ell \\nabla_x K_\\rho(x, x_\\ell)}{\\left(\\sum_\\ell K_\\rho(x, x_\\ell)\\right)^2}\n2706: $$\n2707: \n2708: For the Gaussian kernel, $\\nabla_x K_\\rho(x, x_j) = -\\frac{1}{\\rho^2} K_\\rho(x, x_j) (x - x_j)$. Therefore:\n2709: \n2710: $$\n2711: \\nabla_x w_{ij} = \\frac{-\\frac{1}{\\rho^2} K_\\rho(x, x_j) (x - x_j) \\sum_\\ell K_\\rho(x, x_\\ell) - K_\\rho(x, x_j) \\sum_\\ell \\left(-\\frac{1}{\\rho^2} K_\\rho(x, x_\\ell) (x - x_\\ell)\\right)}{\\left(\\sum_\\ell K_\\rho(x, x_\\ell)\\right)^2}\n2712: $$\n2713: \n2714: Simplifying using $w_{ij} = K_\\rho(x, x_j) / \\sum_\\ell K_\\rho(x, x_\\ell)$:\n2715: \n2716: $$\n2717: \\nabla_x w_{ij} = -\\frac{w_{ij}}{\\rho^2} (x - x_j) + \\frac{w_{ij}}{\\rho^2} \\sum_{\\ell \\in A_k} w_{i\\ell} (x - x_\\ell)\n2718: $$\n2719: \n2720: $$\n2721: = \\frac{1}{\\rho^2} w_{ij} \\left( \\sum_{\\ell \\in A_k} w_{i\\ell} (x - x_\\ell) - (x - x_j) \\right)\n2722: $$\n2723: \n2724: $$\n2725: = \\frac{1}{\\rho^2} \\left( w_{ij} \\sum_{\\ell \\in A_k} w_{i\\ell} (x - x_\\ell) - w_{ij} (x - x_j) \\right)\n2726: $$\n2727: \n2728: which can be rewritten as the formula used in the main proof. $\\square$\n2729: \n2730: **Step 1: First Derivative (Gradient).**\n2731: \n2732: By the chain rule for $V_{\\text{fit}}(x) = g_A(Z_\\rho(x))$:\n2733: \n2734: $$\n2735: \\nabla_x V_{\\text{fit}} = g'_A(Z) \\, \\nabla_x Z_\\rho\n2736: $$\n2737: \n2738: For the Z-score $Z_\\rho = \\frac{d(x) - \\mu_\\rho}{\\sigma'_\\rho}$, we have:\n2739: \n2740: $$\n2741: \\nabla_x Z_\\rho = \\frac{1}{\\sigma'_\\rho} \\left( \\nabla_x d - \\nabla_x \\mu_\\rho \\right) - \\frac{d(x) - \\mu_\\rho}{\\sigma'^2_\\rho} \\nabla_x \\sigma'_\\rho\n2742: $$\n2743: \n2744: **Moment Gradients.** The localized mean depends on $x$ through the weights:\n2745: \n2746: $$\n2747: \\mu_\\rho = \\sum_{j \\in A_k} w_{ij}(\\rho) d_j\n2748: $$\n2749: \n2750: $$\n2751: \\nabla_x \\mu_\\rho = \\sum_{j \\in A_k} (\\nabla_x w_{ij}) d_j\n2752: $$\n2753: \n2754: For the normalized Gaussian weights $w_{ij} = K_\\rho(x, x_j) / \\sum_\\ell K_\\rho(x, x_\\ell)$:\n2755: \n2756: $$\n2757: \\nabla_x w_{ij} = \\frac{1}{\\rho^2} \\left( w_{ij} (x - x_j) - \\sum_{\\ell \\in A_k} w_{i\\ell} w_{ij} (x - x_\\ell) \\right)\n2758: $$\n2759: \n2760: **Critical Telescoping Property:** The normalization constraint $\\sum_j w_{ij} = 1$ implies:\n2761: \n2762: $$\n2763: \\sum_{j \\in A_k} \\nabla_x w_{ij} = 0\n2764: $$\n2765: \n2766: This telescoping property ensures that the gradient of the mean is bounded **independently of $k$** (and thus $N$).\n2767: \n2768: **Step 2: Second Derivative (Hessian).**\n2769: \n2770: Taking another derivative of $\\nabla_x V_{\\text{fit}} = g'_A(Z) \\nabla_x Z$:\n2771: \n2772: $$\n2773: \\nabla^2_x V_{\\text{fit}} = g''_A(Z) \\, (\\nabla_x Z) \\otimes (\\nabla_x Z) + g'_A(Z) \\, \\nabla^2_x Z\n2774: $$\n2775: \n2776: **Term 1 (Outer Product):** The first term is a rank-1 matrix:\n2777: \n2778: $$\n2779: \\left[g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z\\right]_{ab} = g''_A(Z) \\, \\frac{\\partial Z}{\\partial x_a} \\frac{\\partial Z}{\\partial x_b}\n2780: $$\n2781: \n2782: For the Z-score, to leading order (ignoring moment correction terms):\n2783: \n2784: $$\n2785: \\nabla_x Z \\approx \\frac{1}{\\sigma'_\\rho} \\nabla_x d\n2786: $$\n2787: \n2788: Thus:\n2789: \n2790: $$\n2791: g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z \\approx \\frac{g''_A(Z)}{\\sigma'^2_\\rho} \\, \\nabla_x d \\otimes \\nabla_x d\n2792: $$\n2793: \n2794: **Term 2 (Hessian of Z-Score):** The second term involves:\n2795: \n2796: $$\n2797: \\nabla^2_x Z = \\frac{1}{\\sigma'_\\rho} (\\nabla^2_x d - \\nabla^2_x \\mu_\\rho) + \\text{(variance correction terms)}\n2798: $$\n2799: \n2800: The moment Hessian $\\nabla^2_x \\mu_\\rho$ involves second derivatives of the weights $w_{ij}$. By the same telescoping argument, these terms remain bounded.\n2801: \n2802: **Step 3: Explicit Bound.**\n2803: \n2804: Combining both terms and using the bounds:\n2805: - $|g'_A(Z)| \\le g'_{\\max}$, $|g''_A(Z)| \\le g''_{\\max}$ (smoothness of rescale function)\n2806: - $\\|\\nabla_x d\\|_\\infty \\le d'_{\\max}$, $\\|\\nabla^2_x d\\|_\\infty \\le d''_{\\max}$ (regularity of measurement)\n2807: - $\\sigma'_\\rho \\ge \\kappa_{\\text{var,min}}$ (regularization floor)\n2808: - $\\|\\nabla_x \\mu_\\rho\\|, \\|\\nabla^2_x \\mu_\\rho\\| = O(1/\\rho)$ (localization scale dependence)\n2809: \n2810: We obtain the **N-uniform bound**:\n2811: \n2812: $$\n2813: \\|H(x, S)\\| \\le H_{\\max}(\\rho) = \\frac{g''_{\\max} (d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} + \\frac{g'_{\\max} d''_{\\max}}{\\kappa_{\\text{var,min}}} + O(1/\\rho)\n2814: $$\n2815: "
    },
    {
      "directive_type": "remark",
      "label": "unlabeled-remark-240",
      "title": "Geometric Interpretation of the Hessian Terms",
      "start_line": 2817,
      "end_line": 2833,
      "header_lines": [
        2818
      ],
      "content_start": 2820,
      "content_end": 2832,
      "content": "2820: :class: note\n2821: \n2822: The two terms in the Hessian have distinct geometric meanings:\n2823: \n2824: **1. Rank-1 Term (Outer Product):** $g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z$\n2825: - Dominant when the rescale function has high curvature ($g''_A$ large)\n2826: - Aligned with the **gradient direction** of the fitness landscape\n2827: - Creates **anisotropy along level sets**: high curvature perpendicular to level sets\n2828: \n2829: **2. Full Hessian Term:** $g'_A(Z) \\, \\nabla^2_x Z$\n2830: - Captures the **intrinsic curvature** of the Z-score manifold\n2831: - Depends on second derivatives of the measurement function $d(x)$\n2832: - Reflects the **geometry of the problem**, not just the fitness magnitude",
      "metadata": {
        "class": "note"
      },
      "section": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "2817: :::\n2818: \n2819: :::{prf:remark} Geometric Interpretation of the Hessian Terms\n2820: :class: note\n2821: \n2822: The two terms in the Hessian have distinct geometric meanings:\n2823: \n2824: **1. Rank-1 Term (Outer Product):** $g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z$\n2825: - Dominant when the rescale function has high curvature ($g''_A$ large)\n2826: - Aligned with the **gradient direction** of the fitness landscape\n2827: - Creates **anisotropy along level sets**: high curvature perpendicular to level sets\n2828: \n2829: **2. Full Hessian Term:** $g'_A(Z) \\, \\nabla^2_x Z$\n2830: - Captures the **intrinsic curvature** of the Z-score manifold\n2831: - Depends on second derivatives of the measurement function $d(x)$\n2832: - Reflects the **geometry of the problem**, not just the fitness magnitude\n2833: "
    },
    {
      "directive_type": "definition",
      "label": "def-metric-explicit",
      "title": "Emergent Riemannian Metric (Explicit Construction)",
      "start_line": 2839,
      "end_line": 2865,
      "header_lines": [
        2840
      ],
      "content_start": 2842,
      "content_end": 2864,
      "content": "2842: :label: def-metric-explicit\n2843: \n2844: For a walker at position $x$ in swarm state $S$, the **emergent Riemannian metric** is defined as:\n2845: \n2846: $$\n2847: g(x, S) = H(x, S) + \\epsilon_\\Sigma I\n2848: $$\n2849: \n2850: where:\n2851: - $H(x, S) = \\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x)$ is the Hessian from {prf:ref}`thm-explicit-hessian`\n2852: - $\\epsilon_\\Sigma > 0$ is the **regularization parameter**\n2853: - $I$ is the $d \\times d$ identity matrix\n2854: \n2855: The corresponding **diffusion tensor** (inverse of the metric) is:\n2856: \n2857: $$\n2858: D_{\\text{reg}}(x, S) = g(x, S)^{-1} = \\left(H(x, S) + \\epsilon_\\Sigma I\\right)^{-1}\n2859: $$\n2860: \n2861: and the **diffusion coefficient matrix** used in the SDE is:\n2862: \n2863: $$\n2864: \\Sigma_{\\text{reg}}(x, S) = D_{\\text{reg}}(x, S)^{1/2} = \\left(H(x, S) + \\epsilon_\\Sigma I\\right)^{-1/2}",
      "metadata": {
        "label": "def-metric-explicit"
      },
      "section": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "2839: The Hessian alone may not be positive definite (e.g., at saddle points or in flat regions). The regularization ensures well-posedness.\n2840: \n2841: :::{prf:definition} Emergent Riemannian Metric (Explicit Construction)\n2842: :label: def-metric-explicit\n2843: \n2844: For a walker at position $x$ in swarm state $S$, the **emergent Riemannian metric** is defined as:\n2845: \n2846: $$\n2847: g(x, S) = H(x, S) + \\epsilon_\\Sigma I\n2848: $$\n2849: \n2850: where:\n2851: - $H(x, S) = \\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x)$ is the Hessian from {prf:ref}`thm-explicit-hessian`\n2852: - $\\epsilon_\\Sigma > 0$ is the **regularization parameter**\n2853: - $I$ is the $d \\times d$ identity matrix\n2854: \n2855: The corresponding **diffusion tensor** (inverse of the metric) is:\n2856: \n2857: $$\n2858: D_{\\text{reg}}(x, S) = g(x, S)^{-1} = \\left(H(x, S) + \\epsilon_\\Sigma I\\right)^{-1}\n2859: $$\n2860: \n2861: and the **diffusion coefficient matrix** used in the SDE is:\n2862: \n2863: $$\n2864: \\Sigma_{\\text{reg}}(x, S) = D_{\\text{reg}}(x, S)^{1/2} = \\left(H(x, S) + \\epsilon_\\Sigma I\\right)^{-1/2}\n2865: $$"
    },
    {
      "directive_type": "theorem",
      "label": "thm-uniform-ellipticity-explicit",
      "title": "Uniform Ellipticity from Regularization",
      "start_line": 2867,
      "end_line": 2900,
      "header_lines": [
        2868
      ],
      "content_start": 2870,
      "content_end": 2899,
      "content": "2870: :label: thm-uniform-ellipticity-explicit\n2871: \n2872: For any choice of algorithmic parameters $(d, \\rho, \\kappa_{\\text{var,min}}, g_A, A, \\epsilon_\\Sigma)$, the metric $g(x, S)$ is **uniformly elliptic** with explicit bounds:\n2873: \n2874: $$\n2875: c_{\\min}(\\rho) I \\preceq g(x, S) \\preceq c_{\\max} I\n2876: $$\n2877: \n2878: where:\n2879: \n2880: $$\n2881: c_{\\min}(\\rho) = \\epsilon_\\Sigma - \\Lambda_-(\\rho), \\quad c_{\\max} = H_{\\max}(\\rho) + \\epsilon_\\Sigma\n2882: $$\n2883: \n2884: and:\n2885: - $\\Lambda_-(\\rho) \\ge 0$ is the **spectral floor**: the maximum magnitude of negative eigenvalues of $H(x, S)$ over all states\n2886: - $H_{\\max}(\\rho)$ is the **spectral ceiling**: the maximum eigenvalue of $H(x, S)$ over all states\n2887: \n2888: **Sufficient Condition for Positive Definiteness:** If $\\epsilon_\\Sigma > \\Lambda_-(\\rho)$, then $g(x, S) \\succ 0$ for all $x, S$.\n2889: \n2890: **Inverse Bounds (Diffusion Tensor):**\n2891: \n2892: $$\n2893: \\frac{1}{c_{\\max}} I \\preceq D_{\\text{reg}}(x, S) \\preceq \\frac{1}{c_{\\min}(\\rho)} I\n2894: $$\n2895: \n2896: **Square Root Bounds (Diffusion Coefficient Matrix):**\n2897: \n2898: $$\n2899: \\frac{1}{\\sqrt{c_{\\max}}} I \\preceq \\Sigma_{\\text{reg}}(x, S) \\preceq \\frac{1}{\\sqrt{c_{\\min}(\\rho)}} I",
      "metadata": {
        "label": "thm-uniform-ellipticity-explicit"
      },
      "section": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "2867: :::\n2868: \n2869: :::{prf:theorem} Uniform Ellipticity from Regularization\n2870: :label: thm-uniform-ellipticity-explicit\n2871: \n2872: For any choice of algorithmic parameters $(d, \\rho, \\kappa_{\\text{var,min}}, g_A, A, \\epsilon_\\Sigma)$, the metric $g(x, S)$ is **uniformly elliptic** with explicit bounds:\n2873: \n2874: $$\n2875: c_{\\min}(\\rho) I \\preceq g(x, S) \\preceq c_{\\max} I\n2876: $$\n2877: \n2878: where:\n2879: \n2880: $$\n2881: c_{\\min}(\\rho) = \\epsilon_\\Sigma - \\Lambda_-(\\rho), \\quad c_{\\max} = H_{\\max}(\\rho) + \\epsilon_\\Sigma\n2882: $$\n2883: \n2884: and:\n2885: - $\\Lambda_-(\\rho) \\ge 0$ is the **spectral floor**: the maximum magnitude of negative eigenvalues of $H(x, S)$ over all states\n2886: - $H_{\\max}(\\rho)$ is the **spectral ceiling**: the maximum eigenvalue of $H(x, S)$ over all states\n2887: \n2888: **Sufficient Condition for Positive Definiteness:** If $\\epsilon_\\Sigma > \\Lambda_-(\\rho)$, then $g(x, S) \\succ 0$ for all $x, S$.\n2889: \n2890: **Inverse Bounds (Diffusion Tensor):**\n2891: \n2892: $$\n2893: \\frac{1}{c_{\\max}} I \\preceq D_{\\text{reg}}(x, S) \\preceq \\frac{1}{c_{\\min}(\\rho)} I\n2894: $$\n2895: \n2896: **Square Root Bounds (Diffusion Coefficient Matrix):**\n2897: \n2898: $$\n2899: \\frac{1}{\\sqrt{c_{\\max}}} I \\preceq \\Sigma_{\\text{reg}}(x, S) \\preceq \\frac{1}{\\sqrt{c_{\\min}(\\rho)}} I\n2900: $$"
    },
    {
      "directive_type": "proof",
      "label": "unlabeled-proof-325",
      "title": null,
      "start_line": 2902,
      "end_line": 3034,
      "header_lines": [],
      "content_start": 2903,
      "content_end": 3033,
      "content": "2903: \n2904: :::{prf:proof}\n2905: **Step 1: Eigenvalue Bounds for $g(x, S)$.**\n2906: \n2907: Let $\\lambda_1, \\ldots, \\lambda_d$ be the eigenvalues of $H(x, S)$. Then the eigenvalues of $g(x, S) = H(x, S) + \\epsilon_\\Sigma I$ are:\n2908: \n2909: $$\n2910: \\lambda_i(g) = \\lambda_i(H) + \\epsilon_\\Sigma, \\quad i = 1, \\ldots, d\n2911: $$\n2912: \n2913: **Lower Bound:** By definition of the spectral floor:\n2914: \n2915: $$\n2916: \\lambda_i(H) \\ge -\\Lambda_-(\\rho) \\quad \\Rightarrow \\quad \\lambda_i(g) \\ge \\epsilon_\\Sigma - \\Lambda_-(\\rho) = c_{\\min}(\\rho)\n2917: $$\n2918: \n2919: If $\\epsilon_\\Sigma > \\Lambda_-(\\rho)$, then $c_{\\min}(\\rho) > 0$ and $g \\succ 0$ (positive definite).\n2920: \n2921: **Upper Bound:** By the Hessian bound from {prf:ref}`thm-explicit-hessian`:\n2922: \n2923: $$\n2924: \\lambda_i(H) \\le \\|H\\| \\le H_{\\max}(\\rho) \\quad \\Rightarrow \\quad \\lambda_i(g) \\le H_{\\max}(\\rho) + \\epsilon_\\Sigma = c_{\\max}\n2925: $$\n2926: \n2927: Therefore $c_{\\min}(\\rho) I \\preceq g(x, S) \\preceq c_{\\max} I$ in the Loewner ordering.\n2928: \n2929: **Step 2: Inverse and Square Root Bounds.**\n2930: \n2931: For a positive definite matrix $A$ with $c_1 I \\preceq A \\preceq c_2 I$, we have:\n2932: \n2933: $$\n2934: \\frac{1}{c_2} I \\preceq A^{-1} \\preceq \\frac{1}{c_1} I\n2935: $$\n2936: \n2937: Applying this to $D_{\\text{reg}} = g^{-1}$ gives the inverse bounds.\n2938: \n2939: For the square root $\\Sigma_{\\text{reg}} = D_{\\text{reg}}^{1/2} = g^{-1/2}$, the eigenvalues are $1/\\sqrt{\\lambda_i(g)}$, giving:\n2940: \n2941: $$\n2942: \\frac{1}{\\sqrt{c_{\\max}}} I \\preceq \\Sigma_{\\text{reg}} \\preceq \\frac{1}{\\sqrt{c_{\\min}(\\rho)}} I\n2943: $$\n2944: \n2945: **Step 3: Explicit Formulas for the Bounds.**\n2946: \n2947: **Part A: Spectral Ceiling $H_{\\max}(\\rho)$.**\n2948: \n2949: From {prf:ref}`thm-explicit-hessian`, we have:\n2950: \n2951: $$\n2952: H_{\\max}(\\rho) = \\frac{g''_{\\max} (d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} + \\frac{g'_{\\max} d''_{\\max}}{\\kappa_{\\text{var,min}}} + O(1/\\rho)\n2953: $$\n2954: \n2955: where:\n2956: - $g''_{\\max} = \\sup_{z \\in \\mathbb{R}} |g''_A(z)|$ is the maximum second derivative of the rescale function\n2957: - $d'_{\\max} = \\sup_{x \\in \\mathcal{X}} \\|\\nabla_x d(x)\\|$ is the Lipschitz constant of the measurement function\n2958: - $d''_{\\max} = \\sup_{x \\in \\mathcal{X}} \\|\\nabla^2_x d(x)\\|_{\\text{op}}$ is the operator norm of the measurement Hessian\n2959: - The $O(1/\\rho)$ term captures moment correction contributions\n2960: \n2961: **Part B: Spectral Floor $\\Lambda_-(\\rho)$.**\n2962: \n2963: The spectral floor $\\Lambda_-(\\rho)$ bounds the maximum magnitude of negative eigenvalues of $H(x, S)$. From the Hessian decomposition:\n2964: \n2965: $$\n2966: H(x, S) = g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z + g'_A(Z) \\, \\nabla^2_x Z\n2967: $$\n2968: \n2969: **Case 1: Convex Rescale Function ($g''_A \\ge 0$) and Convex Measurement ($\\nabla^2 d \\succeq 0$).**\n2970: \n2971: If both $g''_A(z) \\ge 0$ for all $z$ and $\\nabla^2_x d(x) \\succeq 0$ for all $x$ (positive semi-definite), then both terms in $H$ are positive semi-definite:\n2972: - The outer product $\\nabla_x Z \\otimes \\nabla_x Z \\succeq 0$ (rank-1 positive semi-definite)\n2973: - The scaled Hessian term $g'_A(Z) \\nabla^2_x Z \\succeq 0$ (since $g'_A > 0$ by monotonicity)\n2974: \n2975: Therefore $H \\succeq 0$ and $\\Lambda_-(\\rho) = 0$.\n2976: \n2977: **Case 2: General Case (Allowing Indefinite Hessians).**\n2978: \n2979: When the measurement Hessian $\\nabla^2_x d$ can have negative eigenvalues, or when $g''_A$ can be negative, we must bound the most negative eigenvalue:\n2980: \n2981: $$\n2982: \\Lambda_-(\\rho) = \\sup_{x, S} \\max\\{0, -\\lambda_{\\min}(H(x, S))\\}\n2983: $$\n2984: \n2985: For a symmetric matrix $M = A + B$, we have $\\lambda_{\\min}(M) \\ge \\lambda_{\\min}(A) + \\lambda_{\\min}(B)$. Therefore:\n2986: \n2987: $$\n2988: \\lambda_{\\min}(H) \\ge \\lambda_{\\min}(g''_A(Z) \\, \\nabla Z \\otimes \\nabla Z) + \\lambda_{\\min}(g'_A(Z) \\, \\nabla^2 Z)\n2989: $$\n2990: \n2991: **Term 1 (Outer Product):** The minimum eigenvalue of the rank-1 matrix $g''_A \\nabla Z \\otimes \\nabla Z$ is:\n2992: - If $g''_A(Z) \\ge 0$: $\\lambda_{\\min} = 0$ (has $d-1$ zero eigenvalues)\n2993: - If $g''_A(Z) < 0$: $\\lambda_{\\min} = g''_A(Z) \\|\\nabla Z\\|^2 \\ge -|g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}}$\n2994: \n2995: **Term 2 (Scaled Hessian):** For the second term:\n2996: \n2997: $$\n2998: \\lambda_{\\min}(g'_A(Z) \\nabla^2 Z) = g'_A(Z) \\lambda_{\\min}(\\nabla^2 Z)\n2999: $$\n3000: \n3001: If $\\lambda_{\\min}(\\nabla^2 Z) < 0$ (indefinite), then using $g'_A(Z) \\le g'_{\\max}$ and defining:\n3002: \n3003: $$\n3004: d''_{\\min} = \\inf_{x \\in \\mathcal{X}} \\lambda_{\\min}(\\nabla^2_x d(x))\n3005: $$\n3006: \n3007: we have:\n3008: \n3009: $$\n3010: \\lambda_{\\min}(g'_A(Z) \\nabla^2 Z) \\ge g'_{\\min} \\cdot \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\n3011: $$\n3012: \n3013: where $g'_{\\min} = \\inf_{z} g'_A(z) > 0$ by monotonicity.\n3014: \n3015: **Combined Bound:**\n3016: \n3017: $$\n3018: -\\lambda_{\\min}(H) \\le \\max\\left\\{0, |g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} - g'_{\\min} \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\\right\\} + O(1/\\rho)\n3019: $$\n3020: \n3021: Therefore, the **explicit spectral floor bound** is:\n3022: \n3023: $$\n3024: \\Lambda_-(\\rho) = \\max\\left\\{0, |g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} - g'_{\\min} \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\\right\\} + C_{\\text{moment}}(\\rho)\n3025: $$\n3026: \n3027: where $C_{\\text{moment}}(\\rho) = O(1/\\rho)$ captures the moment correction terms and $d''_{\\min} = \\min\\{0, \\inf_x \\lambda_{\\min}(\\nabla^2 d(x))\\}$ is non-positive when the measurement can be concave.\n3028: \n3029: **Sufficient Condition for Positive Definiteness:** The regularization must satisfy:\n3030: \n3031: $$\n3032: \\epsilon_\\Sigma > \\Lambda_-(\\rho) = \\max\\left\\{0, |g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} - g'_{\\min} \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\\right\\} + C_{\\text{moment}}(\\rho)\n3033: $$",
      "metadata": {},
      "section": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "2902: :::\n2903: \n2904: :::{prf:proof}\n2905: **Step 1: Eigenvalue Bounds for $g(x, S)$.**\n2906: \n2907: Let $\\lambda_1, \\ldots, \\lambda_d$ be the eigenvalues of $H(x, S)$. Then the eigenvalues of $g(x, S) = H(x, S) + \\epsilon_\\Sigma I$ are:\n2908: \n2909: $$\n2910: \\lambda_i(g) = \\lambda_i(H) + \\epsilon_\\Sigma, \\quad i = 1, \\ldots, d\n2911: $$\n2912: \n2913: **Lower Bound:** By definition of the spectral floor:\n2914: \n2915: $$\n2916: \\lambda_i(H) \\ge -\\Lambda_-(\\rho) \\quad \\Rightarrow \\quad \\lambda_i(g) \\ge \\epsilon_\\Sigma - \\Lambda_-(\\rho) = c_{\\min}(\\rho)\n2917: $$\n2918: \n2919: If $\\epsilon_\\Sigma > \\Lambda_-(\\rho)$, then $c_{\\min}(\\rho) > 0$ and $g \\succ 0$ (positive definite).\n2920: \n2921: **Upper Bound:** By the Hessian bound from {prf:ref}`thm-explicit-hessian`:\n2922: \n2923: $$\n2924: \\lambda_i(H) \\le \\|H\\| \\le H_{\\max}(\\rho) \\quad \\Rightarrow \\quad \\lambda_i(g) \\le H_{\\max}(\\rho) + \\epsilon_\\Sigma = c_{\\max}\n2925: $$\n2926: \n2927: Therefore $c_{\\min}(\\rho) I \\preceq g(x, S) \\preceq c_{\\max} I$ in the Loewner ordering.\n2928: \n2929: **Step 2: Inverse and Square Root Bounds.**\n2930: \n2931: For a positive definite matrix $A$ with $c_1 I \\preceq A \\preceq c_2 I$, we have:\n2932: \n2933: $$\n2934: \\frac{1}{c_2} I \\preceq A^{-1} \\preceq \\frac{1}{c_1} I\n2935: $$\n2936: \n2937: Applying this to $D_{\\text{reg}} = g^{-1}$ gives the inverse bounds.\n2938: \n2939: For the square root $\\Sigma_{\\text{reg}} = D_{\\text{reg}}^{1/2} = g^{-1/2}$, the eigenvalues are $1/\\sqrt{\\lambda_i(g)}$, giving:\n2940: \n2941: $$\n2942: \\frac{1}{\\sqrt{c_{\\max}}} I \\preceq \\Sigma_{\\text{reg}} \\preceq \\frac{1}{\\sqrt{c_{\\min}(\\rho)}} I\n2943: $$\n2944: \n2945: **Step 3: Explicit Formulas for the Bounds.**\n2946: \n2947: **Part A: Spectral Ceiling $H_{\\max}(\\rho)$.**\n2948: \n2949: From {prf:ref}`thm-explicit-hessian`, we have:\n2950: \n2951: $$\n2952: H_{\\max}(\\rho) = \\frac{g''_{\\max} (d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} + \\frac{g'_{\\max} d''_{\\max}}{\\kappa_{\\text{var,min}}} + O(1/\\rho)\n2953: $$\n2954: \n2955: where:\n2956: - $g''_{\\max} = \\sup_{z \\in \\mathbb{R}} |g''_A(z)|$ is the maximum second derivative of the rescale function\n2957: - $d'_{\\max} = \\sup_{x \\in \\mathcal{X}} \\|\\nabla_x d(x)\\|$ is the Lipschitz constant of the measurement function\n2958: - $d''_{\\max} = \\sup_{x \\in \\mathcal{X}} \\|\\nabla^2_x d(x)\\|_{\\text{op}}$ is the operator norm of the measurement Hessian\n2959: - The $O(1/\\rho)$ term captures moment correction contributions\n2960: \n2961: **Part B: Spectral Floor $\\Lambda_-(\\rho)$.**\n2962: \n2963: The spectral floor $\\Lambda_-(\\rho)$ bounds the maximum magnitude of negative eigenvalues of $H(x, S)$. From the Hessian decomposition:\n2964: \n2965: $$\n2966: H(x, S) = g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z + g'_A(Z) \\, \\nabla^2_x Z\n2967: $$\n2968: \n2969: **Case 1: Convex Rescale Function ($g''_A \\ge 0$) and Convex Measurement ($\\nabla^2 d \\succeq 0$).**\n2970: \n2971: If both $g''_A(z) \\ge 0$ for all $z$ and $\\nabla^2_x d(x) \\succeq 0$ for all $x$ (positive semi-definite), then both terms in $H$ are positive semi-definite:\n2972: - The outer product $\\nabla_x Z \\otimes \\nabla_x Z \\succeq 0$ (rank-1 positive semi-definite)\n2973: - The scaled Hessian term $g'_A(Z) \\nabla^2_x Z \\succeq 0$ (since $g'_A > 0$ by monotonicity)\n2974: \n2975: Therefore $H \\succeq 0$ and $\\Lambda_-(\\rho) = 0$.\n2976: \n2977: **Case 2: General Case (Allowing Indefinite Hessians).**\n2978: \n2979: When the measurement Hessian $\\nabla^2_x d$ can have negative eigenvalues, or when $g''_A$ can be negative, we must bound the most negative eigenvalue:\n2980: \n2981: $$\n2982: \\Lambda_-(\\rho) = \\sup_{x, S} \\max\\{0, -\\lambda_{\\min}(H(x, S))\\}\n2983: $$\n2984: \n2985: For a symmetric matrix $M = A + B$, we have $\\lambda_{\\min}(M) \\ge \\lambda_{\\min}(A) + \\lambda_{\\min}(B)$. Therefore:\n2986: \n2987: $$\n2988: \\lambda_{\\min}(H) \\ge \\lambda_{\\min}(g''_A(Z) \\, \\nabla Z \\otimes \\nabla Z) + \\lambda_{\\min}(g'_A(Z) \\, \\nabla^2 Z)\n2989: $$\n2990: \n2991: **Term 1 (Outer Product):** The minimum eigenvalue of the rank-1 matrix $g''_A \\nabla Z \\otimes \\nabla Z$ is:\n2992: - If $g''_A(Z) \\ge 0$: $\\lambda_{\\min} = 0$ (has $d-1$ zero eigenvalues)\n2993: - If $g''_A(Z) < 0$: $\\lambda_{\\min} = g''_A(Z) \\|\\nabla Z\\|^2 \\ge -|g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}}$\n2994: \n2995: **Term 2 (Scaled Hessian):** For the second term:\n2996: \n2997: $$\n2998: \\lambda_{\\min}(g'_A(Z) \\nabla^2 Z) = g'_A(Z) \\lambda_{\\min}(\\nabla^2 Z)\n2999: $$\n3000: \n3001: If $\\lambda_{\\min}(\\nabla^2 Z) < 0$ (indefinite), then using $g'_A(Z) \\le g'_{\\max}$ and defining:\n3002: \n3003: $$\n3004: d''_{\\min} = \\inf_{x \\in \\mathcal{X}} \\lambda_{\\min}(\\nabla^2_x d(x))\n3005: $$\n3006: \n3007: we have:\n3008: \n3009: $$\n3010: \\lambda_{\\min}(g'_A(Z) \\nabla^2 Z) \\ge g'_{\\min} \\cdot \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\n3011: $$\n3012: \n3013: where $g'_{\\min} = \\inf_{z} g'_A(z) > 0$ by monotonicity.\n3014: \n3015: **Combined Bound:**\n3016: \n3017: $$\n3018: -\\lambda_{\\min}(H) \\le \\max\\left\\{0, |g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} - g'_{\\min} \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\\right\\} + O(1/\\rho)\n3019: $$\n3020: \n3021: Therefore, the **explicit spectral floor bound** is:\n3022: \n3023: $$\n3024: \\Lambda_-(\\rho) = \\max\\left\\{0, |g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} - g'_{\\min} \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\\right\\} + C_{\\text{moment}}(\\rho)\n3025: $$\n3026: \n3027: where $C_{\\text{moment}}(\\rho) = O(1/\\rho)$ captures the moment correction terms and $d''_{\\min} = \\min\\{0, \\inf_x \\lambda_{\\min}(\\nabla^2 d(x))\\}$ is non-positive when the measurement can be concave.\n3028: \n3029: **Sufficient Condition for Positive Definiteness:** The regularization must satisfy:\n3030: \n3031: $$\n3032: \\epsilon_\\Sigma > \\Lambda_-(\\rho) = \\max\\left\\{0, |g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} - g'_{\\min} \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\\right\\} + C_{\\text{moment}}(\\rho)\n3033: $$\n3034: "
    },
    {
      "directive_type": "remark",
      "label": "unlabeled-remark-459",
      "title": "Algorithmic Control of Ellipticity",
      "start_line": 3036,
      "end_line": 3066,
      "header_lines": [
        3037
      ],
      "content_start": 3039,
      "content_end": 3065,
      "content": "3039: :class: important\n3040: \n3041: The ellipticity bounds are **fully controlled by algorithmic parameters**:\n3042: \n3043: **1. Regularization Parameter $\\epsilon_\\Sigma$:**\n3044: - **Lower bound:** $c_{\\min}(\\rho) = \\epsilon_\\Sigma - \\Lambda_-(\\rho)$\n3045: - Larger $\\epsilon_\\Sigma$ \u2192 stronger regularization \u2192 more isotropic diffusion\n3046: - Must satisfy $\\epsilon_\\Sigma > \\Lambda_-(\\rho)$ for positive definiteness\n3047: \n3048: **2. Localization Scale $\\rho$:**\n3049: - Affects $H_{\\max}(\\rho)$ through the $O(1/\\rho)$ moment correction terms\n3050: - Smaller $\\rho$ \u2192 tighter localization \u2192 larger $H_{\\max}(\\rho)$ \u2192 wider ellipticity gap\n3051: - Larger $\\rho$ \u2192 global averaging \u2192 more stable $H$ \u2192 narrower ellipticity gap\n3052: \n3053: **3. Variance Regularization $\\kappa_{\\text{var,min}}$:**\n3054: - Appears in denominator of Hessian bound\n3055: - Larger $\\kappa_{\\text{var,min}}$ \u2192 smaller $H_{\\max}(\\rho)$ \u2192 better conditioning\n3056: \n3057: **4. Measurement Regularity $(d'_{\\max}, d''_{\\max})$:**\n3058: - Smoother measurement functions \u2192 smaller Hessian bounds\n3059: - Choice of $d$ (reward, diversity, etc.) directly affects geometry\n3060: \n3061: **5. Rescale Function $(g_A, g'_{\\max}, g''_{\\max})$:**\n3062: - Bounds on derivatives control curvature amplification\n3063: - Saturating rescales (e.g., sigmoid) naturally bound curvature\n3064: \n3065: The convergence rate depends on $c_{\\min}(\\rho)$ (see Chapter 5), so there is a **design tradeoff**:",
      "metadata": {
        "class": "important"
      },
      "section": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "3036: :::\n3037: \n3038: :::{prf:remark} Algorithmic Control of Ellipticity\n3039: :class: important\n3040: \n3041: The ellipticity bounds are **fully controlled by algorithmic parameters**:\n3042: \n3043: **1. Regularization Parameter $\\epsilon_\\Sigma$:**\n3044: - **Lower bound:** $c_{\\min}(\\rho) = \\epsilon_\\Sigma - \\Lambda_-(\\rho)$\n3045: - Larger $\\epsilon_\\Sigma$ \u2192 stronger regularization \u2192 more isotropic diffusion\n3046: - Must satisfy $\\epsilon_\\Sigma > \\Lambda_-(\\rho)$ for positive definiteness\n3047: \n3048: **2. Localization Scale $\\rho$:**\n3049: - Affects $H_{\\max}(\\rho)$ through the $O(1/\\rho)$ moment correction terms\n3050: - Smaller $\\rho$ \u2192 tighter localization \u2192 larger $H_{\\max}(\\rho)$ \u2192 wider ellipticity gap\n3051: - Larger $\\rho$ \u2192 global averaging \u2192 more stable $H$ \u2192 narrower ellipticity gap\n3052: \n3053: **3. Variance Regularization $\\kappa_{\\text{var,min}}$:**\n3054: - Appears in denominator of Hessian bound\n3055: - Larger $\\kappa_{\\text{var,min}}$ \u2192 smaller $H_{\\max}(\\rho)$ \u2192 better conditioning\n3056: \n3057: **4. Measurement Regularity $(d'_{\\max}, d''_{\\max})$:**\n3058: - Smoother measurement functions \u2192 smaller Hessian bounds\n3059: - Choice of $d$ (reward, diversity, etc.) directly affects geometry\n3060: \n3061: **5. Rescale Function $(g_A, g'_{\\max}, g''_{\\max})$:**\n3062: - Bounds on derivatives control curvature amplification\n3063: - Saturating rescales (e.g., sigmoid) naturally bound curvature\n3064: \n3065: The convergence rate depends on $c_{\\min}(\\rho)$ (see Chapter 5), so there is a **design tradeoff**:\n3066: - **Large $\\epsilon_\\Sigma$:** Strong convergence guarantees, but less geometric adaptation"
    },
    {
      "directive_type": "definition",
      "label": "def-emergent-manifold",
      "title": "Emergent Riemannian Manifold",
      "start_line": 3072,
      "end_line": 3109,
      "header_lines": [
        3073
      ],
      "content_start": 3075,
      "content_end": 3108,
      "content": "3075: :label: def-emergent-manifold\n3076: \n3077: The metric $g(x, S)$ from {prf:ref}`def-metric-explicit` endows the state space $\\mathcal{X}$ with the structure of a **Riemannian manifold** $(\\mathcal{X}, g_S)$, where the metric depends parametrically on the swarm state $S$.\n3078: \n3079: **Geometric Quantities:**\n3080: \n3081: **1. Metric Tensor (Index Form):**\n3082: \n3083: $$\n3084: g_{ab}(x, S) = \\left[\\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x)\\right]_{ab} + \\epsilon_\\Sigma \\delta_{ab}\n3085: $$\n3086: \n3087: **2. Inverse Metric (Contravariant Tensor):**\n3088: \n3089: $$\n3090: g^{ab}(x, S) = \\left[\\left(\\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x) + \\epsilon_\\Sigma I\\right)^{-1}\\right]_{ab}\n3091: $$\n3092: \n3093: **3. Volume Element:** The Riemannian volume measure is:\n3094: \n3095: $$\n3096: d\\text{Vol}_g = \\sqrt{\\det g(x, S)} \\, dx\n3097: $$\n3098: \n3099: **4. Geodesic Equation:** Curves $\\gamma(t)$ that minimize length satisfy:\n3100: \n3101: $$\n3102: \\frac{d^2 \\gamma^a}{dt^2} + \\Gamma^a_{bc}(x, S) \\frac{d\\gamma^b}{dt} \\frac{d\\gamma^c}{dt} = 0\n3103: $$\n3104: \n3105: where $\\Gamma^a_{bc}$ are the **Christoffel symbols** computed from $g$ via:\n3106: \n3107: $$\n3108: \\Gamma^a_{bc} = \\frac{1}{2} g^{ad} \\left(\\frac{\\partial g_{db}}{\\partial x^c} + \\frac{\\partial g_{dc}}{\\partial x^b} - \\frac{\\partial g_{bc}}{\\partial x^d}\\right)",
      "metadata": {
        "label": "def-emergent-manifold"
      },
      "section": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "3072: With the metric explicitly constructed, we can characterize the induced geometric structure.\n3073: \n3074: :::{prf:definition} Emergent Riemannian Manifold\n3075: :label: def-emergent-manifold\n3076: \n3077: The metric $g(x, S)$ from {prf:ref}`def-metric-explicit` endows the state space $\\mathcal{X}$ with the structure of a **Riemannian manifold** $(\\mathcal{X}, g_S)$, where the metric depends parametrically on the swarm state $S$.\n3078: \n3079: **Geometric Quantities:**\n3080: \n3081: **1. Metric Tensor (Index Form):**\n3082: \n3083: $$\n3084: g_{ab}(x, S) = \\left[\\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x)\\right]_{ab} + \\epsilon_\\Sigma \\delta_{ab}\n3085: $$\n3086: \n3087: **2. Inverse Metric (Contravariant Tensor):**\n3088: \n3089: $$\n3090: g^{ab}(x, S) = \\left[\\left(\\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x) + \\epsilon_\\Sigma I\\right)^{-1}\\right]_{ab}\n3091: $$\n3092: \n3093: **3. Volume Element:** The Riemannian volume measure is:\n3094: \n3095: $$\n3096: d\\text{Vol}_g = \\sqrt{\\det g(x, S)} \\, dx\n3097: $$\n3098: \n3099: **4. Geodesic Equation:** Curves $\\gamma(t)$ that minimize length satisfy:\n3100: \n3101: $$\n3102: \\frac{d^2 \\gamma^a}{dt^2} + \\Gamma^a_{bc}(x, S) \\frac{d\\gamma^b}{dt} \\frac{d\\gamma^c}{dt} = 0\n3103: $$\n3104: \n3105: where $\\Gamma^a_{bc}$ are the **Christoffel symbols** computed from $g$ via:\n3106: \n3107: $$\n3108: \\Gamma^a_{bc} = \\frac{1}{2} g^{ad} \\left(\\frac{\\partial g_{db}}{\\partial x^c} + \\frac{\\partial g_{dc}}{\\partial x^b} - \\frac{\\partial g_{bc}}{\\partial x^d}\\right)\n3109: $$"
    },
    {
      "directive_type": "proposition",
      "label": "prop-geodesics-fitness",
      "title": "Geodesics Favor High-Fitness Regions",
      "start_line": 3111,
      "end_line": 3133,
      "header_lines": [
        3112
      ],
      "content_start": 3114,
      "content_end": 3132,
      "content": "3114: :label: prop-geodesics-fitness\n3115: \n3116: The geodesics of the emergent metric $g(x, S)$ are **biased toward high-fitness regions**. Specifically:\n3117: \n3118: 1. **Shorter distances in high-fitness regions:** For two points $x_1, x_2 \\in \\mathcal{X}$, the Riemannian distance:\n3119: \n3120: $$\n3121: d_g(x_1, x_2) = \\inf_{\\gamma: x_1 \\to x_2} \\int_0^1 \\sqrt{g_{ab}(\\gamma(t), S) \\dot{\\gamma}^a(t) \\dot{\\gamma}^b(t)} \\, dt\n3122: $$\n3123: \n3124: is **smaller** when the path passes through regions of high $V_{\\text{fit}}$ (low curvature, low metric eigenvalues).\n3125: \n3126: 2. **Geodesics avoid high-curvature regions:** The metric eigenvalues are largest where the Hessian $H$ has large positive eigenvalues, which occurs where the fitness landscape is **most convexly curved**. Geodesics bend away from these regions of high metric density to minimize path length.\n3127: \n3128: 3. **Connection to natural gradient:** The inverse metric $g^{-1} = D_{\\text{reg}}$ defines the **natural gradient**:\n3129: \n3130: $$\n3131: \\nabla^{\\text{nat}} V_{\\text{fit}} = g^{-1} \\nabla V_{\\text{fit}}\n3132: $$",
      "metadata": {
        "label": "prop-geodesics-fitness"
      },
      "section": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "3111: :::\n3112: \n3113: :::{prf:proposition} Geodesics Favor High-Fitness Regions\n3114: :label: prop-geodesics-fitness\n3115: \n3116: The geodesics of the emergent metric $g(x, S)$ are **biased toward high-fitness regions**. Specifically:\n3117: \n3118: 1. **Shorter distances in high-fitness regions:** For two points $x_1, x_2 \\in \\mathcal{X}$, the Riemannian distance:\n3119: \n3120: $$\n3121: d_g(x_1, x_2) = \\inf_{\\gamma: x_1 \\to x_2} \\int_0^1 \\sqrt{g_{ab}(\\gamma(t), S) \\dot{\\gamma}^a(t) \\dot{\\gamma}^b(t)} \\, dt\n3122: $$\n3123: \n3124: is **smaller** when the path passes through regions of high $V_{\\text{fit}}$ (low curvature, low metric eigenvalues).\n3125: \n3126: 2. **Geodesics avoid high-curvature regions:** The metric eigenvalues are largest where the Hessian $H$ has large positive eigenvalues, which occurs where the fitness landscape is **most convexly curved**. Geodesics bend away from these regions of high metric density to minimize path length.\n3127: \n3128: 3. **Connection to natural gradient:** The inverse metric $g^{-1} = D_{\\text{reg}}$ defines the **natural gradient**:\n3129: \n3130: $$\n3131: \\nabla^{\\text{nat}} V_{\\text{fit}} = g^{-1} \\nabla V_{\\text{fit}}\n3132: $$\n3133: "
    },
    {
      "directive_type": "proof",
      "label": "unlabeled-proof-558",
      "title": null,
      "start_line": 3135,
      "end_line": 3167,
      "header_lines": [],
      "content_start": 3136,
      "content_end": 3166,
      "content": "3136: \n3137: :::{prf:proof}\n3138: **Part 1: Distance Formula.**\n3139: \n3140: By definition, the Riemannian distance is the infimum of lengths:\n3141: \n3142: $$\n3143: d_g(x_1, x_2) = \\inf_\\gamma \\int_0^1 \\sqrt{\\dot{\\gamma}^T g(\\gamma(t), S) \\dot{\\gamma}} \\, dt\n3144: $$\n3145: \n3146: In a region where $V_{\\text{fit}}$ is high and flat (low curvature), $H$ has small eigenvalues, so $g = H + \\epsilon_\\Sigma I \\approx \\epsilon_\\Sigma I$ (nearly Euclidean). In a region of low fitness and high curvature, $H$ has large eigenvalues, so $g$ is \"stretched\" (larger metric eigenvalues \u2192 longer distances).\n3147: \n3148: Therefore, paths through high-fitness regions incur smaller length than paths through low-fitness regions, even if the Euclidean distances are equal.\n3149: \n3150: **Part 2: Geodesic Deviation.**\n3151: \n3152: The geodesic equation involves the Christoffel symbols $\\Gamma^a_{bc}$, which depend on $\\partial g/\\partial x$. Since:\n3153: \n3154: $$\n3155: \\frac{\\partial g_{ab}}{\\partial x^c} = \\frac{\\partial}{\\partial x^c} \\left[\\nabla^2 V_{\\text{fit}}\\right]_{ab} = \\left[\\nabla^3 V_{\\text{fit}}\\right]_{abc}\n3156: $$\n3157: \n3158: the geodesic curvature is proportional to the **third derivatives of the fitness potential**. Regions of high curvature (large $\\|\\nabla^3 V_{\\text{fit}}\\|$) exert a \"repulsive force\" on geodesics, causing them to deviate.\n3159: \n3160: **Part 3: Natural Gradient Connection.**\n3161: \n3162: The natural gradient is defined as:\n3163: \n3164: $$\n3165: \\nabla^{\\text{nat}} V_{\\text{fit}} = g^{-1} \\nabla V_{\\text{fit}} = D_{\\text{reg}} \\nabla V_{\\text{fit}}\n3166: $$",
      "metadata": {},
      "section": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "3135: :::\n3136: \n3137: :::{prf:proof}\n3138: **Part 1: Distance Formula.**\n3139: \n3140: By definition, the Riemannian distance is the infimum of lengths:\n3141: \n3142: $$\n3143: d_g(x_1, x_2) = \\inf_\\gamma \\int_0^1 \\sqrt{\\dot{\\gamma}^T g(\\gamma(t), S) \\dot{\\gamma}} \\, dt\n3144: $$\n3145: \n3146: In a region where $V_{\\text{fit}}$ is high and flat (low curvature), $H$ has small eigenvalues, so $g = H + \\epsilon_\\Sigma I \\approx \\epsilon_\\Sigma I$ (nearly Euclidean). In a region of low fitness and high curvature, $H$ has large eigenvalues, so $g$ is \"stretched\" (larger metric eigenvalues \u2192 longer distances).\n3147: \n3148: Therefore, paths through high-fitness regions incur smaller length than paths through low-fitness regions, even if the Euclidean distances are equal.\n3149: \n3150: **Part 2: Geodesic Deviation.**\n3151: \n3152: The geodesic equation involves the Christoffel symbols $\\Gamma^a_{bc}$, which depend on $\\partial g/\\partial x$. Since:\n3153: \n3154: $$\n3155: \\frac{\\partial g_{ab}}{\\partial x^c} = \\frac{\\partial}{\\partial x^c} \\left[\\nabla^2 V_{\\text{fit}}\\right]_{ab} = \\left[\\nabla^3 V_{\\text{fit}}\\right]_{abc}\n3156: $$\n3157: \n3158: the geodesic curvature is proportional to the **third derivatives of the fitness potential**. Regions of high curvature (large $\\|\\nabla^3 V_{\\text{fit}}\\|$) exert a \"repulsive force\" on geodesics, causing them to deviate.\n3159: \n3160: **Part 3: Natural Gradient Connection.**\n3161: \n3162: The natural gradient is defined as:\n3163: \n3164: $$\n3165: \\nabla^{\\text{nat}} V_{\\text{fit}} = g^{-1} \\nabla V_{\\text{fit}} = D_{\\text{reg}} \\nabla V_{\\text{fit}}\n3166: $$\n3167: "
    },
    {
      "directive_type": "theorem",
      "label": "thm-algorithmic-tunability",
      "title": "Algorithmic Tunability of the Emergent Geometry",
      "start_line": 3255,
      "end_line": 3283,
      "header_lines": [
        3256
      ],
      "content_start": 3258,
      "content_end": 3282,
      "content": "3258: :label: thm-algorithmic-tunability\n3259: \n3260: The emergent Riemannian geometry is **completely determined** by the algorithmic parameters. Specifically:\n3261: \n3262: **1. Localization Scale $\\rho$:** Controls the spatial extent of geometric structure.\n3263: - Small $\\rho$: Hyper-local geometry, responds to fine-scale features\n3264: - Large $\\rho$: Global geometry, averages over entire landscape\n3265: \n3266: **2. Regularization $\\epsilon_\\Sigma$:** Controls the deviation from Euclidean geometry.\n3267: - Small $\\epsilon_\\Sigma$: Strong geometric adaptation, metric dominated by Hessian $H$\n3268: - Large $\\epsilon_\\Sigma$: Weak geometric adaptation, metric nearly Euclidean $g \\approx \\epsilon_\\Sigma I$\n3269: \n3270: **3. Variance Regularization $\\kappa_{\\text{var,min}}$:** Controls the conditioning of the Z-score.\n3271: - Small $\\kappa_{\\text{var,min}}$: Sensitive to variance collapse, large Hessian bounds\n3272: - Large $\\kappa_{\\text{var,min}}$: Robust to variance collapse, bounded Hessian\n3273: \n3274: **4. Measurement Function $d$:** Determines **what geometric structure emerges**.\n3275: - Reward: Geometry encodes value landscape\n3276: - Diversity: Geometry encodes novelty structure\n3277: - Custom metrics: User-defined geometric inductive biases\n3278: \n3279: **5. Rescale Function $g_A$:** Controls the **amplification** of curvature.\n3280: - Linear: Direct Hessian of Z-score\n3281: - Sigmoid: Saturated curvature, bounded $g''_A$\n3282: - Custom: Tailored curvature profiles",
      "metadata": {
        "label": "thm-algorithmic-tunability"
      },
      "section": "## 9. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "3255: $$\n3256: \n3257: :::{prf:theorem} Algorithmic Tunability of the Emergent Geometry\n3258: :label: thm-algorithmic-tunability\n3259: \n3260: The emergent Riemannian geometry is **completely determined** by the algorithmic parameters. Specifically:\n3261: \n3262: **1. Localization Scale $\\rho$:** Controls the spatial extent of geometric structure.\n3263: - Small $\\rho$: Hyper-local geometry, responds to fine-scale features\n3264: - Large $\\rho$: Global geometry, averages over entire landscape\n3265: \n3266: **2. Regularization $\\epsilon_\\Sigma$:** Controls the deviation from Euclidean geometry.\n3267: - Small $\\epsilon_\\Sigma$: Strong geometric adaptation, metric dominated by Hessian $H$\n3268: - Large $\\epsilon_\\Sigma$: Weak geometric adaptation, metric nearly Euclidean $g \\approx \\epsilon_\\Sigma I$\n3269: \n3270: **3. Variance Regularization $\\kappa_{\\text{var,min}}$:** Controls the conditioning of the Z-score.\n3271: - Small $\\kappa_{\\text{var,min}}$: Sensitive to variance collapse, large Hessian bounds\n3272: - Large $\\kappa_{\\text{var,min}}$: Robust to variance collapse, bounded Hessian\n3273: \n3274: **4. Measurement Function $d$:** Determines **what geometric structure emerges**.\n3275: - Reward: Geometry encodes value landscape\n3276: - Diversity: Geometry encodes novelty structure\n3277: - Custom metrics: User-defined geometric inductive biases\n3278: \n3279: **5. Rescale Function $g_A$:** Controls the **amplification** of curvature.\n3280: - Linear: Direct Hessian of Z-score\n3281: - Sigmoid: Saturated curvature, bounded $g''_A$\n3282: - Custom: Tailored curvature profiles\n3283: "
    }
  ],
  "validation": {
    "ok": true,
    "errors": []
  }
}