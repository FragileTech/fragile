{
  "chapter_index": 4,
  "section_id": "## 4. The Ruppeiner Metric: Algorithmic Construction",
  "directive_count": 17,
  "hints": [
    {
      "directive_type": "definition",
      "label": "def-empirical-entropy",
      "title": "Empirical Entropy Estimator",
      "start_line": 1212,
      "end_line": 1228,
      "header_lines": [
        1213
      ],
      "content_start": 1215,
      "content_end": 1227,
      "content": "1215: :label: def-empirical-entropy\n1216: \n1217: Given $N$ samples $\\{(x_i, v_i)\\}_{i=1}^N$ from the QSD $\\rho_{\\text{QSD}}$, the **empirical entropy estimate** is:\n1218: \n1219: $$\n1220: \\hat{S}_N = -\\frac{k_B}{N} \\sum_{i=1}^N \\log \\hat{\\rho}_{\\text{QSD}}(x_i, v_i) + S_{\\text{ref}}\n1221: $$\n1222: \n1223: where $\\hat{\\rho}_{\\text{QSD}}$ is a kernel density estimate:\n1224: \n1225: $$\n1226: \\hat{\\rho}_{\\text{QSD}}(x, v) = \\frac{1}{N h^d} \\sum_{j=1}^N K\\left(\\frac{\\|x - x_j\\|}{h}\\right) K_v\\left(\\frac{\\|v - v_j\\|}{h_v}\\right)\n1227: $$",
      "metadata": {
        "label": "def-empirical-entropy"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1212: The first ingredient is estimating the thermodynamic entropy $S$ from QSD samples.\n1213: \n1214: :::{prf:definition} Empirical Entropy Estimator\n1215: :label: def-empirical-entropy\n1216: \n1217: Given $N$ samples $\\{(x_i, v_i)\\}_{i=1}^N$ from the QSD $\\rho_{\\text{QSD}}$, the **empirical entropy estimate** is:\n1218: \n1219: $$\n1220: \\hat{S}_N = -\\frac{k_B}{N} \\sum_{i=1}^N \\log \\hat{\\rho}_{\\text{QSD}}(x_i, v_i) + S_{\\text{ref}}\n1221: $$\n1222: \n1223: where $\\hat{\\rho}_{\\text{QSD}}$ is a kernel density estimate:\n1224: \n1225: $$\n1226: \\hat{\\rho}_{\\text{QSD}}(x, v) = \\frac{1}{N h^d} \\sum_{j=1}^N K\\left(\\frac{\\|x - x_j\\|}{h}\\right) K_v\\left(\\frac{\\|v - v_j\\|}{h_v}\\right)\n1227: $$\n1228: "
    },
    {
      "directive_type": "theorem",
      "label": "thm-entropy-estimator-error",
      "title": "Entropy Estimator Consistency and Error Bound",
      "start_line": 1230,
      "end_line": 1253,
      "header_lines": [
        1231
      ],
      "content_start": 1233,
      "content_end": 1252,
      "content": "1233: :label: thm-entropy-estimator-error\n1234: \n1235: Under the following assumptions:\n1236: 1. The QSD $\\rho_{\\text{QSD}}$ is continuously differentiable with bounded derivatives\n1237: 2. The kernel $K$ is a smooth, compactly supported probability density\n1238: 3. Bandwidth $h = h_N$ satisfies $h_N \\to 0$ and $N h_N^d \\to \\infty$ as $N \\to \\infty$\n1239: \n1240: The empirical entropy estimator $\\hat{S}_N$ satisfies:\n1241: \n1242: $$\n1243: \\mathbb{E}[|\\hat{S}_N - S|] = O\\left(\\frac{1}{\\sqrt{N h_N^d}} + h_N^2\\right)\n1244: $$\n1245: \n1246: with high probability (concentration bound):\n1247: \n1248: $$\n1249: \\mathbb{P}[|\\hat{S}_N - S| > \\epsilon] \\leq 2\\exp\\left(-\\frac{N h_N^d \\epsilon^2}{2C}\\right)\n1250: $$\n1251: \n1252: for constant $C$ depending on $\\|\\rho_{\\text{QSD}}\\|_{\\infty}$ and $\\|\\nabla \\rho_{\\text{QSD}}\\|_{\\infty}$.",
      "metadata": {
        "label": "thm-entropy-estimator-error"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1230: :::\n1231: \n1232: :::{prf:theorem} Entropy Estimator Consistency and Error Bound\n1233: :label: thm-entropy-estimator-error\n1234: \n1235: Under the following assumptions:\n1236: 1. The QSD $\\rho_{\\text{QSD}}$ is continuously differentiable with bounded derivatives\n1237: 2. The kernel $K$ is a smooth, compactly supported probability density\n1238: 3. Bandwidth $h = h_N$ satisfies $h_N \\to 0$ and $N h_N^d \\to \\infty$ as $N \\to \\infty$\n1239: \n1240: The empirical entropy estimator $\\hat{S}_N$ satisfies:\n1241: \n1242: $$\n1243: \\mathbb{E}[|\\hat{S}_N - S|] = O\\left(\\frac{1}{\\sqrt{N h_N^d}} + h_N^2\\right)\n1244: $$\n1245: \n1246: with high probability (concentration bound):\n1247: \n1248: $$\n1249: \\mathbb{P}[|\\hat{S}_N - S| > \\epsilon] \\leq 2\\exp\\left(-\\frac{N h_N^d \\epsilon^2}{2C}\\right)\n1250: $$\n1251: \n1252: for constant $C$ depending on $\\|\\rho_{\\text{QSD}}\\|_{\\infty}$ and $\\|\\nabla \\rho_{\\text{QSD}}\\|_{\\infty}$.\n1253: "
    },
    {
      "directive_type": "proof",
      "label": "unlabeled-proof-67",
      "title": null,
      "start_line": 1255,
      "end_line": 1310,
      "header_lines": [],
      "content_start": 1257,
      "content_end": 1309,
      "content": "1257: :::{prf:proof}\n1258: \n1259: **Step 1: Decompose error into bias and variance.**\n1260: \n1261: $$\n1262: \\mathbb{E}[|\\hat{S}_N - S|] \\leq \\underbrace{|\\mathbb{E}[\\hat{S}_N] - S|}_{\\text{bias}} + \\underbrace{\\sqrt{\\text{Var}(\\hat{S}_N)}}_{\\text{stdev}}\n1263: $$\n1264: \n1265: **Step 2: Bound the bias.**\n1266: \n1267: The bias of kernel density estimation is well-known (Silverman, 1986):\n1268: \n1269: $$\n1270: |\\mathbb{E}[\\hat{\\rho}_{\\text{QSD}}(x, v)] - \\rho_{\\text{QSD}}(x, v)| = O(h^2)\n1271: $$\n1272: \n1273: under smoothness assumptions on $\\rho_{\\text{QSD}}$. Propagating through the logarithm:\n1274: \n1275: $$\n1276: |\\mathbb{E}[\\log \\hat{\\rho}_{\\text{QSD}}(x, v)] - \\log \\rho_{\\text{QSD}}(x, v)| = O(h^2)\n1277: $$\n1278: \n1279: Therefore, the bias of $\\hat{S}_N$ is $O(h^2)$.\n1280: \n1281: **Step 3: Bound the variance.**\n1282: \n1283: The variance of the log-density estimate is:\n1284: \n1285: $$\n1286: \\text{Var}(\\log \\hat{\\rho}_{\\text{QSD}}(x, v)) \\leq \\frac{C}{N h^d \\rho_{\\text{QSD}}(x, v)}\n1287: $$\n1288: \n1289: Integrating over the QSD and using Jensen's inequality:\n1290: \n1291: $$\n1292: \\text{Var}(\\hat{S}_N) = O\\left(\\frac{1}{N h^d}\\right)\n1293: $$\n1294: \n1295: **Step 4: Combine via Chebyshev's inequality.**\n1296: \n1297: $$\n1298: \\mathbb{P}[|\\hat{S}_N - \\mathbb{E}[\\hat{S}_N]| > t] \\leq \\frac{\\text{Var}(\\hat{S}_N)}{t^2} = O\\left(\\frac{1}{N h^d t^2}\\right)\n1299: $$\n1300: \n1301: Setting $t = \\epsilon$ and accounting for the bias gives the stated concentration bound.\n1302: \n1303: **Step 5: Optimize bandwidth.**\n1304: \n1305: The total error is $O(1/\\sqrt{N h^d} + h^2)$. Setting the derivative to zero:\n1306: \n1307: $$\n1308: \\frac{d}{dh}\\left(\\frac{1}{\\sqrt{N h^d}} + h^2\\right) = 0 \\quad \\Rightarrow \\quad h^* = O(N^{-1/(d+4)})\n1309: $$",
      "metadata": {},
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1255: :::\n1256: \n1257: :::{prf:proof}\n1258: \n1259: **Step 1: Decompose error into bias and variance.**\n1260: \n1261: $$\n1262: \\mathbb{E}[|\\hat{S}_N - S|] \\leq \\underbrace{|\\mathbb{E}[\\hat{S}_N] - S|}_{\\text{bias}} + \\underbrace{\\sqrt{\\text{Var}(\\hat{S}_N)}}_{\\text{stdev}}\n1263: $$\n1264: \n1265: **Step 2: Bound the bias.**\n1266: \n1267: The bias of kernel density estimation is well-known (Silverman, 1986):\n1268: \n1269: $$\n1270: |\\mathbb{E}[\\hat{\\rho}_{\\text{QSD}}(x, v)] - \\rho_{\\text{QSD}}(x, v)| = O(h^2)\n1271: $$\n1272: \n1273: under smoothness assumptions on $\\rho_{\\text{QSD}}$. Propagating through the logarithm:\n1274: \n1275: $$\n1276: |\\mathbb{E}[\\log \\hat{\\rho}_{\\text{QSD}}(x, v)] - \\log \\rho_{\\text{QSD}}(x, v)| = O(h^2)\n1277: $$\n1278: \n1279: Therefore, the bias of $\\hat{S}_N$ is $O(h^2)$.\n1280: \n1281: **Step 3: Bound the variance.**\n1282: \n1283: The variance of the log-density estimate is:\n1284: \n1285: $$\n1286: \\text{Var}(\\log \\hat{\\rho}_{\\text{QSD}}(x, v)) \\leq \\frac{C}{N h^d \\rho_{\\text{QSD}}(x, v)}\n1287: $$\n1288: \n1289: Integrating over the QSD and using Jensen's inequality:\n1290: \n1291: $$\n1292: \\text{Var}(\\hat{S}_N) = O\\left(\\frac{1}{N h^d}\\right)\n1293: $$\n1294: \n1295: **Step 4: Combine via Chebyshev's inequality.**\n1296: \n1297: $$\n1298: \\mathbb{P}[|\\hat{S}_N - \\mathbb{E}[\\hat{S}_N]| > t] \\leq \\frac{\\text{Var}(\\hat{S}_N)}{t^2} = O\\left(\\frac{1}{N h^d t^2}\\right)\n1299: $$\n1300: \n1301: Setting $t = \\epsilon$ and accounting for the bias gives the stated concentration bound.\n1302: \n1303: **Step 5: Optimize bandwidth.**\n1304: \n1305: The total error is $O(1/\\sqrt{N h^d} + h^2)$. Setting the derivative to zero:\n1306: \n1307: $$\n1308: \\frac{d}{dh}\\left(\\frac{1}{\\sqrt{N h^d}} + h^2\\right) = 0 \\quad \\Rightarrow \\quad h^* = O(N^{-1/(d+4)})\n1309: $$\n1310: "
    },
    {
      "directive_type": "remark",
      "label": "unlabeled-remark-124",
      "title": "Curse of Dimensionality",
      "start_line": 1312,
      "end_line": 1323,
      "header_lines": [
        1313
      ],
      "content_start": 1315,
      "content_end": 1322,
      "content": "1315: :class: warning\n1316: \n1317: The optimal error rate $O(N^{-2/(d+4)})$ degrades rapidly with dimension $d$. For the Fragile Gas in $(x, v)$ space with $d = 3$ spatial dimensions, the effective dimension is $2d = 6$, giving error rate $N^{-1/5}$.\n1318: \n1319: **Mitigation strategies:**\n1320: 1. Use importance reweighting (Chapter 19) to reduce effective dimension\n1321: 2. Exploit separability of position and velocity in the QSD\n1322: 3. Use adaptive bandwidth methods (Silverman, 1986)",
      "metadata": {
        "class": "warning"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1312: :::\n1313: \n1314: :::{prf:remark} Curse of Dimensionality\n1315: :class: warning\n1316: \n1317: The optimal error rate $O(N^{-2/(d+4)})$ degrades rapidly with dimension $d$. For the Fragile Gas in $(x, v)$ space with $d = 3$ spatial dimensions, the effective dimension is $2d = 6$, giving error rate $N^{-1/5}$.\n1318: \n1319: **Mitigation strategies:**\n1320: 1. Use importance reweighting (Chapter 19) to reduce effective dimension\n1321: 2. Exploit separability of position and velocity in the QSD\n1322: 3. Use adaptive bandwidth methods (Silverman, 1986)\n1323: "
    },
    {
      "directive_type": "definition",
      "label": "def-empirical-fisher-information",
      "title": "Empirical Fisher Information Matrix",
      "start_line": 1329,
      "end_line": 1351,
      "header_lines": [
        1330
      ],
      "content_start": 1332,
      "content_end": 1350,
      "content": "1332: :label: def-empirical-fisher-information\n1333: \n1334: For parameter space $\\theta = (\\theta^1, \\ldots, \\theta^p)$ and samples $\\{(x_i, v_i)\\}_{i=1}^N$ from $\\rho_{\\text{QSD}}(\\theta)$, the **empirical Fisher information matrix** is:\n1335: \n1336: $$\n1337: \\hat{I}^{ij}_N(\\theta) = \\frac{1}{N} \\sum_{k=1}^N \\frac{\\partial \\log \\hat{\\rho}_{\\text{QSD}}(x_k, v_k; \\theta)}{\\partial \\theta^i} \\frac{\\partial \\log \\hat{\\rho}_{\\text{QSD}}(x_k, v_k; \\theta)}{\\partial \\theta^j}\n1338: $$\n1339: \n1340: where $\\hat{\\rho}_{\\text{QSD}}(\\cdot; \\theta)$ is the kernel density estimate at parameter value $\\theta$.\n1341: \n1342: **Practical Implementation:** The score function $\\partial \\log \\rho_{\\text{QSD}} / \\partial \\theta^i$ can be estimated via:\n1343: \n1344: 1. **Finite Differences:** Run the algorithm at $\\theta + \\epsilon e_i$ and $\\theta - \\epsilon e_i$, estimate densities $\\hat{\\rho}_+$ and $\\hat{\\rho}_-$, compute:\n1345: \n1346: $$\n1347: \\frac{\\partial \\log \\hat{\\rho}_{\\text{QSD}}}{\\partial \\theta^i} \\approx \\frac{\\log \\hat{\\rho}_+ - \\log \\hat{\\rho}_-}{2\\epsilon}\n1348: $$\n1349: \n1350: 2. **Parametric Bootstrap:** Use the QSD formula {prf:ref}`thm-qsd-canonical-ensemble` to derive analytical expressions for $\\partial \\log \\rho_{\\text{QSD}} / \\partial \\theta^i$",
      "metadata": {
        "label": "def-empirical-fisher-information"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1329: Having estimated the entropy, we now estimate the Fisher information metric, which equals the Ruppeiner metric (Theorem {prf:ref}`thm-ruppeiner-fisher-connection`).\n1330: \n1331: :::{prf:definition} Empirical Fisher Information Matrix\n1332: :label: def-empirical-fisher-information\n1333: \n1334: For parameter space $\\theta = (\\theta^1, \\ldots, \\theta^p)$ and samples $\\{(x_i, v_i)\\}_{i=1}^N$ from $\\rho_{\\text{QSD}}(\\theta)$, the **empirical Fisher information matrix** is:\n1335: \n1336: $$\n1337: \\hat{I}^{ij}_N(\\theta) = \\frac{1}{N} \\sum_{k=1}^N \\frac{\\partial \\log \\hat{\\rho}_{\\text{QSD}}(x_k, v_k; \\theta)}{\\partial \\theta^i} \\frac{\\partial \\log \\hat{\\rho}_{\\text{QSD}}(x_k, v_k; \\theta)}{\\partial \\theta^j}\n1338: $$\n1339: \n1340: where $\\hat{\\rho}_{\\text{QSD}}(\\cdot; \\theta)$ is the kernel density estimate at parameter value $\\theta$.\n1341: \n1342: **Practical Implementation:** The score function $\\partial \\log \\rho_{\\text{QSD}} / \\partial \\theta^i$ can be estimated via:\n1343: \n1344: 1. **Finite Differences:** Run the algorithm at $\\theta + \\epsilon e_i$ and $\\theta - \\epsilon e_i$, estimate densities $\\hat{\\rho}_+$ and $\\hat{\\rho}_-$, compute:\n1345: \n1346: $$\n1347: \\frac{\\partial \\log \\hat{\\rho}_{\\text{QSD}}}{\\partial \\theta^i} \\approx \\frac{\\log \\hat{\\rho}_+ - \\log \\hat{\\rho}_-}{2\\epsilon}\n1348: $$\n1349: \n1350: 2. **Parametric Bootstrap:** Use the QSD formula {prf:ref}`thm-qsd-canonical-ensemble` to derive analytical expressions for $\\partial \\log \\rho_{\\text{QSD}} / \\partial \\theta^i$\n1351: "
    },
    {
      "directive_type": "algorithm",
      "label": "alg-fisher-matrix-estimation",
      "title": "Fisher Information Matrix Estimation via Finite Differences",
      "start_line": 1353,
      "end_line": 1408,
      "header_lines": [
        1354
      ],
      "content_start": 1356,
      "content_end": 1407,
      "content": "1356: :label: alg-fisher-matrix-estimation\n1357: \n1358: **Input:**\n1359: - Current parameter value $\\theta_0 = (\\beta_0, V_0, N_0)$\n1360: - Perturbation size $\\epsilon > 0$ (typically $\\epsilon = 0.01 \\cdot |\\theta_0|$)\n1361: - Number of samples $N$ per run\n1362: - Bandwidth parameters $(h, h_v)$\n1363: \n1364: **Output:**\n1365: - Fisher information matrix $\\hat{I}^{ij}(\\theta_0) \\in \\mathbb{R}^{p \\times p}$\n1366: \n1367: **Procedure:**\n1368: \n1369: 1. **Baseline Run:**\n1370:    - Run Fragile Gas with parameters $\\theta_0$ until QSD convergence\n1371:    - Collect samples $\\{(x_k^{(0)}, v_k^{(0)})\\}_{k=1}^N$\n1372:    - Estimate density $\\hat{\\rho}_0 = \\text{KDE}(\\{(x_k^{(0)}, v_k^{(0)})\\}, h, h_v)$\n1373: \n1374: 2. **Perturbed Runs (for each parameter $i = 1, \\ldots, p$):**\n1375:    - **Forward:** Run at $\\theta_+ = \\theta_0 + \\epsilon e_i$\n1376:      - Collect samples $\\{(x_k^{(i,+)}, v_k^{(i,+)})\\}_{k=1}^N$\n1377:      - Estimate density $\\hat{\\rho}_{i,+}$\n1378:    - **Backward:** Run at $\\theta_- = \\theta_0 - \\epsilon e_i$\n1379:      - Collect samples $\\{(x_k^{(i,-)}, v_k^{(i,-)})\\}_{k=1}^N$\n1380:      - Estimate density $\\hat{\\rho}_{i,-}$\n1381: \n1382: 3. **Compute Score Functions:**\n1383:    For each sample $(x_k^{(0)}, v_k^{(0)})$ from the baseline run:\n1384: \n1385: $$\n1386: s_i^{(k)} = \\frac{\\log \\hat{\\rho}_{i,+}(x_k^{(0)}, v_k^{(0)}) - \\log \\hat{\\rho}_{i,-}(x_k^{(0)}, v_k^{(0)})}{2\\epsilon}\n1387: $$\n1388: \n1389: 4. **Construct Fisher Matrix:**\n1390: \n1391: $$\n1392: \\hat{I}^{ij}(\\theta_0) = \\frac{1}{N} \\sum_{k=1}^N s_i^{(k)} s_j^{(k)}\n1393: $$\n1394: \n1395: 5. **Symmetrize:**\n1396: \n1397: $$\n1398: \\hat{I}^{ij} \\leftarrow \\frac{1}{2}(\\hat{I}^{ij} + \\hat{I}^{ji})\n1399: $$\n1400: \n1401: **Computational Cost:** $O(p \\cdot N)$ Fragile Gas runs, $O(p \\cdot N^2 h^{-d})$ for KDE evaluation.\n1402: \n1403: **Error Bound:** From Theorems {prf:ref}`thm-entropy-estimator-error` and standard finite difference error analysis:\n1404: \n1405: $$\n1406: \\mathbb{E}[\\|\\hat{I}(\\theta_0) - I(\\theta_0)\\|_F] = O\\left(\\epsilon^2 + \\frac{1}{\\sqrt{N h^d}} + h^2\\right)\n1407: $$",
      "metadata": {
        "label": "alg-fisher-matrix-estimation"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1353: :::\n1354: \n1355: :::{prf:algorithm} Fisher Information Matrix Estimation via Finite Differences\n1356: :label: alg-fisher-matrix-estimation\n1357: \n1358: **Input:**\n1359: - Current parameter value $\\theta_0 = (\\beta_0, V_0, N_0)$\n1360: - Perturbation size $\\epsilon > 0$ (typically $\\epsilon = 0.01 \\cdot |\\theta_0|$)\n1361: - Number of samples $N$ per run\n1362: - Bandwidth parameters $(h, h_v)$\n1363: \n1364: **Output:**\n1365: - Fisher information matrix $\\hat{I}^{ij}(\\theta_0) \\in \\mathbb{R}^{p \\times p}$\n1366: \n1367: **Procedure:**\n1368: \n1369: 1. **Baseline Run:**\n1370:    - Run Fragile Gas with parameters $\\theta_0$ until QSD convergence\n1371:    - Collect samples $\\{(x_k^{(0)}, v_k^{(0)})\\}_{k=1}^N$\n1372:    - Estimate density $\\hat{\\rho}_0 = \\text{KDE}(\\{(x_k^{(0)}, v_k^{(0)})\\}, h, h_v)$\n1373: \n1374: 2. **Perturbed Runs (for each parameter $i = 1, \\ldots, p$):**\n1375:    - **Forward:** Run at $\\theta_+ = \\theta_0 + \\epsilon e_i$\n1376:      - Collect samples $\\{(x_k^{(i,+)}, v_k^{(i,+)})\\}_{k=1}^N$\n1377:      - Estimate density $\\hat{\\rho}_{i,+}$\n1378:    - **Backward:** Run at $\\theta_- = \\theta_0 - \\epsilon e_i$\n1379:      - Collect samples $\\{(x_k^{(i,-)}, v_k^{(i,-)})\\}_{k=1}^N$\n1380:      - Estimate density $\\hat{\\rho}_{i,-}$\n1381: \n1382: 3. **Compute Score Functions:**\n1383:    For each sample $(x_k^{(0)}, v_k^{(0)})$ from the baseline run:\n1384: \n1385: $$\n1386: s_i^{(k)} = \\frac{\\log \\hat{\\rho}_{i,+}(x_k^{(0)}, v_k^{(0)}) - \\log \\hat{\\rho}_{i,-}(x_k^{(0)}, v_k^{(0)})}{2\\epsilon}\n1387: $$\n1388: \n1389: 4. **Construct Fisher Matrix:**\n1390: \n1391: $$\n1392: \\hat{I}^{ij}(\\theta_0) = \\frac{1}{N} \\sum_{k=1}^N s_i^{(k)} s_j^{(k)}\n1393: $$\n1394: \n1395: 5. **Symmetrize:**\n1396: \n1397: $$\n1398: \\hat{I}^{ij} \\leftarrow \\frac{1}{2}(\\hat{I}^{ij} + \\hat{I}^{ji})\n1399: $$\n1400: \n1401: **Computational Cost:** $O(p \\cdot N)$ Fragile Gas runs, $O(p \\cdot N^2 h^{-d})$ for KDE evaluation.\n1402: \n1403: **Error Bound:** From Theorems {prf:ref}`thm-entropy-estimator-error` and standard finite difference error analysis:\n1404: \n1405: $$\n1406: \\mathbb{E}[\\|\\hat{I}(\\theta_0) - I(\\theta_0)\\|_F] = O\\left(\\epsilon^2 + \\frac{1}{\\sqrt{N h^d}} + h^2\\right)\n1407: $$\n1408: "
    },
    {
      "directive_type": "remark",
      "label": "unlabeled-remark-222",
      "title": "Practical Guidance on Finite Difference Step Size",
      "start_line": 1410,
      "end_line": 1437,
      "header_lines": [
        1411
      ],
      "content_start": 1413,
      "content_end": 1436,
      "content": "1413: :class: tip\n1414: \n1415: **Critical Parameter:** The perturbation size $\\epsilon$ in Algorithm {prf:ref}`alg-fisher-matrix-estimation` is highly sensitive and must be chosen carefully.\n1416: \n1417: **The Trade-Off:**\n1418: - **Too large** ($\\epsilon \\gg \\sqrt{\\epsilon_{\\text{machine}}}$): Truncation error dominates, as $(\\partial f / \\partial\\theta) \\approx (f(\\theta + \\epsilon) - f(\\theta - \\epsilon))/(2\\epsilon)$ becomes inaccurate.\n1419: - **Too small** ($\\epsilon \\ll \\sqrt{\\epsilon_{\\text{machine}}}$): Catastrophic cancellation\u2014subtracting two nearly equal floating-point numbers amplifies round-off error.\n1420: \n1421: **Recommended Heuristic:**\n1422: \n1423: For double precision ($\\epsilon_{\\text{machine}} \\approx 2.2 \\times 10^{-16}$), use:\n1424: \n1425: $$\n1426: \\epsilon_i = \\delta \\cdot \\max\\{|\\theta_0^i|, \\theta_{\\text{scale}}\\}\n1427: $$\n1428: \n1429: where:\n1430: - $\\delta \\approx \\sqrt[3]{\\epsilon_{\\text{machine}}} \\approx 6 \\times 10^{-6}$ (cube root for a first derivative estimated via a second-order central difference scheme)\n1431: - $\\theta_{\\text{scale}}$ is a characteristic scale (e.g., $\\theta_{\\text{scale}} = 1$ for $\\beta \\sim O(1)$)\n1432: \n1433: **Adaptive Refinement:** If validation checks fail (Algorithm {prf:ref}`alg-ruppeiner-validation`), try:\n1434: 1. Halve $\\epsilon$ and recompute (if truncation error suspected)\n1435: 2. Double $\\epsilon$ and recompute (if round-off error suspected)\n1436: 3. Use higher-order finite difference schemes (4th-order requires 4 evaluations but reduces truncation error to $O(\\epsilon^4)$)",
      "metadata": {
        "class": "tip"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1410: :::\n1411: \n1412: :::{prf:remark} Practical Guidance on Finite Difference Step Size\n1413: :class: tip\n1414: \n1415: **Critical Parameter:** The perturbation size $\\epsilon$ in Algorithm {prf:ref}`alg-fisher-matrix-estimation` is highly sensitive and must be chosen carefully.\n1416: \n1417: **The Trade-Off:**\n1418: - **Too large** ($\\epsilon \\gg \\sqrt{\\epsilon_{\\text{machine}}}$): Truncation error dominates, as $(\\partial f / \\partial\\theta) \\approx (f(\\theta + \\epsilon) - f(\\theta - \\epsilon))/(2\\epsilon)$ becomes inaccurate.\n1419: - **Too small** ($\\epsilon \\ll \\sqrt{\\epsilon_{\\text{machine}}}$): Catastrophic cancellation\u2014subtracting two nearly equal floating-point numbers amplifies round-off error.\n1420: \n1421: **Recommended Heuristic:**\n1422: \n1423: For double precision ($\\epsilon_{\\text{machine}} \\approx 2.2 \\times 10^{-16}$), use:\n1424: \n1425: $$\n1426: \\epsilon_i = \\delta \\cdot \\max\\{|\\theta_0^i|, \\theta_{\\text{scale}}\\}\n1427: $$\n1428: \n1429: where:\n1430: - $\\delta \\approx \\sqrt[3]{\\epsilon_{\\text{machine}}} \\approx 6 \\times 10^{-6}$ (cube root for a first derivative estimated via a second-order central difference scheme)\n1431: - $\\theta_{\\text{scale}}$ is a characteristic scale (e.g., $\\theta_{\\text{scale}} = 1$ for $\\beta \\sim O(1)$)\n1432: \n1433: **Adaptive Refinement:** If validation checks fail (Algorithm {prf:ref}`alg-ruppeiner-validation`), try:\n1434: 1. Halve $\\epsilon$ and recompute (if truncation error suspected)\n1435: 2. Double $\\epsilon$ and recompute (if round-off error suspected)\n1436: 3. Use higher-order finite difference schemes (4th-order requires 4 evaluations but reduces truncation error to $O(\\epsilon^4)$)\n1437: "
    },
    {
      "directive_type": "remark",
      "label": "unlabeled-remark-251",
      "title": "Variance Reduction via Importance Reweighting",
      "start_line": 1439,
      "end_line": 1458,
      "header_lines": [
        1440
      ],
      "content_start": 1442,
      "content_end": 1457,
      "content": "1442: :class: tip\n1443: \n1444: The algorithm can be significantly accelerated using the importance reweighting framework from Chapter 19:\n1445: \n1446: 1. **Single Baseline Run:** Run the Fragile Gas once at $\\theta_0$ with very high $\\beta$ (diversity)\n1447: 2. **Reweight Samples:** For each perturbed parameter $\\theta_i$, reweight the baseline samples using:\n1448: \n1449: $$\n1450: w_k(\\theta_i) = \\frac{\\rho_{\\text{QSD}}(x_k, v_k; \\theta_i)}{\\rho_{\\text{QSD}}(x_k, v_k; \\theta_0)} = \\exp\\left(\\frac{H_{\\text{eff}}(x_k, v_k; \\theta_0) - H_{\\text{eff}}(x_k, v_k; \\theta_i)}{k_B T}\\right)\n1451: $$\n1452: \n1453: 3. **Weighted KDE:** Use weighted kernel density estimation:\n1454: \n1455: $$\n1456: \\hat{\\rho}_{\\text{QSD}}(x, v; \\theta_i) = \\frac{\\sum_{k=1}^N w_k(\\theta_i) K_h(x - x_k) K_{h_v}(v - v_k)}{\\sum_{k=1}^N w_k(\\theta_i)}\n1457: $$",
      "metadata": {
        "class": "tip"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1439: :::\n1440: \n1441: :::{prf:remark} Variance Reduction via Importance Reweighting\n1442: :class: tip\n1443: \n1444: The algorithm can be significantly accelerated using the importance reweighting framework from Chapter 19:\n1445: \n1446: 1. **Single Baseline Run:** Run the Fragile Gas once at $\\theta_0$ with very high $\\beta$ (diversity)\n1447: 2. **Reweight Samples:** For each perturbed parameter $\\theta_i$, reweight the baseline samples using:\n1448: \n1449: $$\n1450: w_k(\\theta_i) = \\frac{\\rho_{\\text{QSD}}(x_k, v_k; \\theta_i)}{\\rho_{\\text{QSD}}(x_k, v_k; \\theta_0)} = \\exp\\left(\\frac{H_{\\text{eff}}(x_k, v_k; \\theta_0) - H_{\\text{eff}}(x_k, v_k; \\theta_i)}{k_B T}\\right)\n1451: $$\n1452: \n1453: 3. **Weighted KDE:** Use weighted kernel density estimation:\n1454: \n1455: $$\n1456: \\hat{\\rho}_{\\text{QSD}}(x, v; \\theta_i) = \\frac{\\sum_{k=1}^N w_k(\\theta_i) K_h(x - x_k) K_{h_v}(v - v_k)}{\\sum_{k=1}^N w_k(\\theta_i)}\n1457: $$\n1458: "
    },
    {
      "directive_type": "proposition",
      "label": "prop-coordinate-transformation-ruppeiner",
      "title": "Coordinate Transformation Formula",
      "start_line": 1464,
      "end_line": 1510,
      "header_lines": [
        1465
      ],
      "content_start": 1467,
      "content_end": 1509,
      "content": "1467: :label: prop-coordinate-transformation-ruppeiner\n1468: \n1469: Let $\\hat{I}^{ij}(\\theta)$ be the Fisher information matrix in parameter space $\\theta = (\\beta, V, N)$, and let $g_R^{ab}(U, V, N)$ be the Ruppeiner metric in thermodynamic space. Then:\n1470: \n1471: $$\n1472: g_R^{ab}(U, V, N) = \\left(\\frac{\\partial \\theta^i}{\\partial U^a}\\right) \\hat{I}^{ij}(\\theta) \\left(\\frac{\\partial \\theta^j}{\\partial U^b}\\right)\n1473: $$\n1474: \n1475: where the Jacobian matrix $J^i_a = \\partial \\theta^i / \\partial U^a$ is computed via thermodynamic relations.\n1476: \n1477: **Explicit Formulas:** For the standard case $(U^1, U^2, U^3) = (U, V, N)$ and $(\\theta^1, \\theta^2, \\theta^3) = (\\beta, V, N)$:\n1478: \n1479: 1. **Energy-Temperature Relation:**\n1480: \n1481: $$\n1482: \\frac{\\partial \\beta}{\\partial U} = -\\frac{\\beta^2}{C_V}\n1483: $$\n1484: \n1485: where $C_V = T (\\partial S / \\partial T)_V$ is the heat capacity.\n1486: \n1487: 2. **Volume and Particle Number:** Trivial since $V$ and $N$ are unchanged:\n1488: \n1489: $$\n1490: \\frac{\\partial V}{\\partial V} = 1, \\quad \\frac{\\partial N}{\\partial N} = 1\n1491: $$\n1492: \n1493: 3. **Jacobian Matrix:**\n1494: \n1495: $$\n1496: J = \\begin{pmatrix}\n1497: -\\beta^2 / C_V & 0 & 0 \\\\\n1498: 0 & 1 & 0 \\\\\n1499: 0 & 0 & 1\n1500: \\end{pmatrix}\n1501: $$\n1502: \n1503: 4. **Ruppeiner Metric:**\n1504: \n1505: $$\n1506: g_R = J^T \\hat{I} J = \\begin{pmatrix}\n1507: \\frac{\\beta^4}{C_V^2} \\hat{I}^{\\beta\\beta} & \\frac{-\\beta^2}{C_V} \\hat{I}^{\\beta V} & \\frac{-\\beta^2}{C_V} \\hat{I}^{\\beta N} \\\\\n1508: \\frac{-\\beta^2}{C_V} \\hat{I}^{V\\beta} & \\hat{I}^{VV} & \\hat{I}^{VN} \\\\\n1509: \\frac{-\\beta^2}{C_V} \\hat{I}^{N\\beta} & \\hat{I}^{NV} & \\hat{I}^{NN}",
      "metadata": {
        "label": "prop-coordinate-transformation-ruppeiner"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1464: The Fisher information matrix computed in \u00a74.3 is in the **parameter space** $\\theta = (\\beta, V, N)$. To obtain the Ruppeiner metric, we must transform to **thermodynamic coordinates** $(U, V, N)$.\n1465: \n1466: :::{prf:proposition} Coordinate Transformation Formula\n1467: :label: prop-coordinate-transformation-ruppeiner\n1468: \n1469: Let $\\hat{I}^{ij}(\\theta)$ be the Fisher information matrix in parameter space $\\theta = (\\beta, V, N)$, and let $g_R^{ab}(U, V, N)$ be the Ruppeiner metric in thermodynamic space. Then:\n1470: \n1471: $$\n1472: g_R^{ab}(U, V, N) = \\left(\\frac{\\partial \\theta^i}{\\partial U^a}\\right) \\hat{I}^{ij}(\\theta) \\left(\\frac{\\partial \\theta^j}{\\partial U^b}\\right)\n1473: $$\n1474: \n1475: where the Jacobian matrix $J^i_a = \\partial \\theta^i / \\partial U^a$ is computed via thermodynamic relations.\n1476: \n1477: **Explicit Formulas:** For the standard case $(U^1, U^2, U^3) = (U, V, N)$ and $(\\theta^1, \\theta^2, \\theta^3) = (\\beta, V, N)$:\n1478: \n1479: 1. **Energy-Temperature Relation:**\n1480: \n1481: $$\n1482: \\frac{\\partial \\beta}{\\partial U} = -\\frac{\\beta^2}{C_V}\n1483: $$\n1484: \n1485: where $C_V = T (\\partial S / \\partial T)_V$ is the heat capacity.\n1486: \n1487: 2. **Volume and Particle Number:** Trivial since $V$ and $N$ are unchanged:\n1488: \n1489: $$\n1490: \\frac{\\partial V}{\\partial V} = 1, \\quad \\frac{\\partial N}{\\partial N} = 1\n1491: $$\n1492: \n1493: 3. **Jacobian Matrix:**\n1494: \n1495: $$\n1496: J = \\begin{pmatrix}\n1497: -\\beta^2 / C_V & 0 & 0 \\\\\n1498: 0 & 1 & 0 \\\\\n1499: 0 & 0 & 1\n1500: \\end{pmatrix}\n1501: $$\n1502: \n1503: 4. **Ruppeiner Metric:**\n1504: \n1505: $$\n1506: g_R = J^T \\hat{I} J = \\begin{pmatrix}\n1507: \\frac{\\beta^4}{C_V^2} \\hat{I}^{\\beta\\beta} & \\frac{-\\beta^2}{C_V} \\hat{I}^{\\beta V} & \\frac{-\\beta^2}{C_V} \\hat{I}^{\\beta N} \\\\\n1508: \\frac{-\\beta^2}{C_V} \\hat{I}^{V\\beta} & \\hat{I}^{VV} & \\hat{I}^{VN} \\\\\n1509: \\frac{-\\beta^2}{C_V} \\hat{I}^{N\\beta} & \\hat{I}^{NV} & \\hat{I}^{NN}\n1510: \\end{pmatrix}"
    },
    {
      "directive_type": "proof",
      "label": "unlabeled-proof-324",
      "title": null,
      "start_line": 1512,
      "end_line": 1527,
      "header_lines": [],
      "content_start": 1514,
      "content_end": 1526,
      "content": "1514: :::{prf:proof}\n1515: \n1516: This is a standard application of the tensor transformation law for metric tensors under coordinate change. Given a metric $\\hat{I}^{ij}$ in coordinates $\\theta^i$ and a coordinate transformation $U^a = U^a(\\theta)$, the pullback metric in coordinates $U^a$ is:\n1517: \n1518: $$\n1519: g_R^{ab} = \\frac{\\partial \\theta^i}{\\partial U^a} \\frac{\\partial \\theta^j}{\\partial U^b} \\hat{I}^{ij}\n1520: $$\n1521: \n1522: The Jacobian $\\partial \\beta / \\partial U$ follows from:\n1523: \n1524: $$\n1525: \\frac{\\partial \\beta}{\\partial U} = \\frac{\\partial \\beta}{\\partial T} \\frac{\\partial T}{\\partial S} \\frac{\\partial S}{\\partial U} = \\left(-\\frac{1}{k_B T^2}\\right) \\left(\\frac{C_V}{T}\\right)^{-1} \\left(\\frac{1}{T}\\right) = -\\frac{\\beta^2}{C_V}\n1526: $$",
      "metadata": {},
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1512: :::\n1513: \n1514: :::{prf:proof}\n1515: \n1516: This is a standard application of the tensor transformation law for metric tensors under coordinate change. Given a metric $\\hat{I}^{ij}$ in coordinates $\\theta^i$ and a coordinate transformation $U^a = U^a(\\theta)$, the pullback metric in coordinates $U^a$ is:\n1517: \n1518: $$\n1519: g_R^{ab} = \\frac{\\partial \\theta^i}{\\partial U^a} \\frac{\\partial \\theta^j}{\\partial U^b} \\hat{I}^{ij}\n1520: $$\n1521: \n1522: The Jacobian $\\partial \\beta / \\partial U$ follows from:\n1523: \n1524: $$\n1525: \\frac{\\partial \\beta}{\\partial U} = \\frac{\\partial \\beta}{\\partial T} \\frac{\\partial T}{\\partial S} \\frac{\\partial S}{\\partial U} = \\left(-\\frac{1}{k_B T^2}\\right) \\left(\\frac{C_V}{T}\\right)^{-1} \\left(\\frac{1}{T}\\right) = -\\frac{\\beta^2}{C_V}\n1526: $$\n1527: "
    },
    {
      "directive_type": "algorithm",
      "label": "alg-ruppeiner-metric-construction",
      "title": "Complete Ruppeiner Metric Construction",
      "start_line": 1529,
      "end_line": 1629,
      "header_lines": [
        1530
      ],
      "content_start": 1532,
      "content_end": 1628,
      "content": "1532: :label: alg-ruppeiner-metric-construction\n1533: \n1534: **Input:**\n1535: - Fragile Gas parameters $(\\alpha, \\beta_{\\text{alg}}, \\gamma, \\sigma_v, \\epsilon_\\Sigma)$\n1536: - Number of samples $N$\n1537: - Perturbation size $\\epsilon$\n1538: - Bandwidth $(h, h_v)$\n1539: \n1540: **Output:**\n1541: - Ruppeiner metric $g_R^{ab}(U, V, N) \\in \\mathbb{R}^{3 \\times 3}$\n1542: - Scalar curvature $R_{\\text{Rupp}}$ (optional)\n1543: - Error estimates and diagnostics\n1544: \n1545: **Procedure:**\n1546: \n1547: **Phase 1: Sample Collection**\n1548: \n1549: 1. Run Fragile Gas with given parameters until QSD convergence (use LSI convergence bound from Chapter 10)\n1550: 2. Collect $N$ samples $\\{(x_k, v_k)\\}_{k=1}^N$ from the QSD\n1551: \n1552: **Phase 2: Thermodynamic Quantities**\n1553: \n1554: 3. Compute internal energy:\n1555: \n1556: $$\n1557: \\hat{U} = \\frac{1}{N} \\sum_{k=1}^N H_{\\text{eff}}(x_k, v_k)\n1558: $$\n1559: \n1560: 4. Compute heat capacity via variance:\n1561: \n1562: $$\n1563: \\hat{C}_V = \\frac{1}{k_B T^2} \\text{Var}(H_{\\text{eff}}) = \\frac{1}{k_B T^2} \\left(\\frac{1}{N}\\sum_k H_{\\text{eff}}(x_k, v_k)^2 - \\hat{U}^2\\right)\n1564: $$\n1565: \n1566: 5. Compute inverse temperature:\n1567: \n1568: $$\n1569: \\hat{\\beta} = \\frac{\\gamma}{\\sigma_v^2}\n1570: $$\n1571: \n1572: **Phase 3: Fisher Information Matrix**\n1573: \n1574: 6. For each parameter direction $i \\in \\{\\beta, V, N\\}$:\n1575:    - Run perturbed simulations at $\\theta_{\\pm} = \\theta_0 \\pm \\epsilon e_i$ (or use importance reweighting)\n1576:    - Estimate densities $\\hat{\\rho}_{\\pm}$ via KDE\n1577:    - Compute score functions $s_i^{(k)} = (\\log \\hat{\\rho}_+ - \\log \\hat{\\rho}_-) / (2\\epsilon)$ at baseline samples\n1578: \n1579: 7. Construct Fisher matrix:\n1580: \n1581: $$\n1582: \\hat{I}^{ij} = \\frac{1}{N} \\sum_{k=1}^N s_i^{(k)} s_j^{(k)}\n1583: $$\n1584: \n1585: 8. Symmetrize: $\\hat{I}^{ij} \\leftarrow (\\hat{I}^{ij} + \\hat{I}^{ji})/2$\n1586: \n1587: **Phase 4: Coordinate Transformation**\n1588: \n1589: 9. Construct Jacobian:\n1590: \n1591: $$\n1592: J = \\begin{pmatrix}\n1593: -\\hat{\\beta}^2 / \\hat{C}_V & 0 & 0 \\\\\n1594: 0 & 1 & 0 \\\\\n1595: 0 & 0 & 1\n1596: \\end{pmatrix}\n1597: $$\n1598: \n1599: 10. Transform to Ruppeiner metric:\n1600: \n1601: $$\n1602: g_R = J^T \\hat{I} J\n1603: $$\n1604: \n1605: **Phase 5: Validation**\n1606: \n1607: 11. Check positive definiteness: Compute eigenvalues of $g_R$, verify all $\\lambda_i > 0$\n1608: 12. Check Maxwell relations (if multiple thermodynamic derivatives available)\n1609: 13. Compute Effective Sample Size (Chapter 19): $\\text{ESS} = N / (1 + \\text{CV}^2(w))$ to assess reliability\n1610: \n1611: **Phase 6: Curvature (Optional)**\n1612: \n1613: 14. If curvature is desired, use Regge calculus (Chapter 14) or closed-form formulas (\u00a76)\n1614: \n1615: **Error Estimate:**\n1616: \n1617: $$\n1618: \\|\\hat{g}_R - g_R\\|_F = O\\left(\\epsilon^2 + \\frac{1}{\\sqrt{N h^d}} + h^2\\right)\n1619: $$\n1620: \n1621: **Computational Complexity:**\n1622: - KDE: $O(N^2 h^{-d})$ per density evaluation\n1623: - Finite differences: $O(p)$ runs of the Fragile Gas\n1624: - Total: $O(p \\cdot N^2 h^{-d})$ where $p = 3$ is the parameter dimension\n1625: \n1626: **Dimension-Dependent Scaling:** For $d$-dimensional state space, the optimal bandwidth is $h^* = O(N^{-1/(d+4)})$, giving:\n1627: \n1628: $$",
      "metadata": {
        "label": "alg-ruppeiner-metric-construction"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1529: :::\n1530: \n1531: :::{prf:algorithm} Complete Ruppeiner Metric Construction\n1532: :label: alg-ruppeiner-metric-construction\n1533: \n1534: **Input:**\n1535: - Fragile Gas parameters $(\\alpha, \\beta_{\\text{alg}}, \\gamma, \\sigma_v, \\epsilon_\\Sigma)$\n1536: - Number of samples $N$\n1537: - Perturbation size $\\epsilon$\n1538: - Bandwidth $(h, h_v)$\n1539: \n1540: **Output:**\n1541: - Ruppeiner metric $g_R^{ab}(U, V, N) \\in \\mathbb{R}^{3 \\times 3}$\n1542: - Scalar curvature $R_{\\text{Rupp}}$ (optional)\n1543: - Error estimates and diagnostics\n1544: \n1545: **Procedure:**\n1546: \n1547: **Phase 1: Sample Collection**\n1548: \n1549: 1. Run Fragile Gas with given parameters until QSD convergence (use LSI convergence bound from Chapter 10)\n1550: 2. Collect $N$ samples $\\{(x_k, v_k)\\}_{k=1}^N$ from the QSD\n1551: \n1552: **Phase 2: Thermodynamic Quantities**\n1553: \n1554: 3. Compute internal energy:\n1555: \n1556: $$\n1557: \\hat{U} = \\frac{1}{N} \\sum_{k=1}^N H_{\\text{eff}}(x_k, v_k)\n1558: $$\n1559: \n1560: 4. Compute heat capacity via variance:\n1561: \n1562: $$\n1563: \\hat{C}_V = \\frac{1}{k_B T^2} \\text{Var}(H_{\\text{eff}}) = \\frac{1}{k_B T^2} \\left(\\frac{1}{N}\\sum_k H_{\\text{eff}}(x_k, v_k)^2 - \\hat{U}^2\\right)\n1564: $$\n1565: \n1566: 5. Compute inverse temperature:\n1567: \n1568: $$\n1569: \\hat{\\beta} = \\frac{\\gamma}{\\sigma_v^2}\n1570: $$\n1571: \n1572: **Phase 3: Fisher Information Matrix**\n1573: \n1574: 6. For each parameter direction $i \\in \\{\\beta, V, N\\}$:\n1575:    - Run perturbed simulations at $\\theta_{\\pm} = \\theta_0 \\pm \\epsilon e_i$ (or use importance reweighting)\n1576:    - Estimate densities $\\hat{\\rho}_{\\pm}$ via KDE\n1577:    - Compute score functions $s_i^{(k)} = (\\log \\hat{\\rho}_+ - \\log \\hat{\\rho}_-) / (2\\epsilon)$ at baseline samples\n1578: \n1579: 7. Construct Fisher matrix:\n1580: \n1581: $$\n1582: \\hat{I}^{ij} = \\frac{1}{N} \\sum_{k=1}^N s_i^{(k)} s_j^{(k)}\n1583: $$\n1584: \n1585: 8. Symmetrize: $\\hat{I}^{ij} \\leftarrow (\\hat{I}^{ij} + \\hat{I}^{ji})/2$\n1586: \n1587: **Phase 4: Coordinate Transformation**\n1588: \n1589: 9. Construct Jacobian:\n1590: \n1591: $$\n1592: J = \\begin{pmatrix}\n1593: -\\hat{\\beta}^2 / \\hat{C}_V & 0 & 0 \\\\\n1594: 0 & 1 & 0 \\\\\n1595: 0 & 0 & 1\n1596: \\end{pmatrix}\n1597: $$\n1598: \n1599: 10. Transform to Ruppeiner metric:\n1600: \n1601: $$\n1602: g_R = J^T \\hat{I} J\n1603: $$\n1604: \n1605: **Phase 5: Validation**\n1606: \n1607: 11. Check positive definiteness: Compute eigenvalues of $g_R$, verify all $\\lambda_i > 0$\n1608: 12. Check Maxwell relations (if multiple thermodynamic derivatives available)\n1609: 13. Compute Effective Sample Size (Chapter 19): $\\text{ESS} = N / (1 + \\text{CV}^2(w))$ to assess reliability\n1610: \n1611: **Phase 6: Curvature (Optional)**\n1612: \n1613: 14. If curvature is desired, use Regge calculus (Chapter 14) or closed-form formulas (\u00a76)\n1614: \n1615: **Error Estimate:**\n1616: \n1617: $$\n1618: \\|\\hat{g}_R - g_R\\|_F = O\\left(\\epsilon^2 + \\frac{1}{\\sqrt{N h^d}} + h^2\\right)\n1619: $$\n1620: \n1621: **Computational Complexity:**\n1622: - KDE: $O(N^2 h^{-d})$ per density evaluation\n1623: - Finite differences: $O(p)$ runs of the Fragile Gas\n1624: - Total: $O(p \\cdot N^2 h^{-d})$ where $p = 3$ is the parameter dimension\n1625: \n1626: **Dimension-Dependent Scaling:** For $d$-dimensional state space, the optimal bandwidth is $h^* = O(N^{-1/(d+4)})$, giving:\n1627: \n1628: $$\n1629: \\text{Total Cost} = O(N^{2 + 2/(d+4)}) \\quad \\text{for } d \\leq 3"
    },
    {
      "directive_type": "theorem",
      "label": "thm-ruppeiner-convergence",
      "title": "Convergence of Algorithmic Ruppeiner Metric",
      "start_line": 1635,
      "end_line": 1665,
      "header_lines": [
        1636
      ],
      "content_start": 1638,
      "content_end": 1664,
      "content": "1638: :label: thm-ruppeiner-convergence\n1639: \n1640: Let $g_R^{ab}(U, V, N)$ be the true Ruppeiner metric defined via $g_R = -\\partial^2 S / \\partial U^a \\partial U^b$, and let $\\hat{g}_R^{ab}$ be the empirical estimate from Algorithm {prf:ref}`alg-ruppeiner-metric-construction` with:\n1641: \n1642: - $N$ samples from the QSD\n1643: - Bandwidth $h = h_N = O(N^{-1/(d+4)})$\n1644: - Perturbation size $\\epsilon = O(N^{-1/3})$\n1645: \n1646: Then, under the regularity conditions:\n1647: 1. QSD is $C^3$ with bounded derivatives up to third order\n1648: 2. Fisher information matrix $I(\\theta)$ is $C^2$ in $\\theta$\n1649: 3. Heat capacity $C_V > C_{\\min} > 0$ (thermodynamic stability)\n1650: 4. Effective sample size $\\text{ESS} \\geq N/10$ (importance reweighting quality)\n1651: \n1652: The algorithmic estimate satisfies:\n1653: \n1654: $$\n1655: \\mathbb{E}[\\|\\hat{g}_R - g_R\\|_F] = O\\left(N^{-\\min\\{2/(d+4), 1/3\\}}\\right)\n1656: $$\n1657: \n1658: with high-probability bound:\n1659: \n1660: $$\n1661: \\mathbb{P}[\\|\\hat{g}_R - g_R\\|_F > \\epsilon] \\leq C \\exp(-N \\epsilon^2 / K)\n1662: $$\n1663: \n1664: for constants $C, K$ depending on the QSD regularity and the heat capacity lower bound $C_{\\min}$.",
      "metadata": {
        "label": "thm-ruppeiner-convergence"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1635: We now establish the main result: the algorithmic construction converges to the true Ruppeiner metric as $N \\to \\infty$.\n1636: \n1637: :::{prf:theorem} Convergence of Algorithmic Ruppeiner Metric\n1638: :label: thm-ruppeiner-convergence\n1639: \n1640: Let $g_R^{ab}(U, V, N)$ be the true Ruppeiner metric defined via $g_R = -\\partial^2 S / \\partial U^a \\partial U^b$, and let $\\hat{g}_R^{ab}$ be the empirical estimate from Algorithm {prf:ref}`alg-ruppeiner-metric-construction` with:\n1641: \n1642: - $N$ samples from the QSD\n1643: - Bandwidth $h = h_N = O(N^{-1/(d+4)})$\n1644: - Perturbation size $\\epsilon = O(N^{-1/3})$\n1645: \n1646: Then, under the regularity conditions:\n1647: 1. QSD is $C^3$ with bounded derivatives up to third order\n1648: 2. Fisher information matrix $I(\\theta)$ is $C^2$ in $\\theta$\n1649: 3. Heat capacity $C_V > C_{\\min} > 0$ (thermodynamic stability)\n1650: 4. Effective sample size $\\text{ESS} \\geq N/10$ (importance reweighting quality)\n1651: \n1652: The algorithmic estimate satisfies:\n1653: \n1654: $$\n1655: \\mathbb{E}[\\|\\hat{g}_R - g_R\\|_F] = O\\left(N^{-\\min\\{2/(d+4), 1/3\\}}\\right)\n1656: $$\n1657: \n1658: with high-probability bound:\n1659: \n1660: $$\n1661: \\mathbb{P}[\\|\\hat{g}_R - g_R\\|_F > \\epsilon] \\leq C \\exp(-N \\epsilon^2 / K)\n1662: $$\n1663: \n1664: for constants $C, K$ depending on the QSD regularity and the heat capacity lower bound $C_{\\min}$.\n1665: "
    },
    {
      "directive_type": "proof",
      "label": "unlabeled-proof-479",
      "title": null,
      "start_line": 1667,
      "end_line": 1741,
      "header_lines": [],
      "content_start": 1669,
      "content_end": 1740,
      "content": "1669: :::{prf:proof}\n1670: \n1671: **Step 1: Decompose the error.**\n1672: \n1673: $$\n1674: \\|\\hat{g}_R - g_R\\|_F \\leq \\underbrace{\\|J^T \\hat{I} J - J^T I J\\|_F}_{\\text{Fisher estimation error}} + \\underbrace{\\|J^T I J - g_R\\|_F}_{\\text{Coordinate transform error}}\n1675: $$\n1676: \n1677: where $J$ is the Jacobian matrix and $I$ is the true Fisher information matrix.\n1678: \n1679: **Step 2: Bound Fisher estimation error.**\n1680: \n1681: From Algorithm {prf:ref}`alg-fisher-matrix-estimation` and Theorem {prf:ref}`thm-entropy-estimator-error`:\n1682: \n1683: $$\n1684: \\|\\hat{I} - I\\|_F = O\\left(\\epsilon^2 + \\frac{1}{\\sqrt{N h^d}} + h^2\\right)\n1685: $$\n1686: \n1687: Using matrix multiplication bounds $\\|AB\\|_F \\leq \\|A\\|_F \\|B\\|_{\\text{op}}$:\n1688: \n1689: $$\n1690: \\|J^T \\hat{I} J - J^T I J\\|_F \\leq \\|J\\|_{\\text{op}}^2 \\|\\hat{I} - I\\|_F\n1691: $$\n1692: \n1693: The Jacobian norm is bounded by:\n1694: \n1695: $$\n1696: \\|J\\|_{\\text{op}} = \\max\\left\\{1, \\frac{\\beta^2}{C_V}\\right\\} \\leq \\frac{\\beta^2}{C_{\\min}}\n1697: $$\n1698: \n1699: using the thermodynamic stability assumption $C_V \\geq C_{\\min} > 0$.\n1700: \n1701: **Step 3: Bound coordinate transform error.**\n1702: \n1703: The identity $g_R = J^T I J$ is exact (this is the definition of the pullback metric), so the second term vanishes:\n1704: \n1705: $$\n1706: \\|J^T I J - g_R\\|_F = 0\n1707: $$\n1708: \n1709: **Step 4: Combine the bounds.**\n1710: \n1711: $$\n1712: \\mathbb{E}[\\|\\hat{g}_R - g_R\\|_F] \\leq \\frac{\\beta^4}{C_{\\min}^2} \\cdot O\\left(\\epsilon^2 + \\frac{1}{\\sqrt{N h^d}} + h^2\\right)\n1713: $$\n1714: \n1715: **Step 5: Optimize the bandwidth and perturbation.**\n1716: \n1717: Choosing $h = N^{-1/(d+4)}$ (standard for KDE) and $\\epsilon = N^{-1/3}$ (balancing finite difference error):\n1718: \n1719: - KDE error: $O(1/\\sqrt{N h^d} + h^2) = O(N^{-2/(d+4)})$\n1720: - Finite difference error: $O(\\epsilon^2) = O(N^{-2/3})$\n1721: \n1722: **Comparing the rates:** Since $d + 4 > 3$ for all $d \\geq 1$, we have $\\frac{2}{d+4} < \\frac{2}{3}$. This implies that for large $N$, the term $N^{-2/3}$ decays much faster than $N^{-2/(d+4)}$. Therefore, the **KDE error always dominates** the finite difference error, and the overall convergence rate is determined by the KDE bandwidth:\n1723: \n1724: $$\n1725: \\mathbb{E}[\\|\\hat{g}_R - g_R\\|_F] = O\\left(N^{-2/(d+4)}\\right) \\quad \\text{for all } d \\geq 1\n1726: $$\n1727: \n1728: **Step 6: High-probability bound.**\n1729: \n1730: Apply Bernstein's concentration inequality for the sum $\\sum_{k=1}^N s_i^{(k)} s_j^{(k)}$ in the Fisher matrix. Under sub-Gaussian tails (satisfied for compactly supported QSD):\n1731: \n1732: $$\n1733: \\mathbb{P}[|\\hat{I}^{ij} - I^{ij}| > t] \\leq 2\\exp\\left(-\\frac{N t^2}{2\\sigma^2 + 2Mt/3}\\right)\n1734: $$\n1735: \n1736: where $\\sigma^2 = \\text{Var}(s_i^{(k)} s_j^{(k)})$ and $M$ is the sub-Gaussian parameter. Setting $t = \\epsilon$ and propagating through the Jacobian transformation:\n1737: \n1738: $$\n1739: \\mathbb{P}[\\|\\hat{g}_R - g_R\\|_F > \\epsilon] \\leq C_1 \\exp(-C_2 N \\epsilon^2)\n1740: $$",
      "metadata": {},
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1667: :::\n1668: \n1669: :::{prf:proof}\n1670: \n1671: **Step 1: Decompose the error.**\n1672: \n1673: $$\n1674: \\|\\hat{g}_R - g_R\\|_F \\leq \\underbrace{\\|J^T \\hat{I} J - J^T I J\\|_F}_{\\text{Fisher estimation error}} + \\underbrace{\\|J^T I J - g_R\\|_F}_{\\text{Coordinate transform error}}\n1675: $$\n1676: \n1677: where $J$ is the Jacobian matrix and $I$ is the true Fisher information matrix.\n1678: \n1679: **Step 2: Bound Fisher estimation error.**\n1680: \n1681: From Algorithm {prf:ref}`alg-fisher-matrix-estimation` and Theorem {prf:ref}`thm-entropy-estimator-error`:\n1682: \n1683: $$\n1684: \\|\\hat{I} - I\\|_F = O\\left(\\epsilon^2 + \\frac{1}{\\sqrt{N h^d}} + h^2\\right)\n1685: $$\n1686: \n1687: Using matrix multiplication bounds $\\|AB\\|_F \\leq \\|A\\|_F \\|B\\|_{\\text{op}}$:\n1688: \n1689: $$\n1690: \\|J^T \\hat{I} J - J^T I J\\|_F \\leq \\|J\\|_{\\text{op}}^2 \\|\\hat{I} - I\\|_F\n1691: $$\n1692: \n1693: The Jacobian norm is bounded by:\n1694: \n1695: $$\n1696: \\|J\\|_{\\text{op}} = \\max\\left\\{1, \\frac{\\beta^2}{C_V}\\right\\} \\leq \\frac{\\beta^2}{C_{\\min}}\n1697: $$\n1698: \n1699: using the thermodynamic stability assumption $C_V \\geq C_{\\min} > 0$.\n1700: \n1701: **Step 3: Bound coordinate transform error.**\n1702: \n1703: The identity $g_R = J^T I J$ is exact (this is the definition of the pullback metric), so the second term vanishes:\n1704: \n1705: $$\n1706: \\|J^T I J - g_R\\|_F = 0\n1707: $$\n1708: \n1709: **Step 4: Combine the bounds.**\n1710: \n1711: $$\n1712: \\mathbb{E}[\\|\\hat{g}_R - g_R\\|_F] \\leq \\frac{\\beta^4}{C_{\\min}^2} \\cdot O\\left(\\epsilon^2 + \\frac{1}{\\sqrt{N h^d}} + h^2\\right)\n1713: $$\n1714: \n1715: **Step 5: Optimize the bandwidth and perturbation.**\n1716: \n1717: Choosing $h = N^{-1/(d+4)}$ (standard for KDE) and $\\epsilon = N^{-1/3}$ (balancing finite difference error):\n1718: \n1719: - KDE error: $O(1/\\sqrt{N h^d} + h^2) = O(N^{-2/(d+4)})$\n1720: - Finite difference error: $O(\\epsilon^2) = O(N^{-2/3})$\n1721: \n1722: **Comparing the rates:** Since $d + 4 > 3$ for all $d \\geq 1$, we have $\\frac{2}{d+4} < \\frac{2}{3}$. This implies that for large $N$, the term $N^{-2/3}$ decays much faster than $N^{-2/(d+4)}$. Therefore, the **KDE error always dominates** the finite difference error, and the overall convergence rate is determined by the KDE bandwidth:\n1723: \n1724: $$\n1725: \\mathbb{E}[\\|\\hat{g}_R - g_R\\|_F] = O\\left(N^{-2/(d+4)}\\right) \\quad \\text{for all } d \\geq 1\n1726: $$\n1727: \n1728: **Step 6: High-probability bound.**\n1729: \n1730: Apply Bernstein's concentration inequality for the sum $\\sum_{k=1}^N s_i^{(k)} s_j^{(k)}$ in the Fisher matrix. Under sub-Gaussian tails (satisfied for compactly supported QSD):\n1731: \n1732: $$\n1733: \\mathbb{P}[|\\hat{I}^{ij} - I^{ij}| > t] \\leq 2\\exp\\left(-\\frac{N t^2}{2\\sigma^2 + 2Mt/3}\\right)\n1734: $$\n1735: \n1736: where $\\sigma^2 = \\text{Var}(s_i^{(k)} s_j^{(k)})$ and $M$ is the sub-Gaussian parameter. Setting $t = \\epsilon$ and propagating through the Jacobian transformation:\n1737: \n1738: $$\n1739: \\mathbb{P}[\\|\\hat{g}_R - g_R\\|_F > \\epsilon] \\leq C_1 \\exp(-C_2 N \\epsilon^2)\n1740: $$\n1741: "
    },
    {
      "directive_type": "corollary",
      "label": "cor-sample-size-ruppeiner",
      "title": "Sample Size Requirements for Target Accuracy",
      "start_line": 1743,
      "end_line": 1765,
      "header_lines": [
        1744
      ],
      "content_start": 1746,
      "content_end": 1764,
      "content": "1746: :label: cor-sample-size-ruppeiner\n1747: \n1748: To achieve Frobenius norm error $\\|\\hat{g}_R - g_R\\|_F < \\delta$ with high probability $1 - \\alpha$:\n1749: \n1750: **Required sample size:**\n1751: \n1752: $$\n1753: N \\geq C \\cdot \\max\\left\\{\\delta^{-\\frac{d+4}{2}}, \\delta^{-\\frac{3}{2}}\\right\\} \\cdot \\log(1/\\alpha)\n1754: $$\n1755: \n1756: where $C$ depends on $\\beta^4 / C_{\\min}^2$, QSD regularity, and kernel bandwidth.\n1757: \n1758: **Examples:**\n1759: \n1760: 1. **Low-dimensional ($d = 2$):** $N = O(\\delta^{-3/2} \\log(1/\\alpha))$\n1761:    - For $\\delta = 0.01$ and $\\alpha = 0.05$: $N \\approx 3 \\times 10^5$\n1762: \n1763: 2. **High-dimensional ($d = 6$):** $N = O(\\delta^{-5} \\log(1/\\alpha))$\n1764:    - For $\\delta = 0.01$ and $\\alpha = 0.05$: $N \\approx 10^{12}$ (intractable!)",
      "metadata": {
        "label": "cor-sample-size-ruppeiner"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1743: :::\n1744: \n1745: :::{prf:corollary} Sample Size Requirements for Target Accuracy\n1746: :label: cor-sample-size-ruppeiner\n1747: \n1748: To achieve Frobenius norm error $\\|\\hat{g}_R - g_R\\|_F < \\delta$ with high probability $1 - \\alpha$:\n1749: \n1750: **Required sample size:**\n1751: \n1752: $$\n1753: N \\geq C \\cdot \\max\\left\\{\\delta^{-\\frac{d+4}{2}}, \\delta^{-\\frac{3}{2}}\\right\\} \\cdot \\log(1/\\alpha)\n1754: $$\n1755: \n1756: where $C$ depends on $\\beta^4 / C_{\\min}^2$, QSD regularity, and kernel bandwidth.\n1757: \n1758: **Examples:**\n1759: \n1760: 1. **Low-dimensional ($d = 2$):** $N = O(\\delta^{-3/2} \\log(1/\\alpha))$\n1761:    - For $\\delta = 0.01$ and $\\alpha = 0.05$: $N \\approx 3 \\times 10^5$\n1762: \n1763: 2. **High-dimensional ($d = 6$):** $N = O(\\delta^{-5} \\log(1/\\alpha))$\n1764:    - For $\\delta = 0.01$ and $\\alpha = 0.05$: $N \\approx 10^{12}$ (intractable!)\n1765: "
    },
    {
      "directive_type": "remark",
      "label": "unlabeled-remark-579",
      "title": "Practical Considerations",
      "start_line": 1767,
      "end_line": 1789,
      "header_lines": [
        1768
      ],
      "content_start": 1770,
      "content_end": 1788,
      "content": "1770: :class: tip\n1771: \n1772: **Computational Feasibility:**\n1773: \n1774: The convergence theorem guarantees correctness but doesn't guarantee tractability. For high-dimensional systems ($d \\geq 4$), direct application is computationally prohibitive. Practical strategies:\n1775: \n1776: 1. **Separability:** Exploit position-velocity factorization in the QSD:\n1777: \n1778: $$\n1779: \\rho_{\\text{QSD}}(x, v) = \\rho_{\\text{spatial}}(x) \\cdot \\rho_{\\text{velocity}}(v)\n1780: $$\n1781: \n1782: allowing separate $3D$ KDE (instead of $6D$).\n1783: \n1784: 2. **Low-Rank Approximation:** If the Fisher matrix is approximately low-rank (common near phase transitions), use randomized SVD.\n1785: \n1786: 3. **Parametric Bootstrap:** Use the analytical QSD formula {prf:ref}`thm-qsd-canonical-ensemble` to derive closed-form score functions, bypassing KDE.\n1787: \n1788: 4. **Subset Selection:** For systems with many thermodynamic variables, compute only the $(U, T)$ block of the Ruppeiner metric (most physically relevant).",
      "metadata": {
        "class": "tip"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1767: :::\n1768: \n1769: :::{prf:remark} Practical Considerations\n1770: :class: tip\n1771: \n1772: **Computational Feasibility:**\n1773: \n1774: The convergence theorem guarantees correctness but doesn't guarantee tractability. For high-dimensional systems ($d \\geq 4$), direct application is computationally prohibitive. Practical strategies:\n1775: \n1776: 1. **Separability:** Exploit position-velocity factorization in the QSD:\n1777: \n1778: $$\n1779: \\rho_{\\text{QSD}}(x, v) = \\rho_{\\text{spatial}}(x) \\cdot \\rho_{\\text{velocity}}(v)\n1780: $$\n1781: \n1782: allowing separate $3D$ KDE (instead of $6D$).\n1783: \n1784: 2. **Low-Rank Approximation:** If the Fisher matrix is approximately low-rank (common near phase transitions), use randomized SVD.\n1785: \n1786: 3. **Parametric Bootstrap:** Use the analytical QSD formula {prf:ref}`thm-qsd-canonical-ensemble` to derive closed-form score functions, bypassing KDE.\n1787: \n1788: 4. **Subset Selection:** For systems with many thermodynamic variables, compute only the $(U, T)$ block of the Ruppeiner metric (most physically relevant).\n1789: "
    },
    {
      "directive_type": "proposition",
      "label": "prop-maxwell-relations-ruppeiner",
      "title": "Maxwell Relations from Ruppeiner Metric",
      "start_line": 1795,
      "end_line": 1840,
      "header_lines": [
        1796
      ],
      "content_start": 1798,
      "content_end": 1839,
      "content": "1798: :label: prop-maxwell-relations-ruppeiner\n1799: \n1800: If the Ruppeiner metric $g_R^{ab}$ is correctly constructed from a thermodynamic entropy $S(U, V, N)$, it must satisfy:\n1801: \n1802: **1. Symmetry:**\n1803: \n1804: $$\n1805: g_R^{ab} = g_R^{ba}\n1806: $$\n1807: \n1808: (automatically satisfied by construction)\n1809: \n1810: **2. Thermodynamic Identities:**\n1811: \n1812: From the definition $g_R^{UU} = -\\partial^2 S / \\partial U^2$, we have:\n1813: \n1814: $$\n1815: \\frac{1}{T^2 C_V} = -\\frac{\\partial^2 S}{\\partial U^2} = g_R^{UU}\n1816: $$\n1817: \n1818: Similarly:\n1819: \n1820: $$\n1821: g_R^{UV} = -\\frac{\\partial^2 S}{\\partial U \\partial V} = \\frac{1}{T} \\frac{\\partial P}{\\partial U} = \\frac{1}{T} \\frac{\\partial P}{\\partial T} \\frac{\\partial T}{\\partial U} = \\frac{1}{T} \\left(\\frac{\\partial P}{\\partial T}\\right)_V \\frac{1}{C_V}\n1822: $$\n1823: \n1824: This provides a **consistency check**: Compute $\\partial P / \\partial T$ independently (e.g., via virial theorem) and verify:\n1825: \n1826: $$\n1827: g_R^{UV} \\stackrel{?}{=} \\frac{1}{T C_V} \\left(\\frac{\\partial P}{\\partial T}\\right)_V\n1828: $$\n1829: \n1830: **3. Positive Definiteness:**\n1831: \n1832: Thermodynamic stability ({prf:ref}`thm-thermodynamic-stability`) requires:\n1833: \n1834: $$\n1835: g_R \\succ 0 \\quad \\text{(all eigenvalues positive)}\n1836: $$\n1837: \n1838: Violation indicates either:\n1839: - Numerical error in the estimation",
      "metadata": {
        "label": "prop-maxwell-relations-ruppeiner"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1795: Having constructed the Ruppeiner metric, we must validate that it satisfies thermodynamic identities.\n1796: \n1797: :::{prf:proposition} Maxwell Relations from Ruppeiner Metric\n1798: :label: prop-maxwell-relations-ruppeiner\n1799: \n1800: If the Ruppeiner metric $g_R^{ab}$ is correctly constructed from a thermodynamic entropy $S(U, V, N)$, it must satisfy:\n1801: \n1802: **1. Symmetry:**\n1803: \n1804: $$\n1805: g_R^{ab} = g_R^{ba}\n1806: $$\n1807: \n1808: (automatically satisfied by construction)\n1809: \n1810: **2. Thermodynamic Identities:**\n1811: \n1812: From the definition $g_R^{UU} = -\\partial^2 S / \\partial U^2$, we have:\n1813: \n1814: $$\n1815: \\frac{1}{T^2 C_V} = -\\frac{\\partial^2 S}{\\partial U^2} = g_R^{UU}\n1816: $$\n1817: \n1818: Similarly:\n1819: \n1820: $$\n1821: g_R^{UV} = -\\frac{\\partial^2 S}{\\partial U \\partial V} = \\frac{1}{T} \\frac{\\partial P}{\\partial U} = \\frac{1}{T} \\frac{\\partial P}{\\partial T} \\frac{\\partial T}{\\partial U} = \\frac{1}{T} \\left(\\frac{\\partial P}{\\partial T}\\right)_V \\frac{1}{C_V}\n1822: $$\n1823: \n1824: This provides a **consistency check**: Compute $\\partial P / \\partial T$ independently (e.g., via virial theorem) and verify:\n1825: \n1826: $$\n1827: g_R^{UV} \\stackrel{?}{=} \\frac{1}{T C_V} \\left(\\frac{\\partial P}{\\partial T}\\right)_V\n1828: $$\n1829: \n1830: **3. Positive Definiteness:**\n1831: \n1832: Thermodynamic stability ({prf:ref}`thm-thermodynamic-stability`) requires:\n1833: \n1834: $$\n1835: g_R \\succ 0 \\quad \\text{(all eigenvalues positive)}\n1836: $$\n1837: \n1838: Violation indicates either:\n1839: - Numerical error in the estimation\n1840: - Thermodynamic instability (near phase transition)"
    },
    {
      "directive_type": "algorithm",
      "label": "alg-ruppeiner-validation",
      "title": "Validation Suite for Ruppeiner Metric",
      "start_line": 1842,
      "end_line": 1907,
      "header_lines": [
        1843
      ],
      "content_start": 1845,
      "content_end": 1906,
      "content": "1845: :label: alg-ruppeiner-validation\n1846: \n1847: **Input:**\n1848: - Empirical Ruppeiner metric $\\hat{g}_R^{ab}$\n1849: - Thermodynamic quantities $(U, T, P, C_V, \\ldots)$\n1850: - Samples $\\{(x_k, v_k)\\}_{k=1}^N$\n1851: \n1852: **Output:**\n1853: - Validation report with pass/fail status for each check\n1854: - Error estimates and diagnostics\n1855: \n1856: **Checks:**\n1857: \n1858: **Check 1: Symmetry**\n1859: \n1860: $$\n1861: \\text{Error}_{\\text{sym}} = \\max_{a,b} |\\hat{g}_R^{ab} - \\hat{g}_R^{ba}|\n1862: $$\n1863: \n1864: **Criterion:** $\\text{Error}_{\\text{sym}} < 10^{-6}$ (machine precision)\n1865: \n1866: **Check 2: Heat Capacity Consistency**\n1867: \n1868: $$\n1869: \\hat{C}_V^{(\\text{variance})} = \\frac{1}{k_B T^2} \\text{Var}(H_{\\text{eff}}) \\quad \\text{vs} \\quad \\hat{C}_V^{(\\text{Ruppeiner})} = \\frac{1}{T^2 \\hat{g}_R^{UU}}\n1870: $$\n1871: \n1872: **Criterion:** $|\\hat{C}_V^{(\\text{variance})} - \\hat{C}_V^{(\\text{Ruppeiner})}| < 0.1 \\cdot \\hat{C}_V^{(\\text{variance)}}$ (10% relative error)\n1873: \n1874: **Check 3: Positive Definiteness**\n1875: \n1876: Compute eigenvalues $\\lambda_1, \\lambda_2, \\lambda_3$ of $\\hat{g}_R$.\n1877: \n1878: **Criterion:** $\\min\\{\\lambda_1, \\lambda_2, \\lambda_3\\} > \\epsilon_{\\text{tol}}$ where $\\epsilon_{\\text{tol}} = 10^{-4}$ (numerical tolerance)\n1879: \n1880: **Check 4: Maxwell Relation (if $P$ is computable)**\n1881: \n1882: $$\n1883: \\text{Error}_{\\text{Maxwell}} = \\left|\\hat{g}_R^{UV} - \\frac{1}{T \\hat{C}_V} \\left(\\frac{\\partial P}{\\partial T}\\right)_V\\right|\n1884: $$\n1885: \n1886: **Criterion:** $\\text{Error}_{\\text{Maxwell}} < 0.2 \\cdot |\\hat{g}_R^{UV}|$ (20% relative error, relaxed due to derivative estimation)\n1887: \n1888: **Check 5: Effective Sample Size**\n1889: \n1890: Compute ESS from importance weights (Chapter 19):\n1891: \n1892: $$\n1893: \\text{ESS} = \\frac{(\\sum_k w_k)^2}{\\sum_k w_k^2}\n1894: $$\n1895: \n1896: **Criterion:** $\\text{ESS} > N/10$ (at least 10% effective samples)\n1897: \n1898: **Reporting:**\n1899: \n1900: - **PASS** if all checks satisfied\n1901: - **WARNING** if Check 2 or 4 fails (statistical error, increase $N$)\n1902: - **FAIL** if Check 3 fails (thermodynamic instability or serious numerical error)\n1903: \n1904: **Remediation:** If FAIL:\n1905: 1. Increase sample size $N$\n1906: 2. Check for phase transition (singularity expected)",
      "metadata": {
        "label": "alg-ruppeiner-validation"
      },
      "section": "## 4. The Ruppeiner Metric: Algorithmic Construction",
      "raw_directive": "1842: :::\n1843: \n1844: :::{prf:algorithm} Validation Suite for Ruppeiner Metric\n1845: :label: alg-ruppeiner-validation\n1846: \n1847: **Input:**\n1848: - Empirical Ruppeiner metric $\\hat{g}_R^{ab}$\n1849: - Thermodynamic quantities $(U, T, P, C_V, \\ldots)$\n1850: - Samples $\\{(x_k, v_k)\\}_{k=1}^N$\n1851: \n1852: **Output:**\n1853: - Validation report with pass/fail status for each check\n1854: - Error estimates and diagnostics\n1855: \n1856: **Checks:**\n1857: \n1858: **Check 1: Symmetry**\n1859: \n1860: $$\n1861: \\text{Error}_{\\text{sym}} = \\max_{a,b} |\\hat{g}_R^{ab} - \\hat{g}_R^{ba}|\n1862: $$\n1863: \n1864: **Criterion:** $\\text{Error}_{\\text{sym}} < 10^{-6}$ (machine precision)\n1865: \n1866: **Check 2: Heat Capacity Consistency**\n1867: \n1868: $$\n1869: \\hat{C}_V^{(\\text{variance})} = \\frac{1}{k_B T^2} \\text{Var}(H_{\\text{eff}}) \\quad \\text{vs} \\quad \\hat{C}_V^{(\\text{Ruppeiner})} = \\frac{1}{T^2 \\hat{g}_R^{UU}}\n1870: $$\n1871: \n1872: **Criterion:** $|\\hat{C}_V^{(\\text{variance})} - \\hat{C}_V^{(\\text{Ruppeiner})}| < 0.1 \\cdot \\hat{C}_V^{(\\text{variance)}}$ (10% relative error)\n1873: \n1874: **Check 3: Positive Definiteness**\n1875: \n1876: Compute eigenvalues $\\lambda_1, \\lambda_2, \\lambda_3$ of $\\hat{g}_R$.\n1877: \n1878: **Criterion:** $\\min\\{\\lambda_1, \\lambda_2, \\lambda_3\\} > \\epsilon_{\\text{tol}}$ where $\\epsilon_{\\text{tol}} = 10^{-4}$ (numerical tolerance)\n1879: \n1880: **Check 4: Maxwell Relation (if $P$ is computable)**\n1881: \n1882: $$\n1883: \\text{Error}_{\\text{Maxwell}} = \\left|\\hat{g}_R^{UV} - \\frac{1}{T \\hat{C}_V} \\left(\\frac{\\partial P}{\\partial T}\\right)_V\\right|\n1884: $$\n1885: \n1886: **Criterion:** $\\text{Error}_{\\text{Maxwell}} < 0.2 \\cdot |\\hat{g}_R^{UV}|$ (20% relative error, relaxed due to derivative estimation)\n1887: \n1888: **Check 5: Effective Sample Size**\n1889: \n1890: Compute ESS from importance weights (Chapter 19):\n1891: \n1892: $$\n1893: \\text{ESS} = \\frac{(\\sum_k w_k)^2}{\\sum_k w_k^2}\n1894: $$\n1895: \n1896: **Criterion:** $\\text{ESS} > N/10$ (at least 10% effective samples)\n1897: \n1898: **Reporting:**\n1899: \n1900: - **PASS** if all checks satisfied\n1901: - **WARNING** if Check 2 or 4 fails (statistical error, increase $N$)\n1902: - **FAIL** if Check 3 fails (thermodynamic instability or serious numerical error)\n1903: \n1904: **Remediation:** If FAIL:\n1905: 1. Increase sample size $N$\n1906: 2. Check for phase transition (singularity expected)\n1907: 3. Inspect QSD convergence (may not have reached equilibrium)"
    }
  ],
  "validation": {
    "ok": true,
    "errors": []
  }
}