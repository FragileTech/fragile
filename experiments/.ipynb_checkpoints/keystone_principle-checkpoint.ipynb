{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Keystone Principle in Action\n",
    "\n",
    "**Didactic Goal**: To demystify the \"intelligence\" of the algorithm by demonstrating the central causal chain of the Keystone Lemma:\n",
    "\n",
    "**High Positional Variance → Detectable Geometric Structure → Corrective Fitness Signal → Targeted Cloning**\n",
    "\n",
    "This notebook makes the abstract concept of an \"error-correction mechanism\" concrete and visual. Skeptics often doubt that simple, local rules can lead to intelligent global behavior; this notebook proves it.\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "From [03_cloning.md](../docs/source/1_euclidean_gas/03_cloning.md), Chapter 6:\n",
    "\n",
    "- **High-Error Set** $H_k(\\epsilon)$: Walkers kinematically isolated in phase space\n",
    "- **Low-Error Set** $L_k(\\epsilon)$: Dense clusters of walkers in phase space\n",
    "- **Fitness Potential** $V_{\\text{fit},i} = (d'_i)^\\beta \\cdot (r'_i)^\\alpha$: Combines diversity and reward signals\n",
    "- **Cloning Probability** $p_i$: Probability walker $i$ gets replaced by a clone\n",
    "\n",
    "## References\n",
    "\n",
    "- Definition 6.3.1 (Geometric Partitioning): `def-unified-high-low-error-sets`\n",
    "- Definition 5.7 (Fitness Potential): `def-fitness-potential-operator`\n",
    "- Lemma 6.5.1 (Geometric Separation): Guarantees $D_H(\\epsilon) > R_L(\\epsilon)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setup and Imports.\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "import panel as pn\n",
    "\n",
    "# Enable Bokeh backend for interactive plots\n",
    "hv.extension('bokeh')\n",
    "pn.extension()\n",
    "\n",
    "from fragile.euclidean_gas import (\n",
    "    EuclideanGas,\n",
    "    EuclideanGasParams,\n",
    "    SimpleQuadraticPotential,\n",
    "    LangevinParams,\n",
    "    CloningParams,\n",
    "    SwarmState,\n",
    "    VectorizedOps,\n",
    ")\n",
    "from fragile.companion_selection import select_companions_softmax\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Create High-Variance Swarm\n",
    "\n",
    "We create a swarm with **two distinct clusters** far apart to induce high positional variance. This sets up the geometric structure that the algorithm will detect and correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a swarm with two distinct clusters.\"\"\"\n",
    "\n",
    "# Parameters\n",
    "N = 100  # Number of walkers\n",
    "d = 2    # Spatial dimension (2D for visualization)\n",
    "device = 'cpu'\n",
    "dtype = torch.float32\n",
    "\n",
    "# Create two clusters\n",
    "N_cluster1 = 30  # High-error cluster (outliers)\n",
    "N_cluster2 = 70  # Low-error cluster (core walkers)\n",
    "\n",
    "# Cluster 1: Outliers far from origin\n",
    "cluster1_center = torch.tensor([5.0, 5.0], dtype=dtype)\n",
    "cluster1_std = 0.5\n",
    "x_cluster1 = cluster1_center + cluster1_std * torch.randn(N_cluster1, d, dtype=dtype)\n",
    "\n",
    "# Cluster 2: Core walkers near origin\n",
    "cluster2_center = torch.tensor([0.0, 0.0], dtype=dtype)\n",
    "cluster2_std = 0.5\n",
    "x_cluster2 = cluster2_center + cluster2_std * torch.randn(N_cluster2, d, dtype=dtype)\n",
    "\n",
    "# Combine clusters\n",
    "x_init = torch.cat([x_cluster1, x_cluster2], dim=0)\n",
    "\n",
    "# Initialize velocities from thermal distribution\n",
    "beta = 1.0\n",
    "v_std = 1.0 / np.sqrt(beta)\n",
    "v_init = v_std * torch.randn(N, d, dtype=dtype)\n",
    "\n",
    "# Create initial state\n",
    "state = SwarmState(x_init, v_init)\n",
    "\n",
    "# Compute initial variance\n",
    "var_x = VectorizedOps.variance_position(state)\n",
    "print(f\"Initial positional variance: {var_x.item():.4f}\")\n",
    "print(f\"Number of walkers in high-error cluster: {N_cluster1}\")\n",
    "print(f\"Number of walkers in low-error cluster: {N_cluster2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: From Variance to Geometry\n",
    "\n",
    "**Goal**: Visualize that high variance creates a geometric partition into High-Error and Low-Error sets.\n",
    "\n",
    "We define the sets based on distance from the swarm centroid:\n",
    "- **High-Error Set** $H_k$: Walkers far from centroid (positional outliers)\n",
    "- **Low-Error Set** $L_k$: Walkers near centroid (core)\n",
    "\n",
    "**Mathematical Definition** (from Definition 6.3.1):\n",
    "- $H_k(\\epsilon)$: Union of outlier clusters whose centers contribute significantly to between-cluster variance\n",
    "- $L_k(\\epsilon)$: Dense clusters satisfying $d_{\\text{alg}}(j, \\ell) \\leq R_L(\\epsilon)$ for all members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define High-Error and Low-Error sets based on geometric structure.\"\"\"\n",
    "\n",
    "# Compute centroid\n",
    "mu_x = torch.mean(state.x, dim=0, keepdim=True)  # [1, d]\n",
    "mu_v = torch.mean(state.v, dim=0, keepdim=True)  # [1, d]\n",
    "\n",
    "# Compute positional error (distance from centroid)\n",
    "positional_error = torch.sqrt(torch.sum((state.x - mu_x)**2, dim=-1))  # [N]\n",
    "\n",
    "# Define threshold: use median to partition into two sets\n",
    "# This is a simplified version for didactic purposes\n",
    "# In practice, the partition is defined by clustering structure (see Definition 6.3.1)\n",
    "threshold = torch.median(positional_error)\n",
    "\n",
    "# Create partition\n",
    "high_error_mask = positional_error > threshold  # High-Error Set H_k\n",
    "low_error_mask = ~high_error_mask  # Low-Error Set L_k\n",
    "\n",
    "print(f\"Partition threshold: {threshold.item():.4f}\")\n",
    "print(f\"High-Error Set |H_k|: {high_error_mask.sum().item()} walkers\")\n",
    "print(f\"Low-Error Set |L_k|: {low_error_mask.sum().item()} walkers\")\n",
    "print(f\"Fraction in H_k: {high_error_mask.float().mean().item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize the geometric partition in phase space.\"\"\"\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "x_np = state.x.numpy()\n",
    "high_error_np = high_error_mask.numpy()\n",
    "\n",
    "# Create color array: red for high-error, blue for low-error\n",
    "colors = ['red' if he else 'blue' for he in high_error_np]\n",
    "labels = ['High-Error (H_k)' if he else 'Low-Error (L_k)' for he in high_error_np]\n",
    "\n",
    "# Create scatter plot\n",
    "scatter = hv.Scatter(\n",
    "    (x_np[:, 0], x_np[:, 1], colors, labels),\n",
    "    kdims=['x', 'y'],\n",
    "    vdims=['color', 'label']\n",
    ").opts(\n",
    "    opts.Scatter(\n",
    "        color='color',\n",
    "        size=8,\n",
    "        alpha=0.6,\n",
    "        width=600,\n",
    "        height=600,\n",
    "        title='Step 1: Geometric Partition from High Variance',\n",
    "        xlabel='Position x₁',\n",
    "        ylabel='Position x₂',\n",
    "        tools=['hover'],\n",
    "        legend_position='top_right'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add centroid marker\n",
    "centroid_marker = hv.Scatter(\n",
    "    ([mu_x[0, 0].item()], [mu_x[0, 1].item()]),\n",
    "    label='Centroid'\n",
    ").opts(\n",
    "    color='black',\n",
    "    marker='x',\n",
    "    size=15,\n",
    "    line_width=3\n",
    ")\n",
    "\n",
    "# Combine plots\n",
    "step1_plot = scatter * centroid_marker\n",
    "step1_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation (Step 1)\n",
    "\n",
    "The plot shows:\n",
    "- **Red points** (High-Error Set $H_k$): Outlier walkers far from centroid\n",
    "- **Blue points** (Low-Error Set $L_k$): Core walkers near centroid\n",
    "- **Black X**: Swarm centroid $\\mu_x$\n",
    "\n",
    "**Key Insight**: High positional variance creates a clear geometric partition. This structure is detectable by the measurement pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: From Geometry to Measurement Signal\n",
    "\n",
    "**Goal**: Show that geometric separation is reliably detected by the distance-measurement pipeline.\n",
    "\n",
    "We use the **companion-pairing mechanism** (Definition 5.1.2) with softmax selection:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(c_i = u) \\propto \\exp\\left(-\\frac{d_{\\text{alg}}(i,u)^2}{2\\epsilon_d^2}\\right)\n",
    "$$\n",
    "\n",
    "Then measure the **raw distance** $d_i := d_{\\text{alg}}(i, c(i))$ where:\n",
    "\n",
    "$$\n",
    "d_{\\text{alg}}(i,j)^2 = \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run companion-pairing and distance-measurement.\"\"\"\n",
    "\n",
    "# Parameters for companion selection\n",
    "epsilon_c = 1.0  # Companion selection range (ε_d in theory)\n",
    "lambda_alg = 0.1  # Velocity weight in algorithmic distance\n",
    "\n",
    "# All walkers are alive (no boundaries in this demo)\n",
    "alive_mask = torch.ones(N, dtype=torch.bool)\n",
    "\n",
    "# Select companions using softmax (distance-dependent)\n",
    "companions = select_companions_softmax(\n",
    "    state.x, state.v, alive_mask,\n",
    "    epsilon=epsilon_c,\n",
    "    lambda_alg=lambda_alg,\n",
    "    exclude_self=True\n",
    ")\n",
    "\n",
    "# Compute raw distances d_i = d_alg(i, c(i))\n",
    "x_companion = state.x[companions]  # [N, d]\n",
    "v_companion = state.v[companions]  # [N, d]\n",
    "\n",
    "pos_diff_sq = torch.sum((state.x - x_companion)**2, dim=-1)  # [N]\n",
    "vel_diff_sq = torch.sum((state.v - v_companion)**2, dim=-1)  # [N]\n",
    "raw_distances = torch.sqrt(pos_diff_sq + lambda_alg * vel_diff_sq)  # [N]\n",
    "\n",
    "# Separate by partition\n",
    "distances_H = raw_distances[high_error_mask]\n",
    "distances_L = raw_distances[low_error_mask]\n",
    "\n",
    "print(f\"Mean distance in H_k: {distances_H.mean().item():.4f} ± {distances_H.std().item():.4f}\")\n",
    "print(f\"Mean distance in L_k: {distances_L.mean().item():.4f} ± {distances_L.std().item():.4f}\")\n",
    "print(f\"Signal separation (D_H - R_L): {(distances_H.mean() - distances_L.mean()).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize distance distribution histograms.\"\"\"\n",
    "\n",
    "# Create histogram data\n",
    "bins = np.linspace(0, max(raw_distances.max().item(), 1.0), 30)\n",
    "\n",
    "# High-Error Set histogram\n",
    "hist_H = hv.Histogram(\n",
    "    np.histogram(distances_H.numpy(), bins=bins),\n",
    "    label='High-Error Set (H_k)'\n",
    ").opts(\n",
    "    color='red',\n",
    "    alpha=0.5,\n",
    "    line_color='darkred'\n",
    ")\n",
    "\n",
    "# Low-Error Set histogram\n",
    "hist_L = hv.Histogram(\n",
    "    np.histogram(distances_L.numpy(), bins=bins),\n",
    "    label='Low-Error Set (L_k)'\n",
    ").opts(\n",
    "    color='blue',\n",
    "    alpha=0.5,\n",
    "    line_color='darkblue'\n",
    ")\n",
    "\n",
    "# Overlay histograms\n",
    "step2_plot = (hist_H * hist_L).opts(\n",
    "    opts.Histogram(\n",
    "        width=700,\n",
    "        height=400,\n",
    "        title='Step 2: Distance Distribution by Partition',\n",
    "        xlabel='Raw Distance d_i',\n",
    "        ylabel='Count',\n",
    "        #label='top_right'\n",
    "    )\n",
    ")\n",
    "\n",
    "step2_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation (Step 2)\n",
    "\n",
    "The histogram shows:\n",
    "- **Red distribution** ($H_k$): Shifted to higher distances → High-error walkers are isolated\n",
    "- **Blue distribution** ($L_k$): Concentrated at lower distances → Low-error walkers are clustered\n",
    "\n",
    "**Key Insight**: The geometric separation is reliably detected by the measurement pipeline. High-error walkers have systematically larger $d_i$ values.\n",
    "\n",
    "**Theoretical Guarantee** (Lemma 5.1.3): $\\mathbb{E}[d_i \\mid i \\in H_k] \\geq D_H(\\epsilon) > R_L(\\epsilon) \\geq \\mathbb{E}[d_j \\mid j \\in L_k]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: From Signal to Fitness\n",
    "\n",
    "**Goal**: Prove that high-error walkers are correctly identified as \"unfit\".\n",
    "\n",
    "The **fitness potential** (Definition 5.7) combines diversity and reward:\n",
    "\n",
    "$$\n",
    "V_{\\text{fit},i} = (d'_i)^\\beta \\cdot (r'_i)^\\alpha\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $d'_i$: Rescaled diversity score (from raw distance $d_i$)\n",
    "- $r'_i$: Rescaled reward score (from raw reward $r_i$)\n",
    "- $\\alpha, \\beta$: Weight parameters\n",
    "\n",
    "**Processing Pipeline**:\n",
    "1. Raw measurement: $d_i$, $r_i$\n",
    "2. Aggregation: $\\mu_d$, $\\sigma_d$, $\\mu_r$, $\\sigma_r$\n",
    "3. Standardization: $z_{d,i} = (d_i - \\mu_d) / \\sigma_d$\n",
    "4. Rescaling: $d'_i = g_A(z_{d,i})$ (monotonic, bounded)\n",
    "5. Composition: $V_{\\text{fit},i} = (d'_i)^\\beta \\cdot (r'_i)^\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute fitness potential V_fit,i for each walker.\"\"\"\n",
    "\n",
    "# Parameters\n",
    "alpha = 1.0  # Reward weight\n",
    "beta = 1.0   # Diversity weight\n",
    "eta = 0.1    # Floor value for rescaling\n",
    "g_A_max = 1.0  # Maximum rescaled value\n",
    "\n",
    "# Step 1: Raw measurements\n",
    "# For this demo, use negative potential as reward (higher is better)\n",
    "potential = SimpleQuadraticPotential()\n",
    "U = potential.evaluate(state.x)  # [N]\n",
    "raw_rewards = -U  # Higher reward = lower potential\n",
    "\n",
    "# Step 2: Aggregation\n",
    "mu_d = raw_distances.mean()\n",
    "sigma_d = raw_distances.std()\n",
    "mu_r = raw_rewards.mean()\n",
    "sigma_r = raw_rewards.std()\n",
    "\n",
    "# Step 3: Standardization\n",
    "z_d = (raw_distances - mu_d) / (sigma_d + 1e-8)\n",
    "z_r = (raw_rewards - mu_r) / (sigma_r + 1e-8)\n",
    "\n",
    "# Step 4: Rescaling with monotonic function g_A\n",
    "# Simple clipping + linear rescaling for demo\n",
    "z_min, z_max = -2.0, 2.0\n",
    "\n",
    "def rescale(z, eta=eta, g_A_max=g_A_max, z_min=z_min, z_max=z_max):\n",
    "    \"\"\"Monotonic rescaling function g_A.\"\"\"\n",
    "    z_clipped = torch.clamp(z, z_min, z_max)\n",
    "    # Linear interpolation from [z_min, z_max] to [eta, g_A_max + eta]\n",
    "    return eta + g_A_max * (z_clipped - z_min) / (z_max - z_min)\n",
    "\n",
    "d_prime = rescale(z_d)\n",
    "r_prime = rescale(z_r)\n",
    "\n",
    "# Step 5: Fitness composition\n",
    "V_fit = (d_prime ** beta) * (r_prime ** alpha)\n",
    "\n",
    "# Separate by partition\n",
    "V_fit_H = V_fit[high_error_mask]\n",
    "V_fit_L = V_fit[low_error_mask]\n",
    "\n",
    "print(f\"Mean fitness in H_k: {V_fit_H.mean().item():.4f} ± {V_fit_H.std().item():.4f}\")\n",
    "print(f\"Mean fitness in L_k: {V_fit_L.mean().item():.4f} ± {V_fit_L.std().item():.4f}\")\n",
    "print(f\"Fitness gap (L_k - H_k): {(V_fit_L.mean() - V_fit_H.mean()).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize correlation between positional error and fitness.\"\"\"\n",
    "\n",
    "# Convert to numpy\n",
    "pos_error_np = positional_error.numpy()\n",
    "V_fit_np = V_fit.numpy()\n",
    "\n",
    "# Create scatter with color-coding\n",
    "scatter_data = hv.Scatter(\n",
    "    (pos_error_np, V_fit_np, colors, labels),\n",
    "    kdims=['Positional Error ||x_i - μ_x||', 'Fitness V_fit,i'],\n",
    "    vdims=['color', 'label']\n",
    ").opts(\n",
    "    opts.Scatter(\n",
    "        color='color',\n",
    "        size=6,\n",
    "        alpha=0.7,\n",
    "        width=700,\n",
    "        height=500,\n",
    "        title='Step 3: Negative Correlation (Error → Fitness)',\n",
    "        xlabel='Positional Error ||x_i - μ_x||',\n",
    "        ylabel='Fitness Potential V_fit,i',\n",
    "        tools=['hover'],\n",
    "        legend_position='top_right'\n",
    "    )\n",
    ")\n",
    "\n",
    "step3_plot = scatter_data\n",
    "step3_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation (Step 3)\n",
    "\n",
    "The scatter plot shows:\n",
    "- **Clear negative correlation**: High positional error → Low fitness\n",
    "- **Red points** (High-Error Set): Clustered at low fitness values\n",
    "- **Blue points** (Low-Error Set): Clustered at high fitness values\n",
    "\n",
    "**Key Insight**: High-error walkers are correctly identified as \"unfit\" by the fitness potential. The diversity signal $d'_i$ acts as an error-correction mechanism.\n",
    "\n",
    "**Theoretical Guarantee**: Fitness potential is bounded in $[V_{\\text{pot,min}}, V_{\\text{pot,max}}]$ with lower values indicating unfitness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: From Fitness to Action\n",
    "\n",
    "**Goal**: Visualize that high-error walkers have high cloning probability and will be replaced.\n",
    "\n",
    "The **cloning probability** (Definition 5.8) is:\n",
    "\n",
    "$$\n",
    "p_i = \\mathbb{E}_{c_i}\\left[\\pi(S_i(c_i))\\right]\n",
    "$$\n",
    "\n",
    "where the **cloning score** is:\n",
    "\n",
    "$$\n",
    "S_i(c_i) = V_{\\text{fit},c_i} - V_{\\text{fit},i}\n",
    "$$\n",
    "\n",
    "and the **cloning gate** is:\n",
    "\n",
    "$$\n",
    "\\pi(S) = \\begin{cases}\n",
    "0 & \\text{if } S \\leq 0 \\\\\n",
    "S/p_{\\max} & \\text{if } 0 < S < p_{\\max} \\\\\n",
    "1 & \\text{if } S \\geq p_{\\max}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Simplified version**: For this demo, we directly use fitness gap as proxy for cloning probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute cloning probabilities.\"\"\"\n",
    "\n",
    "# Parameters\n",
    "p_max = 1.0  # Maximum cloning probability\n",
    "\n",
    "# Compute cloning scores S_i(c_i) = V_fit[c_i] - V_fit[i]\n",
    "V_fit_companion = V_fit[companions]\n",
    "cloning_scores = V_fit_companion - V_fit\n",
    "\n",
    "# Apply cloning gate function π(S)\n",
    "def cloning_gate(S, p_max=p_max):\n",
    "    \"\"\"Cloning gate function π(S).\"\"\"\n",
    "    return torch.clamp(S / p_max, 0.0, 1.0)\n",
    "\n",
    "cloning_probs = cloning_gate(cloning_scores)\n",
    "\n",
    "# Separate by partition\n",
    "p_H = cloning_probs[high_error_mask]\n",
    "p_L = cloning_probs[low_error_mask]\n",
    "\n",
    "print(f\"Mean cloning probability in H_k: {p_H.mean().item():.4f} ± {p_H.std().item():.4f}\")\n",
    "print(f\"Mean cloning probability in L_k: {p_L.mean().item():.4f} ± {p_L.std().item():.4f}\")\n",
    "print(f\"Probability gap (H_k - L_k): {(p_H.mean() - p_L.mean()).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize walkers sized by cloning probability.\"\"\"\n",
    "\n",
    "# Convert to numpy\n",
    "x_np = state.x.numpy()\n",
    "p_np = cloning_probs.numpy()\n",
    "\n",
    "# Scale point sizes by cloning probability (larger = higher p_i)\n",
    "# Base size 5, max size 25\n",
    "sizes = 5 + 20 * p_np\n",
    "\n",
    "# Create scatter plot\n",
    "scatter = hv.Scatter(\n",
    "    (x_np[:, 0], x_np[:, 1], sizes, colors, labels, p_np),\n",
    "    kdims=['x', 'y'],\n",
    "    vdims=['size', 'color', 'label', 'p_i']\n",
    ").opts(\n",
    "    opts.Scatter(\n",
    "        color='color',\n",
    "        size='size',\n",
    "        alpha=0.6,\n",
    "        width=700,\n",
    "        height=700,\n",
    "        title='Step 4: Walkers Sized by Cloning Probability p_i',\n",
    "        xlabel='Position x₁',\n",
    "        ylabel='Position x₂',\n",
    "        tools=['hover'],\n",
    "        legend_position='top_right'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add centroid marker\n",
    "centroid_marker = hv.Scatter(\n",
    "    ([mu_x[0, 0].item()], [mu_x[0, 1].item()]),\n",
    "    label='Centroid'\n",
    ").opts(\n",
    "    color='black',\n",
    "    marker='x',\n",
    "    size=15,\n",
    "    line_width=3\n",
    ")\n",
    "\n",
    "step4_plot = scatter * centroid_marker\n",
    "step4_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation (Step 4)\n",
    "\n",
    "The plot shows:\n",
    "- **Larger points**: Higher cloning probability $p_i$ → Will be replaced\n",
    "- **Red points** (High-Error Set): Systematically larger → High cloning rate\n",
    "- **Blue points** (Low-Error Set): Smaller → Low cloning rate\n",
    "\n",
    "**Key Insight**: The algorithm uses the swarm's geometry to identify \"bad\" walkers (high-error) and systematically replace them with clones of \"good\" walkers (low-error).\n",
    "\n",
    "**Theoretical Guarantee** (Lemma 8.3.1): For any walker $i$ in the unfit set, $p_i \\geq p_u(\\epsilon) > 0$ (non-vanishing cloning pressure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4b: Animate One Cloning Step\n",
    "\n",
    "**Goal**: Show the cloning operator in action—high-error walkers are replaced by clones of low-error walkers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simulate one cloning step and visualize the result.\"\"\"\n",
    "\n",
    "# Create EuclideanGas instance\n",
    "params = EuclideanGasParams(\n",
    "    N=N,\n",
    "    d=d,\n",
    "    potential=SimpleQuadraticPotential(),\n",
    "    langevin=LangevinParams(gamma=1.0, beta=1.0, delta_t=0.01),\n",
    "    cloning=CloningParams(\n",
    "        sigma_x=0.5,\n",
    "        lambda_alg=lambda_alg,\n",
    "        epsilon_c=epsilon_c,\n",
    "        alpha_restitution=0.5,\n",
    "        companion_selection_method='softmax'\n",
    "    ),\n",
    "    device='cpu',\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "gas = EuclideanGas(params)\n",
    "\n",
    "# Apply cloning operator\n",
    "state_cloned = gas.cloning_op.apply(state)\n",
    "\n",
    "# Compute new partition (after cloning)\n",
    "mu_x_new = torch.mean(state_cloned.x, dim=0, keepdim=True)\n",
    "positional_error_new = torch.sqrt(torch.sum((state_cloned.x - mu_x_new)**2, dim=-1))\n",
    "threshold_new = torch.median(positional_error_new)\n",
    "high_error_mask_new = positional_error_new > threshold_new\n",
    "\n",
    "# Convert to numpy\n",
    "x_cloned_np = state_cloned.x.numpy()\n",
    "colors_new = ['red' if he else 'blue' for he in high_error_mask_new.numpy()]\n",
    "\n",
    "# Create before/after scatter plots\n",
    "scatter_before = hv.Scatter(\n",
    "    (x_np[:, 0], x_np[:, 1], colors),\n",
    "    kdims=['x', 'y'],\n",
    "    vdims=['color'],\n",
    "    label='Before Cloning'\n",
    ").opts(\n",
    "    opts.Scatter(\n",
    "        color='color',\n",
    "        size=8,\n",
    "        alpha=0.6,\n",
    "        width=500,\n",
    "        height=500,\n",
    "        xlabel='Position x₁',\n",
    "        ylabel='Position x₂'\n",
    "    )\n",
    ")\n",
    "\n",
    "scatter_after = hv.Scatter(\n",
    "    (x_cloned_np[:, 0], x_cloned_np[:, 1], colors_new),\n",
    "    kdims=['x', 'y'],\n",
    "    vdims=['color'],\n",
    "    label='After Cloning'\n",
    ").opts(\n",
    "    opts.Scatter(\n",
    "        color='color',\n",
    "        size=8,\n",
    "        alpha=0.6,\n",
    "        width=500,\n",
    "        height=500,\n",
    "        xlabel='Position x₁',\n",
    "        ylabel='Position x₂'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Layout side-by-side\n",
    "animation_plot = (scatter_before + scatter_after).opts(\n",
    "    opts.Layout(title='Cloning Operator: Before and After')\n",
    ")\n",
    "\n",
    "animation_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute change in variance after cloning.\"\"\"\n",
    "\n",
    "var_x_before = VectorizedOps.variance_position(state)\n",
    "var_x_after = VectorizedOps.variance_position(state_cloned)\n",
    "\n",
    "print(f\"Positional variance before cloning: {var_x_before.item():.4f}\")\n",
    "print(f\"Positional variance after cloning: {var_x_after.item():.4f}\")\n",
    "print(f\"Variance reduction: {(var_x_before - var_x_after).item():.4f} ({(1 - var_x_after/var_x_before).item():.2%})\")\n",
    "\n",
    "print(f\"\\nHigh-error walkers before: {high_error_mask.sum().item()}\")\n",
    "print(f\"High-error walkers after: {high_error_mask_new.sum().item()}\")\n",
    "print(f\"Reduction in outliers: {(high_error_mask.sum() - high_error_mask_new.sum()).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation (Step 4b)\n",
    "\n",
    "**Before Cloning**: Two distinct clusters with high variance\n",
    "\n",
    "**After Cloning**: \n",
    "- High-error walkers (red) are replaced by clones of low-error walkers (blue)\n",
    "- Outlier cluster shrinks toward the core\n",
    "- Positional variance decreases\n",
    "\n",
    "**Key Insight**: The cloning operator acts as a **variance-reduction mechanism**. It systematically eliminates geometric outliers, driving the swarm toward the optimal region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Keystone Principle Causal Chain\n",
    "\n",
    "This notebook demonstrated the complete causal chain:\n",
    "\n",
    "1. **High Positional Variance** → Creates geometric partition into $H_k$ (outliers) and $L_k$ (core)\n",
    "\n",
    "2. **Detectable Geometric Structure** → Distance measurements $d_i$ reliably separate $H_k$ from $L_k$\n",
    "\n",
    "3. **Corrective Fitness Signal** → Fitness potential $V_{\\text{fit},i}$ correctly identifies high-error walkers as unfit\n",
    "\n",
    "4. **Targeted Cloning** → High-error walkers have high $p_i$ and are systematically replaced\n",
    "\n",
    "## Skeptic's Takeaway\n",
    "\n",
    "> \"I see the feedback loop now. The algorithm isn't just randomly exploring; it uses the swarm's own geometry to identify 'bad' walkers and systematically eliminate them. The intelligence is an emergent property of the measurement pipeline.\"\n",
    "\n",
    "## Mathematical Rigor\n",
    "\n",
    "All claims are rigorously proven in [03_cloning.md](../docs/source/1_euclidean_gas/03_cloning.md):\n",
    "\n",
    "- **Geometric Separation** (Lemma 6.5.1): $D_H(\\epsilon) > R_L(\\epsilon)$ with N-uniform constants\n",
    "- **Signal Separation** (Lemma 5.1.3): $\\mathbb{E}[d_i \\mid i \\in H_k] \\geq D_H(\\epsilon)$\n",
    "- **Non-Vanishing Cloning** (Lemma 8.3.1): $p_i \\geq p_u(\\epsilon) > 0$ for all unfit walkers\n",
    "- **Variance Reduction** (Theorem 8.4): Exponential convergence to low-variance regime\n",
    "\n",
    "The algorithm's \"intelligence\" is not magic—it is a mathematically rigorous consequence of the Keystone Lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Time Evolution of the Swarm\n",
    "\n",
    "**Goal**: Visualize how the swarm evolves over multiple steps, showing the convergence from high variance to low variance as the Keystone Principle systematically eliminates outliers.\n",
    "\n",
    "We will run the full Euclidean Gas algorithm for multiple steps and track:\n",
    "- Position trajectories of all walkers\n",
    "- Variance reduction over time\n",
    "- High-error vs. low-error population dynamics\n",
    "- Convergence to the optimal region (origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Dashboard (Optional)\n",
    "\n",
    "Use Panel to create an interactive dashboard exploring different parameter regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Interactive parameter exploration dashboard.\"\"\"\n",
    "\n",
    "# Parameter widgets\n",
    "epsilon_slider = pn.widgets.FloatSlider(name='ε_c (companion range)', start=0.1, end=5.0, value=1.0, step=0.1)\n",
    "lambda_slider = pn.widgets.FloatSlider(name='λ_alg (velocity weight)', start=0.0, end=1.0, value=0.1, step=0.05)\n",
    "alpha_slider = pn.widgets.FloatSlider(name='α (reward weight)', start=0.0, end=2.0, value=1.0, step=0.1)\n",
    "beta_slider = pn.widgets.FloatSlider(name='β (diversity weight)', start=0.0, end=2.0, value=1.0, step=0.1)\n",
    "\n",
    "@pn.depends(epsilon_slider.param.value, lambda_slider.param.value, alpha_slider.param.value, beta_slider.param.value)\n",
    "def update_visualization(epsilon_c, lambda_alg, alpha, beta):\n",
    "    \"\"\"Update visualization with new parameters.\"\"\"\n",
    "    # Recompute with new parameters\n",
    "    companions_new = select_companions_softmax(\n",
    "        state.x, state.v, alive_mask,\n",
    "        epsilon=epsilon_c,\n",
    "        lambda_alg=lambda_alg,\n",
    "        exclude_self=True\n",
    "    )\n",
    "    \n",
    "    x_companion_new = state.x[companions_new]\n",
    "    v_companion_new = state.v[companions_new]\n",
    "    pos_diff_sq_new = torch.sum((state.x - x_companion_new)**2, dim=-1)\n",
    "    vel_diff_sq_new = torch.sum((state.v - v_companion_new)**2, dim=-1)\n",
    "    raw_distances_new = torch.sqrt(pos_diff_sq_new + lambda_alg * vel_diff_sq_new)\n",
    "    \n",
    "    # Compute fitness with new weights\n",
    "    mu_d_new = raw_distances_new.mean()\n",
    "    sigma_d_new = raw_distances_new.std()\n",
    "    z_d_new = (raw_distances_new - mu_d_new) / (sigma_d_new + 1e-8)\n",
    "    d_prime_new = rescale(z_d_new)\n",
    "    V_fit_new = (d_prime_new ** beta) * (r_prime ** alpha)\n",
    "    \n",
    "    # Compute cloning probabilities\n",
    "    V_fit_companion_new = V_fit_new[companions_new]\n",
    "    cloning_scores_new = V_fit_companion_new - V_fit_new\n",
    "    cloning_probs_new = cloning_gate(cloning_scores_new)\n",
    "    \n",
    "    # Create visualization\n",
    "    sizes_new = 5 + 20 * cloning_probs_new.numpy()\n",
    "    \n",
    "    scatter_new = hv.Scatter(\n",
    "        (x_np[:, 0], x_np[:, 1], sizes_new, colors),\n",
    "        kdims=['x', 'y'],\n",
    "        vdims=['size', 'color']\n",
    "    ).opts(\n",
    "        opts.Scatter(\n",
    "            color='color',\n",
    "            size='size',\n",
    "            alpha=0.6,\n",
    "            width=600,\n",
    "            height=600,\n",
    "            title=f'Cloning Probabilities (ε_c={epsilon_c:.2f}, λ_alg={lambda_alg:.2f}, α={alpha:.2f}, β={beta:.2f})',\n",
    "            xlabel='Position x₁',\n",
    "            ylabel='Position x₂'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return scatter_new * centroid_marker\n",
    "\n",
    "# Create dashboard\n",
    "dashboard = pn.Column(\n",
    "    \"## Interactive Keystone Principle Explorer\",\n",
    "    \"Adjust parameters to see how they affect cloning probabilities.\",\n",
    "    pn.Row(epsilon_slider, lambda_slider),\n",
    "    pn.Row(alpha_slider, beta_slider),\n",
    "    update_visualization\n",
    ")\n",
    "\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run Euclidean Gas for multiple steps and record trajectory.\"\"\"\n",
    "\n",
    "# Create fresh initial state (same setup as before)\n",
    "torch.manual_seed(42)\n",
    "x_init_run = torch.cat([\n",
    "    cluster1_center + cluster1_std * torch.randn(N_cluster1, d, dtype=dtype),\n",
    "    cluster2_center + cluster2_std * torch.randn(N_cluster2, d, dtype=dtype)\n",
    "], dim=0)\n",
    "v_init_run = v_std * torch.randn(N, d, dtype=dtype)\n",
    "\n",
    "# Run parameters\n",
    "n_steps = 50\n",
    "record_every = 2  # Record every N steps for visualization\n",
    "\n",
    "# Run the algorithm\n",
    "trajectory = gas.run(n_steps, x_init=x_init_run, v_init=v_init_run)\n",
    "\n",
    "# Extract trajectory data\n",
    "x_traj = trajectory['x'].numpy()  # [n_steps+1, N, d]\n",
    "v_traj = trajectory['v'].numpy()  # [n_steps+1, N, d]\n",
    "var_x_traj = trajectory['var_x'].numpy()  # [n_steps+1]\n",
    "var_v_traj = trajectory['var_v'].numpy()  # [n_steps+1]\n",
    "\n",
    "print(f\"Run completed: {n_steps} steps\")\n",
    "print(f\"Initial variance: {var_x_traj[0]:.4f}\")\n",
    "print(f\"Final variance: {var_x_traj[-1]:.4f}\")\n",
    "print(f\"Variance reduction: {(var_x_traj[0] - var_x_traj[-1]):.4f} ({(1 - var_x_traj[-1]/var_x_traj[0]):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize variance reduction over time.\"\"\"\n",
    "\n",
    "# Create time series plot\n",
    "time_steps = np.arange(len(var_x_traj))\n",
    "\n",
    "# Position variance curve\n",
    "var_x_curve = hv.Curve(\n",
    "    (time_steps, var_x_traj),\n",
    "    kdims=['Step'],\n",
    "    vdims=['Position Variance'],\n",
    "    label='Position Variance'\n",
    ").opts(\n",
    "    color='blue',\n",
    "    line_width=2,\n",
    "    width=700,\n",
    "    height=400,\n",
    "    title='Variance Reduction Over Time',\n",
    "    xlabel='Step',\n",
    "    ylabel='Variance',\n",
    "    tools=['hover']\n",
    ")\n",
    "\n",
    "# Velocity variance curve\n",
    "var_v_curve = hv.Curve(\n",
    "    (time_steps, var_v_traj),\n",
    "    kdims=['Step'],\n",
    "    vdims=['Velocity Variance'],\n",
    "    label='Velocity Variance'\n",
    ").opts(\n",
    "    color='red',\n",
    "    line_width=2\n",
    ")\n",
    "\n",
    "# Overlay curves\n",
    "variance_plot = (var_x_curve * var_v_curve).opts(\n",
    "    opts.Curve(legend_position='top_right')\n",
    ")\n",
    "\n",
    "variance_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create animated visualization showing swarm evolution over time.\"\"\"\n",
    "\n",
    "# Select frames to visualize (every record_every steps)\n",
    "frame_indices = list(range(0, len(x_traj), record_every))\n",
    "\n",
    "# Create HoloMap for animation\n",
    "scatter_dict = {}\n",
    "\n",
    "for idx in frame_indices:\n",
    "    x_frame = x_traj[idx]  # [N, d]\n",
    "    \n",
    "    # Compute partition for this frame\n",
    "    mu_x_frame = x_frame.mean(axis=0, keepdims=True)\n",
    "    pos_error_frame = np.sqrt(np.sum((x_frame - mu_x_frame)**2, axis=-1))\n",
    "    threshold_frame = np.median(pos_error_frame)\n",
    "    high_error_frame = pos_error_frame > threshold_frame\n",
    "    \n",
    "    # Color code by partition\n",
    "    colors_frame = ['red' if he else 'blue' for he in high_error_frame]\n",
    "    \n",
    "    # Create scatter for this frame\n",
    "    scatter_frame = hv.Scatter(\n",
    "        (x_frame[:, 0], x_frame[:, 1], colors_frame),\n",
    "        kdims=['x', 'y'],\n",
    "        vdims=['color']\n",
    "    ).opts(\n",
    "        opts.Scatter(\n",
    "            color='color',\n",
    "            size=6,\n",
    "            alpha=0.6,\n",
    "            xlim=(-8, 8),\n",
    "            ylim=(-8, 8),\n",
    "            width=600,\n",
    "            height=600,\n",
    "            title=f'Step {idx}: Var = {var_x_traj[idx]:.3f}',\n",
    "            xlabel='Position x₁',\n",
    "            ylabel='Position x₂'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add centroid marker\n",
    "    centroid = hv.Scatter(\n",
    "        ([mu_x_frame[0, 0]], [mu_x_frame[0, 1]]),\n",
    "        label='Centroid'\n",
    "    ).opts(\n",
    "        color='black',\n",
    "        marker='x',\n",
    "        size=12,\n",
    "        line_width=2\n",
    "    )\n",
    "    \n",
    "    # Add origin marker (optimal point)\n",
    "    origin = hv.Scatter(\n",
    "        ([0], [0]),\n",
    "        label='Optimum'\n",
    "    ).opts(\n",
    "        color='green',\n",
    "        marker='+',\n",
    "        size=15,\n",
    "        line_width=3\n",
    "    )\n",
    "    \n",
    "    scatter_dict[idx] = scatter_frame * centroid * origin\n",
    "\n",
    "# Create HoloMap with time dimension\n",
    "swarm_evolution = hv.HoloMap(scatter_dict, kdims='Step')\n",
    "swarm_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize individual walker trajectories (trace paths).\"\"\"\n",
    "\n",
    "# Sample a subset of walkers to avoid clutter\n",
    "n_trace = 10  # Number of walkers to trace\n",
    "trace_indices = np.linspace(0, N-1, n_trace, dtype=int)\n",
    "\n",
    "# Split into two groups for visualization\n",
    "trace_outliers = trace_indices[:n_trace//2]  # Initial outliers\n",
    "trace_core = trace_indices[n_trace//2:]  # Initial core walkers\n",
    "\n",
    "# Create path overlays\n",
    "paths = []\n",
    "\n",
    "for i in trace_outliers:\n",
    "    path = hv.Path([x_traj[:, i, :]], label=f'Outlier {i}').opts(\n",
    "        color='red',\n",
    "        alpha=0.3,\n",
    "        line_width=1.5\n",
    "    )\n",
    "    paths.append(path)\n",
    "\n",
    "for i in trace_core:\n",
    "    path = hv.Path([x_traj[:, i, :]], label=f'Core {i}').opts(\n",
    "        color='blue',\n",
    "        alpha=0.3,\n",
    "        line_width=1.5\n",
    "    )\n",
    "    paths.append(path)\n",
    "\n",
    "# Combine all paths\n",
    "trajectory_plot = hv.Overlay(paths).opts(\n",
    "    opts.Path(\n",
    "        width=700,\n",
    "        height=700,\n",
    "        title='Walker Trajectories (Sample)',\n",
    "        xlabel='Position x₁',\n",
    "        ylabel='Position x₂',\n",
    "        xlim=(-8, 8),\n",
    "        ylim=(-8, 8)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add initial and final positions\n",
    "initial_scatter = hv.Scatter(\n",
    "    (x_traj[0, trace_indices, 0], x_traj[0, trace_indices, 1]),\n",
    "    label='Initial'\n",
    ").opts(\n",
    "    color='orange',\n",
    "    marker='o',\n",
    "    size=10,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "final_scatter = hv.Scatter(\n",
    "    (x_traj[-1, trace_indices, 0], x_traj[-1, trace_indices, 1]),\n",
    "    label='Final'\n",
    ").opts(\n",
    "    color='green',\n",
    "    marker='s',\n",
    "    size=10,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Add origin\n",
    "origin_marker = hv.Scatter(\n",
    "    ([0], [0]),\n",
    "    label='Optimum'\n",
    ").opts(\n",
    "    color='green',\n",
    "    marker='+',\n",
    "    size=20,\n",
    "    line_width=4\n",
    ")\n",
    "\n",
    "trajectory_full = trajectory_plot * initial_scatter * final_scatter * origin_marker\n",
    "trajectory_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Track high-error vs low-error population dynamics.\"\"\"\n",
    "\n",
    "# Compute partition statistics over time\n",
    "high_error_counts = []\n",
    "low_error_counts = []\n",
    "mean_distances_H = []\n",
    "mean_distances_L = []\n",
    "\n",
    "for t in range(len(x_traj)):\n",
    "    x_t = torch.tensor(x_traj[t])\n",
    "    mu_x_t = x_t.mean(dim=0, keepdim=True)\n",
    "    pos_error_t = torch.sqrt(torch.sum((x_t - mu_x_t)**2, dim=-1))\n",
    "    threshold_t = torch.median(pos_error_t)\n",
    "    high_error_t = pos_error_t > threshold_t\n",
    "    \n",
    "    high_error_counts.append(high_error_t.sum().item())\n",
    "    low_error_counts.append((~high_error_t).sum().item())\n",
    "    \n",
    "    # Track mean positional error for each set\n",
    "    mean_distances_H.append(pos_error_t[high_error_t].mean().item())\n",
    "    mean_distances_L.append(pos_error_t[~high_error_t].mean().item())\n",
    "\n",
    "high_error_counts = np.array(high_error_counts)\n",
    "low_error_counts = np.array(low_error_counts)\n",
    "mean_distances_H = np.array(mean_distances_H)\n",
    "mean_distances_L = np.array(mean_distances_L)\n",
    "\n",
    "# Create population dynamics plot\n",
    "high_error_curve = hv.Curve(\n",
    "    (time_steps, high_error_counts),\n",
    "    kdims=['Step'],\n",
    "    vdims=['Count'],\n",
    "    label='High-Error Set |H_k|'\n",
    ").opts(\n",
    "    color='red',\n",
    "    line_width=2\n",
    ")\n",
    "\n",
    "low_error_curve = hv.Curve(\n",
    "    (time_steps, low_error_counts),\n",
    "    kdims=['Step'],\n",
    "    vdims=['Count'],\n",
    "    label='Low-Error Set |L_k|'\n",
    ").opts(\n",
    "    color='blue',\n",
    "    line_width=2\n",
    ")\n",
    "\n",
    "population_plot = (high_error_curve * low_error_curve).opts(\n",
    "    opts.Curve(\n",
    "        width=700,\n",
    "        height=400,\n",
    "        title='Partition Population Dynamics',\n",
    "        xlabel='Step',\n",
    "        ylabel='Number of Walkers',\n",
    "        legend_position='right',\n",
    "        tools=['hover']\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create mean distance plot\n",
    "dist_H_curve = hv.Curve(\n",
    "    (time_steps, mean_distances_H),\n",
    "    kdims=['Step'],\n",
    "    vdims=['Distance'],\n",
    "    label='Mean |H_k| distance'\n",
    ").opts(\n",
    "    color='red',\n",
    "    line_width=2,\n",
    "    line_dash='dashed'\n",
    ")\n",
    "\n",
    "dist_L_curve = hv.Curve(\n",
    "    (time_steps, mean_distances_L),\n",
    "    kdims=['Step'],\n",
    "    vdims=['Distance'],\n",
    "    label='Mean |L_k| distance'\n",
    ").opts(\n",
    "    color='blue',\n",
    "    line_width=2,\n",
    "    line_dash='dashed'\n",
    ")\n",
    "\n",
    "distance_plot = (dist_H_curve * dist_L_curve).opts(\n",
    "    opts.Curve(\n",
    "        width=700,\n",
    "        height=400,\n",
    "        title='Mean Positional Error by Partition',\n",
    "        xlabel='Step',\n",
    "        ylabel='Mean Distance from Centroid',\n",
    "        legend_position='right',\n",
    "        tools=['hover']\n",
    "    )\n",
    ")\n",
    "\n",
    "# Layout vertically\n",
    "dynamics_layout = (population_plot + distance_plot).cols(1)\n",
    "dynamics_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation (Step 5: Time Evolution)\n",
    "\n",
    "The visualizations show:\n",
    "\n",
    "1. **Variance Reduction**: Both position and velocity variance decrease over time, demonstrating convergence\n",
    "2. **Animated Evolution**: The swarm progressively collapses from two distinct clusters toward the optimal point (origin)\n",
    "3. **Walker Trajectories**: Individual paths show how outliers (red) are systematically eliminated and replaced by clones from the core (blue)\n",
    "4. **Population Dynamics**: The partition between high-error and low-error sets evolves as the swarm converges\n",
    "\n",
    "**Key Insights**:\n",
    "- The Keystone Principle operates continuously throughout the run\n",
    "- High-error walkers are repeatedly identified and replaced\n",
    "- The centroid (black X) moves toward the optimum (green +)\n",
    "- Both sets converge in mean distance, indicating the swarm is becoming more concentrated\n",
    "\n",
    "**Theoretical Connection**: This demonstrates **Theorem 8.4** (Variance Reduction): The swarm exhibits exponential convergence to the low-variance regime, driven by the systematic elimination of geometric outliers through targeted cloning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fragile (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
