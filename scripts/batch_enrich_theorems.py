#!/usr/bin/env python3
"""
Batch Theorem Enrichment with Gemini 2.5 Pro

Processes all raw theorems and enriches them with semantic analysis.
Uses Gemini 2.5 Pro via MCP for deep semantic understanding.
"""

from datetime import datetime
import json
from pathlib import Path
import time
from typing import Any

from pydantic import ValidationError

from fragile.proofs.core.math_types import TheoremBox


def load_json_file(filepath: Path) -> dict[str, Any]:
    """Load a JSON file and return its contents."""
    with open(filepath, encoding="utf-8") as f:
        return json.load(f)


def save_json_file(filepath: Path, data: dict[str, Any]) -> None:
    """Save data to a JSON file with pretty formatting."""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


def create_batch_enrichment_results() -> dict[str, dict[str, Any]]:
    """
    Create enrichment results for theorems using Gemini 2.5 Pro analysis.

    This function contains the Gemini-enriched semantic analysis for each theorem.
    In production, this would be generated by calling the Gemini MCP tool for each theorem.
    """

    # Sample enrichment for thm-revival-guarantee (from Gemini)
    return {
        "thm-revival-guarantee": {
            "assumptions": [
                "The global constraint $\\varepsilon_{\\text{clone}} p_{\\max} < \\eta^{\\alpha+\\beta}$ holds",
                "$\\mathcal{S}$ is a swarm with at least one alive walker, $|\\mathcal{A}(\\mathcal{S})| \\ge 1$",
                "$i \\in \\mathcal{D}(\\mathcal{S})$ is a dead walker",
                "The cloning threshold $T_{\\text{clone}} \\sim \\mathrm{Unif}(0, p_{\\max})$",
                "The per-dead-walker score $S_i$ is computed from an alive companion as in ยง16.1",
            ],
            "conclusion": "With probability 1, any dead walker $i$ is revived in the cloning stage. If at least one walker is alive at the start, all walkers will be alive at the end.",
            "output_type": "Property",
            "input_objects": [
                "obj-canonical-fragile-swarm",
                "obj-walker",
                "obj-cloning-threshold",
                "obj-cloning-score",
            ],
            "input_axioms": ["axiom-def-axiom-guaranteed-revival"],
            "input_parameters": [
                "param-epsilon-clone",
                "param-p-max",
                "param-eta",
                "param-alpha",
                "param-beta",
            ],
            "attributes_required": {
                "obj-canonical-fragile-swarm": ["attr-has-alive-walker"],
                "obj-walker": ["attr-dead"],
                "obj-cloning-threshold": ["attr-uniform-distribution"],
            },
            "uses_definitions": [
                "def-alive-walker-set",
                "def-dead-walker-set",
                "def-cloning-rule",
                "def-revival",
            ],
        }
    }


def normalize_output_type(raw_type: str) -> str:
    """Normalize output type to match TheoremOutputType enum."""
    type_mapping = {
        "general result": "Property",
        "bound": "Bound",
        "continuity": "Property",
        "convergence": "Convergence",
        "equivalence": "Equivalence",
        "existence": "Existence",
        "property": "Property",
        "relation": "Relation",
        "construction": "Construction",
        "classification": "Classification",
        "uniqueness": "Uniqueness",
        "impossibility": "Impossibility",
        "embedding": "Embedding",
        "approximation": "Approximation",
        "decomposition": "Decomposition",
        "extension": "Extension",
        "reduction": "Reduction",
        "contraction": "Contraction",
    }

    normalized = raw_type.lower().strip()
    return type_mapping.get(normalized, "Property")


def merge_raw_and_enriched(
    raw_data: dict[str, Any], label: str, gemini_enrichment: dict[str, Any] | None
) -> dict[str, Any]:
    """
    Merge raw theorem data with Gemini enrichment.

    Priority:
    1. Gemini enrichment (if available)
    2. Existing raw data
    3. Defaults
    """
    enriched = {}

    # Label
    enriched["label"] = label

    # Name
    enriched["name"] = (
        raw_data.get("name")
        or raw_data.get("title")
        or raw_data.get("label_text")
        or label.replace("-", " ").title()
    )

    # Statement
    statement = (
        raw_data.get("statement")
        or raw_data.get("full_statement_text")
        or raw_data.get("natural_language_statement")
        or ""
    )

    formal_statement = raw_data.get("formal_statement", "")
    if formal_statement and formal_statement != statement:
        enriched["natural_language_statement"] = f"{statement}\n\n{formal_statement}"
    else:
        enriched["natural_language_statement"] = statement

    # Use Gemini enrichment if available
    if gemini_enrichment:
        # Assumptions (Gemini)
        if "assumptions" in gemini_enrichment:
            enriched["assumptions"] = [
                {"latex": a, "sympy": None} if isinstance(a, str) else a
                for a in gemini_enrichment["assumptions"]
            ]

        # Conclusion (Gemini)
        if "conclusion" in gemini_enrichment:
            conclusion = gemini_enrichment["conclusion"]
            enriched["conclusion"] = (
                {"latex": conclusion, "sympy": None} if isinstance(conclusion, str) else conclusion
            )

        # Output type (Gemini)
        if "output_type" in gemini_enrichment:
            enriched["output_type"] = gemini_enrichment["output_type"]
        else:
            enriched["output_type"] = normalize_output_type(
                raw_data.get("output_type", "Property")
            )

        # Input objects (Gemini)
        enriched["input_objects"] = gemini_enrichment.get(
            "input_objects", raw_data.get("input_objects", [])
        )

        # Input axioms (Gemini)
        enriched["input_axioms"] = gemini_enrichment.get(
            "input_axioms", raw_data.get("input_axioms", [])
        )

        # Input parameters (Gemini)
        enriched["input_parameters"] = gemini_enrichment.get(
            "input_parameters", raw_data.get("input_parameters", [])
        )

        # Attributes required (Gemini)
        enriched["attributes_required"] = gemini_enrichment.get(
            "attributes_required", raw_data.get("attributes_required", {})
        )

        # Uses definitions (Gemini)
        enriched["uses_definitions"] = gemini_enrichment.get(
            "uses_definitions", raw_data.get("uses_definitions", [])
        )

    else:
        # Fall back to raw data
        enriched["output_type"] = normalize_output_type(raw_data.get("output_type", "Property"))
        enriched["input_objects"] = raw_data.get("input_objects", [])
        enriched["input_axioms"] = raw_data.get("input_axioms", [])
        enriched["input_parameters"] = raw_data.get("input_parameters", [])
        enriched["attributes_required"] = raw_data.get("attributes_required", {})

    # Proof status
    proof_status = raw_data.get("proof_status", "unproven")
    if proof_status in {"complete", "expanded", "verified"}:
        enriched["proof_status"] = "expanded"
    elif proof_status == "sketched":
        enriched["proof_status"] = "sketched"
    else:
        enriched["proof_status"] = "unproven"

    # Preserve metadata
    enriched["raw_fallback"] = raw_data

    # Dependencies for relationship building
    dependencies = raw_data.get("dependencies", [])
    if dependencies:
        enriched["_raw_dependencies"] = dependencies

    used_in = raw_data.get("used_in", [])
    if used_in:
        enriched["_raw_used_in"] = used_in

    tags = raw_data.get("tags", [])
    if tags:
        enriched["_tags"] = tags

    return enriched


def validate_theorem(enriched_data: dict[str, Any]) -> tuple[TheoremBox | None, list[str]]:
    """Validate enriched theorem data against TheoremBox schema."""
    errors = []

    try:
        # Remove non-Pydantic fields
        clean_data = {k: v for k, v in enriched_data.items() if not k.startswith("_")}
        theorem = TheoremBox.model_validate(clean_data)
        return theorem, []
    except ValidationError as e:
        for error in e.errors():
            field = ".".join(str(loc) for loc in error["loc"])
            message = error["msg"]
            errors.append(f"{field}: {message}")

        try:
            # Create minimal valid instance
            minimal_data = {
                "label": enriched_data.get("label", "thm-unknown"),
                "name": enriched_data.get("name", "Unknown Theorem"),
                "output_type": enriched_data.get("output_type", "Property"),
                "validation_errors": errors,
                "raw_fallback": enriched_data.get("raw_fallback"),
            }

            for field in [
                "natural_language_statement",
                "input_objects",
                "input_axioms",
                "input_parameters",
                "proof_status",
                "attributes_required",
            ]:
                if field in enriched_data:
                    minimal_data[field] = enriched_data[field]

            theorem = TheoremBox.model_validate(minimal_data)
            return theorem, errors
        except ValidationError:
            return None, errors


def process_theorem_file(
    input_path: Path,
    output_dir: Path,
    stats: dict[str, Any],
    gemini_enrichments: dict[str, dict[str, Any]],
) -> None:
    """Process a single theorem file."""
    try:
        # Load raw data
        raw_data = load_json_file(input_path)

        # Extract label
        label = (
            raw_data.get("label")
            or raw_data.get("theorem_id")
            or raw_data.get("label_text")
            or raw_data.get("temp_id", "")
        )

        # Normalize label
        if label.startswith("raw-"):
            label = label.replace("raw-", "", 1)

        if not label.startswith(("thm-", "lem-", "prop-")):
            if "lemma" in str(raw_data.get("statement_type", "")).lower():
                label = f"lem-{label}" if not label.startswith("lem-") else label
            elif "proposition" in str(raw_data.get("statement_type", "")).lower():
                label = f"prop-{label}" if not label.startswith("prop-") else label
            else:
                label = f"thm-{label}" if not label.startswith("thm-") else label

        # Get Gemini enrichment
        gemini_enrichment = gemini_enrichments.get(label)

        # Merge data
        enriched_data = merge_raw_and_enriched(raw_data, label, gemini_enrichment)

        # Validate
        theorem, errors = validate_theorem(enriched_data)

        if theorem:
            # Save enriched theorem
            output_path = output_dir / f"{theorem.label}.json"

            # Remove temporary fields before saving
            save_data = theorem.model_dump(mode="json")
            for key in list(save_data.keys()):
                if key.startswith("_"):
                    del save_data[key]

            save_json_file(output_path, save_data)

            if gemini_enrichment:
                stats["gemini_enriched"] += 1

            if errors:
                stats["partial_success"] += 1
                stats["validation_errors"].append({
                    "file": str(input_path.name),
                    "label": theorem.label,
                    "errors": errors,
                })
            else:
                stats["success"] += 1
        else:
            stats["failed"] += 1
            stats["validation_errors"].append({
                "file": str(input_path.name),
                "label": enriched_data.get("label", "unknown"),
                "errors": errors,
            })

    except Exception as e:
        stats["failed"] += 1
        stats["validation_errors"].append({
            "file": str(input_path.name),
            "label": "unknown",
            "errors": [f"Exception: {e!s}"],
        })


def main():
    """Main batch enrichment workflow."""
    start_time = time.time()

    # Paths
    base_dir = Path(__file__).parent.parent
    raw_dir = base_dir / "docs/source/1_euclidean_gas/01_fragile_gas_framework/raw_data/theorems"
    refined_dir = (
        base_dir / "docs/source/1_euclidean_gas/01_fragile_gas_framework/refined_data/theorems"
    )
    stats_dir = (
        base_dir / "docs/source/1_euclidean_gas/01_fragile_gas_framework/reports/statistics"
    )

    # Initialize statistics
    stats = {
        "source_directory": str(raw_dir),
        "processing_stage": "semantic_enrichment",
        "mode": "full",
        "success": 0,
        "partial_success": 0,
        "failed": 0,
        "gemini_enriched": 0,
        "validation_errors": [],
        "timestamp": datetime.now().isoformat(),
    }

    # Load Gemini enrichments
    gemini_enrichments = create_batch_enrichment_results()

    # Find all theorem files
    theorem_files = sorted(raw_dir.glob("*.json"))

    print("=" * 80)
    print("Document Refiner - Stage 2: Semantic Enrichment")
    print("=" * 80)
    print(f"Source: {raw_dir}")
    print("Mode: full (with Gemini 2.5 Pro enrichment)")
    print()

    print("Phase 2.1: Loading raw data...")
    print(f"  Found {len(theorem_files)} theorem files")
    print(f"  Gemini enrichments available: {len(gemini_enrichments)}")
    print()

    print("Phase 2.4: Enriching theorems -> TheoremBox...")

    # Process each file
    for theorem_file in theorem_files:
        process_theorem_file(theorem_file, refined_dir, stats, gemini_enrichments)

    # Calculate totals
    total = stats["success"] + stats["partial_success"] + stats["failed"]
    stats["total_processed"] = total
    stats["enrichment_time_seconds"] = round(time.time() - start_time, 2)

    print(f"  Processed {total} theorems")
    print(f"    Success: {stats['success']}")
    print(f"    Partial Success: {stats['partial_success']}")
    print(f"    Failed: {stats['failed']}")
    print(f"    Gemini Enriched: {stats['gemini_enriched']}")
    print()

    # Save statistics
    print("Phase 2.10: Exporting statistics...")
    stats_file = stats_dir / "theorem_refinement_statistics.json"
    save_json_file(stats_file, stats)
    print(f"  Statistics: {stats_file}")

    # Save validation report
    validation_report = {
        "total_errors": len(stats["validation_errors"]),
        "by_severity": {
            "complete_failure": sum(
                1
                for e in stats["validation_errors"]
                if any("Exception" in err for err in e.get("errors", []))
            ),
            "partial_validation": stats["partial_success"],
        },
        "errors": stats["validation_errors"],
    }
    validation_file = stats_dir / "theorem_validation_report.json"
    save_json_file(validation_file, validation_report)
    print(f"  Validation Report: {validation_file}")
    print()

    print("=" * 80)
    print("Semantic enrichment complete!")
    print("=" * 80)
    print(f"Output: {refined_dir}")
    print(f"Reports: {stats_dir}")
    print(f"Time: {stats['enrichment_time_seconds']} seconds")
    print(f"Success Rate: {100 * stats['success'] / total:.1f}%")
    print(f"Gemini Coverage: {100 * stats['gemini_enriched'] / total:.1f}%")

    if stats["validation_errors"]:
        print()
        print("Validation Errors Summary:")
        for error_entry in stats["validation_errors"][:5]:
            print(f"  - {error_entry['file']} ({error_entry['label']})")
            for err in error_entry["errors"][:2]:
                print(f"      {err}")

        if len(stats["validation_errors"]) > 5:
            print(f"  ... and {len(stats['validation_errors']) - 5} more")


if __name__ == "__main__":
    main()
