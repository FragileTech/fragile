{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best reward found: 11811.0000 , efficiency 0.663, Critic: None\n",
      "Walkers iteration 2101 Best reward: 11811.00 Dead walkers: 15.62% Cloned: 23.44%\n",
      "\n",
      "Walkers States: \n",
      "id_walkers shape (64,) Mean: 248511217039606400.000, Std: 5155967118006030336.000, Max: 8759404405908455424.000 Min: -8939735341124625408.000\n",
      "compas_dist shape (64,) Mean: 31.500, Std: 18.473, Max: 63.000 Min: 0.000\n",
      "compas_clone shape (64,) Mean: 33.484, Std: 17.918, Max: 63.000 Min: 0.000\n",
      "processed_rewards shape (64,) Mean: 1.132, Std: 0.465, Max: 2.345 Min: 0.112\n",
      "virtual_rewards shape (64,) Mean: 1.067, Std: 0.038, Max: 1.235 Min: 1.000\n",
      "cum_rewards shape (64,) Mean: 11780.531, Std: 9.426, Max: 11811.000 Min: 11751.000\n",
      "distances shape (64,) Mean: 1.062, Std: 0.558, Max: 1.978 Min: 0.057\n",
      "clone_probs shape (64,) Mean: 0.087, Std: 0.103, Max: 0.393 Min: 0.000\n",
      "will_clone shape (64,) Mean: 0.234, Std: 0.424, Max: 1.000 Min: 0.000\n",
      "alive_mask shape (64,) Mean: 0.844, Std: 0.363, Max: 1.000 Min: 0.000\n",
      "end_condition shape (64,) Mean: 0.156, Std: 0.363, Max: 1.000 Min: 0.000\n",
      "best_reward_found shape None Mean: nan, Std: nan, Max: nan Min: nan\n",
      "best_found shape (128,) Mean: 82.258, Std: 80.210, Max: 255.000 Min: 0.000\n",
      "use_tree shape None Mean: nan, Std: nan, Max: nan Min: nan\n",
      "critic_score shape None Mean: nan, Std: nan, Max: nan Min: nan\n",
      "\n",
      "Env States: \n",
      "game_ends shape (64,) Mean: 0.000, Std: 0.000, Max: 0.000 Min: 0.000\n",
      "rewards shape (64,) Mean: 0.000, Std: 0.000, Max: 0.000 Min: 0.000\n",
      "ends shape (64,) Mean: 0.000, Std: 0.000, Max: 0.000 Min: 0.000\n",
      "\n",
      "Model States: \n",
      "dt shape (64,) Mean: 3.953, Std: 1.110, Max: 7.000 Min: 3.000\n",
      "actions shape (64,) Mean: 4.406, Std: 2.620, Max: 8.000 Min: 0.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from plangym import AtariEnvironment, ParallelEnvironment\n",
    "\n",
    "from fragile.atari.env import AtariEnv\n",
    "from fragile.core.dt_sampler import GaussianDt\n",
    "from fragile.core.env import DiscreteEnv\n",
    "from fragile.core.models import RandomDiscrete\n",
    "from fragile.core.states import States\n",
    "from fragile.core.swarm import Swarm\n",
    "from fragile.core.tree import HistoryTree\n",
    "from fragile.core.walkers import Walkers\n",
    "\n",
    "\n",
    "env = ParallelEnvironment(\n",
    "        env_class=AtariEnvironment,\n",
    "        name=\"MsPacman-ram-v0\",\n",
    "        clone_seeds=True,\n",
    "        autoreset=True,\n",
    "        blocking=False,\n",
    "    )\n",
    "\n",
    "dt = GaussianDt(min_dt=3, max_dt=1000, loc_dt=4, scale_dt=2)\n",
    "swarm = Swarm(\n",
    "    model=lambda x: RandomDiscrete(x, dt_sampler=dt),\n",
    "    walkers=Walkers,\n",
    "    env=lambda: AtariEnv(env),\n",
    "    n_walkers=64,\n",
    "    max_iters=10000,\n",
    "    prune_tree=True,\n",
    "    reward_scale=2,\n",
    "    minimize=False,\n",
    "    tree=HistoryTree,\n",
    "    use_tree=True,\n",
    ")\n",
    "\n",
    "_ = swarm.run_swarm(print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ix = swarm.walkers.states.cum_rewards.argmax()\n",
    "best = swarm.walkers.states.id_walkers[best_ix]\n",
    "path = swarm.tree.get_branch(best, from_hash=True)\n",
    "\n",
    "import time\n",
    "for s, a in zip(path[0][1:], path[1]):\n",
    "    env.step(state=s, action=a)\n",
    "    env.render()\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, obs = env.reset()\n",
    "\n",
    "states = [state.copy() for _ in range(10)]\n",
    "actions = [env.action_space.sample() for _ in range(10)]\n",
    "\n",
    "data = env.step_batch(states=states, actions=actions)\n",
    "new_states, observs, rewards, ends, infos = data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
