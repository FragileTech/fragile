================================================================================
ULTRA-DETAILED REVIEW REPORT: Section 4 "Relaxation Strategies" (Lines 764-966)
File: /home/guillem/fragile/docs/source/1_agent/08_multiagent/06_full_net.md
================================================================================

SUMMARY: Found 11 issues (6 CRITICAL, 3 IMPORTANT, 2 MINOR)

The file appears to be dynamically generated or auto-formatted, preventing direct edits.
This patch file contains all required changes for manual application.

================================================================================
CRITICAL ISSUES (Must Fix):
================================================================================

[ISSUE 1] Line 787 - Missing z ≠ 0 condition
------------------------------------------------------------
LOCATION: def-approximate-equivariance, supremum formula

CURRENT:
```
\sup_{g \in G, z \in \mathcal{Z}} \frac{\|f(\rho(g) z) - \rho(g) f(z)\|}{\|z\|} \leq \epsilon
```

PROBLEM: Division by ||z|| is undefined when z = 0.

FIX:
```
\sup_{g \in G, z \in \mathcal{Z}, z \neq 0} \frac{\|f(\rho(g) z) - \rho(g) f(z)\|}{\|z\|} \leq \epsilon
```

------------------------------------------------------------

[ISSUE 2] Line 792 - Missing measure specification
------------------------------------------------------------
LOCATION: def-approximate-equivariance, equivariance violation formula

CURRENT:
```
\mathcal{V}(f) := \mathbb{E}_{g \sim G, z \sim \mathcal{Z}} \left[ \|f(\rho(g) z) - \rho(g) f(z)\|^2 \right]
```

PROBLEM: For continuous groups like SO(d), need to specify measure (Haar measure).
         "g ~ G" is ambiguous for continuous groups.

FIX:
```
\mathcal{V}(f) := \mathbb{E}_{g \sim \mu_G, z \sim \mathcal{P}_{\mathcal{Z}}} \left[ \|f(\rho(g) z) - \rho(g) f(z)\|^2 \right]
```
is the **equivariance violation**, where $\mu_G$ is the Haar measure on $G$ (or uniform distribution for discrete groups) and $\mathcal{P}_{\mathcal{Z}}$ is a probability measure on $\mathcal{Z}$.

------------------------------------------------------------

[ISSUE 3] Line 796 - Clarify relationship between measures
------------------------------------------------------------
LOCATION: def-approximate-equivariance, Remark

CURRENT:
```
**Remark:** The supremum measure gives a worst-case bound, while $\mathcal{V}(f)$ measures average-case violation used in optimization.
```

FIX:
```
**Remark:** The supremum measure gives a worst-case bound $\epsilon_{\sup}$, while $\mathcal{V}(f)$ measures average-case violation used in optimization. These are related by $\mathcal{V}(f) \leq \epsilon_{\sup}^2 \mathbb{E}[\|z\|^2]$ but can differ significantly in practice.
```

------------------------------------------------------------

[ISSUE 4] Line 862 - Wrong directive type
------------------------------------------------------------
LOCATION: thm-l1-hierarchies header

CURRENT:
```
:::{prf:theorem} L1 and Hierarchies
:label: thm-l1-hierarchies
```

PROBLEM: Line 890 admits "This theorem is heuristic; rigorous proof requires analyzing stochastic gradient dynamics."
         Heuristic arguments should not be labeled as theorems.

FIX:
```
:::{prf:proposition} L1 and Hierarchies
:label: thm-l1-hierarchies
```

NOTE: Keep the label as is (thm-l1-hierarchies) to avoid breaking cross-references.

------------------------------------------------------------

[ISSUE 5] Lines 873-875 - INCORRECT sparsity formula
------------------------------------------------------------
LOCATION: thm-l1-hierarchies, Point 1 (Sparsity)

CURRENT:
```
1. **Sparsity:** The number of large (non-sparse) weights is bounded:
   $$
   \#\{|W_{ij}^{(k\ell)}| \geq \epsilon\} = O(\lambda_{\text{L1}}^{-1})
   $$
   Most weights are driven near zero by L1 penalty.
```

PROBLEM: The O(λ⁻¹) bound is INCORRECT without additional assumptions.
         Correct bound from L1 theory depends on task gradient magnitudes.

FIX:
```
1. **Sparsity:** The number of large (non-sparse) weights is bounded:
   $$
   \#\{|W_{ij}^{(k\ell)}| \geq \epsilon\} \leq \frac{C}{\lambda_{\text{L1}} \epsilon}
   $$
   where $C = O(\max_{ij} \|\partial \mathcal{L}_{\text{task}} / \partial W_{ij}\|_F)$ depends on task gradient magnitudes. Most weights are driven near zero by L1 penalty.
```

------------------------------------------------------------

[ISSUE 6] Lines 913-922 - CRITICAL BUG in L1Scheduler.step()
------------------------------------------------------------
LOCATION: L1Scheduler implementation, step() method

CURRENT:
```python
def step(self, current_violation: float):
    """Update λ_L1 based on measured violation.

    If violation < target: increase λ (push toward more symmetry)
    If violation > target: decrease λ (allow more flexibility)
    """
    error = current_violation - self.target
    self.lambda_L1 *= (1 - self.alpha * error)  # Multiplicative update
    self.lambda_L1 = max(1e-6, min(1.0, self.lambda_L1))  # Clamp
    return self.lambda_L1
```

PROBLEM: The logic is **BACKWARDS**.

Trace through the code:
- If current_violation > target (BAD - too much symmetry breaking):
    → error > 0
    → (1 - alpha * error) < 1
    → lambda_L1 DECREASES  ← WRONG! Should INCREASE to add more regularization

- If current_violation < target (GOOD - could afford more sparsity):
    → error < 0
    → (1 - alpha * error) > 1
    → lambda_L1 INCREASES  ← This part is correct

The docstring is also wrong. Correct behavior:
- High violation → increase λ (more regularization to reduce violation)
- Low violation → decrease λ (less regularization, allow more expressiveness)

FIX:
```python
def __init__(
    self,
    lambda_init: float = 0.01,
    target_violation: float = 0.1,
    adaptation_rate: float = 0.01,
    lambda_min: float = 1e-6,
    lambda_max: float = 1.0,
):
    self.lambda_L1 = lambda_init
    self.target = target_violation
    self.alpha = adaptation_rate
    self.lambda_min = lambda_min
    self.lambda_max = lambda_max

def step(self, current_violation: float):
    """Update λ_L1 based on measured violation.

    If violation > target: increase λ (add regularization to reduce violation)
    If violation < target: decrease λ (reduce regularization, allow more expressiveness)
    """
    error = current_violation - self.target
    self.lambda_L1 *= (1 + self.alpha * error)  # Multiplicative update (FIX: changed sign)
    self.lambda_L1 = max(self.lambda_min, min(self.lambda_max, self.lambda_L1))  # Clamp
    return self.lambda_L1
```

CHANGES:
1. Sign flip: (1 - alpha * error) → (1 + alpha * error)
2. Corrected docstring
3. Made clamping bounds configurable (added lambda_min, lambda_max parameters)

------------------------------------------------------------

================================================================================
IMPORTANT ISSUES (Should Fix):
================================================================================

[ISSUE 7] Line 802 - Missing regularity condition
------------------------------------------------------------
LOCATION: thm-approximate-equivariance-bound

CURRENT:
```
Let $f_{\text{equiv}}: \mathcal{Z} \to \mathcal{Z}$ be strictly $G$-equivariant and $f_{\text{break}}: \mathcal{Z} \to \mathcal{Z}$ be an arbitrary symmetry-breaking term.
```

PROBLEM: Doesn't state conditions ensuring expectation exists (is finite).

FIX:
```
Let $f_{\text{equiv}}: \mathcal{Z} \to \mathcal{Z}$ be strictly $G$-equivariant and $f_{\text{break}}: \mathcal{Z} \to \mathcal{Z}$ be an arbitrary symmetry-breaking term with $\mathbb{E}_{g,z} [\|f_{\text{break}}(\rho(g)z)\|^2] < \infty$.
```

------------------------------------------------------------

[ISSUE 8] Line 878 - Imprecise "insensitive" language
------------------------------------------------------------
LOCATION: thm-l1-hierarchies, Point 2 (Texture zeros)

CURRENT:
```
2. **Texture zeros:** Cross-bundle blocks $(i, j)$ where task loss $\mathcal{L}_{\text{task}}$ is insensitive to $W_{ij}$ have $\|W_{ij}\|_F \approx 0$ (driven to zero by L1 with no opposing gradient).
```

PROBLEM: "Insensitive" is informal. Need mathematical precision.
         Also needs to specify convergence to stationary point.

FIX:
```
2. **Texture zeros:** At a stationary point, cross-bundle blocks $(i, j)$ where $\|\partial \mathcal{L}_{\text{task}} / \partial W_{ij}\|_F < \lambda_{\text{L1}}$ satisfy $W_{ij} = 0$ (driven to zero by L1 with no opposing gradient).
```

------------------------------------------------------------

[ISSUE 9] Lines 880, 884, 888 - Weaken hierarchy claim and fix soft thresholding
------------------------------------------------------------
LOCATION: thm-l1-hierarchies, Point 3 and proof sketch

CURRENT (Line 880):
```
3. **Hierarchy:** Non-zero weights organize into levels with exponential decay (approximately): if $|W^{(1)}| > |W^{(2)}| > \cdots$ are sorted magnitudes, then $|W^{(k+1)}| / |W^{(k)}| \approx \text{const} < 1$.
```

CURRENT (Line 884):
```
**(1) Sparsity:** L1 penalty creates a "soft thresholding" effect. Weights with $|W| < \lambda_{\text{L1}} / |\partial \mathcal{L}_{\text{task}} / \partial W|$ are driven to zero (penalty gradient dominates task gradient). The number of weights exceeding this threshold is $O(1/\lambda_{\text{L1}})$.
```

CURRENT (Line 888):
```
**(3) Hierarchy:** During training, once a weight crosses zero (becomes active), it can grow under task gradients. But nearby small weights continue to be suppressed by L1. This creates a "rich get richer" dynamic: large weights grow, small weights vanish. The distribution becomes hierarchical.
```

PROBLEMS:
- Line 880: Exponential decay is not rigorously proven
- Line 884: Soft thresholding formula is WRONG (should not have division)
- Line 888: Needs acknowledgment that exponential spacing is empirical

FIX (Line 870-880):
```
Under mild regularity conditions (task loss differentiable, optimizer converges to a stationary point), the following hold at convergence:

1. **Sparsity:** The number of large (non-sparse) weights is bounded:
   $$
   \#\{|W_{ij}^{(k\ell)}| \geq \epsilon\} \leq \frac{C}{\lambda_{\text{L1}} \epsilon}
   $$
   where $C = O(\max_{ij} \|\partial \mathcal{L}_{\text{task}} / \partial W_{ij}\|_F)$ depends on task gradient magnitudes. Most weights are driven near zero by L1 penalty.

2. **Texture zeros:** At a stationary point, cross-bundle blocks $(i, j)$ where $\|\partial \mathcal{L}_{\text{task}} / \partial W_{ij}\|_F < \lambda_{\text{L1}}$ satisfy $W_{ij} = 0$ (driven to zero by L1 with no opposing gradient).

3. **Hierarchy:** Non-zero weights tend to organize into levels with the largest weights significantly exceeding smaller ones. Empirically, sorted magnitudes often exhibit approximately exponential decay, though rigorous bounds depend on task structure.
```

FIX (Line 884-889):
```
**(1) Sparsity:** L1 penalty creates a "soft thresholding" effect. Weights with $|\partial \mathcal{L}_{\text{task}} / \partial W| < \lambda_{\text{L1}}$ are driven to zero (penalty gradient dominates task gradient). The number of weights exceeding this threshold is bounded by $C/(\lambda_{\text{L1}} \epsilon)$ where $C$ depends on the maximum task gradient.

**(2) Texture zeros:** If $\partial \mathcal{L}_{\text{task}} / \partial W_{ij} = 0$ (block irrelevant to task), the total gradient is purely from L1: $\partial \mathcal{L}_{\text{total}} / \partial W_{ij} = \lambda_{\text{L1}} \cdot \text{sign}(W_{ij})$, which pushes $W_{ij} \to 0$ at stationarity.

**(3) Hierarchy:** During training, once a weight crosses zero (becomes active), it can grow under task gradients. But nearby small weights continue to be suppressed by L1. This creates a "rich get richer" dynamic: large weights grow, small weights vanish. The distribution tends to become hierarchical, with empirical observations showing approximately exponential spacing in many cases.
```

FIX (Line 890):
```
**Note on rigor:** This proposition presents heuristic arguments based on proximal gradient analysis. Rigorous proofs require analyzing stochastic gradient dynamics and are an active area of research in optimization theory.
```

------------------------------------------------------------

================================================================================
MINOR ISSUES (Recommended):
================================================================================

[ISSUE 10] Lines 899-900 - Docstring class comment
------------------------------------------------------------
LOCATION: L1Scheduler class docstring

CURRENT:
```
"""Adaptive L1 regularization schedule.

If violation < target: increase λ (network has capacity for more sparsity)
If violation > target: decrease λ (reduce regularization, give network flexibility)
"""
```

PROBLEM: After fixing Issue 6, this docstring also needs updating.

FIX:
```
"""Adaptive L1 regularization schedule.

Maintains target equivariance violation by adjusting L1 penalty strength:
- High violation: increase λ to add regularization
- Low violation: decrease λ to allow more expressiveness
"""
```

------------------------------------------------------------

[ISSUE 11] Line 870 - Add stationarity condition
------------------------------------------------------------
LOCATION: thm-l1-hierarchies, regularity conditions

CURRENT:
```
Under mild regularity conditions (task loss differentiable, optimizer converges), the following hold at convergence:
```

FIX:
```
Under mild regularity conditions (task loss differentiable, optimizer converges to a stationary point), the following hold at convergence:
```

------------------------------------------------------------

================================================================================
VERIFIED CORRECT (No Changes Needed):
================================================================================

✓ Lines 816-832: thm-approximate-equivariance-bound proof is CORRECT
  - Correctly derives equality (not inequality)
  - All algebraic steps are valid
  - Conclusion that violation ~ λ² is correct

✓ Lines 849-856: Comparison table is ACCURATE
  - All four strategies correctly described
  - Tradeoffs are appropriate
  - Use cases are well-matched

✓ Lines 937-968: log_sparsity_diagnostics code is CORRECT
  - Sparsity calculation: correct
  - Texture zero detection: correct (Frobenius norm per block)
  - Magnitude sorting: correct

================================================================================
APPLICATION INSTRUCTIONS:
================================================================================

1. The file appears to be auto-generated or auto-formatted, preventing direct Edit tool usage.

2. Apply fixes manually in order of priority:
   - CRITICAL issues first (especially Issue 6 - the L1Scheduler bug)
   - IMPORTANT issues second
   - MINOR issues last

3. After applying fixes, verify no cross-references broke:
   ```bash
   grep -r "thm-l1-hierarchies" docs/source/
   grep -r "def-approximate-equivariance" docs/source/
   ```

4. Test the L1Scheduler fix with a simple unit test:
   ```python
   scheduler = L1Scheduler(lambda_init=0.01, target_violation=0.1)

   # High violation should increase lambda
   lambda_before = scheduler.lambda_L1
   scheduler.step(current_violation=0.2)  # > target
   assert scheduler.lambda_L1 > lambda_before, "Lambda should increase for high violation"

   # Low violation should decrease lambda
   lambda_before = scheduler.lambda_L1
   scheduler.step(current_violation=0.05)  # < target
   assert scheduler.lambda_L1 < lambda_before, "Lambda should decrease for low violation"
   ```

5. Check theorem/proposition numbering is consistent throughout document.

================================================================================
END OF REPORT
================================================================================
