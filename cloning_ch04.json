{
  "$schema": "./math_schema.json",
  "metadata": {
    "title": "The Keystone Principle and the Contractive Nature of Cloning - Chapter 4",
    "document_id": "cloning_ch04",
    "version": "1.0.0",
    "date_created": "2025-10-25",
    "source_document": "docs/source/1_euclidean_gas/03_cloning.md",
    "source_lines": "1142-1334",
    "chapters_covered": ["4. Foundational Assumptions and System Properties"],
    "authors": ["Guillem Duran Ballester"],
    "tags": ["axioms", "foundational-assumptions", "environmental-axioms", "learnability", "velocity-regularization", "diversity-signal"],
    "description": "Complete set of foundational axioms that any valid Fragile Gas instantiation must satisfy for convergence guarantees. Organized into Environmental, Measurement & Signal, and Algorithmic Dynamics axioms.",
    "dependencies": {
      "requires": ["cloning_ch01_ch02.json"],
      "builds_on": ["Fragile Gas framework", "Bounded domain", "Regularity assumptions"]
    },
    "peer_review_status": {
      "status": "in_progress",
      "notes": "Initial extraction from source document"
    }
  },
  "directives": [
    {
      "type": "axiom",
      "label": "ax:lipschitz-fields",
      "title": "(Axiom EG-1): Lipschitz Regularity of Environmental Fields",
      "statement": "The deterministic fields governing the system's kinetic dynamics are locally smooth and globally well-behaved on the compact valid domain $\\mathcal{X}_{\\mathrm{valid}}$. Specifically, there exist finite constants $L_F$ and $L_u$ such that for all $x_1, x_2 \\in \\mathcal{X}_{\\mathrm{valid}}$:\n\n1. **Force Field:** $\\|F(x_1) - F(x_2)\\| \\leq L_F \\|x_1 - x_2\\|$\n2. **Steady Flow Field:** $\\|u(x_1) - u(x_2)\\| \\leq L_u \\|x_1 - x_2\\|$",
      "category": "regularity",
      "axiomatic_parameters": [
        {
          "symbol": "L_F",
          "description": "Lipschitz constant for force field",
          "conditions": "Finite positive constant",
          "typical_values": "Depends on potential function $U(x)$",
          "sensitivity": "Critical for hypocoercive analysis - unbounded $L_F$ breaks drift bounds"
        },
        {
          "symbol": "L_u",
          "description": "Lipschitz constant for steady flow field",
          "conditions": "Finite positive constant",
          "typical_values": "Depends on flow field structure",
          "sensitivity": "Essential for continuity of kinetic operator"
        }
      ],
      "rationale": "This is a standard regularity assumption that ensures the kinetic dynamics do not have infinite gradients or instantaneous velocities, which is essential for the hypocoercive analysis. It guarantees that the one-step change in any walker's state is a well-behaved function of its current state.",
      "failure_modes": [
        {
          "condition": "Force or flow fields are not Lipschitz (discontinuities or unbounded derivatives)",
          "consequence": "Kinetic update operator $\\Psi_{\\text{kin}}$ is no longer continuous",
          "diagnostic": "Small pre-kinetic position changes cause arbitrarily large post-kinetic jumps; hypocoercive drift analysis fails; system dynamics become chaotic"
        }
      ],
      "tags": ["environmental-axiom", "lipschitz", "regularity", "kinetic-dynamics"],
      "related_concepts": [
        {
          "label": "hypocoercive-analysis",
          "description": "Lipschitz regularity is prerequisite for controlled dissipation transfer",
          "relationship": "enables"
        }
      ]
    },
    {
      "type": "axiom",
      "label": "ax:safe-harbor",
      "title": "(Axiom EG-2): Existence of a Safe Harbor",
      "statement": "There exists a compact set $C_{\\mathrm{safe}} \\subset \\mathcal{X}_{\\mathrm{valid}}$ and a reward threshold $R_{\\mathrm{safe}}$ such that:\n\n1. $C_{\\mathrm{safe}}$ lies strictly inside the valid domain: $d(x, \\partial \\mathcal{X}_{\\mathrm{valid}}) \\geq \\delta_{\\mathrm{safe}} > 0$ for every $x \\in C_{\\mathrm{safe}}$.\n2. The positional reward is strictly better inside the safe harbor: $\\max_{y \\in C_{\\mathrm{safe}}} R_{\\mathrm{pos}}(y) \\geq R_{\\mathrm{safe}}$ and $R_{\\mathrm{pos}}(x) < R_{\\mathrm{safe}}$ for all $x \\notin C_{\\mathrm{safe}}$.",
      "category": "environmental",
      "axiomatic_parameters": [
        {
          "symbol": "C_{\\mathrm{safe}}",
          "description": "Safe harbor region (compact subset strictly inside valid domain)",
          "conditions": "Compact, $d(x, \\partial \\mathcal{X}_{\\mathrm{valid}}) \\geq \\delta_{\\mathrm{safe}} > 0$ for all $x \\in C_{\\mathrm{safe}}$",
          "typical_values": "Interior region away from boundary",
          "sensitivity": "Size and location determine extinction probability"
        },
        {
          "symbol": "R_{\\mathrm{safe}}",
          "description": "Reward threshold distinguishing safe harbor from boundary",
          "conditions": "Achievable within safe harbor, not achievable outside",
          "typical_values": "Depends on reward landscape",
          "sensitivity": "Determines strength of boundary contraction"
        },
        {
          "symbol": "\\delta_{\\mathrm{safe}}",
          "description": "Minimum distance from safe harbor to boundary",
          "conditions": "Strictly positive",
          "typical_values": "Application-dependent",
          "sensitivity": "Larger values increase robustness to noise"
        }
      ],
      "rationale": "This structural assumption on the reward landscape is the engine for the boundary potential's contractive drift. It guarantees that walkers near the boundary are demonstrably 'unfit' compared to those in the interior, ensuring they will be preferentially cloned inwards. This provides the inward pull necessary to counteract the diffusive expansion from the kinetic noise.",
      "failure_modes": [
        {
          "condition": "No safe harbor exists (highest reward near boundary)",
          "consequence": "Cloning drives swarm toward boundary instead of away; boundary potential $W_b$ experiences expansive drift; swarm extinction becomes inevitable",
          "diagnostic": "Drift inequality for $W_b$ fails; extinction probability increases exponentially"
        }
      ],
      "tags": ["environmental-axiom", "safe-harbor", "boundary-control", "reward-landscape"],
      "related_concepts": [
        {
          "label": "boundary-drift-contraction",
          "description": "Safe harbor axiom enables Chapter 11's proof of $W_b$ contraction",
          "relationship": "enables"
        }
      ]
    },
    {
      "type": "axiom",
      "label": "ax:non-deceptive-landscape",
      "title": "(Axiom EG-3): Non-Deceptive Landscape",
      "statement": "The environment is **non-deceptive**. A sufficient geometric separation between two walkers guarantees a minimal, non-zero difference in their raw positional rewards. Formally, there exist constants $L_{\\text{grad}} > 0$ and $\\kappa_{\\text{raw},r} > 0$ such that:\n\nIf $\\|x - y\\| \\geq L_{\\text{grad}}$, then $|R_{\\mathrm{pos}}(y) - R_{\\mathrm{pos}}(x)| \\geq \\kappa_{\\text{raw},r}$.",
      "category": "learnability",
      "axiomatic_parameters": [
        {
          "symbol": "L_{\\text{grad}}",
          "description": "Minimum distance for guaranteed reward difference",
          "conditions": "Positive constant",
          "typical_values": "Depends on reward landscape scale",
          "sensitivity": "Determines detectability of geometric diversity"
        },
        {
          "symbol": "\\kappa_{\\text{raw},r}",
          "description": "Minimum reward difference for separated walkers",
          "conditions": "Strictly positive",
          "typical_values": "Proportional to reward landscape gradients",
          "sensitivity": "Critical for Keystone Principle - zero value causes deceptive plateau failure"
        }
      ],
      "rationale": "This is the most important 'learnability' axiom. It forges the critical link between the geometric diversity signal and the reward signal, preventing the algorithm from getting stuck on deceptive plateaus. It is the direct input for proving the 'intelligence' of the fitness metric in the Keystone Principle.",
      "failure_modes": [
        {
          "condition": "Deceptive plateau: large geometric diversity with constant reward",
          "consequence": "Signal decoupling - diversity channel active, reward channel dead; fitness dominated by geometry alone; swarm churns indefinitely without progress",
          "diagnostic": "Loss of corrective feedback loop in Keystone Principle (Chapter 7); cloning becomes random reshuffling"
        }
      ],
      "tags": ["learnability-axiom", "non-deceptive", "reward-signal", "keystone-prerequisite"],
      "related_concepts": [
        {
          "label": "keystone-principle",
          "description": "Non-deceptive landscape is essential for intelligent fitness targeting (Chapter 7)",
          "relationship": "critical-for"
        }
      ]
    },
    {
      "type": "axiom",
      "label": "ax:velocity-regularization",
      "title": "(Axiom EG-4): Velocity Regularization via Reward",
      "statement": "The total reward function $R(x,v)$ is designed to actively penalize high kinetic energy. It is composed of the positional reward $R_{\\text{pos}}(x)$ and a quadratic velocity regularization term:\n\n$$R_{\\text{total}}(x, v) := R_{\\text{pos}}(x) - c_{v\\_reg} \\|v\\|^2$$\n\nwhere $c_{v\\_reg}$ is a strictly positive constant $c_{v\\_reg} > 0$.",
      "category": "algorithmic-dynamics",
      "axiomatic_parameters": [
        {
          "symbol": "c_{v\\_reg}",
          "description": "Velocity regularization coefficient",
          "conditions": "Strictly positive: $c_{v\\_reg} > 0$",
          "typical_values": "Tuned to balance position optimization vs. kinetic stability",
          "sensitivity": "Critical for synergistic dissipation - zero value causes unbounded velocity variance expansion"
        }
      ],
      "rationale": "This axiom is a critical safety mechanism within the synergistic dissipation framework. While the cloning operator contracts positional variance $V_{\\text{Var},x}$ but causes bounded expansion of velocity variance $V_{\\text{Var},v}$, the velocity regularization term ensures this expansion remains controlled and bounded.\n\n**Two key functions:**\n1. **Bounding velocity variance expansion**: Walkers with anomalously high velocity become 'unfit' and are preferentially cloned, resetting to companion velocities.\n2. **Enabling kinetic dissipation**: Prevents kinetic energy from growing beyond the capacity of Langevin friction to dissipate it.",
      "failure_modes": [
        {
          "condition": "Velocity regularization disabled ($c_{v\\_reg} = 0$)",
          "consequence": "Unbounded velocity variance expansion - high-velocity walkers persist and proliferate; synergistic framework breaks; kinetic instability and increased extinction risk",
          "diagnostic": "Velocity component of Lyapunov function fails to contract; $V_{\\text{Var},v}$ grows without bound"
        }
      ],
      "tags": ["algorithmic-axiom", "velocity-regularization", "synergistic-dissipation", "kinetic-stability"],
      "related_concepts": [
        {
          "label": "synergistic-lyapunov",
          "description": "Velocity regularization ensures cloning's $V_{\\text{Var},v}$ expansion remains bounded (Chapter 3)",
          "relationship": "enables"
        }
      ]
    },
    {
      "type": "axiom",
      "label": "ax:active-diversity",
      "title": "(Axiom EG-5): Active Diversity Signal",
      "statement": "The diversity channel of the fitness potential is active. The dynamics weight $\\beta$ is strictly positive:\n\n$$\\beta > 0$$",
      "category": "algorithmic-dynamics",
      "axiomatic_parameters": [
        {
          "symbol": "\\beta",
          "description": "Diversity channel weight in fitness potential",
          "conditions": "Strictly positive: $\\beta > 0$",
          "typical_values": "Tuned relative to reward weight $\\alpha$",
          "sensitivity": "Critical for Keystone Principle - zero value eliminates convergence detection"
        }
      ],
      "rationale": "This is a fundamental assumption for the Keystone Principle's proof of intelligent targeting. It ensures that the algorithm pays attention to the reliable geometric signal generated by the **phase-space** companion kernel. This signal is the primary mechanism that allows the algorithm to detect its own lack of convergence and escape deceptive reward landscapes.",
      "failure_modes": [
        {
          "condition": "Diversity signal disabled ($\\beta = 0$)",
          "consequence": "Fitness becomes $V_{\\text{fit}} = (r')^\\alpha$ - blind to geometric diversity; loss of convergence detection; on flat plateaus, all walkers have same fitness, cloning ceases, swarm stalls",
          "diagnostic": "Entire Keystone Principle fails; no mechanism to force adaptation"
        }
      ],
      "tags": ["algorithmic-axiom", "diversity-signal", "keystone-prerequisite", "convergence-detection"],
      "related_concepts": [
        {
          "label": "keystone-principle",
          "description": "Active diversity is foundation for fitness gap creation (Chapter 7-8)",
          "relationship": "critical-for"
        }
      ]
    }
  ],
  "dependency_graph": {
    "nodes": [
      {"label": "ax:lipschitz-fields", "type": "axiom", "level": 0},
      {"label": "ax:safe-harbor", "type": "axiom", "level": 0},
      {"label": "ax:non-deceptive-landscape", "type": "axiom", "level": 0},
      {"label": "ax:velocity-regularization", "type": "axiom", "level": 0},
      {"label": "ax:active-diversity", "type": "axiom", "level": 0}
    ],
    "edges": []
  },
  "constants_glossary": {
    "L_F": {
      "symbol": "L_F",
      "value": "finite positive constant (system-dependent)",
      "description": "Lipschitz constant for force field $F(x)$",
      "units": "1/length",
      "defined_in": "ax:lipschitz-fields",
      "used_in": ["ax:lipschitz-fields"]
    },
    "L_u": {
      "symbol": "L_u",
      "value": "finite positive constant (system-dependent)",
      "description": "Lipschitz constant for steady flow field $u(x)$",
      "units": "dimensionless",
      "defined_in": "ax:lipschitz-fields",
      "used_in": ["ax:lipschitz-fields"]
    },
    "delta_safe": {
      "symbol": "\\delta_{\\mathrm{safe}}",
      "value": "strictly positive (application-dependent)",
      "description": "Minimum distance from safe harbor to boundary",
      "units": "length",
      "defined_in": "ax:safe-harbor",
      "used_in": ["ax:safe-harbor"]
    },
    "R_safe": {
      "symbol": "R_{\\mathrm{safe}}",
      "value": "application-dependent threshold",
      "description": "Reward threshold distinguishing safe harbor",
      "units": "reward units",
      "defined_in": "ax:safe-harbor",
      "used_in": ["ax:safe-harbor"]
    },
    "L_grad": {
      "symbol": "L_{\\text{grad}}",
      "value": "positive constant (landscape-dependent)",
      "description": "Minimum distance for guaranteed reward difference",
      "units": "length",
      "defined_in": "ax:non-deceptive-landscape",
      "used_in": ["ax:non-deceptive-landscape"]
    },
    "kappa_raw_r": {
      "symbol": "\\kappa_{\\text{raw},r}",
      "value": "strictly positive (landscape-dependent)",
      "description": "Minimum reward difference for separated walkers",
      "units": "reward units",
      "defined_in": "ax:non-deceptive-landscape",
      "used_in": ["ax:non-deceptive-landscape"]
    },
    "c_v_reg": {
      "symbol": "c_{v\\_reg}",
      "value": "strictly positive tuning parameter",
      "description": "Velocity regularization coefficient",
      "units": "reward per velocity^2",
      "defined_in": "ax:velocity-regularization",
      "used_in": ["ax:velocity-regularization"]
    },
    "beta": {
      "symbol": "\\beta",
      "value": "strictly positive tuning parameter",
      "description": "Diversity channel weight in fitness potential",
      "units": "dimensionless",
      "defined_in": "ax:active-diversity",
      "used_in": ["ax:active-diversity"]
    }
  },
  "notation_index": {
    "F(x)": "Force field (negative gradient of potential)",
    "u(x)": "Steady flow field",
    "C_{\\mathrm{safe}}": "Safe harbor region (compact subset)",
    "R_{\\mathrm{pos}}(x)": "Positional reward function",
    "R_{\\text{total}}(x,v)": "Total reward including velocity regularization",
    "\\Psi_{\\text{kin}}": "Kinetic operator",
    "\\Psi_{\\text{clone}}": "Cloning operator",
    "V_{\\text{Var},x}": "Positional variance component of Lyapunov function",
    "V_{\\text{Var},v}": "Velocity variance component of Lyapunov function",
    "W_b": "Boundary potential term in Lyapunov function",
    "V_{\\text{fit}}": "Fitness potential"
  }
}
