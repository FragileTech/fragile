{
  "stage": "directives",
  "directive_type": "algorithm",
  "generated_at": "2025-11-09T10:55:47.869675+00:00",
  "source_documents": [
    "02_euclidean_gas",
    "03_cloning",
    "06_convergence"
  ],
  "document_count": 3,
  "total_count": 5,
  "items": [
    {
      "directive_type": "algorithm",
      "label": "alg-euclidean-gas",
      "title": "Euclidean Gas Update",
      "start_line": 153,
      "end_line": 199,
      "header_lines": [
        154
      ],
      "content_start": 156,
      "content_end": 198,
      "content": "156: :label: alg-euclidean-gas\n157: \n158: Given a swarm state $\\mathcal S_t=(w_1,\\dots,w_N)$ with walkers $w_i=(x_i,v_i,s_i)$, the Euclidean Gas performs one update as follows:\n159: \n160: 1.  **Cemetery check.** If all walkers are dead (no alive indices in $\\mathcal A_t$) return the cemetery state; otherwise continue.\n161: 2.  **Measurement stage.** For every alive walker $i\\in\\mathcal A_t$ sample a companion $c_{\\mathrm{pot}}(i)$ from the algorithmic distance-weighted kernel $\\mathbb C_\\epsilon(\\mathcal S_t,i)$, then compute raw reward $r_i:=R(x_i,v_i)$ and algorithmic distance $d_i:=d_{\\text{alg}}(i,c_{\\mathrm{pot}}(i))$ as defined in Section 3.3 and detailed in {ref}`Stage 2 <sec-eg-stage2>`.\n162: 3.  **Patched standardisation.** Aggregate the raw reward and distance vectors with the empirical operator and apply the regularized standard deviation from {prf:ref}`def-statistical-properties-measurement` to obtain standardized scores with floor $\\sigma'_{\\min,\\mathrm{patch}} = \\sqrt{\\kappa_{\\mathrm{var,min}}+\\varepsilon_{\\mathrm{std}}^2}$.\n163: 4.  **Logistic rescale.** Apply the Canonical Logistic Rescale Function ({prf:ref}`def-canonical-logistic-rescale-function-example`) to the standardized reward and distance components, producing positive outputs $r'_i$ and $d'_i$. Combine them with the canonical exponents to freeze the potential vector $V_{\\text{fit},i}=(d'_i)^\\beta (r'_i)^\\alpha$ with floor $\\eta^{\\alpha+\\beta}$.\n164: 5.  **Clone/Persist gate.** For each walker draw a clone companion $c_{\\mathrm{clone}}(i)$ from the same algorithmic distance-weighted kernel and threshold $T_i\\sim\\mathrm{Unif}(0,p_{\\max})$, compute the canonical score $S_i:=\\big(V_{\\text{fit},c_{\\mathrm{clone}}(i)}-V_{\\text{fit},i}\\big)/(V_{\\text{fit},i}+\\varepsilon_{\\mathrm{clone}})$, and clone when $S_i>T_i$. Cloned walkers are grouped by companion and undergo a momentum-conserving inelastic collision: positions reset to the companion's position plus Gaussian jitter ($\\sigma_x$), while velocities are updated via center-of-mass calculation with random rotation and restitution coefficient $\\alpha_{\\text{restitution}}$, as detailed in {ref}`Stage 3 <sec-eg-stage3>` and Definition 5.7.4 of `03_cloning.md`. Otherwise the walker persists unchanged. The intermediate swarm sets every status to alive before the kinetic step.\n165: 6.  **Kinetic perturbation.** Update each alive clone or survivor by applying the **BAOAB splitting integrator** for one step of underdamped Langevin dynamics with force $F(x)=\\nabla R_{\\mathrm{pos}}(x)$ and noise scales $(\\sigma_v,\\sigma_x)$.\n166: 7.  **Status refresh.** Set the new status $s_i^{(t+1)}=\\mathbf 1_{\\mathcal X_{\\mathrm{valid}}}(x_i^+)$ and output the updated swarm $\\mathcal S_{t+1}$.\n167: \n168: **Euclidean Gas Algorithm**\n169: \n170: $$\n171: \\begin{aligned}\n172: & \\textbf{Input:} \\mathcal S_t = \\{(x_i^{(t)}, v_i^{(t)}, s_i^{(t)})\\}_{i=1}^N\\text{; and parameters } \\alpha, \\beta, \\varepsilon_{\\mathrm{std}}, \\eta, \\tau, p_{\\max}, \\varepsilon_{\\mathrm{clone}}, \\sigma_x, \\alpha_{\\text{restitution}}, \\sigma_v, \\\\\n173: & \\qquad \\sigma'_{\\mathrm{patch}}, g_A, \\mathbb C_i, Q_{\\delta}, \\Psi_{\\mathrm{kin,BAOAB}}. \\\\\n174: & \\textbf{If } |\\mathcal A_t| = 0: \\textbf{ return } \\delta_{\\mathcal S_t} \\quad \\text{\\# Cemetery absorption} \\\\\n175: \\\\\n176: & \\underline{\\text{Stage 2a: Raw vectors on alive set}} \\\\\n177: & \\dots \\quad \\text{\\# Unchanged} \\\\\n178: \\\\\n179: & \\underline{\\text{Stage 2b: Patched standardisation}} \\\\\n180: & \\dots \\quad \\text{\\# Unchanged} \\\\\n181: \\\\\n182: & \\underline{\\text{Stage 2c: Logistic rescale of components}} \\\\\n183: & \\dots \\quad \\text{\\# Unchanged} \\\\\n184: \\\\\n185: & \\underline{\\text{Stage 2d: Assemble full vectors with floors}} \\\\\n186: & \\dots \\quad \\text{\\# Unchanged} \\\\\n187: \\\\\n188: & \\underline{\\text{Stage 3: Cloning transition}} \\\\\n189: & \\dots \\quad \\text{\\# Unchanged logic, produces } (x_i^{(t+\\frac{1}{2})}, v_i^{(t+\\frac{1}{2})}) \\\\\n190: \\\\\n191: & \\underline{\\text{Stage 4: Langevin perturbation and status refresh}} \\\\\n192: & \\mathcal S_{\\mathrm{pert}} \\sim \\Psi_{\\mathrm{kin,BAOAB}}(\\{(x_i^{(t+\\frac{1}{2})}, v_i^{(t+\\frac{1}{2})})\\}, \\cdot) \\quad \\text{\\# BAOAB Langevin step with velocity capping} \\\\\n193: & \\textbf{For each } i = 1..N: \\\\\n194: & \\quad (x_i^{(t+1)}, v_i^{(t+1)}) \\leftarrow \\text{draw from kinetic step output} \\\\\n195: & \\quad s_i^{(t+1)} \\leftarrow \\mathbf 1_{\\mathcal X_{\\mathrm{valid}}}(x_i^{(t+1)}) \\\\\n196: & \\textbf{Return } \\mathcal S_{t+1}\n197: \\end{aligned}\n198: ",
      "metadata": {
        "label": "alg-euclidean-gas"
      },
      "section": "## 3. Definition: the Euclidean Gas",
      "references": [
        "sec-eg-stage2",
        "def-statistical-properties-measurement",
        "def-canonical-logistic-rescale-function-example",
        "sec-eg-stage3"
      ],
      "raw_directive": "153: ### **3.1 Euclidean Gas algorithm (canonical pipeline)**\n154: \n155: :::{prf:algorithm} Euclidean Gas Update\n156: :label: alg-euclidean-gas\n157: \n158: Given a swarm state $\\mathcal S_t=(w_1,\\dots,w_N)$ with walkers $w_i=(x_i,v_i,s_i)$, the Euclidean Gas performs one update as follows:\n159: \n160: 1.  **Cemetery check.** If all walkers are dead (no alive indices in $\\mathcal A_t$) return the cemetery state; otherwise continue.\n161: 2.  **Measurement stage.** For every alive walker $i\\in\\mathcal A_t$ sample a companion $c_{\\mathrm{pot}}(i)$ from the algorithmic distance-weighted kernel $\\mathbb C_\\epsilon(\\mathcal S_t,i)$, then compute raw reward $r_i:=R(x_i,v_i)$ and algorithmic distance $d_i:=d_{\\text{alg}}(i,c_{\\mathrm{pot}}(i))$ as defined in Section 3.3 and detailed in {ref}`Stage 2 <sec-eg-stage2>`.\n162: 3.  **Patched standardisation.** Aggregate the raw reward and distance vectors with the empirical operator and apply the regularized standard deviation from {prf:ref}`def-statistical-properties-measurement` to obtain standardized scores with floor $\\sigma'_{\\min,\\mathrm{patch}} = \\sqrt{\\kappa_{\\mathrm{var,min}}+\\varepsilon_{\\mathrm{std}}^2}$.\n163: 4.  **Logistic rescale.** Apply the Canonical Logistic Rescale Function ({prf:ref}`def-canonical-logistic-rescale-function-example`) to the standardized reward and distance components, producing positive outputs $r'_i$ and $d'_i$. Combine them with the canonical exponents to freeze the potential vector $V_{\\text{fit},i}=(d'_i)^\\beta (r'_i)^\\alpha$ with floor $\\eta^{\\alpha+\\beta}$.\n164: 5.  **Clone/Persist gate.** For each walker draw a clone companion $c_{\\mathrm{clone}}(i)$ from the same algorithmic distance-weighted kernel and threshold $T_i\\sim\\mathrm{Unif}(0,p_{\\max})$, compute the canonical score $S_i:=\\big(V_{\\text{fit},c_{\\mathrm{clone}}(i)}-V_{\\text{fit},i}\\big)/(V_{\\text{fit},i}+\\varepsilon_{\\mathrm{clone}})$, and clone when $S_i>T_i$. Cloned walkers are grouped by companion and undergo a momentum-conserving inelastic collision: positions reset to the companion's position plus Gaussian jitter ($\\sigma_x$), while velocities are updated via center-of-mass calculation with random rotation and restitution coefficient $\\alpha_{\\text{restitution}}$, as detailed in {ref}`Stage 3 <sec-eg-stage3>` and Definition 5.7.4 of `03_cloning.md`. Otherwise the walker persists unchanged. The intermediate swarm sets every status to alive before the kinetic step.\n165: 6.  **Kinetic perturbation.** Update each alive clone or survivor by applying the **BAOAB splitting integrator** for one step of underdamped Langevin dynamics with force $F(x)=\\nabla R_{\\mathrm{pos}}(x)$ and noise scales $(\\sigma_v,\\sigma_x)$.\n166: 7.  **Status refresh.** Set the new status $s_i^{(t+1)}=\\mathbf 1_{\\mathcal X_{\\mathrm{valid}}}(x_i^+)$ and output the updated swarm $\\mathcal S_{t+1}$.\n167: \n168: **Euclidean Gas Algorithm**\n169: \n170: $$\n171: \\begin{aligned}\n172: & \\textbf{Input:} \\mathcal S_t = \\{(x_i^{(t)}, v_i^{(t)}, s_i^{(t)})\\}_{i=1}^N\\text{; and parameters } \\alpha, \\beta, \\varepsilon_{\\mathrm{std}}, \\eta, \\tau, p_{\\max}, \\varepsilon_{\\mathrm{clone}}, \\sigma_x, \\alpha_{\\text{restitution}}, \\sigma_v, \\\\\n173: & \\qquad \\sigma'_{\\mathrm{patch}}, g_A, \\mathbb C_i, Q_{\\delta}, \\Psi_{\\mathrm{kin,BAOAB}}. \\\\\n174: & \\textbf{If } |\\mathcal A_t| = 0: \\textbf{ return } \\delta_{\\mathcal S_t} \\quad \\text{\\# Cemetery absorption} \\\\\n175: \\\\\n176: & \\underline{\\text{Stage 2a: Raw vectors on alive set}} \\\\\n177: & \\dots \\quad \\text{\\# Unchanged} \\\\\n178: \\\\\n179: & \\underline{\\text{Stage 2b: Patched standardisation}} \\\\\n180: & \\dots \\quad \\text{\\# Unchanged} \\\\\n181: \\\\\n182: & \\underline{\\text{Stage 2c: Logistic rescale of components}} \\\\\n183: & \\dots \\quad \\text{\\# Unchanged} \\\\\n184: \\\\\n185: & \\underline{\\text{Stage 2d: Assemble full vectors with floors}} \\\\\n186: & \\dots \\quad \\text{\\# Unchanged} \\\\\n187: \\\\\n188: & \\underline{\\text{Stage 3: Cloning transition}} \\\\\n189: & \\dots \\quad \\text{\\# Unchanged logic, produces } (x_i^{(t+\\frac{1}{2})}, v_i^{(t+\\frac{1}{2})}) \\\\\n190: \\\\\n191: & \\underline{\\text{Stage 4: Langevin perturbation and status refresh}} \\\\\n192: & \\mathcal S_{\\mathrm{pert}} \\sim \\Psi_{\\mathrm{kin,BAOAB}}(\\{(x_i^{(t+\\frac{1}{2})}, v_i^{(t+\\frac{1}{2})})\\}, \\cdot) \\quad \\text{\\# BAOAB Langevin step with velocity capping} \\\\\n193: & \\textbf{For each } i = 1..N: \\\\\n194: & \\quad (x_i^{(t+1)}, v_i^{(t+1)}) \\leftarrow \\text{draw from kinetic step output} \\\\\n195: & \\quad s_i^{(t+1)} \\leftarrow \\mathbf 1_{\\mathcal X_{\\mathrm{valid}}}(x_i^{(t+1)}) \\\\\n196: & \\textbf{Return } \\mathcal S_{t+1}\n197: \\end{aligned}\n198: \n199: $$",
      "_registry_context": {
        "stage": "directives",
        "document_id": "02_euclidean_gas",
        "chapter_index": 3,
        "chapter_file": "chapter_3.json",
        "section_id": "## 3. Definition: the Euclidean Gas"
      }
    },
    {
      "directive_type": "algorithm",
      "label": "alg-greedy-pairing",
      "title": "Sequential Stochastic Greedy Pairing Algorithm",
      "start_line": 1461,
      "end_line": 1500,
      "header_lines": [
        1462
      ],
      "content_start": 1464,
      "content_end": 1499,
      "content": "1464: :label: alg-greedy-pairing\n1465: \n1466: ALGORITHM: GreedyPairing(alive_walkers, epsilon_d)\n1467: -------------------------------------------------\n1468: INPUT:\n1469:   alive_walkers: A list of k walker objects.\n1470:   epsilon_d: The interaction range for diversity.\n1471: OUTPUT:\n1472:   companion_map: A dictionary representing the pairing.\n1473: \n1474: 1.  unpaired_set ← a set containing all walkers from alive_walkers\n1475: 2.  companion_map ← an empty dictionary\n1476: \n1477: 3.  WHILE len(unpaired_set) > 1:\n1478: 4.      i ← unpaired_set.pop()  // Select and remove a walker\n1479: \n1480: 5.      // Prepare to compute the probability distribution\n1481: 6.      companions ← list(unpaired_set)\n1482: 7.      weights ← empty list of floats\n1483: 8.\n1484: 9.      FOR j IN companions:\n1485: 10.         dist_sq = algorithmic_distance(i.state, j.state)^2\n1486: 11.         weight = exp(-dist_sq / (2 * epsilon_d^2))\n1487: 12.         weights.append(weight)\n1488: \n1489: 13.     // Normalize weights to get probabilities\n1490: 14.     total_weight = sum(weights)\n1491: 15.     probabilities = [w / total_weight for w in weights]\n1492: \n1493: 16.     // Sample the companion based on the probabilities\n1494: 17.     c_i ← sample_from(companions, probabilities)\n1495: \n1496: 18.     // Finalize the pair\n1497: 19.     unpaired_set.remove(c_i)\n1498: 20.     companion_map[i] ← c_i\n1499: 21.     companion_map[c_i] ← i",
      "metadata": {
        "label": "alg-greedy-pairing"
      },
      "section": "## 5. The Measurement and Interaction Pipeline",
      "references": [],
      "raw_directive": "1461: The following pseudocode provides a concrete implementation of this operator.\n1462: \n1463: :::{prf:algorithm} Sequential Stochastic Greedy Pairing Algorithm\n1464: :label: alg-greedy-pairing\n1465: \n1466: ALGORITHM: GreedyPairing(alive_walkers, epsilon_d)\n1467: -------------------------------------------------\n1468: INPUT:\n1469:   alive_walkers: A list of k walker objects.\n1470:   epsilon_d: The interaction range for diversity.\n1471: OUTPUT:\n1472:   companion_map: A dictionary representing the pairing.\n1473: \n1474: 1.  unpaired_set ← a set containing all walkers from alive_walkers\n1475: 2.  companion_map ← an empty dictionary\n1476: \n1477: 3.  WHILE len(unpaired_set) > 1:\n1478: 4.      i ← unpaired_set.pop()  // Select and remove a walker\n1479: \n1480: 5.      // Prepare to compute the probability distribution\n1481: 6.      companions ← list(unpaired_set)\n1482: 7.      weights ← empty list of floats\n1483: 8.\n1484: 9.      FOR j IN companions:\n1485: 10.         dist_sq = algorithmic_distance(i.state, j.state)^2\n1486: 11.         weight = exp(-dist_sq / (2 * epsilon_d^2))\n1487: 12.         weights.append(weight)\n1488: \n1489: 13.     // Normalize weights to get probabilities\n1490: 14.     total_weight = sum(weights)\n1491: 15.     probabilities = [w / total_weight for w in weights]\n1492: \n1493: 16.     // Sample the companion based on the probabilities\n1494: 17.     c_i ← sample_from(companions, probabilities)\n1495: \n1496: 18.     // Finalize the pair\n1497: 19.     unpaired_set.remove(c_i)\n1498: 20.     companion_map[i] ← c_i\n1499: 21.     companion_map[c_i] ← i\n1500: ",
      "_registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 5,
        "chapter_file": "chapter_5.json",
        "section_id": "## 5. The Measurement and Interaction Pipeline"
      }
    },
    {
      "directive_type": "algorithm",
      "label": "alg-param-selection",
      "title": "Parameter Selection for Optimal Convergence",
      "start_line": 1953,
      "end_line": 2034,
      "header_lines": [
        1954
      ],
      "content_start": 1956,
      "content_end": 2033,
      "content": "1956: :label: alg-param-selection\n1957: \n1958: **Input:** Problem dimension $d$, budget $N$, landscape curvature estimate $\\lambda_{\\min}$\n1959: \n1960: **Goal:** Choose $(\\gamma, \\lambda, \\sigma_v, \\tau, d_{\\text{safe}}, \\kappa_{\\text{wall}})$ to maximize $\\kappa_{\\text{total}}$ while keeping $C_{\\text{total}}$ reasonable.\n1961: \n1962: **Step 1: Balance friction and cloning**\n1963: \n1964: Choose $\\gamma \\sim \\lambda$ to avoid bottlenecks:\n1965: \n1966: $$\n1967: \\gamma = \\lambda = \\sqrt{\\lambda_{\\min}}\n1968: $$\n1969: \n1970: **Justification:**\n1971: - If $\\gamma \\ll \\lambda$: velocity thermalization is the bottleneck ($\\kappa_{\\text{total}} \\sim 2\\gamma$)\n1972: - If $\\lambda \\ll \\gamma$: positional contraction is the bottleneck ($\\kappa_{\\text{total}} \\sim \\lambda$)\n1973: - Balanced: $\\kappa_{\\text{total}} \\sim \\min(2\\gamma, \\lambda) = \\sqrt{\\lambda_{\\min}}$\n1974: \n1975: **Step 2: Choose noise intensity for exploration**\n1976: \n1977: Set thermal noise to match desired exploration scale $\\sigma_{\\text{explore}}$:\n1978: \n1979: $$\n1980: \\sigma_v = \\sqrt{\\gamma \\sigma_{\\text{explore}}^2}\n1981: $$\n1982: \n1983: **Justification:** The equilibrium positional variance is:\n1984: \n1985: $$\n1986: V_{\\text{Var},x}^{\\text{eq}} \\sim \\frac{\\sigma_v^2 \\tau^2}{\\gamma \\lambda} \\sim \\sigma_{\\text{explore}}^2\n1987: $$\n1988: \n1989: **Step 3: Choose timestep from stability**\n1990: \n1991: Use CFL-like condition:\n1992: \n1993: $$\n1994: \\tau = \\frac{c_{\\text{CFL}}}{\\sqrt{\\gamma \\lambda_{\\max}}}\n1995: $$\n1996: \n1997: where $\\lambda_{\\max}$ is the largest curvature and $c_{\\text{CFL}} \\sim 0.1 - 0.5$.\n1998: \n1999: **Justification:** Ensures:\n2000: - BAOAB stability: $\\gamma \\tau \\ll 1$\n2001: - Symplectic accuracy: $\\sqrt{\\lambda_{\\max}} \\tau \\ll 1$\n2002: - Weak error: $O(\\tau^2)$ corrections negligible\n2003: \n2004: **Step 4: Set boundary parameters for safety**\n2005: \n2006: Choose Safe Harbor distance from swarm variance:\n2007: \n2008: $$\n2009: d_{\\text{safe}} = 3\\sqrt{V_{\\text{Var},x}^{\\text{eq}}} \\sim 3\\sigma_{\\text{explore}}\n2010: $$\n2011: \n2012: Choose boundary stiffness from extinction tolerance:\n2013: \n2014: $$\n2015: \\kappa_{\\text{wall}} = \\frac{\\lambda f_{\\text{typical}}}{\\Delta f_{\\text{desired}}}\n2016: $$\n2017: \n2018: to ensure $P(\\text{extinction per step}) \\lesssim e^{-\\Theta(N)}$.\n2019: \n2020: **Step 5: Scale with swarm size**\n2021: \n2022: For dimension $d$ and desired Wasserstein accuracy $\\epsilon_W$:\n2023: \n2024: $$\n2025: N \\geq \\left(\\frac{\\sigma_v^2 \\tau}{\\epsilon_W^2 \\kappa_W}\\right)^d\n2026: $$\n2027: \n2028: **Output:** Optimized parameters $(\\gamma^*, \\lambda^*, \\sigma_v^*, \\tau^*, d_{\\text{safe}}^*, \\kappa_{\\text{wall}}^*)$\n2029: \n2030: **Expected performance:**\n2031: \n2032: $$\n2033: \\kappa_{\\text{total}} \\sim \\sqrt{\\lambda_{\\min}}, \\quad",
      "metadata": {
        "label": "alg-param-selection"
      },
      "section": "## 5. Explicit Parameter Dependence and Convergence Rates",
      "references": [],
      "raw_directive": "1953: Based on the explicit formulas, here is a practical strategy for choosing parameters:\n1954: \n1955: :::{prf:algorithm} Parameter Selection for Optimal Convergence\n1956: :label: alg-param-selection\n1957: \n1958: **Input:** Problem dimension $d$, budget $N$, landscape curvature estimate $\\lambda_{\\min}$\n1959: \n1960: **Goal:** Choose $(\\gamma, \\lambda, \\sigma_v, \\tau, d_{\\text{safe}}, \\kappa_{\\text{wall}})$ to maximize $\\kappa_{\\text{total}}$ while keeping $C_{\\text{total}}$ reasonable.\n1961: \n1962: **Step 1: Balance friction and cloning**\n1963: \n1964: Choose $\\gamma \\sim \\lambda$ to avoid bottlenecks:\n1965: \n1966: $$\n1967: \\gamma = \\lambda = \\sqrt{\\lambda_{\\min}}\n1968: $$\n1969: \n1970: **Justification:**\n1971: - If $\\gamma \\ll \\lambda$: velocity thermalization is the bottleneck ($\\kappa_{\\text{total}} \\sim 2\\gamma$)\n1972: - If $\\lambda \\ll \\gamma$: positional contraction is the bottleneck ($\\kappa_{\\text{total}} \\sim \\lambda$)\n1973: - Balanced: $\\kappa_{\\text{total}} \\sim \\min(2\\gamma, \\lambda) = \\sqrt{\\lambda_{\\min}}$\n1974: \n1975: **Step 2: Choose noise intensity for exploration**\n1976: \n1977: Set thermal noise to match desired exploration scale $\\sigma_{\\text{explore}}$:\n1978: \n1979: $$\n1980: \\sigma_v = \\sqrt{\\gamma \\sigma_{\\text{explore}}^2}\n1981: $$\n1982: \n1983: **Justification:** The equilibrium positional variance is:\n1984: \n1985: $$\n1986: V_{\\text{Var},x}^{\\text{eq}} \\sim \\frac{\\sigma_v^2 \\tau^2}{\\gamma \\lambda} \\sim \\sigma_{\\text{explore}}^2\n1987: $$\n1988: \n1989: **Step 3: Choose timestep from stability**\n1990: \n1991: Use CFL-like condition:\n1992: \n1993: $$\n1994: \\tau = \\frac{c_{\\text{CFL}}}{\\sqrt{\\gamma \\lambda_{\\max}}}\n1995: $$\n1996: \n1997: where $\\lambda_{\\max}$ is the largest curvature and $c_{\\text{CFL}} \\sim 0.1 - 0.5$.\n1998: \n1999: **Justification:** Ensures:\n2000: - BAOAB stability: $\\gamma \\tau \\ll 1$\n2001: - Symplectic accuracy: $\\sqrt{\\lambda_{\\max}} \\tau \\ll 1$\n2002: - Weak error: $O(\\tau^2)$ corrections negligible\n2003: \n2004: **Step 4: Set boundary parameters for safety**\n2005: \n2006: Choose Safe Harbor distance from swarm variance:\n2007: \n2008: $$\n2009: d_{\\text{safe}} = 3\\sqrt{V_{\\text{Var},x}^{\\text{eq}}} \\sim 3\\sigma_{\\text{explore}}\n2010: $$\n2011: \n2012: Choose boundary stiffness from extinction tolerance:\n2013: \n2014: $$\n2015: \\kappa_{\\text{wall}} = \\frac{\\lambda f_{\\text{typical}}}{\\Delta f_{\\text{desired}}}\n2016: $$\n2017: \n2018: to ensure $P(\\text{extinction per step}) \\lesssim e^{-\\Theta(N)}$.\n2019: \n2020: **Step 5: Scale with swarm size**\n2021: \n2022: For dimension $d$ and desired Wasserstein accuracy $\\epsilon_W$:\n2023: \n2024: $$\n2025: N \\geq \\left(\\frac{\\sigma_v^2 \\tau}{\\epsilon_W^2 \\kappa_W}\\right)^d\n2026: $$\n2027: \n2028: **Output:** Optimized parameters $(\\gamma^*, \\lambda^*, \\sigma_v^*, \\tau^*, d_{\\text{safe}}^*, \\kappa_{\\text{wall}}^*)$\n2029: \n2030: **Expected performance:**\n2031: \n2032: $$\n2033: \\kappa_{\\text{total}} \\sim \\sqrt{\\lambda_{\\min}}, \\quad\n2034: T_{\\text{mix}} \\sim \\frac{5}{\\sqrt{\\lambda_{\\min}}}",
      "_registry_context": {
        "stage": "directives",
        "document_id": "06_convergence",
        "chapter_index": 5,
        "chapter_file": "chapter_5.json",
        "section_id": "## 5. Explicit Parameter Dependence and Convergence Rates"
      }
    },
    {
      "directive_type": "algorithm",
      "label": "alg-projected-gradient-ascent",
      "title": "Projected Gradient Ascent for Parameter Optimization",
      "start_line": 3299,
      "end_line": 3403,
      "header_lines": [
        3300
      ],
      "content_start": 3302,
      "content_end": 3402,
      "content": "3302: :label: alg-projected-gradient-ascent\n3303: \n3304: **Input:**\n3305: - Landscape: $(\\lambda_{\\min}, \\lambda_{\\max}, d)$\n3306: - Constraints: $(N_{\\max}, \\lambda_{\\max}, V_{\\max}, \\ldots)$\n3307: - Initial guess: $\\mathbf{P}_0$ (from closed-form solution)\n3308: \n3309: **Output:** Optimal parameters $\\mathbf{P}^*$, achieved rate $\\kappa_{\\text{total}}^*$\n3310: \n3311: **Algorithm:**\n3312: \n3313: ```python\n3314: def optimize_parameters_constrained(landscape, constraints, P_init, max_iter=100):\n3315:     P = P_init\n3316:     alpha = 0.1  # Step size\n3317: \n3318:     for iter in range(max_iter):\n3319:         # Step 1: Compute current rates\n3320:         kappa = compute_rates(P, landscape)\n3321:         #   kappa = [kappa_x(P), kappa_v(P), kappa_W(P), kappa_b(P)]\n3322: \n3323:         kappa_total = min(kappa)\n3324: \n3325:         # Step 2: Identify active constraints (rates equal to minimum)\n3326:         active = [i for i in range(4) if abs(kappa[i] - kappa_total) < 1e-6]\n3327: \n3328:         # Step 3: Compute subgradient\n3329:         if len(active) == 1:\n3330:             # Unique minimum: gradient is M_kappa[active[0], :]\n3331:             grad = M_kappa[active[0], :]\n3332:         else:\n3333:             # Multiple minima: convex combination of gradients\n3334:             grad = mean(M_kappa[active, :], axis=0)\n3335: \n3336:         # Step 4: Gradient ascent step\n3337:         P_new = P * (1 + alpha * grad)  # Multiplicative update\n3338: \n3339:         # Step 5: Project onto feasible set\n3340:         P_new = project_onto_constraints(P_new, constraints)\n3341: \n3342:         # Step 6: Check convergence\n3343:         rel_change = norm(P_new - P) / norm(P)\n3344:         if rel_change < 1e-4:\n3345:             break\n3346: \n3347:         # Step 7: Adaptive step size\n3348:         kappa_new = min(compute_rates(P_new, landscape))\n3349:         if kappa_new > kappa_total:\n3350:             alpha *= 1.2  # Increase step (things are improving)\n3351:         else:\n3352:             alpha *= 0.5  # Decrease step (overshot)\n3353:             P_new = P     # Reject step\n3354: \n3355:         P = P_new\n3356: \n3357:     return P, kappa_total\n3358: ```\n3359: \n3360: **Helper functions:**\n3361: \n3362: ```python\n3363: def compute_rates(P, landscape):\n3364:     \"\"\"Compute all four rates from parameters.\"\"\"\n3365:     lambda_val = P['lambda']\n3366:     gamma = P['gamma']\n3367:     tau = P['tau']\n3368:     lambda_alg = P['lambda_alg']\n3369:     epsilon_c = P['epsilon_c']\n3370:     kappa_wall = P['kappa_wall']\n3371: \n3372:     # Use formulas from Chapter 7\n3373:     c_fit = estimate_fitness_correlation(lambda_alg, epsilon_c)\n3374: \n3375:     kappa_x = lambda_val * c_fit * (1 - 0.1*tau)\n3376:     kappa_v = 2 * gamma * (1 - 0.1*tau)\n3377:     kappa_W = 0.5 * gamma / (1 + gamma/landscape['lambda_min'])\n3378:     kappa_b = min(lambda_val, kappa_wall + gamma)\n3379: \n3380:     return [kappa_x, kappa_v, kappa_W, kappa_b]\n3381: \n3382: def project_onto_constraints(P, constraints):\n3383:     \"\"\"Project parameters onto feasible set.\"\"\"\n3384:     P_proj = P.copy()\n3385: \n3386:     # Box constraints\n3387:     if 'N_max' in constraints:\n3388:         P_proj['N'] = min(P['N'], constraints['N_max'])\n3389:     if 'lambda_max' in constraints:\n3390:         P_proj['lambda'] = min(P['lambda'], constraints['lambda_max'])\n3391: \n3392:     # Stability constraints\n3393:     P_proj['tau'] = min(P['tau'], 0.5/P['gamma'])\n3394:     P_proj['tau'] = min(P_proj['tau'], 1/sqrt(constraints['lambda_max']))\n3395: \n3396:     # Positivity\n3397:     for key in P_proj:\n3398:         P_proj[key] = max(P_proj[key], 1e-6)\n3399: \n3400:     # Restitution bound\n3401:     P_proj['alpha_rest'] = clip(P['alpha_rest'], 0, 1)\n3402: ",
      "metadata": {
        "label": "alg-projected-gradient-ascent"
      },
      "section": "## 6. Spectral Analysis of Parameter Coupling",
      "references": [],
      "raw_directive": "3299: When constraints are active (e.g., limited memory $N \\leq N_{\\max}$ or communication budget $\\lambda \\leq \\lambda_{\\max}$), we need iterative optimization.\n3300: \n3301: :::{prf:algorithm} Projected Gradient Ascent for Parameter Optimization\n3302: :label: alg-projected-gradient-ascent\n3303: \n3304: **Input:**\n3305: - Landscape: $(\\lambda_{\\min}, \\lambda_{\\max}, d)$\n3306: - Constraints: $(N_{\\max}, \\lambda_{\\max}, V_{\\max}, \\ldots)$\n3307: - Initial guess: $\\mathbf{P}_0$ (from closed-form solution)\n3308: \n3309: **Output:** Optimal parameters $\\mathbf{P}^*$, achieved rate $\\kappa_{\\text{total}}^*$\n3310: \n3311: **Algorithm:**\n3312: \n3313: ```python\n3314: def optimize_parameters_constrained(landscape, constraints, P_init, max_iter=100):\n3315:     P = P_init\n3316:     alpha = 0.1  # Step size\n3317: \n3318:     for iter in range(max_iter):\n3319:         # Step 1: Compute current rates\n3320:         kappa = compute_rates(P, landscape)\n3321:         #   kappa = [kappa_x(P), kappa_v(P), kappa_W(P), kappa_b(P)]\n3322: \n3323:         kappa_total = min(kappa)\n3324: \n3325:         # Step 2: Identify active constraints (rates equal to minimum)\n3326:         active = [i for i in range(4) if abs(kappa[i] - kappa_total) < 1e-6]\n3327: \n3328:         # Step 3: Compute subgradient\n3329:         if len(active) == 1:\n3330:             # Unique minimum: gradient is M_kappa[active[0], :]\n3331:             grad = M_kappa[active[0], :]\n3332:         else:\n3333:             # Multiple minima: convex combination of gradients\n3334:             grad = mean(M_kappa[active, :], axis=0)\n3335: \n3336:         # Step 4: Gradient ascent step\n3337:         P_new = P * (1 + alpha * grad)  # Multiplicative update\n3338: \n3339:         # Step 5: Project onto feasible set\n3340:         P_new = project_onto_constraints(P_new, constraints)\n3341: \n3342:         # Step 6: Check convergence\n3343:         rel_change = norm(P_new - P) / norm(P)\n3344:         if rel_change < 1e-4:\n3345:             break\n3346: \n3347:         # Step 7: Adaptive step size\n3348:         kappa_new = min(compute_rates(P_new, landscape))\n3349:         if kappa_new > kappa_total:\n3350:             alpha *= 1.2  # Increase step (things are improving)\n3351:         else:\n3352:             alpha *= 0.5  # Decrease step (overshot)\n3353:             P_new = P     # Reject step\n3354: \n3355:         P = P_new\n3356: \n3357:     return P, kappa_total\n3358: ```\n3359: \n3360: **Helper functions:**\n3361: \n3362: ```python\n3363: def compute_rates(P, landscape):\n3364:     \"\"\"Compute all four rates from parameters.\"\"\"\n3365:     lambda_val = P['lambda']\n3366:     gamma = P['gamma']\n3367:     tau = P['tau']\n3368:     lambda_alg = P['lambda_alg']\n3369:     epsilon_c = P['epsilon_c']\n3370:     kappa_wall = P['kappa_wall']\n3371: \n3372:     # Use formulas from Chapter 7\n3373:     c_fit = estimate_fitness_correlation(lambda_alg, epsilon_c)\n3374: \n3375:     kappa_x = lambda_val * c_fit * (1 - 0.1*tau)\n3376:     kappa_v = 2 * gamma * (1 - 0.1*tau)\n3377:     kappa_W = 0.5 * gamma / (1 + gamma/landscape['lambda_min'])\n3378:     kappa_b = min(lambda_val, kappa_wall + gamma)\n3379: \n3380:     return [kappa_x, kappa_v, kappa_W, kappa_b]\n3381: \n3382: def project_onto_constraints(P, constraints):\n3383:     \"\"\"Project parameters onto feasible set.\"\"\"\n3384:     P_proj = P.copy()\n3385: \n3386:     # Box constraints\n3387:     if 'N_max' in constraints:\n3388:         P_proj['N'] = min(P['N'], constraints['N_max'])\n3389:     if 'lambda_max' in constraints:\n3390:         P_proj['lambda'] = min(P['lambda'], constraints['lambda_max'])\n3391: \n3392:     # Stability constraints\n3393:     P_proj['tau'] = min(P['tau'], 0.5/P['gamma'])\n3394:     P_proj['tau'] = min(P_proj['tau'], 1/sqrt(constraints['lambda_max']))\n3395: \n3396:     # Positivity\n3397:     for key in P_proj:\n3398:         P_proj[key] = max(P_proj[key], 1e-6)\n3399: \n3400:     # Restitution bound\n3401:     P_proj['alpha_rest'] = clip(P['alpha_rest'], 0, 1)\n3402: \n3403:     return P_proj",
      "_registry_context": {
        "stage": "directives",
        "document_id": "06_convergence",
        "chapter_index": 6,
        "chapter_file": "chapter_6.json",
        "section_id": "## 6. Spectral Analysis of Parameter Coupling"
      }
    },
    {
      "directive_type": "algorithm",
      "label": "alg-adaptive-tuning",
      "title": "Adaptive Parameter Tuning",
      "start_line": 3488,
      "end_line": 3580,
      "header_lines": [
        3489
      ],
      "content_start": 3491,
      "content_end": 3579,
      "content": "3491: :label: alg-adaptive-tuning\n3492: \n3493: **Input:**\n3494: - Swarm system (black box)\n3495: - Initial parameter guess $\\mathbf{P}_0$\n3496: - Measurement window $T_{\\text{sample}}$\n3497: \n3498: **Output:** Tuned parameters $\\mathbf{P}_{\\text{tuned}}$\n3499: \n3500: **Algorithm:**\n3501: \n3502: ```python\n3503: def adaptive_tuning(swarm_system, P_init, n_iterations=10, T_sample=1000):\n3504:     \"\"\"\n3505:     Iteratively improve parameters using empirical measurements.\n3506:     \"\"\"\n3507:     P = P_init\n3508: \n3509:     for iter in range(n_iterations):\n3510:         # Step 1: Run swarm for T_sample steps\n3511:         trajectory = swarm_system.run(P, steps=T_sample)\n3512: \n3513:         # Step 2: Estimate rates from trajectory\n3514:         kappa_emp = estimate_rates_from_trajectory(trajectory)\n3515:         #   Returns: [kappa_x_emp, kappa_v_emp, kappa_W_emp, kappa_b_emp]\n3516: \n3517:         # Step 3: Identify bottleneck\n3518:         i_bottleneck = argmin(kappa_emp)\n3519:         kappa_min = kappa_emp[i_bottleneck]\n3520: \n3521:         bottleneck_names = ['Position', 'Velocity', 'Wasserstein', 'Boundary']\n3522:         print(f\"Iter {iter}: Bottleneck = {bottleneck_names[i_bottleneck]}, \"\n3523:               f\"κ = {kappa_min:.4f}\")\n3524: \n3525:         # Step 4: Compute adjustment direction using sensitivity matrix\n3526:         grad = M_kappa[i_bottleneck, :]  # Which parameters affect bottleneck?\n3527: \n3528:         # Step 5: Adaptive step size based on gap to target\n3529:         # Estimate achievable rate from landscape (if known roughly)\n3530:         kappa_target = estimate_achievable_rate(swarm_system)\n3531:         gap = kappa_target - kappa_min\n3532: \n3533:         if gap > 0:\n3534:             alpha = 0.2 * gap / kappa_min  # Proportional adjustment\n3535:         else:\n3536:             alpha = 0.05  # Small refinement\n3537: \n3538:         # Step 6: Update parameters\n3539:         P_new = {}\n3540:         for j, param_name in enumerate(param_names):\n3541:             P_new[param_name] = P[param_name] * (1 + alpha * grad[j])\n3542: \n3543:         # Step 7: Project onto feasible set\n3544:         P_new = project_onto_constraints(P_new, get_system_constraints())\n3545: \n3546:         # Step 8: Validate improvement\n3547:         trajectory_new = swarm_system.run(P_new, steps=T_sample//2)\n3548:         kappa_new = estimate_rates_from_trajectory(trajectory_new)\n3549: \n3550:         if min(kappa_new) > min(kappa_emp):\n3551:             P = P_new  # Accept\n3552:             print(f\"  → Accepted: κ_new = {min(kappa_new):.4f}\")\n3553:         else:\n3554:             alpha *= 0.5  # Reduce step size, try again\n3555:             print(f\"  → Rejected: Reducing step size\")\n3556: \n3557:     return P\n3558: \n3559: def estimate_rates_from_trajectory(trajectory):\n3560:     \"\"\"\n3561:     Extract empirical convergence rates from swarm trajectory.\n3562: \n3563:     Method: Fit exponential decay to Lyapunov components:\n3564:         V_i(t) ≈ C_i/κ_i + (V_i(0) - C_i/κ_i) * exp(-κ_i * t)\n3565: \n3566:     Extract κ_i from exponential fit.\n3567:     \"\"\"\n3568:     # Extract Lyapunov components over time\n3569:     V_Var_x = [compute_variance(traj.positions) for traj in trajectory]\n3570:     V_Var_v = [compute_variance(traj.velocities) for traj in trajectory]\n3571:     V_W = [compute_wasserstein(traj, reference) for traj in trajectory]\n3572:     W_b = [compute_boundary_potential(traj) for traj in trajectory]\n3573: \n3574:     # Fit exponential decay: V(t) = C + A * exp(-kappa * t)\n3575:     kappa_x = fit_exponential_rate(V_Var_x, trajectory.times)\n3576:     kappa_v = fit_exponential_rate(V_Var_v, trajectory.times)\n3577:     kappa_W = fit_exponential_rate(V_W, trajectory.times)\n3578:     kappa_b = fit_exponential_rate(W_b, trajectory.times)\n3579: ",
      "metadata": {
        "label": "alg-adaptive-tuning"
      },
      "section": "## 6. Spectral Analysis of Parameter Coupling",
      "references": [],
      "raw_directive": "3488: When the landscape is unknown or model assumptions are violated, adapt parameters based on measured convergence.\n3489: \n3490: :::{prf:algorithm} Adaptive Parameter Tuning\n3491: :label: alg-adaptive-tuning\n3492: \n3493: **Input:**\n3494: - Swarm system (black box)\n3495: - Initial parameter guess $\\mathbf{P}_0$\n3496: - Measurement window $T_{\\text{sample}}$\n3497: \n3498: **Output:** Tuned parameters $\\mathbf{P}_{\\text{tuned}}$\n3499: \n3500: **Algorithm:**\n3501: \n3502: ```python\n3503: def adaptive_tuning(swarm_system, P_init, n_iterations=10, T_sample=1000):\n3504:     \"\"\"\n3505:     Iteratively improve parameters using empirical measurements.\n3506:     \"\"\"\n3507:     P = P_init\n3508: \n3509:     for iter in range(n_iterations):\n3510:         # Step 1: Run swarm for T_sample steps\n3511:         trajectory = swarm_system.run(P, steps=T_sample)\n3512: \n3513:         # Step 2: Estimate rates from trajectory\n3514:         kappa_emp = estimate_rates_from_trajectory(trajectory)\n3515:         #   Returns: [kappa_x_emp, kappa_v_emp, kappa_W_emp, kappa_b_emp]\n3516: \n3517:         # Step 3: Identify bottleneck\n3518:         i_bottleneck = argmin(kappa_emp)\n3519:         kappa_min = kappa_emp[i_bottleneck]\n3520: \n3521:         bottleneck_names = ['Position', 'Velocity', 'Wasserstein', 'Boundary']\n3522:         print(f\"Iter {iter}: Bottleneck = {bottleneck_names[i_bottleneck]}, \"\n3523:               f\"κ = {kappa_min:.4f}\")\n3524: \n3525:         # Step 4: Compute adjustment direction using sensitivity matrix\n3526:         grad = M_kappa[i_bottleneck, :]  # Which parameters affect bottleneck?\n3527: \n3528:         # Step 5: Adaptive step size based on gap to target\n3529:         # Estimate achievable rate from landscape (if known roughly)\n3530:         kappa_target = estimate_achievable_rate(swarm_system)\n3531:         gap = kappa_target - kappa_min\n3532: \n3533:         if gap > 0:\n3534:             alpha = 0.2 * gap / kappa_min  # Proportional adjustment\n3535:         else:\n3536:             alpha = 0.05  # Small refinement\n3537: \n3538:         # Step 6: Update parameters\n3539:         P_new = {}\n3540:         for j, param_name in enumerate(param_names):\n3541:             P_new[param_name] = P[param_name] * (1 + alpha * grad[j])\n3542: \n3543:         # Step 7: Project onto feasible set\n3544:         P_new = project_onto_constraints(P_new, get_system_constraints())\n3545: \n3546:         # Step 8: Validate improvement\n3547:         trajectory_new = swarm_system.run(P_new, steps=T_sample//2)\n3548:         kappa_new = estimate_rates_from_trajectory(trajectory_new)\n3549: \n3550:         if min(kappa_new) > min(kappa_emp):\n3551:             P = P_new  # Accept\n3552:             print(f\"  → Accepted: κ_new = {min(kappa_new):.4f}\")\n3553:         else:\n3554:             alpha *= 0.5  # Reduce step size, try again\n3555:             print(f\"  → Rejected: Reducing step size\")\n3556: \n3557:     return P\n3558: \n3559: def estimate_rates_from_trajectory(trajectory):\n3560:     \"\"\"\n3561:     Extract empirical convergence rates from swarm trajectory.\n3562: \n3563:     Method: Fit exponential decay to Lyapunov components:\n3564:         V_i(t) ≈ C_i/κ_i + (V_i(0) - C_i/κ_i) * exp(-κ_i * t)\n3565: \n3566:     Extract κ_i from exponential fit.\n3567:     \"\"\"\n3568:     # Extract Lyapunov components over time\n3569:     V_Var_x = [compute_variance(traj.positions) for traj in trajectory]\n3570:     V_Var_v = [compute_variance(traj.velocities) for traj in trajectory]\n3571:     V_W = [compute_wasserstein(traj, reference) for traj in trajectory]\n3572:     W_b = [compute_boundary_potential(traj) for traj in trajectory]\n3573: \n3574:     # Fit exponential decay: V(t) = C + A * exp(-kappa * t)\n3575:     kappa_x = fit_exponential_rate(V_Var_x, trajectory.times)\n3576:     kappa_v = fit_exponential_rate(V_Var_v, trajectory.times)\n3577:     kappa_W = fit_exponential_rate(V_W, trajectory.times)\n3578:     kappa_b = fit_exponential_rate(W_b, trajectory.times)\n3579: \n3580:     return [kappa_x, kappa_v, kappa_W, kappa_b]",
      "_registry_context": {
        "stage": "directives",
        "document_id": "06_convergence",
        "chapter_index": 6,
        "chapter_file": "chapter_6.json",
        "section_id": "## 6. Spectral Analysis of Parameter Coupling"
      }
    }
  ]
}