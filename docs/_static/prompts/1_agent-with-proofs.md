## 01_foundations/01_definitions.md

:::{prf:definition} Bounded-Rationality Controller
:label: def-bounded-rationality-controller

The agent is a controller with internal state

$$
Z_t := (K_t, z_{n,t}, z_{\mathrm{tex},t}) \in \mathcal{Z}=\mathcal{K}\times\mathcal{Z}_n\times\mathcal{Z}_{\mathrm{tex}},

$$
and internal components (Encoder/Shutter, World Model, Critic, Policy). Its evolution is driven only by the observable interaction stream at the interface (observations/feedback) and by its own outgoing control signals (actions).

:::

:::{prf:definition} Boundary / Markov Blanket
:label: def-boundary-markov-blanket

The boundary variables at time $t$ are the interface tuple

$$
B_t := (x_t,\ r_t,\ d_t,\ \iota_t,\ a_t),

$$
where:
- $x_t\in\mathcal{X}$ is the observation (input sample),
- $r_t\in\mathbb{R}$ is the boundary reward sample (evaluation of the reward 1-form/flux; scalar in the conservative case),
- $d_t\in\{0,1\}$ is termination (absorbing event / task boundary; corresponds to $\Gamma_{\text{term}}$ and $\tau_{\text{term}}$ in Definition {prf:ref}`def-terminal-boundary`),
- $\iota_t$ denotes any additional side channels (costs, constraints, termination reasons, privileged signals),
- $a_t\in\mathcal{A}$ is action (control signal sent outward).

:::

:::{prf:definition} Environment as Generative Process
:label: def-environment-as-generative-process

The "environment" is the conditional law of future interface signals given past interface history. Concretely it is a (possibly history-dependent) kernel on incoming boundary signals conditional on the boundary history:

$$
P_{\partial}(x_{t+1}, r_{t+1}, d_{t+1}, \iota_{t+1}\mid B_{\le t}).

$$
In the Markov case this reduces to the familiar RL kernel

$$
P_{\partial}(x_{t+1}, r_{t+1}, d_{t+1}, \iota_{t+1}\mid B_t),

$$
but the **interpretation changes**: $P_{\partial}$ is not "a dataset generator"; it is the **input-output law** that the controller must cope with under partial observability and model mismatch.

This is the categorical move: we do not assume access to the environment's latent variables; we work only with the **law over observable interface variables**.

:::

:::{prf:definition} Agent symmetry group; operational
:label: def-agent-symmetry-group-operational

Let:
- $G_{\text{obj}}$ be an **objective/feedback gauge** acting on scalar feedback signals (e.g., change of units or baseline shift). A common choice is the positive affine group

  $$
  G_{\text{obj}} := \{(a,b): a>0,\ r\mapsto ar+b\}.

  $$
  (If representing value as a unit-norm phase variable, one may instead use $U(1)$; {ref}`sec-defect-functionals-implementing-regulation`.C treats the real-valued case via projective heads.)
- $G_{\text{spatial}}$ be an **observation gauge** acting on raw observations $x$ (e.g., pose/translation/rotation; choose $SE(3)$, $SE(2)$, $\mathrm{Sim}(2)$, or a task-specific subgroup depending on sensors).
- $S_{|\mathcal{K}|}$ be the **symbol-permutation symmetry** of the discrete macro register: relabeling code indices is unobservable if downstream components depend only on embeddings $\{e_k\}$.
- $\mathrm{Symp}(2n,\mathbb{R})$ be an optional **phase-space symmetry** acting on canonical latent coordinates $z=(q,p)\in\mathbb{R}^{2n}$ when the world model is parameterized as a symplectic/Hamiltonian system {cite}`greydanus2019hamiltonian` ({ref}`sec-defect-functionals-implementing-regulation`.B).

The (candidate) total symmetry group is the direct product

$$
\mathcal{G}_{\mathbb{A}}
:=
G_{\text{obj}}
\times
G_{\text{spatial}}
\times
S_{|\mathcal{K}|}
\times
\mathrm{Symp}(2n,\mathbb{R}).

$$
**Internal vs. external symmetries.**
- **Internal (objective) gauge:** transformations of the scalar feedback scale/offset (and any potentials) that should not qualitatively change the policy update direction.
- **External (observation) gauge:** transformations of the input stream that change *pose* but not *identity*.

**Principle of covariance (engineering requirement).** The internal maps of the agent should be invariant/equivariant under $\mathcal{G}_{\mathbb{A}}$ in the following typed sense:
- **Shutter $E$**: canonicalize or quotient $G_{\text{spatial}}$ before discretization, so the macro register is approximately invariant:

  $$
  K(x)\approx K(g\cdot x)\quad (g\in G_{\text{spatial}}),

  $$
  while $z_n$ carries structured nuisance parameters (pose/basis/disturbance coordinates) and $z_{\mathrm{tex}}$ carries reconstruction-only texture ({ref}`sec-the-shutter-as-a-vq-vae`, {ref}`sec-defect-functionals-implementing-regulation`.A).
- **World model $S$ and policy $\pi$:** be covariant to symbol permutations $S_{|\mathcal{K}|}$ by treating $K$ only through its embedding $e_K$ (not the integer label) and by using permutation-invariant diagnostics.
- **Critic/value and dual variables:** enforce stability and constraint satisfaction in a way that is robust to re-scaling/offset of the scalar feedback ({ref}`sec-defect-functionals-implementing-regulation`.C, {ref}`sec-adaptive-multipliers-learned-penalties-setpoints-and-calibration`).

These are *requirements on representations and interfaces*, not philosophical claims: if an invariance is not enforced, the corresponding failure modes (symmetry blindness, brittle scaling, uncontrolled drift) become more likely and harder to debug.

:::

## 01_foundations/02_control_loop.md

:::{prf:definition} State-Space Sensitivity Metric
:label: def-state-space-sensitivity-metric

The **value-curvature component** of the state-space sensitivity metric at a point $z$ in the latent space is defined
from the symmetric Hessian of the value function, with a PSD proxy used when enforcing metric positivity:

$$
(G_V)_{ij} = \frac{\partial^2 V}{\partial z_i \partial z_j} \quad \text{(theory)}, \qquad
(G_V)_{ij} \approx c_V\,\frac{\partial V}{\partial z_i}\frac{\partial V}{\partial z_j} \quad \text{(Gauss--Newton proxy)}.

$$

The **complete** metric used elsewhere is $G = G_V + \lambda_G G_\pi$ with $G_\pi$ the state-space Fisher component (Definition {prf:ref}`def-complete-latent-space-metric`). Units: $[(G_V)_{ij}]=\mathrm{nat}\,[z]^{-2}$ if $z$ is measured in units $[z]$; in the proxy form, $c_V$ carries units $\mathrm{nat}^{-1}$.
:::

:::{prf:definition} Complete Latent Space Metric
:label: def-complete-latent-space-metric

The complete state-space sensitivity metric on $\mathcal{Z}$ is defined as:

$$
G_{ij}(z) = \underbrace{(G_V)_{ij}(z)}_{\text{Hessian (value curvature)}} + \lambda_G \underbrace{(G_\pi)_{ij}(z)}_{\text{Fisher (control sensitivity)}},
\qquad
(G_\pi)_{ij}(z) := \mathbb{E}_{a \sim \pi} \left[ \frac{\partial \log \pi(a|z)}{\partial z_i} \frac{\partial \log \pi(a|z)}{\partial z_j} \right].

$$

Units: the Fisher term has units $[z]^{-2}$; therefore $\lambda_G$ carries the same units as $V$ (here $\mathrm{nat}$) so both addends match.
:::

:::{prf:definition} Causal Enclosure Condition
:label: def-causal-enclosure-condition

**Causal Enclosure Condition (Markov sufficiency).** With the nuisance/texture split ({ref}`sec-the-shutter-as-a-vq-vae`), let $(K_t, z_{n,t}, z_{\mathrm{tex},t}, K^{\text{act}}_t)$ be the internal state/action process and define the macrostate $K_t:=\Pi(Z_t)$ (projection to the discrete register). The macro-model requirement is the conditional independence

$$
K_{t+1}\ \perp\!\!\!\perp\ (z_{n,t}, z_{\mathrm{tex},t})\ \big|\ (K_t,K^{\text{act}}_t),

$$
equivalently the vanishing of a conditional mutual information:

$$
I(K_{t+1};z_{n,t},z_{\mathrm{tex},t}\mid K_t,K^{\text{act}}_t)=0.

$$
:::

:::{prf:definition} Closure Defect
:label: def-closure-defect

**Closure Defect (kernel-level).** Write the micro-dynamics as a Markov kernel $P(dz'\mid z,a)$ and let $P_\Pi(\cdot\mid z,a)$ be the pushforward kernel on $\mathcal{K}$ induced by $\Pi$. A learned macro-dynamics kernel $\bar{P}(\cdot\mid k,a)$ is enclosure-correct iff

$$
P_\Pi(\cdot\mid z,a)=\bar{P}(\cdot\mid \Pi(z),a)
\quad\text{for }P\text{-a.e. }z.

$$
A canonical defect functional is the expected divergence

$$
\delta_{\text{CE}}
:=
\mathbb{E}_{z,a}\Big[D_{\mathrm{KL}}\big(P_\Pi(\cdot\mid z,a)\ \Vert\ \bar{P}(\cdot\mid \Pi(z),a)\big)\Big].

$$
:::

:::{prf:assumption} Regularity Conditions for the Fragile Agent
:label: asm-regularity-conditions

1. **Smoothness:** $V \in C^2(\mathcal{Z})$ --- the Hessian exists and is continuous
2. **Positive Definiteness:** $G(z) \succ 0$ for all $z \in \mathcal{Z}$ --- the metric is non-degenerate
3. **Lipschitz Dynamics:** $\|f(z_1, a) - f(z_2, a)\| \leq L\|z_1 - z_2\|$ --- no discontinuities
4. **Bounded State Space:** $\mathcal{Z}$ is compact, or $V$ has appropriate growth at infinity
:::

:::{prf:definition} Local Conditioning Scale
:label: def-local-conditioning-scale

Let $(\mathcal{Z}, G)$ be the Riemannian latent manifold. Define a local scale parameter $\Theta: \mathcal{Z} \to \mathbb{R}^+$ as the trace of the inverse metric:

$$
\Theta(z) := \frac{1}{d} \operatorname{Tr}\left( G^{-1}(z) \right)

$$
where $d = \dim(\mathcal{Z})$. The corresponding **precision / coupling coefficient** is
$\beta_{\text{cpl}}(z) = [\Theta(z)]^{-1}$. When entropy regularization is tied to geometry, interpret
$\beta_{\text{cpl}}$ as a local inverse temperature; in an isothermal approximation where $\beta_{\text{cpl}}$ is
constant, set $\beta_{\text{cpl}} = 1/T_c$.
Units: if $z$ carries units $[z]$, then $[G]=\mathrm{nat}\,[z]^{-2}$ implies $[\Theta]=[z]^2/\mathrm{nat}$ and $[\beta_{\text{cpl}}]=\mathrm{nat}/[z]^2$ (dimensionless when $z$ is normalized).

:::

:::{prf:lemma} Variance-Curvature Correspondence
:label: lem-variance-curvature-correspondence

The covariance of the policy $\pi(a|z)$ is coupled to the curvature/sensitivity encoded by $G$. In entropy-regularized control, a natural scaling is:

$$
\Sigma_\pi(z) \propto \beta_{\text{cpl}}(z)^{-1} \cdot G^{-1}(z)

$$
*Proof (sketch).* In maximum-entropy control / exponential-family models, stationary distributions over latent states
often take an exponential form $p(z)\propto \exp(-V(z)/T_c)$. In an isothermal approximation where
$\beta_{\text{cpl}}$ is constant, identify $T_c = \beta_{\text{cpl}}^{-1}$. Matching this form with a geometry-aware
update implies that policy covariance scales inversely with the sensitivity metric. Deviations can be measured by a
**consistency defect** $\mathcal{D}_{\beta_{\text{cpl}}} := \|\nabla \log p + \beta_{\text{cpl}} \nabla_A V\|_G^2$.

:::

:::{prf:definition} Entropy-Regularized Objective Functional
:label: def-entropy-regularized-objective-functional

Let $d\mu_G:=\sqrt{|G|}\,dz$ be the Riemannian volume form on $\mathcal{Z}$ and let $p(z)$ be a probability density with respect to $d\mu_G$. For a (dimensionless) trade-off coefficient $T_c\ge 0$, define

$$
\mathcal{F}[p,\pi]
:=
\int_{\mathcal{Z}} p(z)\Big(V(z) - T_c\,H(\pi(\cdot\mid z))\Big)\,d\mu_G,

$$
where $H(\pi(\cdot\mid z)) := -\mathbb{E}_{a\sim \pi(\cdot\mid z)}[\log \pi(a\mid z)]$ is the per-state policy entropy (in nats). Because $V$ and $H$ are measured in nats ({ref}`sec-units-and-dimensional-conventions`), $T_c$ is dimensionless.

:::

:::{prf:definition} Belief Density
:label: def-belief-density

Let $p(z,s)\ge 0$ be a density with respect to $d\mu_G$ representing the agent's belief (or belief-weight) over latent coordinates. In closed-system idealizations one may impose $\int_{\mathcal{Z}}p(z,s)\,d\mu_G=1$; in open-system implementations with explicit projections/reweightings we track the unnormalized mass and renormalize when needed ({ref}`sec-intrinsic-motivation-maximum-entropy-exploration`).

:::

:::{prf:definition} Transport Field
:label: def-transport-field

Let $v\in\Gamma(T\mathcal{Z})$ be a vector field describing the instantaneous transport of belief mass on $\mathcal{Z}$. In a value-gradient-flow idealization (used only for intuition), one may take

$$
v^i(z) := -G^{ij}(z)\frac{\partial V}{\partial z^j},

$$
so transport points in the direction of decreasing $V$ (Riemannian steepest descent). Units: if computation time is measured in solver units, then $[v]=[z]/\mathrm{solver\ time}$ (map to $\mathrm{step}$ using the $t \leftrightarrow s$ budget in {ref}`sec-the-chronology-temporal-distinctions`).

:::

:::{prf:lemma} Continuity Equation for Transport
:label: lem-continuity-equation-for-transport

If the belief density evolves only by deterministic transport under $v$ (no internal sources/sinks), then it satisfies the continuity equation

$$
\frac{\partial p}{\partial s} + \nabla_i \left( p v^i \right) = 0

$$
where $\nabla_i$ denotes the Levi-Civita covariant derivative associated with $G$.

:::

:::{prf:definition} Source Residual
:label: def-source-residual

In general, belief evolution may include additional update effects (e.g. approximation error, off-manifold steps, or explicit projection/reweighting). We collect these into a residual/source term $\sigma(z,s)$:

$$
\frac{\partial p}{\partial s} + \operatorname{div}_G(p v) = \sigma

$$
Interpreting $\sigma$:
1. If $\sigma>0$ on a region, belief mass is being created there beyond pure transport; this indicates an **ungrounded internal update** relative to the transport model.
2. If $\sigma<0$, belief mass is being removed beyond pure transport (aggressive forgetting or projection).
3. Integrating over any measurable region $U\subseteq\mathcal{Z}$ and applying the divergence theorem yields the exact mass balance

   $$
   \frac{d}{ds}\int_U p\,d\mu_G
   =
   -\oint_{\partial U}\langle p v,n\rangle\,dA_G
   +\int_U \sigma\,d\mu_G.

   $$
   For {math}`U=\mathcal{Z}` this relates net mass change to boundary flux and the integrated residual.

:::

:::{prf:proposition} Mass Conservation in a Closed Enclosure
:label: prop-mass-conservation-in-a-closed-enclosure

If $\sigma\equiv 0$ and the boundary flux vanishes (e.g. $\langle p v,n\rangle=0$ on $\partial\mathcal{Z}$), then the total belief mass

$$
\mathcal{V}(s):=\int_{\mathcal{Z}}p(z,s)\,d\mu_G

$$
is constant in time.

*Proof.* Applying the divergence theorem on the Riemannian manifold:

$$
\frac{d\mathcal{V}}{ds} = \int_{\mathcal{Z}} \frac{\partial p}{\partial s} d\mu_G = -\int_{\mathcal{Z}} \operatorname{div}_G(p v) d\mu_G = -\int_{\partial \mathcal{Z}} \langle p v, n \rangle dA = 0

$$
assuming there is no net boundary contribution and no internal source term. In applications we do not estimate $\sigma$ pointwise; instead we monitor surrogate checks (e.g. BoundaryCheck and coupling-window metrics) that are sensitive to persistent boundary decoupling (Sections 3 and 15).

:::

:::{prf:definition} Observation Inflow Form
:label: def-observation-inflow-form

Let $j \in \Omega^{d-1}(\partial \mathcal{Z})$ be the **observation inflow form**. This form represents the rate of information entering the model through the interface.

:::

:::{prf:theorem} Generalized Conservation of Belief
:label: thm-generalized-conservation-of-belief

The evolution of the belief density $p$ satisfies the **Global Balance Equation**:

$$
\frac{d}{ds}\int_{\mathcal{Z}}p\,d\mu_G
=
-\oint_{\partial \mathcal{Z}} \langle p v,n\rangle\,dA_G
\;+\;
\int_{\mathcal{Z}} \sigma\,d\mu_G.

$$
where $n$ is the outward unit normal and $dA_G$ is the induced boundary area element. (Equivalently, if $\iota:\partial\mathcal{Z}\hookrightarrow \mathcal{Z}$ is the inclusion map, then the boundary flux is the pullback $\iota^*(p v\;\lrcorner\; d\mu_G)$.)

**The Architectural Sieve Condition (Node 13: BoundaryCheck).** The idealized "fully grounded" regime corresponds to $\sigma\approx 0$ in the interior: net changes in internal belief mass should be attributable to boundary influx and explicit projection events. Operationally we do not estimate $\sigma$ pointwise; instead Node 13 and the coupling-window diagnostics (Theorem {prf:ref}`thm-information-stability-window-operational`) enforce that the macro register remains coupled to boundary data (non-collapse of $I(X;K)$) and does not saturate ($H(K)$ stays below $\log|\mathcal{K}|$).

$$
\frac{d\mathcal{V}}{ds}
=
-\oint_{\partial \mathcal{Z}} \langle p v,n\rangle\,dA_G,

$$
in the case $\sigma\equiv 0$.

Here $\langle p v,n\rangle$ is the outward flux density across the boundary (negative values correspond to net inflow).

**Distinction: boundary-driven updates vs ungrounded updates**

1.  **Valid learning (boundary-driven):** The belief changes because there is non-negligible boundary flux, i.e. new observations justify updating the internal state.
2.  **Ungrounded update (internal source):** The belief changes despite negligible boundary flux, corresponding to $\sigma>0$ under the transport model. Operationally, this is a warning sign that internal rollouts are decoupled from the data stream and should be treated as unreliable for control until re-grounded.

:::

:::{prf:corollary} Boundary filter interpretation
:label: cor-boundary-filter-interpretation

Sieve Nodes 13-16 (Boundary/Overload/Starve/Align) can be interpreted as monitoring a trace-like coupling between bulk and boundary (informally: whether internal degrees of freedom remain supported by boundary evidence), analogous in spirit to the trace map $\operatorname{Tr}: H^1(\mathcal{Z}) \to H^{1/2}(\partial \mathcal{Z})$:

*   **Mode B.E (Injection):** Occurs when interface inflow exceeds the effective capacity of the manifold (Levin capacity), breaking the assumed operating regime.
*   **Mode B.D (Starvation):** Occurs when interface inflow is too weak, causing the internal information volume to decay (catastrophic forgetting).

:::

## 03_architecture/01_compute_tiers.md

:::{prf:definition} Attentive Routing Law
:label: def-attentive-routing-law

$$
w_i(x) := \frac{\exp\left(\frac{\langle k_i(z), q(z,f) \rangle}{\tau(z)}\right)}{\sum_{j=1}^{N_c} \exp\left(\frac{\langle k_j(z), q(z,f) \rangle}{\tau(z)}\right)}

$$
where $k_i(z) = U(z)\,\text{base\_query}_i$ and $\tau(z)$ is the metric-aware temperature. With `covariant_attn=False`, $U(z)=I$ and $\text{base\_query}_i = c_i$, reducing to dot-product routing on chart centers. This mechanism is **permutation invariant**: shuffling the memory order of the chart tokens merely shuffles the output indices without changing the underlying topology or geometry.

:::

:::{prf:definition} The Macro-State Tree
:label: def-the-macro-state-tree

Let $\mathcal{T}$ be a rooted tree representing the hierarchical partition of the state space.

1. The **root** represents the entire observation space $\mathcal{X}$.
2. **Level 1 nodes** correspond to charts $K_{\text{chart}} \in \{1, \dots, N_c\}$.
3. **Level 2 nodes** correspond to codes $K_{\text{code}} \in \{1, \dots, N_v\}$ within a chart.
4. Edges represent the containment relationship (refinement of the partition).

Equip the vertex set $V(\mathcal{T})$ with the graph metric $d_{\mathcal{T}}$ (shortest path length).

:::

:::{prf:lemma} Gromov Hyperbolicity
:label: lem-gromov-hyperbolicity

The tree metric space $(\mathcal{T}, d_{\mathcal{T}})$ is $0$-hyperbolic in the sense of Gromov. That is, for any geodesic triangle, each side is contained in the $0$-neighborhood of the union of the other two sides.
*Proof.* Standard result for simplicial trees. $\square$

:::

:::{prf:corollary} The Hyperbolic Embedding
:label: cor-the-hyperbolic-embedding

There exists a quasi-isometric embedding $\iota: V(\mathcal{T}) \hookrightarrow \mathbb{H}^n$ into $n$-dimensional hyperbolic space such that the depth in the tree correlates with the hyperbolic distance from a basepoint. In the upper half-space model $\mathbb{H}^n = \{(x, y) : y > 0\}$ with metric $ds^2 = (dx^2 + dy^2)/y^2$, tree depth $\ell$ maps to $\log(1/y)$; equivalently, in the Poincare ball model, depth maps to $\tanh^{-1}(r)$ where $r \in [0,1)$ is the radial coordinate.

This identifies the **discrete macro-register** $K_t = (K_{\text{chart}}, K_{\text{code}})$ as the bulk of a hyperbolic geometry. Navigating from the root to a leaf corresponds to moving from the interior of $\mathbb{H}^n$ toward the ideal boundary $\partial_\infty \mathbb{H}^n$, increasing information resolution at each step.

:::

:::{prf:definition} The Local Fibre Structure
:label: def-the-local-fibre-structure

We model the latent space $\mathcal{Z}$ as a disjoint union of fibres over the discrete index set $\mathcal{K}$:

$$
\mathcal{Z} = \bigsqcup_{k \in \mathcal{K}} \mathcal{Z}_n^{(k)}, \qquad \mathcal{Z}_n^{(k)} \cong \mathbb{R}^{d_n}.

$$
For each macro-symbol $k \in \mathcal{K}$, the fibre $\mathcal{Z}_n^{(k)}$ represents the **structured nuisance** space (local pose/basis coordinates).

The interpolation of this discrete structure into a continuous manifold is achieved by the Attentive Atlas ({ref}`sec-tier-the-attentive-atlas`), which provides soft transition functions (partitions of unity) $\{w_i(x)\}$ that interpolate between fibres in overlap regions.

:::

:::{prf:proposition} Texture as the Ideal Boundary
:label: prop-texture-as-the-ideal-boundary

Let $\mathcal{M}$ be the Riemannian manifold constructed above. The **texture residual** $z_{\text{tex}}$ corresponds to the behavior of the state at the **conformal boundary at infinity**, $\partial_\infty \mathbb{H}^n$.

*Proof (Construction).*

1. Consider a sequence of refining codes $(K_{\text{chart}}^{(n)}, K_{\text{code}}^{(n)})$ representing a path $\gamma$ in the tree $\mathcal{T}$ extending to infinite depth.
2. As the depth $n \to \infty$, the volume of the region covered by code $K^{(n)}$ in the observation space $\mathcal{X}$ shrinks to zero (assuming a non-degenerate shutter).
3. In the hyperbolic metric of the latent space, the distance from the basepoint $d(o, \gamma(n)) \to \infty$.
4. The residual $z_{\text{tex}}$ is defined as the information remaining after finite truncation at level $n$. Specifically, $z_{\text{tex}} = \Delta_{\text{total}} - z_n$.
5. If we interpret the encoding process as a flow toward the boundary of $\mathbb{H}^n$, then $z_{\text{tex}}$ represents the **transverse coordinates** at the cutoff surface $\Sigma_\epsilon$.
6. Taking the limit $\epsilon \to 0$, $z_{\text{tex}}$ maps to the **limit set** $\Lambda \subset \partial_\infty \mathbb{H}^n$. The mathematical structure parallels the AdS/CFT bulk-boundary correspondence: the fields $(K, z_n)$ reconstruct $(x)$ up to a cutoff; $z_{\text{tex}}$ is the UV (high-frequency) data living strictly at the conformal boundary. $\square$

**Operational Implication:**
This formalizes why $z_{\text{tex}}$ must be excluded from dynamics ($S_t$) and control ($\pi_\theta$). The dynamics $S_t$ operate on the **bulk** (finite-energy excitations inside the hyperbolic volume). The texture $z_{\text{tex}}$ lives at the **boundary at infinity** (infinite energy / zero scale). Coupling the bulk dynamics to the boundary fluctuations violates the separation of scales and leads to the Labyrinthine failure mode (Mode T.C).

:::

:::{prf:definition} The Latent Metric Tensor
:label: def-the-latent-metric-tensor

Working in the upper half-space model where depth $\rho \in [0, \infty)$ corresponds to $y = e^{-\rho}$, the metric $ds^2$ on the global latent space $\mathcal{Z}$ takes the form:

$$
ds^2 = d\rho^2 + d\sigma_{\mathcal{K}}^2 + e^{-2\rho} \|dz_n\|^2

$$
where:

* $\rho$ is the resolution depth (hierarchy level), with $\rho = 0$ at the root and $\rho \to \infty$ at the boundary.
* $d\sigma_{\mathcal{K}}^2$ is the (discrete) metric on tree branches at fixed depth—operationally, it counts the number of chart/code transitions.
* $\|dz_n\|^2$ is the Euclidean metric on the structured nuisance $z_n$.
* The factor $e^{-2\rho}$ indicates that as resolution increases (deeper in the tree), the effective magnitude of nuisance variations shrinks exponentially relative to the macroscopic decision branches.

**Rigorous Interpretation of $z_n$:**
The structured nuisance $z_n$ is not stochastic noise; it is the **tangent space coordinate** on the horosphere (surface of constant depth $\rho$) determined by the active macro-symbol $K$. Horospheres in hyperbolic space are intrinsically flat (zero curvature), which is why local linear control theory (LTI approximations) applies within a single chart, even though the global geometry is hyperbolic.

:::

:::{prf:definition} The Peeling Step
:label: def-the-peeling-step

At layer $\ell$, the input signal $x^{(\ell)}$ is decomposed into a structural component (the **Effective Theory** at scale $\ell$) and a residual component (the **High-Frequency Fluctuations**).

1. **Analysis (Encoding):** The block identifies the macro-symbol $K^{(\ell)}$ and structured nuisance $z_n^{(\ell)}$ that best approximate $x^{(\ell)}$:

$$
(K^{(\ell)}, z_n^{(\ell)}) = \mathcal{E}^{(\ell)}(x^{(\ell)})

$$
2. **Synthesis (Effective Reconstruction):** The block generates the signal explained by this structure:

$$
\hat{x}^{(\ell)} = \mathcal{D}^{(\ell)}(K^{(\ell)}, z_n^{(\ell)})

$$
3. **Residual Computation (Texture Extraction):** The unexplained signal is isolated:

$$
z_{\text{tex}}^{(\ell)} = x^{(\ell)} - \hat{x}^{(\ell)}

$$
:::

:::{prf:definition} The Rescaling Operator / Renormalization
:label: def-the-rescaling-operator-renormalization

To prevent signal decay (vanishing activations) without using skip connections, we explicitly renormalize the residual to unit variance before passing it to the next scale:

$$
x^{(\ell+1)} = \frac{z_{\text{tex}}^{(\ell)}}{\sigma^{(\ell)} + \epsilon}, \qquad \sigma^{(\ell)} = \sqrt{\mathrm{Var}(z_{\text{tex}}^{(\ell)}) + \epsilon}

$$
The scalar $\sigma^{(\ell)}$ is stored as a state variable (the **scale factor**) for the decoding pass.

:::

:::{prf:definition} Total Reconstruction
:label: def-total-reconstruction

The original signal is reconstructed by summing the contributions of all scales, modulated by their respective scale factors. Define $\Pi^{(\ell)} := \prod_{j=0}^{\ell-1} \sigma^{(j)}$ with the convention $\Pi^{(0)} = 1$ (empty product). Then:

$$
\hat{x} = \sum_{\ell=0}^{L-1} \Pi^{(\ell)} \cdot \hat{x}^{(\ell)} + \Pi^{(L)} \cdot x^{(L)}

$$
:::

:::{prf:proposition} Gradient Preservation via Orthogonality
:label: prop-gradient-preservation-via-orthogonality

Let $W$ be a weight matrix satisfying $W^T W = I$ (semi-orthogonality). Then:
1. All singular values of $W$ equal 1.
2. The backward gradient $\nabla_x \mathcal{L} = W^T \nabla_y \mathcal{L}$ satisfies $\|\nabla_x \mathcal{L}\| = \|\nabla_y \mathcal{L}\|$.
3. Neither explosion nor vanishing occurs across the layer.

*Proof.* For semi-orthogonal $W$, the singular values are exactly 1. The Jacobian $\partial y / \partial x = W$ has $\|W\|_2 = 1$. By the chain rule, gradient norms are preserved. $\square$

This is why the gradient flow table ({ref}`sec-orthonormal-constraints-for-atlas-charts`) shows Preserved for orthogonal $W$ versus Explodes or vanishes for arbitrary $W$.

:::

:::{prf:proposition} Forward Activation Stability
:label: prop-forward-activation-stability

With variance rescaling:
1. $\mathrm{Var}(x^{(\ell)}) = 1$ for all $\ell$ (by construction).
2. Non-linearities (GELU) operate in their active region, avoiding saturation.
3. The backward gradient is scaled by $1/\sigma^{(\ell)}$, amplifying gradients for fine-scale layers.

**Gradient Amplification Analysis:** Let the loss $\mathcal{L}$ depend on the output of block $\ell$. The gradient flowing back to block $\ell-1$ includes the factor:

$$
\frac{\partial x^{(\ell)}}{\partial z_{\text{tex}}^{(\ell-1)}} = \frac{1}{\sigma^{(\ell-1)}}

$$
Since each block successfully explains part of the signal, the residual standard deviation $\sigma^{(\ell)} < 1$ (the texture has less variance than the unit-normalized input). This implies:
- **Without rescaling:** inputs to deeper layers decay exponentially ($\|x^{(\ell)}\| \to 0$), killing activations.
- **With rescaling:** inputs $x^{(\ell)}$ remain $O(1)$ (unit variance), keeping non-linearities in their active region.
- **Gradient amplification:** the backward gradient includes the factor $1/\sigma^{(\ell-1)} > 1$, counteracting the natural decay of fine-scale influence on the global loss.

This prevents the **Spectral Bias** where neural networks preferentially learn low frequencies and ignore high-frequency structure.

:::

:::{prf:theorem} Dynamical Isometry without Skip Connections
:label: thm-dynamical-isometry-without-skip-connections

A stacked TopoEncoder with:
1. OrthogonalLinear layers satisfying $\|W^T W - I\|_F < \epsilon_{\text{orth}}$,
2. Variance rescaling at each scale transition,
3. Spectral normalization with $\sigma_{\max}(W_\ell) \leq K$,

achieves approximate dynamical isometry: the singular values of the input-output Jacobian $J = \partial \hat{x} / \partial x$ satisfy $\sigma_i(J) \in [1/\kappa, \kappa]$ for a condition number $\kappa = O(K^L \cdot \prod_\ell (1 + \epsilon_{\text{orth}}))$.

*Proof sketch.* Each layer contributes a factor with singular values in $[1-\epsilon, 1+\epsilon]$ (orthogonality) or $[0, K]$ (spectral norm). The variance rescaling ensures activations remain $O(1)$, preventing saturation. The product of $L$ such factors yields the stated bound. $\square$

:::

:::{prf:definition} Factorized Jump Operator
:label: def-factorized-jump-operator

For each chart $i$, define:
- An **encoder** $B_i: \mathbb{R}^{d_n} \to \mathbb{R}^r$ that lifts local coordinates to the global tangent space.
- A **decoder** $A_j: \mathbb{R}^r \to \mathbb{R}^{d_n}$ that projects from the global tangent space to chart $j$'s coordinates.
- Bias terms $c_i \in \mathbb{R}^r$ and $d_j \in \mathbb{R}^{d_n}$.

The transition $L_{i \to j}$ is then:

$$
L_{i \to j}(z) = A_j(B_i z + c_i) + d_j

$$
:::

:::{prf:proposition} Parameter Efficiency
:label: prop-parameter-efficiency

The factorized parameterization requires $O(K \cdot r \cdot d_n)$ parameters instead of $O(K^2 \cdot d_n^2)$.

*Proof.* Each chart contributes one encoder $B_i \in \mathbb{R}^{r \times d_n}$, one decoder $A_i \in \mathbb{R}^{d_n \times r}$, and bias vectors $c_i \in \mathbb{R}^r$, $d_i \in \mathbb{R}^{d_n}$. Total: $K(r \cdot d_n + d_n \cdot r + r + d_n) = O(K \cdot r \cdot d_n)$. $\square$

For typical values ($K = 64$, $d_n = 16$, $r = 8$), this yields $64 \times (2 \times 8 \times 16 + 8 + 16) = 17,920$ parameters—approximately a $58\times$ reduction compared to the naive $\sim 10^6$.

:::

:::{prf:definition} Overlap Consistency Loss
:label: def-overlap-consistency-loss

For a pair of charts $(i, j)$ with non-empty overlap, define the pairwise consistency loss as:

$$
\mathcal{L}_{\text{jump}}^{(i,j)} = \mathbb{E}_{x : w_i(x) > \tau, \, w_j(x) > \tau} \left[ \left\| z_n^{(j)} - L_{i \to j}(z_n^{(i)}) \right\|^2 \right]

$$
where $z_n^{(i)}$ and $z_n^{(j)}$ are the nuisance coordinates computed independently by chart $i$ and chart $j$'s encoders, and $w_i(x), w_j(x)$ are the soft router weights. The total overlap consistency loss sums over all overlapping pairs:

$$
\mathcal{L}_{\text{jump}} = \sum_{i < j} \mathcal{L}_{\text{jump}}^{(i,j)}

$$
**Intuition:** If the encoder correctly identifies that $x$ belongs to both charts, then applying the jump operator to chart $i$'s encoding should yield chart $j$'s encoding. Any discrepancy indicates that the transition functions are inconsistent with the actual data manifold.

**Implementation Details:**

1. **Overlap Detection:** A point $x$ is in the overlap $U_i \cap U_j$ if both router weights exceed a threshold:

   $$
   \mathbf{1}[x \in U_i \cap U_j] \approx \mathbf{1}[w_i(x) > \tau] \cdot \mathbf{1}[w_j(x) > \tau]

   $$
   With soft routers ({ref}`sec-tier-the-attentive-atlas`), we use the product $w_i(x) \cdot w_j(x)$ as a soft indicator.

2. **Sampling Overlaps:** Computing all $K^2$ pairs is expensive. We sample:
   - The top-2 charts per point (from router weights).
   - Random chart pairs with probability proportional to their co-activation frequency.

3. **Symmetry Penalty (Optional):** To encourage approximate invertibility:

   $$
   \mathcal{L}_{\text{inv}} = \mathbb{E}_{x, i, j} \left[ \left\| z_n^{(i)} - L_{j \to i}(L_{i \to j}(z_n^{(i)})) \right\|^2 \right]

   $$
:::

## 03_architecture/02_disentangled_vae.md

:::{prf:definition} The Three-Channel Latent Decomposition
:label: def-three-channel-latent

The internal state at time $t$ decomposes as:

$$
Z_t = (K_t, z_{n,t}, z_{\mathrm{tex},t})
$$

where:

1. $K_t = (K_{\mathrm{chart}}, K_{\mathrm{code}})$ is the discrete macro state. $K_{\mathrm{chart}}$
   selects an atlas chart, and $K_{\mathrm{code}}$ selects a local code within that chart.
2. $z_{n,t} \in \mathbb{R}^{d_n}$ is the structured nuisance (pose, basis, gauge residual).
3. $z_{\mathrm{tex},t} \in \mathbb{R}^{d_{\mathrm{tex}}}$ is reconstruction-only texture.

The geometry latent used by the decoder is

$$
z_{\mathrm{geo}} = c_{\mathrm{bar}} + z_{q,\mathrm{st}} + z_n
$$

where $c_{\mathrm{bar}}$ is the chart center mixture and $z_{q,\mathrm{st}}$ is the straight-through
quantized code.
:::

:::{prf:definition} The Golden Rule of Causal Enclosure
:label: def-causal-enclosure

The macro symbol must satisfy the causal enclosure property:

$$
P(K_{t+1} \mid K_t, a_t) \text{ is sharply concentrated}
$$

and texture independence:

$$
I(K_{t+1}; Z_{\mathrm{tex},t} \mid K_t, a_t) = 0.
$$

Optionally, in the strongest form, nuisance independence also holds:

$$
I(K_{t+1}; Z_{n,t} \mid K_t, a_t) = 0.
$$

That is: nuisance and texture should not be required to predict the next macro symbol once action
is accounted for.
:::

:::{prf:definition} The Total TopoEncoder Loss
:label: def-total-disentangled-loss

The compound loss is:

$$
\mathcal{L}_{\text{total}} =
\mathcal{L}_{\text{recon}} + \mathcal{L}_{\text{vq}} + \lambda_{\text{ent}}\,\mathcal{L}_{\text{entropy}} +
\lambda_{\text{cons}}\,\mathcal{L}_{\text{consistency}} +
\sum_{i \in \text{tiers}} \lambda_i \mathcal{L}_i +
\lambda_{\text{jump}}\,\mathcal{L}_{\text{jump}} +
\lambda_{\text{sup}}\,\mathcal{L}_{\text{sup}}.
$$

Where:

- $\mathcal{L}_{\text{recon}} = \|x - \hat{x}\|^2$ (MSE reconstruction).
- $\mathcal{L}_{\text{vq}}$ is the codebook + commitment loss.
- $\mathcal{L}_{\text{entropy}}$ is routing entropy (encourages sharp routing).
- $\mathcal{L}_{\text{consistency}}$ aligns encoder and decoder routing.
- Tiered losses include variance, diversity, separation, codebook centering, chart center
  separation, residual scale, window, disentangle, orthogonality, code entropy, per-chart code
  entropy, KL prior, orbit, and VICReg invariance.
- $\mathcal{L}_{\text{jump}}$ enforces chart transition consistency when the jump operator is
  enabled.
- $\mathcal{L}_{\text{sup}}$ applies supervised topology when labels are available.

Learned precisions can reweight reconstruction, VQ, and supervised terms when enabled.
:::

:::{prf:definition} The Closure Ratio
:label: def-closure-ratio

Let $K$ be the chart assignment and $N_c$ the number of charts. Define

$$
\rho_{\text{close}} = 1 - \frac{H(K \mid X)}{\log N_c}
\;=\; \frac{I(X;K)}{\log N_c}.
$$

Values near 1 indicate sharp, informative routing; values near 0 indicate diffuse routing.
:::

:::{prf:definition} Hierarchical Latent Stack
:label: def-hierarchical-latent

A multi-scale atlas uses a hierarchy of discrete chart codes:

$$
Z_t = (K_t^{(0)}, K_t^{(1)}, \ldots, K_t^{(L)}, z_{n,t}, z_{\mathrm{tex},t})
$$

where each level $\ell$ has its own chart set and codebook, and higher levels capture coarser
structure.
:::

## 03_architecture/03_optimization.md

:::{prf:definition} Preconditioned Update
:label: def-preconditioned-update
The preconditioned update is

$$
\theta_{t+1} = \theta_t - \eta_t M_t g_t,
$$
with $M_t$ SPD and $g_t = \nabla\mathcal{V}(\theta_t)$.
:::

:::{prf:theorem} Preconditioned Descent (Sufficient Condition)
:label: thm-preconditioned-descent
Under A1--A2, if

$$
0 < \eta_t < \frac{2 m_{\min}}{L m_{\max}^2},
$$
then the update in Definition {prf:ref}`def-preconditioned-update` satisfies

$$
\mathcal{V}(\theta_{t+1}) \le \mathcal{V}(\theta_t) - \left(\eta_t m_{\min} - \frac{L}{2}\eta_t^2 m_{\max}^2\right)
\|g_t\|^2.
$$
In particular, $\mathcal{V}$ decreases whenever $g_t \ne 0$. If
$\eta_t = 2 m_{\min} / (L m_{\max}^2)$, the right-hand side yields nonincrease.
:::

:::{prf:proof}
By $L$-smoothness (A1), for $d_t := \eta_t M_t g_t$,

$$
\mathcal{V}(\theta_t - d_t)
\le \mathcal{V}(\theta_t) - g_t^\top d_t + \frac{L}{2}\|d_t\|^2.
$$
Since $M_t$ is SPD with eigenvalues in $[m_{\min}, m_{\max}]$ (A2), we have
$g_t^\top M_t g_t \ge m_{\min}\|g_t\|^2$ and
$\|M_t g_t\|^2 \le m_{\max}^2\|g_t\|^2$. Substituting yields

$$
\mathcal{V}(\theta_{t+1})
\le \mathcal{V}(\theta_t) - \eta_t m_{\min}\|g_t\|^2 + \frac{L}{2}\eta_t^2 m_{\max}^2\|g_t\|^2.
$$
The right-hand side is strictly smaller than $\mathcal{V}(\theta_t)$ whenever
$\eta_t < 2 m_{\min} / (L m_{\max}^2)$ and yields nonincrease at equality. \qedhere
:::

:::{prf:definition} Relative Trust Region (Mach Limit)
:label: def-relative-trust-region
A relative trust region is the constraint

$$
\|\theta_{t+1} - \theta_t\| \le \kappa\,(\|\theta_t\| + \epsilon_\theta),
$$
with $\kappa \in (0,1)$ and $\epsilon_\theta \ge 0$. The case $\epsilon_\theta = 0$ recovers the strict relative form,
while $\epsilon_\theta > 0$ avoids degeneracy at $\|\theta_t\| = 0$.
:::

:::{prf:lemma} Trust-Region Scaling Preserves Descent
:label: lem-trust-region-scaling
Let $d_t = \eta_t M_t g_t$ with $\eta_t$ satisfying Theorem {prf:ref}`thm-preconditioned-descent`. Define the scaled step

$$
\tilde d_t = s d_t, \qquad s := \min\left(1, \frac{\kappa(\|\theta_t\| + \epsilon_\theta)}{\|d_t\|} \right).
$$
Then $\mathcal{V}(\theta_t - \tilde d_t) \le \mathcal{V}(\theta_t)$.
:::

:::{prf:proof}
If $d_t = 0$, then $\theta_{t+1} = \theta_t$ and the inequality holds trivially. Otherwise, for $s \in (0,1]$,
$\mathcal{V}(\theta_t - s d_t)$ satisfies the smoothness bound

$$
\mathcal{V}(\theta_t - s d_t)
\le \mathcal{V}(\theta_t) - s g_t^\top d_t + \frac{L}{2}s^2 \|d_t\|^2.
$$
The right-hand side is a quadratic in $s$ with positive linear term and nonnegative curvature. Since
Theorem {prf:ref}`thm-preconditioned-descent` guarantees descent at $s=1$, any smaller $s$ preserves
nonincreasing $\mathcal{V}$. \qedhere
:::

:::{prf:proposition} Discrete Varentropy Brake
:label: prop-varentropy-brake-discrete
Let $T_t > 0$ be the cognitive temperature and $V_H(\theta_t)$ the varentropy. Define

$$
T_{t+1} = T_t\left(1 - \frac{\eta_T}{1 + \gamma V_H(\theta_t)}\right),
$$
with $\eta_T \in (0,1)$ and $\gamma > 0$. Then

1. $0 < T_{t+1} \le T_t$ (temperature is positive and nonincreasing), and
2. $|T_{t+1} - T_t| \le \eta_T T_t / (1 + \gamma V_H(\theta_t))$ (cooling is slowed when $V_H$ is large).
:::

:::{prf:proof}
Since $\eta_T \in (0,1)$ and $V_H \ge 0$, the multiplier lies in $(0,1]$, proving positivity and monotone
nonincrease. The second statement follows immediately from the update definition. \qedhere
:::

:::{prf:definition} Gradient-Momentum Alignment
:label: def-gradient-alignment
Let $g_t$ be the gradient and $m_t$ a momentum estimate. Define the alignment score

$$
a_t := g_t^\top m_t.
$$
:::

:::{prf:proposition} Alignment-Triggered Step Damping
:label: prop-alignment-step-damping
Let $a_t := g_t^\top m_t$ and fix a damping factor $\rho \in (0,1]$. Define

$$
\eta_t^{+} :=
\begin{cases}
\rho\,\eta_t & \text{if } a_t < 0, \\
\eta_t & \text{otherwise}.
\end{cases}
$$
If $\eta_t$ satisfies Theorem {prf:ref}`thm-preconditioned-descent`, then the damped step with $\eta_t^{+}$ also
decreases $\mathcal{V}$.
:::

:::{prf:proof}
The update with $\eta_t^{+}$ is a scaled version of the step with $\eta_t$, with scaling factor in $(0,1]$.
By the same smoothness inequality used in the proof of Theorem {prf:ref}`thm-preconditioned-descent`, any reduction
in step size preserves nonincreasing $\mathcal{V}$ as long as the original step was in the descent regime.
Equivalently, this is a special case of the scaling argument in Lemma {prf:ref}`lem-trust-region-scaling`. \qedhere
:::

:::{prf:proposition} SNR-Gated Step Size
:label: prop-snr-gate
Under A1--A3 and the stochastic update $\theta_{t+1} = \theta_t - \eta_t M_t \hat g_t$, assume $g_t \ne 0$. The
expected Lyapunov change satisfies

$$
\mathbb{E}[\mathcal{V}(\theta_{t+1})\mid\theta_t]
\le \mathcal{V}(\theta_t)
- \eta_t m_{\min}\|g_t\|^2
+ \frac{L}{2}\eta_t^2 m_{\max}^2\left(\|g_t\|^2 + \sigma^2\right).
$$
Consequently, a sufficient condition for expected descent is

$$
\eta_t
\le
\frac{2 m_{\min}}{L m_{\max}^2\left(1 + \sigma^2 / \|g_t\|^2\right)}
= \frac{2 m_{\min}}{L m_{\max}^2}\cdot\frac{\mathrm{SNR}}{1+\mathrm{SNR}},
$$
with $\mathrm{SNR} := \|g_t\|^2 / \sigma^2$. If $g_t = 0$, the sufficient bound reduces to $\eta_t = 0$.
:::

:::{prf:proof}
Apply the smoothness bound with $d_t = \eta_t M_t \hat g_t$ and take conditional expectations.
Use $\mathbb{E}[\hat g_t] = g_t$ and
$\mathbb{E}[\|\hat g_t\|^2] = \|g_t\|^2 + \mathbb{E}[\|\xi_t\|^2] \le \|g_t\|^2 + \sigma^2$.
Then apply the eigenvalue bounds from A2 and solve for $\eta_t$ to make the coefficient negative. \qedhere
:::

:::{prf:definition} Log-LR Conduction Update
:label: def-log-lr-conduction
Let $\eta_i > 0$ be per-group learning rates and $x_i := \log(\eta_i)$. Define

$$
 x_i^{+} = x_i + \frac{k}{2}(x_{i-1} - 2 x_i + x_{i+1}),
$$
with Neumann boundary conditions $x_0 = x_1$, $x_{n+1} = x_n$ and conductivity $k \in [0,1]$.
:::

:::{prf:proposition} Conduction Contracts LR Disparities
:label: prop-conduction-contracts
Let $L$ be the path-graph Laplacian on $n$ groups and define the energy

$$
E(x) := \frac{1}{2} x^\top L x = \frac{1}{2}\sum_{i=1}^{n-1}(x_{i+1} - x_i)^2.
$$
The update in Definition {prf:ref}`def-log-lr-conduction` is gradient descent on $E$ with step size $k/2$, and for
$k \in [0,1]$ it satisfies $E(x^{+}) \le E(x)$.
:::

:::{prf:proof}
We have $\nabla E(x) = L x$ for the path graph. The update is
$x^{+} = x - (k/2) L x$, i.e. gradient descent with step size $k/2$.
Because $L$ is symmetric positive semidefinite with eigenvalues bounded by $\lambda_{\max} \le 4$ for a path graph,
step size $k/2 \le 1/2$ ensures $(1 - (k/2)\lambda)^2 \le 1$ for all eigenvalues. Therefore
$E(x^{+}) = \tfrac{1}{2} x^\top (I - (k/2)L)^\top L (I - (k/2)L)x \le E(x)$. \qedhere
:::

:::{prf:theorem} Thermodynamic Governor Stability (Conditional)
:label: thm-optimizer-conditional-stability
Under A1--A5, with updates that apply:
1. preconditioned descent (Theorem {prf:ref}`thm-preconditioned-descent`),
2. trust-region scaling (Lemma {prf:ref}`lem-trust-region-scaling`),
3. alignment-triggered step damping (Proposition {prf:ref}`prop-alignment-step-damping`),
4. varentropy brake (Proposition {prf:ref}`prop-varentropy-brake-discrete`),
5. SNR gating (Proposition {prf:ref}`prop-snr-gate`), and
6. log-LR conduction (Proposition {prf:ref}`prop-conduction-contracts`),

the optimizer produces a nonincreasing Lyapunov objective in the deterministic case and ensures expected descent
under bounded noise. Learning rates remain positive and coherent across adjacent groups, and the temperature schedule
obeys the adiabatic (varentropy) constraint.
:::

:::{prf:remark}
:label: rem-agent-optimization-conditional
These guarantees are conditional and local. They ensure stability and controlled descent, not global optimality.
Violations of A1--A5 (e.g., non-smooth losses, unbounded noise, misordered groups) void the guarantees.
:::

## 04_control/01_exploration.md

:::{prf:definition} Macro Path Distribution
:label: def-macro-path-distribution

Fix a horizon $H\in\mathbb{N}$ and a (possibly stochastic) policy $\pi(a\mid k)$. The induced distribution over length-$H$ macro state-action trajectories

$$
\xi := (K_t, A_t, K_{t+1}, A_{t+1}, \dots, A_{t+H-1}, K_{t+H})
    \in \mathcal{K}\times(\mathcal{A}\times\mathcal{K})^H

$$
conditioned on $K_t=k$ is

$$
P_\pi(\xi\mid k)
:=
\prod_{h=0}^{H-1}\pi(A_{t+h}\mid K_{t+h})\ \bar{P}(K_{t+h+1}\mid K_{t+h},A_{t+h}).

$$
(For continuous $\mathcal{A}$, interpret $P_\pi(\xi\mid k)$ as a density with respect to the action reference measure.)

:::

:::{prf:definition} Causal Path Entropy
:label: def-causal-path-entropy

The causal path entropy at $(k,H)$ under $\pi$ is the cumulative policy entropy along paths
$\xi\in\Gamma_H(k)$ induced by $\pi$ and $\bar{P}$:

$$
S_c(k,H;\pi)
:= \sum_{h=0}^{H-1} \mathbb{E}_{\xi\sim P_\pi(\cdot\mid k)}
\left[ H\!\left(\pi(\cdot\mid K_{t+h})\right) \right].

$$
Only policy randomness contributes; stochasticity in $\bar{P}$ does not add entropy credit.
The expectation is taken under the path law induced by $\pi$ and $\bar{P}$.
This quantity is well-typed because the macro register is discrete; for continuous $\mathcal{A}$, interpret
$H(\pi(\cdot\mid k))$ as a differential entropy with respect to the action reference measure.

:::

:::{prf:definition} Exploration Gradient, metric form
:label: def-exploration-gradient-metric-form

Let $z_{\text{macro}}=e_k\in\mathbb{R}^{d_m}$ denote the code embedding of $k$ ({ref}`sec-the-shutter-as-a-vq-vae`), and let $G$ be the relevant metric on the macro chart ({ref}`sec-second-order-sensitivity-value-defines-a-local-metric`). Define the exploration gradient as the metric gradient of state-action path entropy:

$$
\mathbf{g}_{\text{expl}}(e_k) := T_c\ \nabla_G S_c(k,H;\pi),

$$
where $T_c>0$ is the cognitive temperature ({prf:ref}`def-cognitive-temperature`). Operationally, gradients are taken through the continuous pre-quantization coordinates (straight-through VQ estimator); in the strictly symbolic limit, the gradient becomes a discrete preference ordering induced by $S_c(k,H;\pi)$.

**Interpretation (Exploration / Reachability).** $S_c(k,H;\pi)$ measures how much action-level randomness the
agent injects along trajectories from $k$ under $\pi$. Increasing $S_c$ preserves **agent-controlled reachability**:
the policy avoids committing to a narrow action sequence, independent of environmental stochasticity.

:::

:::{prf:definition} MaxEnt RL objective on macrostates
:label: def-maxent-rl-objective-on-macrostates

Let $\mathcal{R}(k,a)$ be an instantaneous reward/cost-rate term ({ref}`sec-re-typing-standard-rl-primitives-as-interface-signals`, {ref}`sec-the-hjb-correspondence`) and let $\gamma\in(0,1)$ be the discount factor (dimensionless). The maximum-entropy objective is

$$
J_{T_c}(\pi)
:=
\mathbb{E}_\pi\left[\sum_{t\ge 0}\gamma^t\left(\mathcal{R}(K_t,K^{\text{act}}_t) + T_c\,\mathcal{H}(\pi(\cdot\mid K_t))\right)\right],

$$
where $\mathcal{H}$ is Shannon entropy. This is the standard "utility + entropy regularization" objective.

**Regimes.**
- $T_c\to 0$: $\pi$ collapses toward determinism; behavior can be brittle under distribution shift.
- $T_c\to\infty$: $\pi$ approaches maximal entropy; behavior becomes overly random and may degrade grounding (BarrierScat).
- The useful regime is intermediate: enough entropy to remain robust, enough utility to remain directed.

:::

:::{prf:proposition} Soft Bellman form, discrete actions
:label: prop-soft-bellman-form-discrete-actions

Assume finite $\mathcal{A}$. Define the soft state value

$$
V^*(k) := \max_{\pi} \ \mathbb{E}\Big[\sum_{t\ge 0}\gamma^t(\mathcal{R}+T_c\mathcal{H})\ \Big|\ K_0=k\Big].

$$
Then $V^*$ satisfies the entropic Bellman fixed point

$$
V^*(k)
=
T_c \log \sum_{a\in\mathcal{A}}
\exp\!\left(\frac{1}{T_c}\left(\mathcal{R}(k,a)+\gamma\,\mathbb{E}_{k'\sim\bar{P}(\cdot\mid k,a)}[V^*(k')]\right)\right),

$$
and the corresponding optimal policy is the softmax policy

$$
\pi^*(a\mid k)\propto
\exp\!\left(\frac{1}{T_c}\left(\mathcal{R}(k,a)+\gamma\,\mathbb{E}[V^*(k')]\right)\right).

$$
*Proof sketch.* Standard convex duality / log-sum-exp variational identity: maximizing expected reward plus entropy yields a softmax (exponential-family) distribution; substituting back produces the log-partition recursion. (This is the "soft"/MaxEnt Bellman equation used in SAC-like methods.)

**Consequence.** The same mathematics can be read as:
1) maximize reward while retaining policy entropy (MaxEnt RL), or
2) maximize reachability/diversity of future macro state-action trajectories (intrinsic motivation).

:::

:::{prf:definition} Causal Path Space
:label: def-causal-path-space

For a macrostate $k\in\mathcal{K}$ and horizon $H$, define the future macro state-action path space

$$
\Gamma_H(k)
:=
\left\{(k_0,a_0,k_1,a_1,\dots,a_{H-1},k_H)\in\mathcal{K}^{H+1}\times\mathcal{A}^H : k_0 = k\right\}.

$$
:::

:::{prf:definition} Path Probability
:label: def-path-probability

$P_\pi(\xi\mid k)$ is the induced state-action path probability from {prf:ref}`def-macro-path-distribution`.

:::

:::{prf:definition} Causal Entropy
:label: def-causal-entropy

$S_c(k,H;\pi)$ is the causal path entropy from {prf:ref}`def-causal-path-entropy`, i.e., the cumulative policy
entropy along the induced path measure $P_\pi(\cdot\mid k)$.

:::

:::{prf:definition} Exploration gradient, covariant form
:label: def-exploration-gradient-covariant-form

On a macro chart with metric $G$ ({ref}`sec-second-order-sensitivity-value-defines-a-local-metric`),

$$
\mathbf{g}_{\text{expl}}(e_k) := T_c\,\nabla_G S_c(k,H;\pi).

$$
:::

:::{prf:theorem} Equivalence of Entropy-Regularized Control Forms; discrete macro
:label: thm-equivalence-of-entropy-regularized-control-forms-discrete-macro

Assume:
1. finite macro alphabet $\mathcal{K}$ and (for simplicity) finite action set $\mathcal{A}$,
2. an enclosure-consistent macro kernel $\bar{P}(k'\mid k,a)$,
3. bounded reward flux $\mathcal{R}(k,a)$,
4. discount factor $\gamma \in (0,1)$ (ensures convergence of infinite-horizon sums).

Then the following are equivalent characterizations of the same optimal control law:

1. **MaxEnt control (utility + freedom):** $\pi^*$ maximizes $J_{T_c}(\pi)$ from {prf:ref}`def-maxent-rl-objective-on-macrostates`.
2. **Exponentially tilted trajectory measure (KL-regularization).** Fix a reference (prior) policy $\pi_0(a\mid k)$ with full support (uniform when $\mathcal{A}$ is finite). Consider the infinite-horizon trajectory measure. The optimal controlled path law admits an exponential-family form relative to the reference measure induced by $\pi_0$ and $\bar{P}$:

   $$
   P^*(\omega\mid K_t=k)\ \propto\
   P_0(\omega \mid k)\,
   \exp\!\left(\frac{1}{T_c}\sum_{h=0}^{\infty}\gamma^h\,\mathcal{R}(K_{t+h},K^{\text{act}}_{t+h})\right),

   $$
   where $P_0(\omega \mid k) := \prod_{h=0}^{\infty}\pi_0(K^{\text{act}}_{t+h}\mid K_{t+h})\,\bar{P}(K_{t+h+1}\mid K_{t+h},K^{\text{act}}_{t+h})$ is the reference trajectory measure, and the normalizer is the (state-dependent) path-space normalizing constant. (For finite-horizon $H$, replace $\infty$ with $H-1$; the equivalence holds for any horizon.)
3. **Soft Bellman optimality:** the optimal value function $V^*$ satisfies the soft Bellman recursion of {prf:ref}`prop-soft-bellman-form-discrete-actions`, and $\pi^*$ is the corresponding softmax policy.

Moreover, the path-space log-normalizer is (up to scaling) the soft value. Gradients of the log-normalizer therefore induce a well-defined exploration direction in any differentiable macro coordinate system. The link between soft optimality and path entropy is cleanest when stated as a KL-regularized variational identity: if $P_0(\omega\mid k)$ denotes the reference trajectory measure induced by $\pi_0$ and $\bar{P}$, then

$$
\log Z(k)
=
\sup_{P(\cdot\mid k)}
\left\{
\frac{1}{T_c}\,\mathbb{E}_{P}\!\left[\sum_{h=0}^{\infty}\gamma^h\,\mathcal{R}\right]
-D_{\mathrm{KL}}(P(\cdot\mid k)\Vert P_0(\cdot\mid k))
\right\},

$$
and the optimizer is exactly the exponentially tilted law {math}`P^*`. In the special case where {math}`P_0` is uniform (or treated as constant), the KL term differs from Shannon path entropy by an additive constant, recovering the standard "maximize entropy subject to expected reward" view. The finite-horizon version replaces $\infty$ with $H-1$; as $H \to \infty$, the finite-horizon solution converges to the stationary infinite-horizon optimum.

*Proof sketch.* Set up the constrained variational problem "maximize path entropy subject to an expected reward constraint." The Euler-Lagrange condition yields an exponential-family distribution on paths. The normalizer obeys dynamic programming and equals the soft value. Differentiating the log-normalizer yields the corresponding exploration-gradient direction.

:::

## 04_control/02_belief_dynamics.md

:::{prf:definition} Belief operator
:label: def-belief-operator

Let $\varrho_t\in\mathbb{C}^{d\times d}$ satisfy $\varrho_t\succeq 0$ and $\mathrm{Tr}(\varrho_t)=1$. Diagonal $\varrho_t$ reduces to a classical probability vector; non-diagonal terms can be used to encode correlations/uncertainty structure in a learned feature basis.

:::

:::{prf:definition} GKSL generator
:label: def-gksl-generator

A continuous-time, Markovian, completely-positive trace-preserving (CPTP) evolution has a generator of the Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) form {cite}`gorini1976completely,lindblad1976generators`:

$$
\frac{d\varrho}{ds}
=
\underbrace{-i[H,\varrho]}_{\text{conservative drift}}
\;+\;
\underbrace{\sum_{j} \gamma_j\left(L_j\varrho L_j^\dagger-\frac12\{L_j^\dagger L_j,\varrho\}\right)}_{\text{dissipative update}},

$$
where {math}`H=H^\dagger` is Hermitian, {math}`\gamma_j\ge 0` are rates, and {math}`\{L_j\}` are (learned) operators.

**Operational interpretation (within this document).**
- The commutator term is a structured way to represent **reversible internal prediction** (it preserves $\mathrm{Tr}(\varrho)$ and the spectrum of $\varrho$).
- The dissipator is a structured way to represent **irreversible assimilation / disturbance** while preserving positivity and trace.

This is a modeling choice, not a claim about literal quantum physics: it is used here purely as a convenient, well-posed parametrization of CPTP belief updates.

*Note (WFR Correspondence).* In the **classical limit** (diagonal density matrix $\varrho = \mathrm{diag}(p)$), the GKSL generator reduces to a Markov jump process on the diagonal probabilities $p_k$. This classical master equation is **rigorously equivalent** to a gradient flow in the Wasserstein-Fisher-Rao metric ({prf:ref}`def-the-wfr-action`, {ref}`sec-connection-to-gksl-master-equation`): transport corresponds to continuous probability flow, reaction corresponds to jump-induced mass redistribution {cite}`maas2011gradient,mielke2011gradient`. The commutator term vanishes for diagonal states (no coherences to rotate). For full quantum states, see {cite}`carlen2014wasserstein` for the quantum Wasserstein gradient flow theory.

:::

## 04_control/03_coupling_window.md

:::{prf:definition} Grounding rate
:label: def-grounding-rate

Let $G_t:=I(X_t;K_t)$ be the symbolic mutual information injected through the boundary (Node 13). The *grounding rate* is the average information inflow per step:

$$
\lambda_{\text{in}} := \mathbb{E}[G_t].

$$
Units: $[\lambda_{\text{in}}]=\mathrm{nat/step}$.

:::

:::{prf:definition} Mixing rate
:label: def-mixing-rate

Let $S_t:=H(K_t)$ be the macro entropy. The *mixing rate* is the expected entropy growth not attributable to purposeful exploration:

$$
\lambda_{\text{mix}} := \mathbb{E}[(S_{t+1}-S_t)_+].

$$
Units: $[\lambda_{\text{mix}}]=\mathrm{nat/step}$.

:::

:::{prf:theorem} Information-stability window; operational
:label: thm-information-stability-window-operational

A necessary condition for stable, grounded macrostates is the existence of constants $0<\epsilon<\log|\mathcal{K}|$ such that, along typical trajectories,

$$
\epsilon \le I(X_t;K_t) \quad\text{and}\quad H(K_t)\le \log|\mathcal{K}|-\epsilon,

$$
and the net entropy balance satisfies

$$
\lambda_{\text{in}} \gtrsim \lambda_{\text{mix}}.

$$
Violations correspond to identifiable barrier modes:
- If $I(X;K)\approx 0$: under-coupling - ungrounded inference / decoupling (Mode D.C).
- If $H(K)\approx \log|\mathcal{K}|$: over-coupling or dispersion - symbol dispersion (BarrierScat).

*Remark.* This theorem is intentionally stated at the level of measurable information quantities (Gate Nodes) so it can be audited online; strengthening it to a sufficient condition requires specifying the macro kernel class and a contraction inequality (e.g. log-Sobolev / Doeblin-type conditions).

:::

## 05_geometry/01_metric_law.md

:::{prf:definition} DPI / boundary-capacity constraint
:label: def-dpi-boundary-capacity-constraint

Consider the boundary stream $(X_t)_{t\ge 0}$ and the induced internal state process $(Z_t)_{t\ge 0}$ produced by the shutter (Definition {prf:ref}`def-bounded-rationality-controller`). Because all internal state is computed from boundary influx and internal memory, any information in the bulk must be mediated by a finite-capacity channel. Operationally, the data-processing constraint is:

$$
I_{\text{bulk}} \;\le\; C_{\partial},

$$
where $C_{\partial}$ is the effective information capacity of the boundary channel and $I_{\text{bulk}}$ is the amount of information the agent can stably maintain in $\mathcal{Z}$ without violating Causal Enclosure (no internal source term $\sigma$; Definition {prf:ref}`def-source-residual`).
Units: $[I_{\text{bulk}}]=[C_{\partial}]=\mathrm{nat}$.

:::

:::{prf:definition} Information density and bulk information volume
:label: def-information-density-and-bulk-information-volume

Let $\rho(z,s)$ denote the probability density of the agent's belief state at position $z \in \mathcal{Z}$ and computation time $s$, **defined with respect to the Riemannian volume measure** $d\mu_G = \sqrt{|G|}\,dz^n$. The **information density** $\rho_I(z,s)\ge 0$ is defined as the local Shannon entropy density:

$$
\rho_I(z,s) := -\rho(z,s) \log \rho(z,s),

$$
with units of nats per unit Riemannian volume ($n=\dim\mathcal{Z}$).

*Remark.* Integrating $\rho_I$ over $\mathcal{Z}$ with respect to $d\mu_G$ yields the coordinate-invariant differential entropy $h[\rho] = -\int \rho \log \rho \, d\mu_G$. By defining $\rho$ as a density with respect to $d\mu_G$ (rather than Lebesgue measure $dz$), the entropy is automatically invariant under coordinate changes---no explicit metric correction term is needed.

:::

:::{prf:definition} Bulk Information Volume
:label: def-a-bulk-information-volume

Define the bulk information volume over a region $\Omega\subseteq\mathcal{Z}$ by

$$
I_{\text{bulk}}(\Omega) := \int_{\Omega} \rho_I(z,s)\, d\mu_G.

$$
When $\Omega=\mathcal{Z}$ we write $I_{\text{bulk}}:=I_{\text{bulk}}(\mathcal{Z})$. This is conceptually distinct from the probability-mass balance in {ref}`Section 2.11 <sec-variance-value-duality-and-information-conservation>`; here the integral measures grounded structure in nats.

:::

:::{prf:definition} Boundary capacity: area law at finite resolution
:label: def-boundary-capacity-area-law-at-finite-resolution

Let $dA_G$ be the induced $(n-1)$-dimensional area form on $\partial\mathcal{Z}$. If the boundary interface has a minimal resolvable scale $\ell>0$ (pixel/token floor), then an operational capacity bound is an area law:

$$
C_{\partial}(\partial\mathcal{Z})
:=
\frac{1}{\eta_\ell}\oint_{\partial\mathcal{Z}} dA_G,

$$
where $\eta_\ell$ is the effective boundary area-per-nat at resolution $\ell$ (a resolution-dependent constant set by the interface).
Units: $[\eta_\ell]=[dA_G]/\mathrm{nat}$ and $[\ell]$ is the chosen boundary resolution length scale.

*Remark (discrete macro specialization).* For the split shutter, the most conservative computable proxy is

$$
C_{\partial}\ \approx\ \mathbb{E}[I(X_t;K_t)]\ \le\ \log|\mathcal{K}|,

$$
which is exactly Node 13 (BoundaryCheck) and Theorem {prf:ref}`thm-information-stability-window-operational`'s grounding condition.

:::

:::{prf:theorem} Capacity-constrained metric law
:label: thm-capacity-constrained-metric-law

Under the regularity and boundary-clamping hypotheses stated in {ref}`Appendix A <sec-appendix-a-full-derivations>`, and under the soundness condition that bulk structure is boundary-grounded (no internal source term $\sigma$ on $\operatorname{int}(\mathcal{Z})$; Definition {prf:ref}`def-source-residual`), stationarity of a capacity-constrained curvature functional implies

$$
R_{ij} - \frac{1}{2}R\,G_{ij} + \Lambda G_{ij} = \kappa\, T_{ij},

$$
where $\Lambda$ and $\kappa$ are constants and $T_{ij}$ is the **total Risk Tensor** induced by the reward field. *Units:* $\Lambda$ has the same units as curvature ($[R]\sim [z]^{-2}$), and $\kappa$ is chosen so that $\kappa\,T_{ij}$ matches those curvature units.

*Operational reading.* Curvature is the geometric mechanism that prevents the internal information volume (Definition 18.1.2a) from exceeding the boundary's information bandwidth (Definition {prf:ref}`def-a-bulk-information-volume`) while remaining grounded.

**Implementation hook.** The squared residual of this identity defines a capacity-consistency regularizer $\mathcal{L}_{\text{cap-metric}}$; see {ref}`Appendix B <sec-appendix-b-units-parameters-and-coefficients>` for the consolidated list of loss definitions and naming conventions.

:::

:::{prf:definition} Extended Risk Tensor with Maxwell Stress
:label: def-extended-risk-tensor

The total Risk Tensor $T_{ij}$ decomposes into gradient and curl contributions:

$$
T_{ij} = T_{ij}^{\text{gradient}} + T_{ij}^{\text{Maxwell}},

$$
where:

1. **Gradient Stress** (from scalar potential $\Phi$):

$$
T_{ij}^{\text{gradient}} = \partial_i \Phi \, \partial_j \Phi - \frac{1}{2}G_{ij} \|\nabla\Phi\|_G^2

$$
2. **Maxwell Stress** (from {prf:ref}`def-value-curl` $\mathcal{F}$):

$$
T_{ij}^{\text{Maxwell}} = \mathcal{F}_{ik}\mathcal{F}_j^{\;k} - \frac{1}{4}G_{ij}\mathcal{F}^{kl}\mathcal{F}_{kl}

$$
*Units:* $[T_{ij}] = \mathrm{nat}^2/[z]^2$.

**Conservative Limit:** When $\mathcal{F} = 0$ (Definition {prf:ref}`def-conservative-reward-field`), the Maxwell term vanishes and we recover the standard gradient-only risk tensor.

**Non-Conservative Case:** When $\mathcal{F} \neq 0$, the Maxwell stress contributes additional terms to the curvature equation.

:::

:::{prf:definition} Capacity saturation diagnostic
:label: def-capacity-saturation-diagnostic

Compute the capacity saturation ratio:

$$
\nu_{\text{cap}}(s) := \frac{I_{\text{bulk}}(s)}{C_{\partial}},

$$
where $I_{\text{bulk}}(s) = \int_{\mathcal{Z}} \rho_I(z,s)\, d\mu_G$ per Definition 18.1.2a.

*Interpretation:*
- $\nu_{\text{cap}} \ll 1$: Under-utilized capacity; the agent may be compressing excessively (lossy representation).
- $\nu_{\text{cap}} \approx 1$: Operating at capacity limit; geometry must regulate to prevent overflow.
- $\nu_{\text{cap}} > 1$: **Violation** of the DPI constraint (Definition {prf:ref}`def-dpi-boundary-capacity-constraint`); indicates ungrounded structure.

*Cross-reference:* When $\nu_{\text{cap}} > 1$, the curvature correction (Theorem {prf:ref}`thm-capacity-constrained-metric-law`) is insufficient. This triggers geometric reflow---the metric $G$ must increase $|G|$ (expand volume) to bring $I_{\text{bulk}}$ back within bounds.

:::

## 05_geometry/02_wfr_geometry.md

:::{prf:definition} The Generalized WFR Action
:label: def-the-wfr-action

The squared WFR distance $d^2_{\mathrm{WFR}}(\rho_0, \rho_1)$ is the infimum of the generalized energy functional:

$$
\mathcal{E}[\rho, v, r] = \int_0^1 \int_{\mathcal{Z}} \left( \underbrace{\|v_s(z)\|_G^2}_{\text{Transport Cost}} + \underbrace{\lambda^2 |r_s(z)|^2}_{\text{Reaction Cost}} - \underbrace{2\langle \mathbf{A}(z), v_s(z) \rangle}_{\text{Vector Potential}} \right) d\rho_s(z) \, ds

$$
subject to the **Unbalanced Continuity Equation**:

$$
\partial_s \rho + \nabla \cdot (\rho v) = \rho r

$$
where:
- $v_s(z) \in T_z\mathcal{Z}$ is the **velocity field** (transport/flow)
- $r_s(z) \in \mathbb{R}$ is the **reaction rate** (growth/decay of mass)
- $\lambda > 0$ is the **length-scale parameter** balancing transport and reaction
- $G$ is the Riemannian metric on the continuous fibres ({ref}`Section 2.5 <sec-second-order-sensitivity-value-defines-a-local-metric>`)
- $\mathbf{A}(z)$ is the **vector potential** satisfying $d\mathbf{A} = \mathcal{F}$ (the {prf:ref}`def-value-curl`)

*Units:* $[\mathbf{A}] = \mathrm{nat}/[\text{length}]$.

**Conservative Limit:** When $\mathcal{F} = 0$ (Definition {prf:ref}`def-conservative-reward-field`), we can choose the gauge $\mathbf{A} = 0$ and recover the standard WFR action without the vector potential term.

**Non-Conservative Case:** When $\mathcal{F} \neq 0$, the vector potential term couples the transport velocity to the solenoidal component of the reward field. The Euler-Lagrange equations of this action yield the Lorentz-Langevin equation (Definition {prf:ref}`def-bulk-drift-continuous-flow`).

*Remark (Gauge Invariance).* The action is invariant under gauge transformations $\mathbf{A} \to \mathbf{A} + d\chi$ for any scalar $\chi$, since $d(d\chi) = 0$. We fix the gauge via the Coulomb condition $\delta\mathbf{A} = 0$ (divergence-free).

*Forward reference (Boundary Conditions).* {ref}`Section 23.5 <sec-wfr-boundary-conditions-waking-vs-dreaming>` specifies how boundary conditions on $\partial\mathcal{Z}$ (sensory and motor boundaries) constrain the WFR dynamics: **Waking** imposes Dirichlet (sensors) + Neumann (motors) BCs; **Dreaming** imposes reflective BCs on both, enabling recirculating flow without external input.

:::

:::{prf:remark} Units
:label: rem-units

$[v] = \text{length}/\text{time}$, $[r] = 1/\text{time}$, and $[\lambda] = \text{length}$. The ratio $\|v\|/(\lambda |r|)$ determines whether transport or reaction dominates.

:::

:::{prf:definition} Canonical length-scale
:label: def-canonical-length-scale

Let $G$ be the latent metric on $\mathcal{Z}$. The canonical choice for $\lambda$ is the **geodesic injectivity radius**:

$$
\lambda := \min_{z \in \mathcal{Z}} \text{inj}_G(z),

$$
where $\text{inj}_G(z)$ is the injectivity radius at $z$ -- the largest $r$ such that the exponential map $\exp_z: T_z\mathcal{Z} \to \mathcal{Z}$ is a diffeomorphism on $B_r(0)$.

*Default value.* If the injectivity radius is unknown or the metric is learned, a practical default is:

$$
\lambda_{\text{default}} = \sqrt{\frac{\text{tr}(G^{-1})}{n}} \approx \text{mean characteristic length of } \mathcal{Z}.

$$
This corresponds to the RMS geodesic step size in an isotropic metric.

*Cross-reference:* The screening length $\ell_{\text{screen}} = 1/\kappa$ from {ref}`Section 24.2 <sec-the-bulk-potential-screened-poisson-equation>` plays an analogous role for temporal horizons; $\lambda$ plays the corresponding role for spatial horizons in the WFR geometry.

:::

:::{prf:proposition} Limiting Regimes
:label: prop-limiting-regimes

The WFR metric seamlessly unifies discrete and continuous dynamics:

1. **Continuous Movement (Flow):** When moving within a chart, $r \approx 0$. The dynamics are dominated by $\nabla \cdot (\rho v)$, and the metric reduces to $W_2$ (Wasserstein-2). This recovers the Riemannian manifold structure of the nuisance fibres.

2. **Discrete Movement (Jump):** When the flow reaches a topological obstruction (chart boundary without overlap), transport becomes infinitely expensive. It becomes cheaper to use the source term $r$:
   - $r < 0$ on the old chart (mass destruction)
   - $r > 0$ on the new chart (mass creation)
   This recovers the **Fisher-Rao metric** on the discrete simplex $\Delta^{|\mathcal{K}|}$.

3. **Mixed Regime (Overlap):** In chart overlaps, both $v$ and $r$ are active. The optimal path smoothly interpolates between transport and reaction.

*Proof sketch.* The cone-space representation of WFR (lifting $\rho$ to $(\sqrt{\rho}, \sqrt{\rho} \cdot z)$) shows that the WFR geodesic projects to a $W_2$ geodesic when $r = 0$, and to a Fisher-Rao geodesic when $v = 0$. $\square$

:::

:::{prf:theorem} Classical Master Equation as WFR Gradient Flow
:label: thm-classical-master-equation-wfr

Let $\mathcal{K} = \{1, \ldots, K\}$ be a finite state space with transition rates $W_{jk} \geq 0$ (rate of jumping from $k$ to $j$). The classical master equation

$$
\dot{p}_j = \sum_{k} W_{jk} p_k - W_{kj} p_j
$$

is the **gradient flow** of the relative entropy $H(p \| \pi) = \sum_j p_j \log(p_j / \pi_j)$ with respect to a discrete Wasserstein-type metric, where $\pi$ is the stationary distribution satisfying detailed balance {cite}`maas2011gradient,mielke2011gradient,chow2012fokker`.

:::

:::{prf:corollary} GKSL Classical Limit
:label: cor-gksl-classical-limit

When the GKSL density matrix is diagonal, $\varrho = \mathrm{diag}(p_1, \ldots, p_K)$, the GKSL equation reduces to a classical master equation with rates

$$
W_{jk} = \sum_\ell \gamma_\ell |\langle j | L_\ell | k \rangle|^2.
$$

The commutator term $-i[H, \varrho]$ vanishes identically for diagonal states. By Theorem {prf:ref}`thm-classical-master-equation-wfr`, this evolution is a gradient flow in WFR geometry.

:::

:::{prf:remark} Full Quantum Case
:label: rem-full-quantum-wfr

For non-diagonal density matrices (quantum coherences), the appropriate geometric structure is the **quantum Wasserstein distance** of Carlen \& Maas {cite}`carlen2014wasserstein,carlen2017gradient`. The GKSL equation is the gradient flow of quantum relative entropy with respect to this metric. This framework handles coherences but is more complex than the classical WFR theory used here.

:::

:::{prf:definition} WFR World Model
:label: def-wfr-world-model

The policy outputs a generalized velocity field $(v, r)$ to minimize the WFR path length to the target distribution (goal).

```python
import torch
import torch.nn as nn
from typing import Tuple

class WFRWorldModel(nn.Module):
    """
    Unified World Model using Unbalanced Optimal Transport dynamics.

    Predicts the 'Generalized Velocity' (v, r) for belief particles.
    No separate 'discrete' and 'continuous' modules.
    """

    def __init__(
        self,
        macro_embed_dim: int,
        nuisance_dim: int,
        action_dim: int,
        hidden_dim: int = 256,
    ):
        super().__init__()
        # Input: particle state + action
        # State includes: macro embedding, nuisance coords, mass (weight)
        input_dim = macro_embed_dim + nuisance_dim + 1 + action_dim

        # Single MLP backbone for unified dynamics
        self.dynamics_net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
        )

        # Head 1: Transport velocity (Riemannian motion on fibre)
        self.head_v = nn.Linear(hidden_dim, nuisance_dim)

        # Head 2: Reaction rate (Fisher-Rao mass creation/destruction)
        self.head_r = nn.Linear(hidden_dim, 1)

    def forward(
        self,
        z_t: torch.Tensor,           # [B, D] latent state (macro_embed + nuisance)
        mass_t: torch.Tensor,        # [B, 1] particle weight (belief mass)
        action_t: torch.Tensor,      # [B, A] action
        dt: float = 0.1,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Predict next state via WFR dynamics.

        Returns:
            z_next: [B, D] next latent state
            mass_next: [B, 1] next particle mass
            v_t: [B, nuisance_dim] transport velocity
            r_t: [B, 1] reaction rate
        """
        # Unified prediction
        inp = torch.cat([z_t, mass_t, action_t], dim=-1)
        feat = self.dynamics_net(inp)

        v_t = self.head_v(feat)  # Transport velocity
        r_t = self.head_r(feat)  # Reaction rate (log-growth)

        # Integrate dynamics (Euler step)
        # Position update (Transport): z' = z + v * dt
        z_next = z_t.clone()
        z_next[..., -self.head_v.out_features:] += v_t * dt

        # Mass update (Reaction): m' = m * exp(r * dt)
        # If r > 0: hypothesis gaining probability (jumping in)
        # If r < 0: hypothesis losing probability (jumping out)
        mass_next = mass_t * torch.exp(r_t * dt)

        return z_next, mass_next, v_t, r_t
```

**How this handles the "Jump" seamlessly:**

- **Deep inside a Chart:** Model predicts $r \approx 0$ and $v \neq 0$. Particle moves normally.
- **Approaching a Boundary:** Model sees invalid description (high prediction error). Predicts $r < 0$ for current chart, $r > 0$ for neighboring chart particles.
- **Result:** Probability mass smoothly "tunnels" between charts without hard discrete switching.

:::

:::{prf:definition} Scale-Dependent Teleportation Cost
:label: def-scale-dependent-teleportation-cost

$$
\lambda^{(\ell)} \propto \sigma^{(\ell)} \quad \text{(jump cost scales with residual variance)}

$$
where $\sigma^{(\ell)}$ is the scale factor from Definition {prf:ref}`def-the-rescaling-operator-renormalization`.

**Interpretation:**
- **Layer 0 (Bulk / IR):** High $\lambda^{(0)}$. Jumping is expensive; macro-structure is rigid. Transport dominates.
- **Layer $L$ (Texture / UV):** Low $\lambda^{(L)}$. "Mass" (texture details) can appear/disappear cheaply. Reaction dominates.

**Correspondence with Cosmological Constant:**
In the capacity-constrained metric law ({ref}`Section 18 <sec-capacity-constrained-metric-law-geometry-from-interface-limits>`, Theorem {prf:ref}`thm-capacity-constrained-metric-law`), the term $\Lambda G_{ij}$ plays the role of a baseline curvature. The correspondence is:

$$
\Lambda^{(\ell)} \sim \frac{1}{(\lambda^{(\ell)})^2}

$$
- Bulk (low $\Lambda$): Flat, rigid, transport-dominated
- Boundary (high $\Lambda$): Curved, fluid, reaction-dominated

:::

:::{prf:theorem} WFR Stress-Energy Tensor; variational form
:label: thm-wfr-stress-energy-tensor-variational-form

Let the WFR action be

$$
\mathcal{S}_{\mathrm{WFR}}
=
\frac12\int_0^T\int_{\mathcal{Z}}
\rho\left(\|v\|_G^2+\lambda^2 r^2\right)\,d\mu_G\,ds,

$$
with continuity equation

$$
\partial_s\rho+\nabla\!\cdot(\rho v)=\rho r.

$$
Define

$$
T_{ij}:=
-\frac{2}{\sqrt{|G|}}\frac{\delta(\sqrt{|G|}\,\mathcal{L}_{\mathrm{WFR}})}{\delta G^{ij}}
\quad\text{(holding }\rho,v,r\text{ fixed).}

$$
Then

$$
T_{ij}=\rho\,v_i v_j + P\,G_{ij},
\qquad
P=\frac12\,\rho\left(\|v\|_G^2+\lambda^2 r^2\right),

$$
which is the perfect-fluid form with reaction contributing an additive pressure term
{math}`P_{\mathrm{react}}=\tfrac12\lambda^2\rho r^2`.

*Proof sketch.* Vary $\mathcal{S}_{\mathrm{WFR}}$ with respect to $G^{ij}$ while holding
$(\rho,v,r)$ fixed. Use $\delta\|v\|_G^2=-v_i v_j\,\delta G^{ij}$ and
$\delta d\mu_G=-\tfrac12 G_{ij}\delta G^{ij}d\mu_G$, then collect terms to match
$\delta\mathcal{S}_{\mathrm{WFR}}=-\tfrac12\int T_{ij}\delta G^{ij}d\mu_G\,ds$.
See {ref}`Appendix C <sec-appendix-c-wfr-stress-energy-tensor>` for the full derivation. $\square$

:::

:::{prf:definition} WFR Consistency Loss / WFRCheck
:label: def-wfr-consistency-loss-wfrcheck

The cone-space representation linearizes WFR locally. From $\partial_s \rho = \rho r - \nabla \cdot (\rho v)$ and $u = \sqrt{\rho}$, we have $\partial_s u = \frac{\rho r - \nabla \cdot (\rho v)}{2\sqrt{\rho}}$. Define the consistency loss:

$$
\mathcal{L}_{\mathrm{WFR}} = \left\| \sqrt{\rho_{t+1}} - \sqrt{\rho_t} - \frac{\Delta t}{2\sqrt{\rho_t}}\left(\rho_t r_t - \nabla \cdot (\rho_t v_t)\right) \right\|_{L^2}^2

$$
This penalizes deviations from the unbalanced continuity equation.

**Practical implementation:**

```python
def compute_wfr_consistency_loss(
    rho_t: torch.Tensor,       # [B, K] belief over charts at time t
    rho_t1: torch.Tensor,      # [B, K] belief over charts at time t+1
    v_t: torch.Tensor,         # [B, K, d_n] transport velocity per chart
    r_t: torch.Tensor,         # [B, K] reaction rate per chart
    dt: float = 0.1,
    eps: float = 1e-6,
) -> torch.Tensor:
    """
    Compute WFR consistency loss (cone-space formulation).

    Penalizes violation of unbalanced continuity equation.
    """
    sqrt_rho_t = torch.sqrt(rho_t + eps)
    sqrt_rho_t1 = torch.sqrt(rho_t1 + eps)

    # Approximate divergence term (finite difference)
    # In practice, use automatic differentiation if v is differentiable
    div_rho_v = torch.zeros_like(rho_t)  # Placeholder for nabla . (rho v)

    # Predicted change in sqrt(rho) from: d/ds sqrt(rho) = (rho*r - div(rho*v)) / (2*sqrt(rho))
    predicted_delta = (dt / (2 * sqrt_rho_t + eps)) * (rho_t * r_t - div_rho_v)

    # Actual change
    actual_delta = sqrt_rho_t1 - sqrt_rho_t

    # L2 loss
    loss = ((actual_delta - predicted_delta) ** 2).mean()

    return loss
```

:::

## 05_geometry/03_holographic_gen.md

:::{prf:definition} Manifold Boundary and Interior
:label: def-manifold-boundary-and-interior

Let $\mathcal{Z}$ be the latent manifold with Poincare disk model. The **boundary** is the $(n-1)$-dimensional limit set:

$$
\partial\mathcal{Z} := \{z \in \mathbb{C}^n : |z| = 1\}.

$$
The **interior** (or bulk) is the open disk:

$$
\text{int}(\mathcal{Z}) := \{z \in \mathbb{C}^n : |z| < 1\}.

$$
These are standard differential geometry terms; the boundary is the ideal boundary at infinity in the hyperbolic metric.

:::

:::{prf:definition} Hyperbolic Volume Growth
:label: def-hyperbolic-volume-growth

With metric $G_{ij} = \frac{4\delta_{ij}}{(1-|z|^2)^2}$, the volume of a hyperbolic ball $B_r(0)$ grows exponentially:

$$
\mathrm{Vol}(B_r(0)) = 4\pi \sinh^2\!\left(\frac{r}{2}\right) \;\approx\; \pi e^r \quad \text{as } r \to \infty.

$$
Units: $[\mathrm{Vol}] = [z]^2$.

:::

:::{prf:definition} The Entropic Force
:label: def-the-entropic-force

The "Free Energy" of a state at radius $r$ is dominated by the entropic volume term $S(r) = 2 \operatorname{artanh}(r)$. To maximize entropy (fill the capacity), the agent experiences a radial force:

$$
F_{\text{entropy}}(z) = \nabla_G S(z) = \frac{(1-|z|^2)}{2} \cdot \frac{z}{|z|}

$$
This accounts for the Poincaré metric conformal factor. The drift magnitude decreases near the boundary ($|z| \to 1$), ensuring the agent asymptotically approaches but never reaches it.

Units: $[F_{\text{entropy}}] = [z]/\tau$.

:::

:::{prf:proposition} Isotropic Radial Expansion
:label: prop-isotropic-radial-expansion

If acting alone (no policy steering), the entropic drift produces the isotropic expansion:

$$
r(\tau) = \tanh(\tau/2)

$$
This represents isotropic diffusion---expanding uniformly in all directions.

*Proof.* The overdamped equation $\dot{r} = (1-r^2)/2$ (from the Riemannian gradient of $U(z) = -2\operatorname{artanh}(|z|)$) integrates to $r(\tau) = \tanh(\tau/2 + \operatorname{artanh}(r_0))$. For $r_0 = 0$, we get $r(\tau) = \tanh(\tau/2)$. $\square$

:::

:::{prf:definition} Hyperbolic Information Potential
:label: def-hyperbolic-information-potential

The **information potential** $U: \mathbb{D} \to \mathbb{R}$ is the negative hyperbolic distance from the origin:

$$
U(z) := -d_{\mathbb{D}}(0, z) = -2 \operatorname{artanh}(|z|) = -\log\!\left(\frac{1+|z|}{1-|z|}\right).

$$
Units: $[U] = \mathrm{nat}$.

*Remark (Thermodynamic Interpretation).* At origin ($z=0$): $U = 0$ (maximum potential, maximum entropy). At boundary ($|z| \to 1$): $U \to -\infty$ (minimum potential, fully specified). The depth $-U(z)$ measures the **information content** of the state.

:::

:::{prf:proposition} Riemannian Gradient of $U$
:label: prop-riemannian-gradient-of

The gradient in the Poincaré metric is:

$$
\nabla_G U(z) = G^{-1} \nabla U = -\frac{(1-|z|^2)}{2} \hat{z}, \quad \text{where } \hat{z} = \frac{z}{|z|}.

$$
The **entropic drift** (negative gradient) pushes radially outward:

$$
-\nabla_G U(z) = \frac{(1-|z|^2)}{2} \hat{z}.

$$
*Remark (Connection to {ref}`Section 7.11 <sec-the-geometry-of-the-latent-space-a-hyperbolic-hierarchy>`).* The Poincare coordinate $z$ relates to depth via $\rho = d_{\mathbb{D}}(0, z) = 2\operatorname{artanh}(|z|)$. Chart transitions are handled by the WFR jump process ({ref}`Section 22.2 <sec-the-coupled-jump-diffusion-sde>`), governed by the {prf:ref}`def-the-wfr-action`.

**Cross-references:** Definition {prf:ref}`def-information-density-and-bulk-information-volume`, Theorem {prf:ref}`thm-capacity-constrained-metric-law`.

:::

:::{prf:proposition} SO(D) Symmetry at Origin
:label: prop-so-d-symmetry-at-origin

At $z = 0$:
1. The metric is isotropic: $G(0) = 4I$
2. The entropic force vanishes: $F_{\text{entropy}}(0) = 0$
3. The system has full rotational symmetry $SO(D)$

*Cross-reference (Gauge Breaking):* This $SO(D)$ symmetry is the special case where the stabilizer subgroup $H_0 = \{e\}$ is trivial. In multi-agent settings, this symmetry is spontaneously broken via the Higgs mechanism (Theorem {prf:ref}`thm-higgs-mechanism`), yielding massive gauge bosons and effective agent masses.

:::

:::{prf:definition} The Control Field
:label: def-the-control-field

The Policy $\pi_\theta(a|z)$ outputs a **control field** $u_\pi(z)$ on the tangent bundle $T\mathbb{D}$:

$$
u_\pi(z) = G^{-1}(z) \cdot \mathbb{E}_{a \sim \pi_\theta}[a]

$$
This vector field represents the **Information Preference** of the agent (or the User).

Units: $[u_\pi] = [z]/\tau$.

*Remark (Context-Conditioning).* {ref}`Section 23.6 <sec-relationship-to-the-context-conditioned-framework>` generalizes this to **context-conditioned policies** $\pi(a|z,c)$ where the context $c \in \mathcal{C}$ unifies: RL action spaces, classification label spaces, and LLM prompt spaces. The control field becomes $u_\pi(z,c) = G^{-1}(z) \cdot \nabla_z \Phi_{\text{eff}}(z,K,c)$ where the {prf:ref}`def-effective-potential` depends on task context.

:::

:::{prf:definition} Control Field at Origin
:label: def-control-field-at-origin

At $\tau=0$, the total drift is:

$$
F_{\text{total}} = F_{\text{entropy}} + u_\pi(0)

$$
Since $F_{\text{entropy}}(0) = 0$ (isotropic), the initial trajectory is determined **entirely** by $u_\pi(0)$.

:::

:::{prf:theorem} Unified Control Interpretation
:label: thm-unified-control-interpretation

The control field $u_\pi$ admits three equivalent interpretations:

| **Mode**                     | **Control Field $u_\pi$**                          | **Interpretation**                         |
|------------------------------|----------------------------------------------------|--------------------------------------------|
| **RL**                       | $u_\pi = G^{-1} \nabla_z V_{\text{critic}}$        | Points toward high-value regions           |
| **Conditioned Generation**   | $u_\pi = G^{-1} \cdot \text{embed}(\text{prompt})$ | Clamped to user's prompt embedding         |
| **Unconditional (Dreaming)** | $u_\pi = 0$                                        | Pure thermal fluctuation selects direction |

*Proof.* In all cases, $u_\pi$ is a tangent vector at $z$. The RL case follows from the policy gradient theorem {cite}`sutton1999policy`; the generation case follows from treating the prompt as a target direction; the unconditional case reduces to pure Langevin dynamics where noise breaks symmetry. $\square$

:::

:::{prf:theorem} Angular Symmetry Breaking {cite}`strogatz2015nonlinear`
:label: thm-angular-symmetry-breaking

In the **overdamped limit** of the second-order geodesic Langevin equation (Definition {prf:ref}`def-bulk-drift-continuous-flow`, Theorem {prf:ref}`thm-overdamped-limit`), the generation dynamics decompose into radial expansion and angular symmetry breaking.

**Radial dynamics (monotonic expansion):** The radial coordinate $r = |z|$ satisfies:

$$
dr = \frac{1-r^2}{2}\,d\tau + \frac{1-r^2}{2}\sqrt{2T_c}\,dW_r

$$
with drift $\frac{1-r^2}{2} > 0$ for all $r \in [0,1)$. The origin is not a fixed point; the drift pushes trajectories outward (though stochastic fluctuations can temporarily reverse this at small $r$).

**Angular dynamics (symmetry breaking):** In polar coordinates $z = re^{i\theta}$, the angular evolution satisfies:

$$
d\theta = \frac{u_\pi^\theta}{r}\,d\tau + \frac{1-r^2}{2r}\sqrt{2T_c}\,dW_\theta

$$
where $u_\pi^\theta = u_\pi \cdot \hat{\theta}$ is the tangential component of the control field.

**Phase Transition:** The $SO(D)$ rotational symmetry undergoes spontaneous breaking controlled by the dimensionless ratio:

$$
\eta(r) := \frac{|u_\pi^\theta|^2}{T_c} \cdot \frac{2r^2}{(1-r^2)^2}

$$
- **Symmetric phase** ($\eta \ll 1$): Angular noise dominates; direction randomizes
- **Broken phase** ($\eta \gg 1$): Policy dominates; direction determined by $u_\pi$

*Proof.* Starting from the second-order geodesic Langevin equation (Definition {prf:ref}`def-bulk-drift-continuous-flow`) with the Poincaré metric $G_{ij} = \frac{4\delta_{ij}}{(1-r^2)^2}$, we take the overdamped limit (Theorem {prf:ref}`thm-overdamped-limit`). The overdamped position SDE in Cartesian coordinates is:

$$
dz^k = -G^{kj}\partial_j U\, d\tau + u_\pi^k\, d\tau + \sqrt{2T_c}(G^{-1/2})^{kj}\,dW^j_\tau

$$
where $G^{-1/2} = \frac{1-r^2}{2}I$. Converting to polar coordinates via Itô's lemma:
- Radial: $dr = \langle dz, \hat{r}\rangle + \frac{1}{2}\text{tr}(\text{Hess}_r \cdot \Sigma)$ where $\Sigma = 2T_c G^{-1}$
- Angular: $d\theta = \langle dz, \hat{\theta}/r\rangle + \frac{1}{2}\text{tr}(\text{Hess}_\theta \cdot \Sigma)$

The Itô corrections vanish for the radial component (since $\partial^2 r/\partial z^i\partial z^j$ is traceless) and contribute a drift correction for angular that cancels with geometric terms. The stated SDEs follow after simplification.

**Critical temperature:** The symmetry-breaking ratio $\eta(r)$ compares the squared angular drift to the angular diffusion coefficient. At characteristic radius $r_*$, setting $\eta(r_*) = 1$ defines the critical temperature.

**Direction freeze-out:** As $r$ increases toward the boundary, $\eta(r) \to \infty$ (the denominator $(1-r^2)^2 \to 0$), causing the angular distribution to concentrate. The direction selected at early times persists to the boundary. $\square$

:::

:::{prf:axiom} Bulk-Boundary Decoupling
:label: ax-bulk-boundary-decoupling

The state decomposition $Z = (K, z_n, z_{\text{tex}})$ satisfies a **partition condition**:

1. **Interior (Planning Domain):** The trajectory $z(\tau)$ evolves strictly on the manifold $\mathcal{Z} = \mathcal{K} \times \mathcal{Z}_n$. It contains no texture component. Planning depends only on geometry and topology:

$$
\dot{z} = f(z, u_\pi) \quad (\text{No } z_{\text{tex}} \text{ dependence})

$$
2. **Boundary Interface:** Texture $z_{\text{tex}}$ is a stochastic component that exists **only** at the interface where the internal state meets the external observation:

$$
z_{\text{tex}} \sim \mathcal{N}(0, \Sigma(z_{\text{final}}))

$$
Formally, the partition condition is:

$$
\frac{\partial}{\partial z_{\text{tex}}} \left[ \dot{z}^k, \lambda_{\text{jump}}, u_\pi \right] = 0 \quad \forall \tau \in [0, \tau_{\text{stop}})

$$
:::

:::{prf:definition} Boundary Texture Distribution
:label: def-boundary-texture-distribution

At the terminal position $z_{\text{final}}$, texture is sampled from a **geometry-dependent** Gaussian:

$$
z_{\text{tex}} \sim \mathcal{N}\big(0,\, \Sigma(z_{\text{final}})\big),

$$
where the covariance matrix is:

$$
\Sigma(z) = \sigma_{\text{tex}}^2 \cdot G^{-1}(z) = \sigma_{\text{tex}}^2 \cdot \frac{(1-|z|^2)^2}{4} I.

$$
Units: $[\Sigma] = [z_{\text{tex}}]^2$.

:::

:::{prf:proposition} Conformal Texture Scaling
:label: prop-conformal-texture-scaling

The texture variance scales with the inverse metric:

| **Region** | **$\lvert z\rvert$** | **$\Sigma(z)$**                            | **Interpretation**        |
|------------|----------------------|--------------------------------------------|---------------------------|
| Origin     | $\approx 0$          | $\sigma_{\text{tex}}^2/4 \cdot I$          | Moderate texture (coarse) |
| Mid-disk   | $\approx 0.5$        | $\sigma_{\text{tex}}^2 \cdot 9/64 \cdot I$ | Reduced texture           |
| Boundary   | $\to 1$              | $\to 0$                                    | Deterministic texture     |

*Remark (Conformal suppression).* Near the boundary (high resolution/specificity), the metric $G$ diverges, so $G^{-1} \to 0$ and texture fluctuations are suppressed.

:::

:::{prf:definition} Boundary Decoder
:label: def-boundary-decoder

The Decoder $\mathcal{D}$ is the **only** component that sees texture. It performs the **boundary synthesis**:

$$
x = \mathcal{D}(z_{\text{final}}, z_{\text{tex}})

$$
where:
- $z_{\text{final}} = (e_K, z_n)$: Determines the shape, physics, and causal structure
- $z_{\text{tex}}$: "Paints" the high-frequency details onto that structure

:::

:::{prf:proposition} Epistemic Barrier
:label: prop-epistemic-barrier

The partition condition enforces **BarrierEpi** (Epistemic Limit): The agent does not waste capacity predicting the noise---it only predicts the *statistics* of the noise ($\Sigma$).

:::

:::{prf:definition} Stopping Criterion
:label: def-stopping-criterion

The flow terminates when the radial coordinate exceeds a cutoff:

$$
\tau_{\text{stop}} := \inf\{\tau \ge 0 : |z(\tau)| \ge R_{\text{cutoff}}\}

$$
This is equivalent to the information stopping criterion $I_{\text{bulk}}(z) \ge C_\partial$ (Theorem {prf:ref}`thm-capacity-constrained-metric-law`).
In practice, choose $R_{\text{cutoff}} = 1 - \varepsilon$ with $\varepsilon$ tied to Levin length/resolution. This is a
computational cutoff, not a terminal task boundary.

**Algorithm 21.3.7 (Boundary Texture Sampling).**

```python
import torch

def sample_boundary_texture(
    z_final: torch.Tensor,        # [B, D] final semantic position
    texture_dim: int,             # Dimension of texture space
    sigma_tex: float = 1.0,       # Base texture std dev
) -> torch.Tensor:
    """
    Sample texture with geometry-dependent variance.

    Implements Definition 21.3.2:
        z_tex ~ N(0, Sigma(z_final))
        Sigma(z) = sigma_tex^2 * G^{-1}(z)

    The partition condition (Axiom 21.3.1) ensures this is called
    ONLY at terminal time, not during interior dynamics.

    Cross-ref: Proposition 21.3.3 (Conformal Scaling)
    """
    B = z_final.shape[0]

    # Compute G^{-1}(z) = (1 - |z|^2)^2 / 4
    r_sq = (z_final ** 2).sum(dim=-1, keepdim=True)  # [B, 1]
    one_minus_r_sq = torch.clamp(1.0 - r_sq, min=1e-6)
    G_inv_scale = (one_minus_r_sq ** 2) / 4.0  # [B, 1]

    # Texture std = sigma_tex * sqrt(G^{-1})
    texture_std = sigma_tex * torch.sqrt(G_inv_scale)  # [B, 1]

    # Sample isotropic Gaussian, then scale
    z_tex = torch.randn(B, texture_dim, device=z_final.device)
    z_tex = z_tex * texture_std  # broadcast scaling

    return z_tex
```

:::

## 05_geometry/04_equations_motion.md

:::{prf:definition} Mass Tensor
:label: def-mass-tensor

We define the **inertial mass tensor** $\mathbf{M}(z)$ as the capacity-constrained metric:

$$
\mathbf{M}(z) := G(z).

$$
This definition has the following operational consequences:
- **High curvature regions** (large $G$) have larger effective mass, yielding smaller velocity updates per unit force
- **Low curvature regions** (small $G$) have smaller effective mass, yielding larger velocity updates per unit force

Units: $[\mathbf{M}_{ij}] = [z]^{-2}$ (same as metric).

*Remark (Risk-Metric Coupling).* Combined with the Metric Law (Theorem {prf:ref}`thm-capacity-constrained-metric-law`), this yields a causal chain:

$$
\text{High risk } T_{ij} \;\Rightarrow\; \text{Large } G_{ij} \;\Rightarrow\; \text{Large } \mathbf{M}_{ij} \;\Rightarrow\; \text{Reduced step size}

$$
The metric-weighted step size decreases in high-curvature (high-risk) regions without explicit penalty terms.

:::

:::{prf:definition} Extended Onsager-Machlup Action
:label: def-extended-onsager-machlup-action

Let $(\mathcal{Z}, G)$ be the latent Riemannian manifold with the capacity-constrained metric ({ref}`Section 18 <sec-capacity-constrained-metric-law-geometry-from-interface-limits>`). For a path $z: [0, T] \to \mathcal{Z}$, the extended Onsager-Machlup action is:

$$
S_{\mathrm{OM}}[z] = \int_0^T \left( \frac{1}{2}\mathbf{M}(z)\|\dot{z}\|^2 + \Phi_{\text{eff}}(z) + \frac{T_c}{12}\,R(z) + T_c \cdot H_{\pi}(z) \right) ds,

$$
where:
- $\mathbf{M}(z)\|\dot{z}\|^2 = G_{ij}(z)\,\dot{z}^i\,\dot{z}^j$ is the kinetic energy (mass = metric)
- $\Phi_{\text{eff}}(z)$ is the effective potential (Definition {prf:ref}`def-effective-potential`)
- $R(z)$ is the scalar curvature of the metric $G$
- $H_{\pi}(z) = -\mathbb{E}_{a \sim \pi}[\log \pi(a|z)]$ is the policy entropy
- $T_c > 0$ is the {prf:ref}`def-cognitive-temperature` (cf. {ref}`Section 21.2 <sec-policy-control-field>`)

Units: $[S_{\mathrm{OM}}] = \mathrm{nat}$.

*Remark (Curvature Correction).* The term $\frac{T_c}{12}R(z)$ is a stochastic correction that accounts for the path-measure distortion on curved spaces. In flat space ($R = 0$), this term vanishes. The entropy term $T_c H_{\pi}$ ensures the agent prefers stochastic policies in uncertain regions.

:::

:::{prf:proposition} Mass Scaling Near Boundary
:label: prop-mass-scaling-near-boundary

For the Poincare disk, the mass tensor scales as:

$$
\mathbf{M}(z) = \frac{4}{(1-|z|^2)^2} I_d \quad \xrightarrow{|z| \to 1} \quad +\infty.

$$
The metric diverges as $|z| \to 1$, which bounds all finite-action trajectories to the interior of the disk.

*Proof.* Direct evaluation of the Poincare metric. The factor $(1-|z|^2)^{-2}$ diverges as $|z| \to 1$. $\square$

:::

:::{prf:proposition} Most Probable Path
:label: prop-most-probable-path

For the controlled diffusion

$$
dz^k = b^k(z)\,ds + \sqrt{2T_c}\,\sigma^{kj}(z)\,dW^j_s,

$$
where $\sigma \sigma^T = G^{-1}$, the most probable path connecting $z(0) = z_0$ and $z(T) = z_1$ minimizes the Onsager-Machlup action $S_{\mathrm{OM}}[z]$ subject to the boundary conditions.

*Proof sketch.* This follows from the Girsanov theorem and the Cameron-Martin formula adapted to Riemannian manifolds. See {cite}`ikeda1989stochastic` Chapter V or {ref}`Appendix A.4 <sec-appendix-a-full-derivations>` for details. $\square$

:::

:::{prf:definition} Second-Order Geodesic Langevin Equation
:label: def-bulk-drift-continuous-flow

The agent's state evolves as a **particle with position $z$ and momentum $p$** on the Riemannian manifold $(\mathcal{Z}, G)$. The dynamics are given by the coupled **second-order Langevin SDE**:

$$
\begin{cases}
dz^k = G^{kj}(z)\, p_j\, ds \\[8pt]
dp_k = \left[ -\partial_k \Phi_{\text{eff}} - \gamma\, p_k + \beta_{\text{curl}}\, \mathcal{F}_{kj}\, G^{j\ell}\, p_\ell - \Gamma^m_{k\ell}\, G^{\ell j}\, p_j\, p_m + u_{\pi,k} \right] ds + \sqrt{2\gamma T_c}\, (G^{1/2})_{kj}\, dW^j_s
\end{cases}

$$
where:
- $\Phi_{\text{eff}}$ is the **effective potential** (Definition {prf:ref}`def-effective-potential`)
- $\gamma > 0$ is the **friction coefficient** (damping rate)
- $\mathcal{F}_{ij} = \partial_i \mathcal{R}_j - \partial_j \mathcal{R}_i$ is the **Value Curl** tensor (Definition {prf:ref}`def-value-curl`)
- $\beta_{\text{curl}} \ge 0$ is the **curl coupling strength** (dimensionless)
- $\Gamma^m_{k\ell}$ are the **Christoffel symbols** of the Levi-Civita connection (Proposition {prf:ref}`prop-explicit-christoffel-symbols-for-poincare-disk`)
- $u_{\pi,k}$ is the **control field** from the policy (Definition {prf:ref}`def-the-control-field`)
- $T_c$ is the **cognitive temperature** (Definition {prf:ref}`def-cognitive-temperature`)
- $W_s$ is a standard Wiener process

*Units:* $[z] = \text{length}$, $[p] = \text{length}/\tau$, $[\gamma] = 1/\tau$, $[\Phi_{\text{eff}}] = \mathrm{nat}$, $[T_c] = \mathrm{nat}$.

**Interpretation:** The position evolves via the momentum (kinematic relation), while the momentum evolves under:

1. **Gradient force**: $-\nabla\Phi_{\text{eff}}$ — force from effective potential
2. **Friction**: $-\gamma p$ — damping toward equilibrium
3. **Lorentz force**: $\beta_{\text{curl}} \mathcal{F} G^{-1} p$ — velocity-dependent force from Value Curl (perpendicular to velocity)
4. **Geodesic correction**: $-\Gamma(G^{-1}p, G^{-1}p)$ — parallel transport on curved space
5. **Control field**: $u_\pi$ — policy-induced force
6. **Thermal noise**: $\sqrt{2\gamma T_c} G^{1/2} dW$ — fluctuation-dissipation balanced noise

**Hamiltonian Structure:** The deterministic part ($T_c = 0$, $\gamma = 0$) derives from the Hamiltonian:

$$
H(z, p) = \frac{1}{2} G^{ij}(z)\, p_i\, p_j + \Phi_{\text{eff}}(z).

$$
The friction and noise terms implement an **Ornstein-Uhlenbeck thermostat** that samples the Boltzmann distribution $\rho(z,p) \propto \exp(-H(z,p)/T_c)$.

**Conservative Limit:** When $\mathcal{F} = 0$ (Definition {prf:ref}`def-conservative-reward-field`), the Lorentz term vanishes and we recover the standard geodesic Langevin equation.

**Non-Conservative Dynamics:** When $\mathcal{F} \neq 0$, the Lorentz force induces rotational dynamics. Trajectories may converge to limit cycles rather than fixed points (Theorem {prf:ref}`thm-ness-existence`).

*Remark (BAOAB Integration).* This second-order system is integrated using the Boris-BAOAB scheme (Definition {prf:ref}`def-baoab-splitting`), which preserves the Boltzmann distribution to $O(h^2)$ and handles the velocity-dependent Lorentz force via the Boris rotation.

:::

:::{prf:proposition} Explicit Christoffel Symbols for Poincaré Disk
:label: prop-explicit-christoffel-symbols-for-poincare-disk

For the Poincare disk model with metric $G_{ij} = \frac{4\delta_{ij}}{(1-|z|^2)^2}$, the Christoffel symbols in Cartesian coordinates are:

$$
\Gamma^k_{ij}(z) = \frac{2}{1-|z|^2}\left(\delta^k_i z_j + \delta^k_j z_i - \delta_{ij} z^k\right).

$$
The geodesic correction term $\Gamma^k_{ij}\dot{z}^i\dot{z}^j$ contracts to:

$$
\Gamma^k_{ij}\dot{z}^i\dot{z}^j = \frac{4(z \cdot \dot{z})}{1-|z|^2}\dot{z}^k - \frac{2|\dot{z}|^2}{1-|z|^2}z^k.

$$
*Proof.* Direct computation from $\Gamma^k_{ij} = \frac{1}{2}G^{k\ell}(\partial_i G_{j\ell} + \partial_j G_{i\ell} - \partial_\ell G_{ij})$ using $\partial_m[(1-|z|^2)^{-2}] = 4z_m(1-|z|^2)^{-3}$. $\square$

*Geometric interpretation:* The first term $(z \cdot \dot{z})\dot{z}$ accelerates motion radially when moving outward; the second term $|\dot{z}|^2 z$ provides centripetal correction. Together they ensure geodesics are circular arcs perpendicular to the boundary.

:::

:::{prf:definition} Mass Evolution - Jump Process
:label: def-mass-evolution-jump-process

The importance weight $m(s)$ evolves according to a coupled jump-diffusion:

$$
dm = m \cdot r(z, a)\,ds + m \cdot (\eta - 1)\,dN_s,

$$
where:
- $r(z, a)$ is the **reaction rate** from the WFR dynamics ({ref}`Section 20.2 <sec-the-wfr-metric>`)
- $N_s$ is a Poisson process with intensity $\lambda_{\text{jump}}(z)$
- $\eta$ is the multiplicative jump factor (typically $\eta > 1$ for jumps to higher-value charts)

*Interpretation:* Between jumps, mass evolves smoothly via the reaction term $r$. At jump times, the mass is rescaled by factor $\eta$, and the position is teleported via the chart transition operator $L_{i \to j}$.

:::

:::{prf:proposition} Jump Intensity from Value Discontinuity
:label: prop-jump-intensity-from-value-discontinuity

The jump intensity $\lambda_{\text{jump}}(z)$ is determined by the value difference across chart boundaries:

$$
\lambda_{\text{jump}}(z) = \lambda_0 \cdot \exp\left(\beta_{\text{ent}} \cdot \left( V_{\text{target}}(L(z)) - V_{\text{source}}(z) - c_{\text{transport}} \right) \right),

$$
where:
- $\lambda_0 > 0$ is a base jump rate
- $\beta_{\text{ent}} > 0$ is the inverse temperature (sharpness)
- $V_{\text{target}}$ and $V_{\text{source}}$ are the value functions on the target and source charts
- $L: \mathcal{Z}_{\text{source}} \to \mathcal{Z}_{\text{target}}$ is the chart transition operator
- $c_{\text{transport}} \ge 0$ is the transport cost (WFR term)

*Remark (SMC Interpretation).* The mass $m(s)$ is precisely the **importance weight** in Sequential Monte Carlo (SMC) / particle filtering. The agent is a single-particle realization of the WFR flow from {ref}`Section 20 <sec-wasserstein-fisher-rao-geometry-unified-transport-on-hybrid-state-spaces>`. Multiple particles can be used for ensemble-based generation.

**Cross-references:** {ref}`Section 20.2 <sec-the-wfr-metric>` ({prf:ref}`def-the-wfr-action`), {ref}`Section 20.6 <sec-the-unified-world-model>` (WFR world model), {ref}`Section 11 <sec-intrinsic-motivation-maximum-entropy-exploration>` (Filtering and projection).

:::

:::{prf:definition} Effective Potential
:label: def-effective-potential

The unified effective potential is:

$$
\Phi_{\text{eff}}(z, K) = \alpha\, U(z) + (1 - \alpha)\, V_{\text{critic}}(z, K) + \gamma_{risk}\, \Psi_{\text{risk}}(z),

$$
where:
- $U(z) = -d_{\mathbb{D}}(0, z) = -2\operatorname{artanh}(|z|)$ is the **hyperbolic information potential** (Definition {prf:ref}`def-hyperbolic-information-potential`)
- $V_{\text{critic}}(z, K)$ is the **learned value/critic function** on chart $K$ ({ref}`Section 2.7 <sec-the-hjb-correspondence>`)
- $\Psi_{\text{risk}}(z) = \frac{1}{2}\operatorname{tr}(T_{ij} G^{ij})$ is the **risk-stress contribution** (Theorem {prf:ref}`thm-capacity-constrained-metric-law`)
- $\alpha \in [0, 1]$ is the generation-vs-control hyperparameter
- $\gamma_{risk} \ge 0$ is the risk aversion coefficient

Units: $[\Phi_{\text{eff}}] = \mathrm{nat}$.

:::

:::{prf:proposition} Mode Interpretation
:label: prop-mode-interpretation

The parameter $\alpha$ interpolates between pure generation and pure control:

| Regime              | $\alpha$ Value      | Behavior                                                       |
|---------------------|---------------------|----------------------------------------------------------------|
| **Pure Generation** | $\alpha = 1$        | Flow follows $-\nabla_G U$ (holographic expansion, {ref}`Section 21 <sec-radial-generation-entropic-drift-and-policy-control>`) |
| **Pure Control**    | $\alpha = 0$        | Flow follows $-\nabla_G V_{\text{critic}}$ (policy gradient)   |
| **Hybrid**          | $\alpha \in (0, 1)$ | Balanced generation and control                                |

*Remark (Risk Modulation).* The $\gamma_{risk}$ term provides an additional penalty in high-stress regions (large $T_{ij}$), which further discourages risky trajectories beyond the geometric slowdown from Mass=Metric.

:::

:::{prf:corollary} Gradient Decomposition
:label: cor-gradient-decomposition

The gradient of the effective potential decomposes as:

$$
\nabla_G \Phi_{\text{eff}} = \alpha\, \nabla_G U + (1 - \alpha)\, \nabla_G V_{\text{critic}} + \gamma_{risk}\, \nabla_G \Psi_{\text{risk}}.

$$
For the Poincare disk model, the first term simplifies to:

$$
\nabla_G U = -\frac{(1-|z|^2)}{2}\, \hat{z}, \qquad \hat{z} = \frac{z}{|z|}.

$$
**Cross-references:** Definition {prf:ref}`def-hyperbolic-information-potential`, {ref}`Section 2.7 <sec-the-hjb-correspondence>` (Critic $V$), Section 14.2 (MaxEnt control), Theorem {prf:ref}`thm-capacity-constrained-metric-law`.

*Forward reference (Scalar Field Interpretation).* {ref}`Section 24 <sec-the-reward-field-value-forms-and-hodge-geometry>`
provides the complete field-theoretic interpretation of $V_{\text{critic}}$: the Critic solves the **Screened Poisson
Equation** (Theorem {prf:ref}`thm-the-hjb-helmholtz-correspondence`) with rewards as boundary flux (scalar charges in the
conservative case; Definition {prf:ref}`def-the-reward-flux`), the Value represents **Gibbs Free Energy** (Axiom
{prf:ref}`ax-the-boltzmann-value-law`), and the Value Hessian induces a **Conformal Coupling** to the metric (Definition
{prf:ref}`def-value-metric-conformal-coupling`).

:::

:::{prf:definition} Cognitive Temperature
:label: def-cognitive-temperature

The **cognitive temperature** $T_c > 0$ is the exploration-exploitation tradeoff parameter that controls:

1. **Diffusion magnitude:** The thermal noise term in the geodesic SDE scales as $\sqrt{2T_c}\,dW$
2. **Boltzmann policy:** The softmax temperature in $\pi(a|z) \propto \exp(Q(z,a)/T_c)$
3. **Free energy tradeoff:** The entropy-energy balance $\Phi = E - T_c S$

*Units:* nat (dimensionless in natural units where $k_B = 1$).

*Correspondence:* $T_c$ is the agent-theoretic analogue of thermodynamic temperature $k_B T$ in statistical mechanics.
:::

:::{prf:definition} Boris-BAOAB Splitting
:label: def-baoab-splitting

The Boris-BAOAB integrator splits the Lorentz-Langevin dynamics into five substeps per time step $h$:

1. **B** (half kick + Boris rotation):
   - Half-kick from gradient: $p^- \leftarrow p - \frac{h}{2}\nabla\Phi(z)$
   - Boris rotation (if $\mathcal{F} \neq 0$):
     - $t \leftarrow \frac{h}{2}\beta_{\text{curl}} G^{-1}\mathcal{F}$ (rotation vector)
     - $p' \leftarrow p^- + p^- \times t$
     - $s \leftarrow \frac{2t}{1 + |t|^2}$
     - $p^+ \leftarrow p^- + p' \times s$
   - Half-kick from gradient: $p \leftarrow p^+ - \frac{h}{2}\nabla\Phi(z)$

2. **A** (half drift): $z \leftarrow \operatorname{Exp}_z\left(\frac{h}{2} G^{-1}(z)\, p\right)$

3. **O** (thermostat): $p \leftarrow c_1 p + c_2\, G^{1/2}(z)\, \xi$, where $\xi \sim \mathcal{N}(0, I)$

4. **A** (half drift): $z \leftarrow \operatorname{Exp}_z\left(\frac{h}{2} G^{-1}(z)\, p\right)$

5. **B** (half kick + Boris rotation): Same as step 1

where $c_1 = e^{-\gamma h}$ and $c_2 = \sqrt{(1 - c_1^2) T_c}$.

**Conservative Limit:** When $\mathcal{F} = 0$, the Boris rotation is identity and we recover standard BAOAB.

*Remark (Boris Rotation).* The Boris algorithm is a volume-preserving integrator for magnetic-like forces. It rotates the momentum around the local Value Curl axis, preserving the norm $|p|$ while changing direction. This ensures the Lorentz force does no net work, consistent with physics.

*Remark (O-step).* The O-step implements the **Ornstein-Uhlenbeck thermostat**, which exactly preserves the Maxwell-Boltzmann momentum distribution $p \sim \mathcal{N}(0, T_c G)$.

:::

:::{prf:proposition} BAOAB Preserves Boltzmann
:label: prop-baoab-preserves-boltzmann

The BAOAB integrator preserves the Boltzmann distribution $\rho(z, p) \propto \exp(-\Phi_{\text{eff}}(z)/T_c - \|p\|_G^2 / (2T_c))$ to second order in $h$.

*Proof sketch.* The symmetric splitting B-A-O-A-B ensures time-reversibility of the deterministic steps. The O-step exactly samples the Maxwell-Boltzmann momentum distribution. Together, these guarantee that $\rho$ is a fixed point of the numerical flow up to $O(h^3)$ errors. See {cite}`leimkuhler2016computation`. $\square$

*Remark (Comparison to Euler-Maruyama).* Euler-Maruyama has $O(h)$ bias in the stationary distribution, whereas BAOAB achieves $O(h^2)$. For long trajectories, this difference is critical.

:::

:::{prf:theorem} Overdamped Limit
:label: thm-overdamped-limit

Consider the second-order SDE from Definition {prf:ref}`def-bulk-drift-continuous-flow` with friction coefficient $\gamma$:

$$
m\,\ddot{z}^k + \gamma\,\dot{z}^k - \beta_{\text{curl}} G^{km}\mathcal{F}_{mj}\dot{z}^j + G^{kj}\partial_j\Phi + \Gamma^k_{ij}\dot{z}^i\dot{z}^j = \sqrt{2T_c}\,\left(G^{-1/2}\right)^{kj}\,\xi^j,

$$
where $m$ is the "inertial mass" and $\xi$ is white noise. In the limit $\gamma \to \infty$ with $m$ fixed (or equivalently, $m \to 0$ with $\gamma$ fixed), the dynamics reduce to the first-order Langevin equation:

$$
dz^k = \left[\mathcal{M}_\gamma(z)\right]^{k}{}_{j}\left(-G^{j\ell}(z)\,\partial_\ell\Phi_{\text{gen}}(z)\right) ds + \sqrt{2T_c}\,\left(G^{-1/2}(z)\right)^{kj}\,dW^j_s.

$$
*Proof sketch.* In the high-friction limit, velocity equilibrates instantaneously to
$\dot{z} \approx \mathcal{M}_\gamma(-G^{-1}\nabla\Phi)$. The geodesic term
$\Gamma(\dot{z},\dot{z}) \sim O(|\dot{z}|^2) = O(\gamma^{-2})$ is negligible. What remains is the curl-corrected
gradient flow with diffusion. See {ref}`Appendix A.4 <sec-appendix-a-full-derivations>` for the full singular
perturbation analysis. $\square$

:::

:::{prf:corollary} Recovery of Holographic Flow
:label: cor-recovery-of-holographic-flow

Setting $\alpha = 1$ (pure generation), $T_c \to 0$ (deterministic limit), and $\mathcal{F} = 0$ (conservative case) in the overdamped equation recovers the holographic gradient flow from {ref}`Section 21.2 <sec-policy-control-field>`:

$$
\dot{z} = -G^{-1}(z)\,\nabla U(z).

$$
For the Poincare disk, this gives $\dot{z} = \frac{(1-|z|^2)}{2}\,z$, which integrates to $|z(\tau)| = \tanh(\tau/2)$.

*Proof.* Direct substitution of $\Phi_{\text{gen}} = U$ into the overdamped equation. The explicit solution for the radial coordinate $r(\tau) = |z(\tau)|$ satisfies $\dot{r} = \frac{1-r^2}{2}$, which integrates to $r(\tau) = \tanh(\tau/2 + \operatorname{artanh}(r_0))$. For $r_0 = 0$, we get $r(\tau) = \tanh(\tau/2)$. $\square$

*Remark.* This proves that the "ad-hoc" holographic law from {ref}`Section 21 <sec-radial-generation-entropic-drift-and-policy-control>` is actually the **optimal control trajectory** for the geometry defined in {ref}`Section 18 <sec-capacity-constrained-metric-law-geometry-from-interface-limits>`, vindicating the intuition.

:::

:::{prf:corollary} Fokker-Planck Duality {cite}`risken1996fokkerplanck`
:label: cor-fokker-planck-duality

The stationary distribution of the overdamped SDE is:

$$
p_*(z) \propto \exp\left(-\frac{\Phi_{\text{gen}}(z)}{T_c}\right)\,\sqrt{|G(z)|},

$$
where $|G| = \det(G)$ is the metric determinant. This is the Boltzmann distribution on the curved manifold.

*Proof.* The Fokker-Planck equation for the overdamped dynamics is:

$$
\partial_s p = \nabla_i\left( G^{ij}\left( p\,\partial_j\Phi + T_c\,\partial_j p \right) \right).

$$
Setting $\partial_s p = 0$ and using detailed balance gives $p \propto e^{-\Phi/T_c} \sqrt{|G|}$. The $\sqrt{|G|}$ factor accounts for the Riemannian volume form. $\square$

**Cross-references:** {ref}`Section 21.2 <sec-policy-control-field>` (Langevin dynamics), Theorem {prf:ref}`thm-equivalence-of-entropy-regularized-control-forms-discrete-macro`, {ref}`Section 2.11 <sec-variance-value-duality-and-information-conservation>` (Belief density evolution).

:::

:::{prf:definition} Agent Lifecycle Phases
:label: def-agent-lifecycle-phases


| Phase           | Time Interval                | Dynamics                         | Texture      | Key Operations                                                                         |
|-----------------|------------------------------|----------------------------------|--------------|----------------------------------------------------------------------------------------|
| **1. Init**     | $\tau = 0$                   | $z(0) = 0$                       | None         | Initialize at origin; $p(0) \sim \mathcal{N}(0, T_c G(0))$                             |
| **2. Kick**     | $[0, \tau_{kick}]$           | Langevin at origin               | None         | Apply symmetry-breaking control $u_\pi$ (Def. {prf:ref}`def-the-control-field`)        |
| **3. Bulk**     | $[\tau_{kick}, \tau_{stop}]$ | BAOAB + Jumps                    | **Firewall** | Geodesic flow with chart transitions                                                   |
| **4. Boundary** | $\tau = \tau_{stop}$         | $\lVert z\rVert \geq R_{cutoff}$ | Sampled      | Sample texture $z_{tex} \sim \mathcal{N}(0, \Sigma(z))$                                |
| **5. Decode**   | Post-$\tau_{stop}$           | —                                | Used         | $x = \text{Decoder}(e_K, z_n, z_{tex})$                                                |

*Remark.* The **Texture Firewall** (Axiom {prf:ref}`ax-bulk-boundary-decoupling`) ensures that $\partial_{z_{tex}} \dot{z} = 0$ throughout the bulk phase—texture is completely invisible to the dynamics.

**Algorithm 22.6.2 (Full Agent Loop).**

```python
def run_agent_loop(
    policy: Policy,
    decoder: Decoder,
    T_c: float,
    gamma: float,
    h: float,
    R_cutoff: float = 0.95,
    max_steps: int = 1000,
) -> torch.Tensor:
    """
    Execute the full agent lifecycle from init to decode.

    Returns: Generated output x
    """
    B, d = 1, policy.latent_dim
    device = policy.device

    # ===== Phase 1: Init =====
    z = torch.zeros(B, d, device=device)
    p = torch.randn(B, d, device=device) * math.sqrt(T_c * 4.0)  # G(0) = 4I
    K = torch.zeros(B, dtype=torch.long, device=device)
    m = torch.ones(B, device=device)
    state = GeodesicState(z=z, p=p, K=K, m=m, s=0.0)

    # ===== Phase 2: Kick =====
    # Apply symmetry-breaking control at origin
    u_pi = policy.symmetry_breaking_kick(z, mode='generation')

    # ===== Phase 3: Bulk (with Texture Firewall) =====
    for step in range(max_steps):
        # Compute effective potential gradient
        grad_Phi = compute_effective_potential_gradient(
            state.z, state.K, policy.value_fn, alpha=0.5
        )

        # Update control field
        u_pi = policy.control_field(state.z, state.K)

        # BAOAB step (texture is invisible here)
        state = geodesic_baoab_step(
            state, grad_Phi, u_pi, T_c, gamma, h,
            jump_rate_fn=policy.jump_rate,
            chart_transition_fn=policy.chart_transition
        )

        # Check boundary condition
        z_norm = torch.sqrt((state.z ** 2).sum(dim=-1))
        if (z_norm >= R_cutoff).all():
            break

    # ===== Phase 4: Boundary - Sample texture =====
    z_tex = sample_holographic_texture(state.z, sigma_tex=0.1)

    # ===== Phase 5: Decode =====
    embedding = policy.chart_embedding(state.K)  # e_K
    x = decoder(embedding, state.z, z_tex)

    return x
```

:::

:::{prf:proposition} Phase Transition Interpretation
:label: prop-phase-transition-interpretation

The agent lifecycle corresponds to a thermodynamic phase transition:

| Phase | Thermodynamic Analogy | Order Parameter |
|-------|----------------------|-----------------|
| Init (gas) | High entropy, symmetric | $\lVert z\rVert = 0$ |
| Kick (nucleation) | Symmetry breaking | $u_\pi \neq 0$ |
| Bulk (liquid) | Directed flow | $0 < \lVert z\rVert < R_{cutoff}$ |
| Boundary (solid) | Crystallization | $\lVert z\rVert \geq R_{cutoff}$ |

:::

:::{prf:definition} Einstein Relation on Manifolds
:label: def-einstein-relation-on-manifolds

The fluctuation-dissipation relation requires:

$$
\sigma^2(z) = \frac{2\gamma(z)\, T_c}{G(z)},

$$
where $\sigma^2$ is the noise variance. This ensures the correct equilibrium distribution.

:::

:::{prf:proposition} Automatic Phase Transitions
:label: prop-automatic-phase-transitions

With adaptive temperature $T_c(z)$ satisfying the Einstein relation:

| Regime                      | Metric $G(z)$ | Effective Noise | Phase Behavior                |
|-----------------------------|---------------|-----------------|-------------------------------|
| **Uncertain** (near origin) | Small         | Large           | Gas phase (exploration)       |
| **Certain** (near boundary) | Large         | Small           | Solid phase (crystallization) |

*Remark.* This automatic phase transition emerges from the geometry alone---no explicit temperature schedule is needed.

:::

:::{prf:definition} Fisher-Covariance Duality
:label: def-fisher-covariance-duality

The inverse relationship between uncertainty and metric:

$$
G(z) \approx \Sigma^{-1}(z),

$$
where $\Sigma(z)$ is the posterior covariance of the belief at $z$. This duality underlies the Mass=Metric principle (Definition {prf:ref}`def-mass-tensor`).

**Algorithm 22.7.4 (Adaptive Temperature).**

```python
def adaptive_temperature(
    z: torch.Tensor,
    base_T: float,
    certainty_scale: float = 1.0,
) -> torch.Tensor:
    """
    Compute adaptive temperature based on local geometry.

    T_c(z) = base_T * (1 - |z|^2)^2 / 4

    This maintains constant effective noise: sigma^2 * G = 2 * gamma * T_c
    """
    r_sq = (z ** 2).sum(dim=-1, keepdim=True)
    # Conformal factor inverse: G^{-1} = (1-|z|^2)^2 / 4
    inv_conformal = (1.0 - r_sq + 1e-8) ** 2 / 4.0
    return base_T * inv_conformal * certainty_scale
```

:::

:::{prf:corollary} Deterministic Boundary
:label: cor-deterministic-boundary

As $|z| \to 1$:

$$
T_c(z) \to 0, \qquad \text{noise} \to 0.

$$
The agent becomes deterministic at the boundary, ensuring reproducible outputs.

:::

## 06_fields/01_boundary_interface.md

:::{prf:definition} Symplectic Boundary Manifold
:label: def-symplectic-boundary-manifold

The agent's interface is a symplectic manifold $(\partial\mathcal{Z}, \omega)$ with canonical coordinates $(q, p) \in T^*\mathcal{M}$ where:
- $q \in \mathcal{Q}$ is the **position bundle** (sensory configuration)
- $p \in T^*_q\mathcal{Q}$ is the **momentum bundle** (motor flux)

The symplectic form is:

$$
\omega = \sum_{i=1}^n dq^i \wedge dp_i.

$$
Units: $[\omega] = [q][p] = \mathrm{nat}$.

*Remark (Causal Structure).* The symplectic structure encodes causality: observations fix "where" the belief state is (position), while actions fix "how" it flows outward (momentum/flux). These cannot be treated symmetrically as static fields.

:::

:::{prf:definition} Dirichlet Boundary Condition --- Sensors
:label: def-dirichlet-boundary-condition-sensors

The sensory input stream $\phi(x)$ imposes a **Dirichlet** (position-clamping) condition on the belief density:

$$
\rho_{\partial}^{\text{sense}}(q, t) = \delta(q - q_{\text{obs}}(t)),

$$
where $q_{\text{obs}}(t) = E_\phi(x_t)$ is the encoded observation. This clamps the *configuration* of the belief state.

*Interpretation:* Information flow from environment to agent (observation).

:::

:::{prf:definition} Neumann Boundary Condition --- Motors
:label: def-neumann-boundary-condition-motors

The motor output stream $A(x)$ imposes a **Neumann** (flux-clamping) condition:

$$
\nabla \rho \cdot \mathbf{n} \big|_{\partial\mathcal{Z}_{\text{motor}}} = j_{\text{motor}}(p, t),

$$
where $j_{\text{motor}}$ is the motor current density determined by the policy:

$$
j_{\text{motor}} = D_A(u_\pi) = \text{Decoder}(z, u_\pi, z_{\text{tex,motor}}).

$$
*Interpretation:* Information flow from agent to environment (action).

Units: $[j_{\text{motor}}] = \mathrm{nat}/\text{step}$.

:::

:::{prf:proposition} Symplectic Duality Principle
:label: prop-symplectic-duality-principle

Under the canonical transformation $(q, p) \mapsto (p, -q)$:
- Dirichlet conditions become Neumann conditions
- Sensors become motors
- Perception becomes action

This duality is the mathematical foundation for the symmetric treatment of sensing and actuation.

*Proof sketch.* The symplectic form $\omega$ is invariant under canonical transformations. The Legendre transform $\mathcal{L}: T\mathcal{Q} \to T^*\mathcal{Q}$ maps velocity to momentum, exchanging position-fixing (Dirichlet) for flux-fixing (Neumann). $\square$

**Cross-references:** {ref}`sec-the-interface-and-observation-inflow` (Observation inflow), Definition {prf:ref}`def-dirichlet-boundary-condition-sensors`.

:::

:::{prf:definition} Visual Atlas — Perception
:label: def-visual-atlas-perception

The Visual Atlas $\mathcal{A}_{\text{vis}} = \{(U_\alpha, \phi_\alpha, e_\alpha^{\text{vis}})\}_{\alpha \in \mathcal{K}_{\text{vis}}}$ is a chart atlas on the sensory manifold $\mathcal{Q}$ with:
- **Charts** $U_\alpha \subset \mathcal{Q}$: Objects, Scenes, Viewpoints
- **Chart maps** $\phi_\alpha: U_\alpha \to \mathbb{R}^{d_{\text{vis}}}$: Local coordinates
- **Codebook embeddings** $e_\alpha^{\text{vis}} \in \mathbb{R}^{d_m}$: Discrete macro codes

*Input:* Raw observations $\phi_{\text{raw}}$ (pixels, sensors).
*Output:* Latent state $z \in \mathcal{Z}$ (configuration).

:::

:::{prf:definition} Action Atlas --- Actuation
:label: def-action-atlas-actuation

The Action Atlas $\mathcal{A}_{\text{act}} = \{(V_\beta, \psi_\beta, e_\beta^{\text{act}})\}_{\beta \in \mathcal{K}_{\text{act}}}$ is a chart atlas on the motor manifold $T^*\mathcal{Q}$ with:
- **Charts** $V_\beta \subset T^*\mathcal{Q}$: Gaits, Grasps, Tool Affordances (topologically distinct control regimes)
- **Chart maps** $\psi_\beta: V_\beta \to \mathbb{R}^{d_{\text{act}}}$: Local motor coordinates
- **Codebook embeddings** $e_\beta^{\text{act}} \in \mathbb{R}^{d_m}$: Action primitive codes

*Input:* Intention $u_{\text{intent}} \in T_z\mathcal{Z}$ (from Policy, {ref}`sec-policy-control-field`).
*Output:* Actuation $a_{\text{raw}}$ (torques, voltages).

*Remark (Jump Operator in Action Atlas).* The **Jump Operator** $L_{\beta \to \beta'}$ in the Action Atlas represents **Task Switching**: transitioning from one control primitive to another (e.g., "Walk" $\to$ "Jump", "Grasp" $\to$ "Release"). This mirrors the chart transition operator in the Visual Atlas ({ref}`sec-the-unified-world-model`).

:::

:::{prf:theorem} Atlas Duality via Legendre Transform
:label: thm-atlas-duality-via-legendre-transform

The Visual and Action Atlases are related by the Legendre transform $\mathcal{L}: T\mathcal{Q} \to T^*\mathcal{Q}$:

$$
\mathcal{A}_{\text{act}} = \mathcal{L}(\mathcal{A}_{\text{vis}}),

$$
where the chart transition functions satisfy:

$$
\psi_\beta \circ \mathcal{L} \circ \phi_\alpha^{-1} = \nabla_{\dot{q}} L(q, \dot{q})

$$
for Lagrangian $L(q, \dot{q}) = \frac{1}{2}\|\dot{q}\|_G^2 - V(q)$.

*Proof.* **Step 1 (Legendre transform definition).** The Legendre transform of a convex Lagrangian $L(q,\dot{q})$ is defined by:

$$
\mathcal{L}: T\mathcal{Q} \to T^*\mathcal{Q}, \qquad (q, \dot{q}) \mapsto \left(q, \frac{\partial L}{\partial \dot{q}}\right).

$$
For $L = \frac{1}{2}\|\dot{q}\|_G^2 - V(q)$, this gives $p = G(q)\dot{q}$, which is invertible when $G > 0$.

**Step 2 (Symplectic preservation).** The Legendre transform is a diffeomorphism that pulls back the canonical symplectic form $\omega_{T^*\mathcal{Q}} = dp \wedge dq$ to the Poincare-Cartan form $\omega_{T\mathcal{Q}} = d\theta_L$ where $\theta_L = \frac{\partial L}{\partial \dot{q}^i}dq^i$. This ensures that Hamiltonian flow on $T^*\mathcal{Q}$ corresponds to Lagrangian flow on $T\mathcal{Q}$.

**Step 3 (Chart compatibility).** Let $(U_\alpha, \phi_\alpha)$ be a chart in $\mathcal{A}_{\text{vis}}$ with coordinates $(q^\alpha, \dot{q}^\alpha)$. Define the induced action chart $(V_\beta, \psi_\beta)$ by $V_\beta = \mathcal{L}(U_\alpha \times T_{U_\alpha}\mathcal{Q})$ with coordinates $(q^\alpha, p^\alpha)$. The transition function is:

$$
\psi_\beta \circ \mathcal{L} \circ \phi_\alpha^{-1}: (q^\alpha, \dot{q}^\alpha) \mapsto (q^\alpha, G_{\alpha\beta}(q)\dot{q}^\beta),

$$
which is smooth and invertible by positive-definiteness of $G$. $\square$

*Remark (Why Legendre?).* The Legendre transform is the unique smooth map relating configuration-velocity (perception) to configuration-momentum (action) that:
1. Preserves the symplectic structure (Proposition {prf:ref}`prop-symplectic-duality-principle`)
2. Interchanges Dirichlet and Neumann boundary conditions ({ref}`sec-the-symplectic-interface-position-momentum-duality`)
3. Maps kinetic energy to Hamiltonian dynamics

*Cross-reference:* The metric $G$ appearing here is the capacity-constrained metric from Theorem {prf:ref}`thm-capacity-constrained-metric-law`, ensuring that the "mass" in the Legendre relation $p = G\dot{q}$ is the same "mass" that determines geodesic inertia (Definition {prf:ref}`def-mass-tensor`).

:::

:::{prf:definition} The Holographic Shutter — Unified Interface
:label: def-the-holographic-shutter-unified-interface

The Shutter is extended from {ref}`sec-the-shutter-as-a-vq-vae` to a symmetric tuple:

$$
\mathbb{S} = (\mathcal{A}_{\text{vis}}, \mathcal{A}_{\text{act}}),

$$
where:
- **Ingress (Perception):** $E_\phi: \mathcal{Q} \to \mathcal{Z}$ via Visual Atlas
- **Egress (Actuation):** $D_A: T_z\mathcal{Z} \times \mathcal{Z} \to T^*\mathcal{Q}$ via Action Atlas
- **Proprioception (Inverse Model):** $E_A: T^*\mathcal{Q} \to T_z\mathcal{Z}$ maps realized actions back to intentions

**Cross-references:** {ref}`sec-the-shutter-as-a-vq-vae` (VQ-VAE Shutter), {ref}`sec-tier-the-attentive-atlas` (AttentiveAtlasEncoder), {ref}`sec-decoder-architecture-overview-topological-decoder` (TopologicalDecoder).

:::

:::{prf:definition} Motor Texture Decomposition
:label: def-motor-texture-decomposition

The motor output decomposes as:

$$
a_t = (K^{\text{act}}_t, z_{n,\text{motor}}, z_{\text{tex,motor}}),

$$
where:
- $K^{\text{act}}_t \in \mathcal{K}_{\text{act}}$ is the **discrete motor macro** (action primitive/chart index)
- $z_{n,\text{motor}} \in \mathbb{R}^{d_{\text{motor},n}}$ is **motor nuisance** (impedance, compliance, force distribution)
- $z_{\text{tex,motor}} \in \mathbb{R}^{d_{\text{motor,tex}}}$ is **motor texture** (tremor, fine-grained noise, micro-corrections)

*Remark (Parallel to Visual Decomposition).* This mirrors the visual decomposition $(K_t, z_{n,t}, z_{\text{tex},t})$ from {ref}`sec-the-shutter-as-a-vq-vae`:

| Component                 | Visual Domain                 | Motor Domain                              |
|---------------------------|-------------------------------|-------------------------------------------|
| **Macro (discrete)**      | Object/Scene chart $K$        | Action primitive $K^{\text{act}}$         |
| **Nuisance (continuous)** | Pose/viewpoint $z_n$          | Compliance/impedance $z_{n,\text{motor}}$ |
| **Texture (residual)**    | Pixel detail $z_{\text{tex}}$ | Tremor/noise $z_{\text{tex,motor}}$       |

:::

:::{prf:definition} Compliance Tensor
:label: def-compliance-tensor

The motor nuisance encodes the **compliance tensor**:

$$
C_{ij}(z_{n,\text{motor}}) = \frac{\partial a^i}{\partial f^j},

$$
where $f$ is the external force/feedback. This determines how the motor output responds to perturbations:
- **High compliance** ($C$ large): Soft, yielding response (safe interaction)
- **Low compliance** ($C$ small): Stiff, precise response (accurate positioning)

Units: $[C_{ij}] = [a]/[f]$.

:::

:::{prf:definition} Motor Texture Distribution
:label: def-motor-texture-distribution

At the motor boundary, texture is sampled from a geometry-dependent Gaussian:

$$
z_{\text{tex,motor}} \sim \mathcal{N}(0, \Sigma_{\text{motor}}(z)),

$$
where:

$$
\Sigma_{\text{motor}}(z) = \sigma_{\text{motor}}^2 \cdot G_{\text{motor}}^{-1}(z) = \sigma_{\text{motor}}^2 \cdot \frac{(1-|z|^2)^2}{4} I_{d_{\text{motor,tex}}}.

$$
This follows the same conformal scaling as visual texture (Definition {prf:ref}`def-boundary-texture-distribution`), ensuring consistent thermodynamic behavior.

:::

:::{prf:axiom} Motor Texture Firewall
:label: ax-motor-texture-firewall

Motor texture is decoupled from the Bulk dynamics:

$$
\partial_{z_{\text{tex,motor}}} \dot{z} = 0, \qquad \partial_{z_{\text{tex,motor}}} u_\pi = 0.

$$
The policy $\pi_\theta$ operates on $(K, z_n, A, z_{n,\text{motor}})$ but **never** on $(z_{\text{tex}}, z_{\text{tex,motor}})$.

*Remark (Sim-to-Real Gap).* The **motor texture variance** $\sigma_{\text{motor}}^2$ is the mathematical definition of the "Sim-to-Real gap":
- **Simulation:** $\sigma_{\text{motor}} \approx 0$ (deterministic, no tremor)
- **Reality:** $\sigma_{\text{motor}} > 0$ (friction, sensor noise, motor tremor)
- **Robustness:** The Bulk policy $u_\pi$ is invariant; only the Action Decoder learns to manage domain-specific noise.

**Cross-references:** {ref}`sec-the-retrieval-texture-firewall` (Texture Firewall), Axiom {prf:ref}`ax-bulk-boundary-decoupling`.

:::

:::{prf:definition} Cycle Phases
:label: def-cycle-phases


| Phase             | Process            | Information Flow                      | Entropy Change               |
|-------------------|--------------------|---------------------------------------|------------------------------|
| **I. Perception** | Compression        | Mutual information $I(X;K)$ extracted | $\Delta S_{\text{bulk}} < 0$ |
| **II. Dreaming**  | Internal evolution | No external exchange                  | $\Delta S = 0$ (isentropic)  |
| **III. Action**   | Expansion          | Mutual information $I(A;K)$ injected  | $\Delta S_{\text{bulk}} > 0$ |

*Remark (Statistical mechanics analogy).* This cycle is structurally analogous to a Stirling cycle in thermodynamics.

:::

:::{prf:theorem} Perception as Compression
:label: thm-perception-as-compression

During perception, the agent compresses external entropy into internal free energy:

$$
W_{\text{compress}} = T_c \cdot I(X_t; K_t) \geq 0,

$$
where $T_c$ is the cognitive temperature ({prf:ref}`def-cognitive-temperature`) and $I(X_t; K_t)$ is the mutual information extracted from the observation $X_t$ into the macro-state $K_t$.

*Mechanism:* The Visual Encoder $E_\phi$ compresses high-entropy raw data $\phi_{\text{raw}}$ into a low-entropy macro-state $z$. The "heat" absorbed is the raw sensory stream.

*Information-theoretic interpretation:* Entropy decreases ($\Delta S < 0$). The Information Bottleneck cost bounds the compression.

:::

:::{prf:theorem} Action as Expansion
:label: thm-action-as-expansion

During action, the agent expands internal free energy into external control:

$$
W_{\text{expand}} = T_c \cdot I(K^{\text{act}}_t; K_t) \geq 0,

$$
where $I(K^{\text{act}}_t; K_t)$ is the mutual information injected from the intention into the motor output.

*Mechanism:* The Action Decoder $D_A$ "expands" the low-entropy Intention $u_\pi$ into high-dimensional motor commands $a_{\text{raw}}$, injecting motor texture.

*Information-theoretic interpretation:* Entropy increases ($\Delta S > 0$). The agent injects stochastic texture into motor outputs.

:::

:::{prf:definition} Dreaming as Unitary Evolution
:label: def-dreaming-as-unitary-evolution

In the dreaming phase, the internal dynamics are approximately unitary (energy-conserving):

$$
\partial_s \rho + [H_{\text{internal}}, \rho]_{\text{Poisson}} = 0,

$$
where $H_{\text{internal}}$ is the effective Hamiltonian:

$$
H_{\text{internal}}(z, p) = \frac{1}{2}\|p\|_{G^{-1}}^2 + V_{\text{critic}}(z).

$$
*Mechanism:* The agent is decoupled from the boundary (adiabatic/isolated). The Bulk evolves under Hamiltonian dynamics (BAOAB integrator with $\gamma \to 0$).

*Information-theoretic interpretation:* Isentropic ($\Delta S = 0$). Internal planning proceeds without information exchange with the environment.

:::

:::{prf:proposition} Carnot Efficiency Bound
:label: prop-carnot-efficiency-bound

The agent's efficiency in converting sensory information to control information is bounded:

$$
\eta = \frac{I(K^{\text{act}}_t; K_t)}{I(X_t; K_t)} \leq 1 - \frac{T_{\text{motor}}}{T_{\text{sensor}}},

$$
where $T_{\text{sensor}}$ and $T_{\text{motor}}$ are the effective temperatures at the sensory and motor boundaries.

*Interpretation:* Perfect efficiency ($\eta = 1$) requires $T_{\text{motor}} = 0$ (deterministic motors) or $T_{\text{sensor}} \to \infty$ (infinite sensory entropy). Real systems operate at $\eta < 1$.

**Cross-references:** {ref}`sec-adaptive-thermodynamics` (Adaptive Thermodynamics), {ref}`sec-the-equivalence-theorem` (MaxEnt Control).

*Forward reference (Reward as Heat).* {ref}`sec-the-bulk-potential-screened-poisson-equation` establishes that Reward is the thermodynamic **heat input** that drives the cycle: the Boltzmann-Value Law (Axiom {prf:ref}`ax-the-boltzmann-value-law`) identifies $V(z) = E(z) - T_c S(z)$ as Gibbs Free Energy, and Theorem {prf:ref}`thm-wfr-consistency-value-creates-mass` proves that WFR dynamics materialize the agent in high-value regions ("Value Creates Mass").

:::

:::{prf:definition} Waking: Boundary Clamping
:label: def-waking-boundary-clamping

During waking ($u_\pi \neq 0$), the sensory stream creates a high-mass source at the encoded location:

$$
\rho_{\partial}^{\text{sense}}(z, t) = \delta(z - z_{\text{obs}}(t)) \quad \text{(Dirichlet)},

$$
and the motor stream creates a flux sink:

$$
\nabla_n \rho \cdot \mathbf{n} = j_{\text{motor}}(u_\pi) \quad \text{(Neumann)}.

$$
The internal belief $\rho_{\text{bulk}}$ evolves to minimize the **WFR Geodesic Distance** to $\rho_{\partial}$:
- **Small Error** ($d_{\text{WFR}} < \lambda$): Transport dominates ($v$ term). The agent smoothly tracks the observation.
- **Large Error** ($d_{\text{WFR}} > \lambda$): Reaction dominates ($r$ term). The agent "teleports" (Surprise/Saccade) to the new reality via chart jump.

:::

:::{prf:definition} Dreaming: Reflective Boundary
:label: def-dreaming-reflective-boundary

During dreaming ($u_\pi = 0$), the sensory stream is cut. The boundary condition becomes **Reflective**:

$$
\nabla_n \rho \cdot \mathbf{n} = 0 \quad \text{(Reflective/Neumann-zero)}.

$$
The system is closed:
- Total mass is conserved: $\int_{\mathcal{Z}} \rho\, r\, d\mu_G = 0$
- Dynamics are driven purely by the internal potential $V_{\text{critic}}(z)$
- No information enters or leaves the boundary

:::

:::{prf:theorem} WFR Mode Switching
:label: thm-wfr-mode-switching

The transition from waking to dreaming corresponds to a **boundary condition phase transition**:

| Mode         | Sensory BC                             | Motor BC             | Internal Flow | Information Balance       |
|--------------|----------------------------------------|----------------------|---------------|---------------------------|
| **Waking**   | Dirichlet ($\delta$-clamp)             | Neumann (flux-clamp) | Source-driven | $\oint j_{\text{in}} > 0$ |
| **Dreaming** | Reflective ($\nabla \rho \cdot n = 0$) | Reflective           | Recirculating | $\oint j = 0$             |

:::

:::{prf:proposition} Grounding Rate via Boundary Flux
:label: prop-grounding-rate-via-boundary-flux

The grounding rate (cf. Definition 16.1.1) is:

$$
G_t = \oint_{\partial\mathcal{Z}_{\text{sense}}} j_{\text{obs}} \cdot dA - \oint_{\partial\mathcal{Z}_{\text{motor}}} j_{\text{motor}} \cdot dA,

$$
which is:
- **Positive** during waking (net information inflow from sensors)
- **Zero** during dreaming (closed system)
- **Negative** during pure actuation (net information outflow to motors)

**Cross-references:** {ref}`sec-the-wfr-metric` (WFR Action), {ref}`sec-the-unified-world-model` (WFR World Model), {ref}`sec-the-interface-and-observation-inflow` (Observation Inflow).

:::

:::{prf:definition} Context Space
:label: def-context-space

The **Context Space** $\mathcal{C}$ is a manifold parameterizing the control/conditioning signal for the agent:

$$
\mathcal{C} := \{c : c \text{ specifies a boundary condition on } \partial\mathcal{Z}\}.

$$
The context determines the target distribution at the motor boundary via the effective potential:

$$
\pi(a | z, c) \propto \exp\left(-\frac{1}{T_c} \Phi_{\text{eff}}(z, K, c)\right).

$$
Units: $[\mathcal{C}]$ inherits from the task domain.

:::

:::{prf:definition} Context Instantiation Functor
:label: def-context-instantiation-functor

The Context Space admits a functor $\mathcal{I}: \mathbf{Task} \to \mathcal{C}$ with three canonical instantiations:

| Task Domain        | Context $c \in \mathcal{C}$ | Motor Output $a$           | Effective Potential $\Phi_{\text{eff}}$      |
|--------------------|-----------------------------|----------------------------|----------------------------------------------|
| **RL**             | Action space $\mathcal{A}$  | Motor command (torques)    | $V_{\text{critic}}(z, K)$                    |
| **Classification** | Label space $\mathcal{Y}$   | Class prediction $\hat{y}$ | $-\log p(y\mid z)$ (cross-entropy)           |
| **LLM**            | Prompt space $\mathcal{P}$  | Token sequence             | $-\log p(\text{token}\mid z, \text{prompt})$ |

*Key Insight:* In all cases, the context $c$ functions as the **symmetry-breaking boundary condition** that determines which direction the holographic expansion takes at the origin.

:::

:::{prf:theorem} Universal Context Structure
:label: thm-universal-context-structure

All context instantiations share the same geometric structure:

1. **Embedding:** $c \mapsto e_c \in \mathbb{R}^{d_c}$ maps the context to a latent vector
2. **Symmetry-Breaking Kick:** $e_c$ determines the initial control field:

   $$
   u_\pi(0) = G^{-1}(0) \cdot e_c = \frac{1}{4} e_c

   $$
   (at the Poincare disk origin where $G(0) = 4I$)
3. **Motor Distribution:** The output distribution is:

   $$
   \pi(a | z, c) = \text{softmax}\left(-\frac{\Phi_{\text{eff}}(z, K, c)}{T_c}\right)

   $$
*Proof.* The holographic expansion ({ref}`sec-radial-generation-entropic-drift-and-policy-control`) is invariant to the interpretation of the control field $u_\pi$. Whether $u_\pi$ encodes "go left" (RL), "class = cat" (classification), or "continue with tone = formal" (LLM), the bulk dynamics follow the same geodesic SDE ({ref}`sec-the-equations-of-motion-geodesic-jump-diffusion`). The interpretation is purely a boundary condition. $\square$

:::

:::{prf:definition} Context-Conditioned WFR
:label: def-context-conditioned-wfr

The WFR dynamics ({ref}`sec-the-wfr-metric`) generalize to context-conditioned form:

$$
\partial_s \rho + \nabla \cdot (\rho\, v_c) = \rho\, r_c,

$$
where:
- $v_c(z) = -G^{-1}(z) \nabla_z \Phi_{\text{eff}}(z, K, c) + u_\pi(z, c)$ is the context-conditioned velocity
- $r_c(z)$ is the context-conditioned reaction rate (chart jumps influenced by context)

:::

:::{prf:corollary} Prompt = Action = Label
:label: cor-prompt-action-label

The following are isomorphic as boundary conditions on $\partial\mathcal{Z}$:

$$
\text{RL Action} \;\cong\; \text{Classification Label} \;\cong\; \text{LLM Prompt}.

$$
Each specifies:
1. **Which chart** to route to (discrete macro $K$ or $A$)
2. **Where in the chart** to aim (continuous nuisance $z_n$ or $z_{n,\text{motor}}$)
3. **What texture** to inject (visual or motor texture)

*Remark (Unified Training Objective).* This isomorphism enables transfer learning across task domains: an agent trained on RL can be fine-tuned for classification by reinterpreting the action space as label space, with the same holographic dynamics.

**Cross-references:** {ref}`sec-policy-control-field` (Control Field), Theorem {prf:ref}`thm-unified-control-interpretation`, Definition {prf:ref}`def-effective-potential`.

*Forward reference (Effective Potential Resolution).* {ref}`sec-the-bulk-potential-screened-poisson-equation`
resolves the meaning of $\Phi_{\text{eff}} = V_{\text{critic}}$: the Critic solves the **Screened Poisson Equation** to
compute the potential from boundary reward flux (scalar charges in the conservative case). The discount factor $\gamma$
determines the screening length $\ell = c_{\text{info}} \Delta t / (-\ln\gamma)$ (natural units: $1/(-\ln\gamma)$)
(Corollary {prf:ref}`cor-discount-as-screening-length`),
explaining why distant rewards are exponentially suppressed in policy.

:::

## 06_fields/02_reward_field.md

:::{prf:definition} The Reward 1-Form
:label: def-reward-1-form

Let $\mathcal{R}$ be a differential 1-form on the latent manifold $(\mathcal{Z}, G)$. The **instantaneous reward rate** received by the agent moving with velocity $v \in T_z\mathcal{Z}$ is:

$$
r_t = \langle \mathcal{R}(z), v \rangle_G = \mathcal{R}_i(z) \dot{z}^i.

$$

*Units:* $[\mathcal{R}] = \mathrm{nat}/[\text{length}]$.

The cumulative reward along a trajectory $\gamma: [0,T] \to \mathcal{Z}$ is the **line integral**:

$$
R_{\text{cumulative}} = \int_\gamma \mathcal{R} = \int_0^T \mathcal{R}_i(\gamma(t)) \dot{\gamma}^i(t) \, dt.

$$

*Remark.* Instantaneous reward depends on both position $z$ and velocity $\dot{z}$. A stationary agent ($\dot{z} = 0$) receives zero instantaneous reward.

:::

:::{prf:definition} The Reward Flux (Boundary Form)
:label: def-the-reward-flux

The environment provides reward via a boundary 1-form $J_r$ on $\partial\Omega$. Let
$\iota:\partial\Omega\hookrightarrow\Omega$ be the inclusion. The boundary condition is the pullback
$\iota^*\mathcal{R} = J_r$, and the cumulative boundary reward along a boundary trajectory
$\gamma_\partial$ is:

$$
\int_{\gamma_\partial} J_r = \text{Cumulative Boundary Reward}.

$$
In the discrete limit, this manifests as samples $r_t = J_r(\partial_t)$ deposited at the boundary
coordinates $(t, z_{\text{boundary}})$.

*Units:* $[J_r] = \mathrm{nat}/[\text{length}]$, $[r_t] = \mathrm{nat}$.

*Relation to 1-form:* For any surface $\Sigma$ with boundary $\partial\Sigma$, Stokes' theorem gives
$\oint_{\partial\Sigma}\mathcal{R}=\int_\Sigma d\mathcal{R}$. In the conservative case
($\mathcal{R}=d\Phi$), boundary reward reduces to Dirichlet/Neumann data for $\Phi$ (equivalently a
boundary source density $\sigma_r$).

:::

:::{prf:definition} Terminal Boundary (End/Death Flags)
:label: def-terminal-boundary

Let $\Gamma_{\text{term}} \subset \mathcal{Z}$ denote the terminal subset representing end/death flags.
Define the stopping time $\tau_{\text{term}} := \inf\{t \ge 0 : z_t \in \Gamma_{\text{term}}\}$ and
kill the process upon hitting $\Gamma_{\text{term}}$. For the conservative value PDE, impose a
Dirichlet condition $V|_{\Gamma_{\text{term}}} = V_{\text{term}}$ (often $0$ or a terminal payoff).
In WFR form, include a killing rate $\kappa_{\text{term}}(z) \ge 0$ or a reaction term $r<0$
concentrated on $\Gamma_{\text{term}}$.

*Terminal vs holographic boundary.* $\Gamma_{\text{term}}$ is a task boundary and is separate from the
holographic boundary of the hyperbolic space.

*Computational cutoff.* For numerics we truncate the hyperbolic disk at $\lvert z\rvert = 1-\varepsilon$,
with $\varepsilon$ tied to the Levin length/resolution. This is a computational boundary for stability,
not a physical terminal set.

:::

:::{prf:theorem} Hodge Decomposition of the Reward Field
:label: thm-hodge-decomposition

On a compact latent Riemannian manifold $(\mathcal{Z}, G)$ with boundary (or on a complete manifold
with suitable decay and boundary conditions), the Reward 1-form $\mathcal{R}$ decomposes into:

$$
\mathcal{R} = \underbrace{d\Phi}_{\text{Gradient}} + \underbrace{\delta \Psi}_{\text{Solenoidal}} + \underbrace{\eta}_{\text{Harmonic}}

$$
where:
1. **$\Phi \in \Omega^0(\mathcal{Z})$** (Scalar Potential): The conservative/optimizable component. $d\Phi$ is an exact form.
2. **$\Psi \in \Omega^2(\mathcal{Z})$** (Vector Potential): The rotational/cyclic component. $\delta\Psi$ is a coexact form (divergence-free).
3. **$\eta \in \mathcal{H}^1(\mathcal{Z})$** (Harmonic Flux): Topological cycles from manifold holes. Satisfies $d\eta = 0$ and $\delta\eta = 0$.

We identify $\Phi$ with the critic value $V$ (the exact component), so $d\Phi = dV$; the conservative case corresponds
to $A=0$.
Define the non-exact component $A := \delta\Psi + \eta$, so $\mathcal{R} = d\Phi + A$ and $\mathcal{F} = dA$.

*Units:* $[\Phi] = \mathrm{nat}$, $[\Psi] = \mathrm{nat}$, $[\eta] = \mathrm{nat}/[\text{length}]$.

*Proof sketch.* The Hodge decomposition follows from the orthogonal decomposition of $L^2(\Omega^1)$
into exact, coexact, and harmonic forms (with absolute/relative boundary conditions fixed when
$\partial\mathcal{Z}\neq\varnothing$). The Hodge Laplacian $\Delta_H = d\delta + \delta d$ has kernel
equal to the harmonic forms. The explicit solution uses the Green's operator
$G = (\Delta_H)^{-1}$ on the orthogonal complement of harmonic forms:
$\Phi = \delta G \mathcal{R}$, $\Psi = d G \mathcal{R}$,
$\eta = \mathcal{R} - d\Phi - \delta\Psi$. $\square$

:::

:::{prf:definition} The Value Curl (Vorticity Tensor)
:label: def-value-curl

The **Value Curl** is the exterior derivative of the reward form:

$$
\mathcal{F} := d\mathcal{R} = dA = d\delta\Psi.

$$
In coordinates: $\mathcal{F}_{ij} = \partial_i \mathcal{R}_j - \partial_j \mathcal{R}_i$.

*Units:* $[\mathcal{F}] = \mathrm{nat}/[\text{length}]^2$ (curvature of value).

**Properties:**
1. $\mathcal{F}$ is antisymmetric: $\mathcal{F}_{ij} = -\mathcal{F}_{ji}$
2. $\mathcal{F}$ satisfies the Bianchi identity: $d\mathcal{F} = 0$
3. $\mathcal{F}$ is gauge-invariant: if $\mathcal{R} \to \mathcal{R} + d\chi$, then $\mathcal{F} \to \mathcal{F}$

:::

:::{prf:definition} Conservative Reward Field
:label: def-conservative-reward-field

The reward field $\mathcal{R}$ is **conservative** if and only if:

$$
\mathcal{F} = d\mathcal{R} = 0 \quad \text{(curl-free)}.

$$
Equivalently, $\mathcal{R} = d\Phi$ for some scalar potential $\Phi$ (the solenoidal and harmonic components vanish).

**Conservative Special Case:** When $\mathcal{F} = 0$ everywhere, we recover standard scalar value functions with $V(z) = \Phi(z)$. The cumulative reward around any closed loop vanishes:

$$
\oint_\gamma \mathcal{R} = \int_\Sigma d\mathcal{R} = \int_\Sigma \mathcal{F} = 0.

$$

*Remark.* Standard RL assumes conservative reward fields. The scalar value function $V(s)$ exists precisely because path-independence holds.

:::

:::{prf:proposition} Value Cycle Detection
:label: prop-value-cycle-detection

The Value Curl $\mathcal{F}$ can be estimated from trajectory data. For a closed loop $\gamma$ in latent space:

$$
\oint_\gamma \mathcal{R} = \int_\Sigma \mathcal{F} \, d\Sigma \neq 0 \implies \text{Non-conservative rewards.}

$$
**Diagnostic:** Non-zero circulation $\oint_\gamma \mathcal{R}$ indicates non-conservative structure.
Using accumulated TD-error around closed loops is a heuristic that requires approximate loop closure
and a consistent reward estimator.

:::

:::{prf:theorem} The HJB-Helmholtz Correspondence {cite}`bellman1957dynamic,evans2010pde`
:label: thm-the-hjb-helmholtz-correspondence

Let the temporal discount rate be $\lambda := -\ln\gamma / \Delta t$ and define the **spatial screening mass** $\kappa := \lambda / c_{\text{info}}$ (so $\gamma = e^{-\lambda \Delta t}$). The Bellman condition

$$
V(z) = \mathbb{E}[r + \gamma V(z')]

$$
approaches the following PDE in the limit $\Delta t \to 0$:

$$
\boxed{-\Delta_G V(z) + \kappa^2 V(z) = \rho_r(z)}

$$
where:
- $\Delta_G = \frac{1}{\sqrt{|G|}} \partial_i \left( \sqrt{|G|} G^{ij} \partial_j \right)$ is the **Laplace-Beltrami operator** on the manifold $(\mathcal{Z}, G)$
- $\kappa^2$ is the "mass" of the scalar field, causing the influence of distant rewards to decay exponentially
- $\rho_r(z)$ is the scalar source density associated with the conservative component of $\mathcal{R}$
  (bulk density plus boundary flux data; see Definition {prf:ref}`def-the-reward-flux`)

*Proof sketch.* Consider the continuous-time limit of the Bellman equation for a diffusion process
$dz = b(z) dt + \sigma(z) dW$ with $\sigma\sigma^T = 2T_c G^{-1}$. Expanding
$V(z') = V(z + dz)$ to second order and taking expectations, with instantaneous reward rate
$r := \mathcal{R}_i(z) b^i(z,a)$:

$$
V(z) = r \Delta t + \gamma \mathbb{E}[V(z')] \approx r \Delta t + (1 - \kappa \Delta t)\left(V + \nabla_A V \cdot b \Delta t + T_c \Delta_G V \Delta t\right).

$$
Rearranging and dividing by $\Delta t$, then taking $\Delta t \to 0$:

$$
\kappa V = r + \nabla_A V \cdot b + T_c \Delta_G V.
Here $\nabla_A V := \nabla V - A$ with $A := \delta\Psi + \eta$ the non-conservative component of $\mathcal{R}$
(conservative case: $A=0$).

$$
For the stationary case ($b = 0$) and absorbing the temperature into the source term, this yields the Helmholtz equation $-\Delta_G V + \kappa^2 V = \rho_r$. Details in {ref}`sec-appendix-a-full-derivations`. $\square$

Units: $[\kappa] = 1/\text{length}$, $[\Delta_G V] = \mathrm{nat}/\text{length}^2$, $[\rho_r] = \mathrm{nat}/\text{length}^2$.

*Cross-reference (Relativistic Extension):* This **elliptic** Helmholtz equation assumes instantaneous value propagation. When agents interact across spatial or computational separation with finite information speed $c_{\text{info}}$, the equation generalizes to the **hyperbolic Klein-Gordon equation**: $(\frac{1}{c^2}\partial_t^2 - \Delta_G + \kappa^2)V = \rho_r$. See Theorem {prf:ref}`thm-hjb-klein-gordon` in {ref}`sec-the-hyperbolic-value-equation`.

*Cross-reference (Gauge-Covariant Generalization):* When dynamics must be invariant under local nuisance
transformations ({ref}`sec-local-gauge-symmetry-nuisance-bundle`), covariant derivatives
act on vector-valued belief fields (or nuisance orientation multiplets) rather than on the scalar
value $V$. Only if $V$ is chosen to transform in a non-trivial representation does the Helmholtz
operator become $-D_\mu D^\mu + \kappa^2$.

:::

:::{prf:remark} Dimensional Consistency of the Helmholtz Equation
:label: rem-helmholtz-dimensions

The screened Poisson equation $-\Delta_G V + \kappa^2 V = \rho_r$ requires careful dimensional analysis. The naive expression $\kappa = -\ln\gamma$ appears dimensionless, which would be inconsistent with $[\Delta_G] = [\text{length}]^{-2}$.

The resolution is to separate temporal and spatial scales. Define the temporal discount rate $\lambda := -\ln\gamma / \Delta t$ (units $1/[\text{time}]$), then convert to the spatial screening mass $\kappa := \lambda / c_{\text{info}}$ (units $1/[\text{length}]$). This makes $\kappa^2$ commensurate with $[\Delta_G] = [\text{length}]^{-2}$.

**In natural units** (used throughout this document): We set $\Delta t = 1$ and $c_{\text{info}} = 1$, making $\kappa = -\ln\gamma$ numerically equal to the screening mass.

**In SI units**: The proper relationship is:

$$
\kappa_{\text{phys}} = \frac{-\ln\gamma}{c_{\text{info}} \Delta t}, \qquad [\kappa_{\text{phys}}] = \frac{1}{\text{length}}

$$

The screening length $\ell_{\text{screen}} = 1/\kappa$ thus depends on both the temporal horizon ($\gamma$) and the information propagation speed $c_{\text{info}}$. Slower propagation (smaller $c_{\text{info}}$) shortens the effective horizon in latent space.

:::

:::{prf:proposition} Green's Function Interpretation
:label: prop-green-s-function-interpretation

The Critic computes the **Green's function** of the screened Laplacian on the latent geometry:

$$
V(z) = \int_{\Omega} G_\kappa(z, z') \rho_r(z') \, d\mu_G(z') + \mathcal{B}_{\partial\Omega}[G_\kappa, V],

$$
where $G_\kappa(z, z')$ is the Green's function satisfying
$(-\Delta_G + \kappa^2) G_\kappa(z, \cdot) = \delta_z$, and $\mathcal{B}_{\partial\Omega}$ encodes the
chosen boundary condition (Dirichlet/Neumann). In the pure boundary-source case with density
$\sigma_r$:

$$
V(z) = \int_{\partial\Omega} G_\kappa(z, z') \sigma_r(z') \, d\Sigma(z').

$$

*Remark.* The value at $z$ is a weighted integral of bulk sources plus boundary flux, with weights
given by the Green's function. This is a superposition principle: the Helmholtz equation is linear.

:::

:::{prf:proposition} Green's Function Decay
:label: prop-green-s-function-decay

On a manifold with bounded curvature, the Green's function decays exponentially:

$$
G_\kappa(z, z') \sim \frac{1}{d_G(z, z')^{(d-1)/2}} \exp\left(-\kappa \cdot d_G(z, z')\right),

$$
where $d_G$ is the geodesic distance and $d$ is the dimension.

:::

:::{prf:corollary} Discount as Screening Length
:label: cor-discount-as-screening-length

The discount factor $\gamma$ determines a characteristic **screening length**:

$$
\ell_{\text{screen}} = \frac{1}{\kappa} = \frac{c_{\text{info}} \Delta t}{-\ln\gamma} = \frac{c_{\text{info}}}{\lambda}.

$$
where $\lambda := -\ln\gamma / \Delta t$.
For $\gamma = 0.99$ and $c_{\text{info}} \Delta t = 1$: $\ell_{\text{screen}} \approx 100$ steps.

*Interpretation:* Rewards at geodesic distance $> \ell_{\text{screen}}$ from state $z$ are exponentially suppressed in their contribution to $V(z)$. This is the **temporal horizon** recast as a **spatial horizon** in latent space.

*Note:* Numerical values below assume natural units ($c_{\text{info}} \Delta t = 1$).

**Table 24.2.5 (Discount-Screening Correspondence).**

| Discount $\gamma$ | Screening Mass $\kappa$ | Screening Length $\ell$ | Interpretation                    |
|-------------------|-------------------------|-------------------------|-----------------------------------|
| $\gamma \to 1$    | $\kappa \to 0$          | $\ell \to \infty$       | Infinite horizon (massless field) |
| $\gamma = 0.99$   | $\kappa \approx 0.01$   | $\ell \approx 100$      | Standard RL                       |
| $\gamma = 0.9$    | $\kappa \approx 0.1$    | $\ell \approx 10$       | Short horizon                     |
| $\gamma \to 0$    | $\kappa \to \infty$     | $\ell \to 0$            | Myopic (infinitely massive)       |

**Cross-references:** {ref}`sec-the-hjb-correspondence` (HJB Equation), Theorem {prf:ref}`thm-capacity-constrained-metric-law`.

:::

:::{prf:axiom} The Generalized Boltzmann-Value Law
:label: ax-the-boltzmann-value-law

The scalar potential $\Phi(z)$ from the Hodge decomposition (Theorem {prf:ref}`thm-hodge-decomposition`) represents the **Gibbs Free Energy** of the state $z$:

$$
\Phi(z) = E(z) - T_c S(z),

$$
where:
- $E(z)$ is the **task risk/cost** at state $z$
- $S(z)$ is the **exploration entropy** (measure of uncertainty/optionality)
- $T_c$ is the **cognitive temperature** ({prf:ref}`def-cognitive-temperature`, {ref}`sec-hyperbolic-volume-and-entropic-drift`)

*Units:* $[\Phi] = [E] = [T_c S] = \mathrm{nat}$.

**Conservative Case ($\mathcal{F} = 0$):** When the Value Curl vanishes, $\mathcal{R} = d\Phi$ and the scalar potential $\Phi$ is the complete value function $V(z)$.

**Non-Conservative Case ($\mathcal{F} \neq 0$):** The scalar potential $\Phi$ captures only the optimizable component of the reward field. The solenoidal component $\delta\Psi$ creates additional cyclic dynamics.

:::

:::{prf:definition} Canonical Ensemble {cite}`sutton2018rl`
:label: def-canonical-ensemble

This potential induces a probability measure on the manifold via the **Canonical Ensemble**:

$$
P_{\text{stationary}}(z) = \frac{1}{Z} \exp\left(\frac{V(z)}{T_c}\right),

$$
where $Z = \int_{\mathcal{Z}} \exp(V(z)/T_c) \, d\mu_G(z)$ is the partition function.

*Sign Convention:* If $V$ is "Reward" (higher is better), use $+V/T_c$. If $V$ is "Cost" (lower is better), use $-V/T_c$. Throughout this document we use the **Reward convention** unless otherwise noted.

:::

:::{prf:theorem} WFR Consistency: Value Creates Mass
:label: thm-wfr-consistency-value-creates-mass

In the WFR dynamics ({prf:ref}`def-the-wfr-action`, {ref}`sec-wasserstein-fisher-rao-geometry-unified-transport-on-hybrid-state-spaces`), the reaction rate $r(z)$ in the unbalanced continuity equation is determined by the value function:

$$
r(z) = \frac{1}{s_r} \left( V(z) - \bar{V} \right),

$$
where $\bar{V} = \mathbb{E}_\rho[V]$ is the mean value and $s_r$ is the reaction time scale (computation time).

*Consequence:* The mass evolution satisfies:

$$
\dot{m}(s) = m(s) \cdot r(z(s)) \propto m(s) \cdot (V(z(s)) - \bar{V}).

$$

Probability density increases in regions where $V > \bar{V}$ and decreases where $V < \bar{V}$.

*Proof.* The WFR optimal reaction rate minimizes $\int \lambda^2 r^2 \, d\rho$ subject to the constraint that the endpoint marginals match. The solution is $r \propto (V - \bar{V})$, where $V$ appears because it determines the target stationary distribution. $\square$

:::

:::{prf:corollary} Conservative Equilibrium Distribution
:label: cor-equilibrium-distribution

**Conservative Case ($\mathcal{F} = 0$):** At equilibrium ($\partial_s \rho = 0$), the WFR dynamics with reaction rate $r(z) \propto (\Phi(z) - \bar{\Phi})$ converge to the Boltzmann distribution:

$$
\rho_\infty(z) \propto \exp\left(\frac{\Phi(z)}{T_c}\right),

$$
which is exactly the canonical ensemble (Definition {prf:ref}`def-canonical-ensemble`).

*Remark.* In the conservative case, the stationary distribution has zero probability current ($J = 0$). The distribution concentrates in high-$\Phi$ regions with concentration controlled by $T_c$.

:::

:::{prf:theorem} Non-Equilibrium Steady State (NESS)
:label: thm-ness-existence

**Non-Conservative Case ($\mathcal{F} \neq 0$):** If the Value Curl does not vanish, the stationary distribution $\rho_\infty$ is a **Non-Equilibrium Steady State** satisfying:

1. **Stationarity:** $\partial_s \rho_\infty = 0$
2. **Persistent Current:** The probability current $J = \rho v - D\nabla\rho$ is non-zero and divergence-free: $\nabla \cdot J = 0$ but $J \neq 0$
3. **Entropy Production:** The system continually produces entropy at rate:

$$
\dot{S}_i = \int_{\mathcal{Z}} \frac{\|J\|_G^2}{\rho D} \, d\mu_G > 0

$$

*Remark.* The probability density $\rho_\infty$ is time-independent, but individual trajectories circulate indefinitely. This distinguishes NESS from true equilibrium (where $J = 0$).

:::

:::{prf:proposition} NESS Decomposition
:label: prop-ness-decomposition

The probability current in a NESS decomposes into:

$$
J = J_{\text{gradient}} + J_{\text{cyclic}}

$$
where:
- $J_{\text{gradient}} = -D\rho\nabla\ln\rho + \rho\nabla\Phi$ derives from the scalar potential
- $J_{\text{cyclic}} = \rho \cdot v_{\text{curl}}$ derives from the solenoidal component

At stationarity, $\nabla \cdot J = 0$, but only $J_{\text{gradient}} = 0$ at true equilibrium. NESS has $J_{\text{cyclic}} \neq 0$.

:::

:::{prf:corollary} The Varentropy-Stability Relation (Cognitive Heat Capacity)
:label: cor-varentropy-stability

Let $\mathcal{I}(a|z) = -\ln \pi(a|z)$ be the surprisal of an action. Define the **Policy Varentropy** $V_H(z)$ as the variance of the surprisal under the Boltzmann policy:

$$
V_H(z) := \mathrm{Var}_{a \sim \pi}[\mathcal{I}(a|z)] = \mathbb{E}_{\pi}\left[ \left( \ln \pi(a|z) + H(\pi) \right)^2 \right].

$$
*Units:* $\mathrm{nat}^2$.

Under the Boltzmann-Value Law (Axiom {prf:ref}`ax-the-boltzmann-value-law`), the Varentropy equals the **Heat Capacity** $C_v$ of the decision state:

$$
V_H(z) = \beta_{\text{ent}}^2 \mathrm{Var}_\pi[Q] = C_v,

$$
where $\beta_{\text{ent}} = 1/T_c$ is the inverse cognitive temperature. Equivalently:

$$
V_H(z) = T_c \frac{\partial H(\pi)}{\partial T_c}.

$$
**Operational Consequence:**
1. **Thermal Stability:** $V_H$ measures the sensitivity of the agent's exploration strategy to changes in the cognitive temperature $T_c$.
2. **Phase Transitions:** A divergence or spike in $V_H$ signals a second-order phase transition (critical point) where the policy is bifurcating from a single mode to multiple modes (or collapsing).
3. **Governor Constraint:** To ensure quasi-static evolution (reversible learning), the annealing rate $\dot{T}_c$ must satisfy the adiabatic condition:

$$
|\dot{T}_c| \ll \frac{T_c}{\sqrt{V_H(z)}}.

$$
*Proof:* See Appendix {ref}`E.8 <sec-appendix-e-proof-of-corollary-varentropy-stability>`.

:::

:::{prf:definition} Value-Metric Conformal Coupling
:label: def-value-metric-conformal-coupling

We model the effect of Value on the Metric $G$ as a **Conformal Transformation**:

$$
\tilde{G}_{ij}(z) = \Omega^2(z) \cdot G_{ij}(z),

$$
where the conformal factor $\Omega(z)$ depends on the **Hessian of the Value**:

$$
\Omega(z) = 1 + \alpha_{\text{conf}} \cdot \|\nabla^2_G V(z)\|_{\text{op}},

$$
with $\alpha_{\text{conf}} \ge 0$ the conformal coupling strength and $\|\cdot\|_{\text{op}}$ the operator norm.

Units: $[\Omega] = 1$ (dimensionless), $[\alpha_{\text{conf}}] = \text{length}^2/\mathrm{nat}$.

:::

:::{prf:proposition} Risk-Curvature Mechanism
:label: prop-risk-curvature-mechanism

The conformal factor encodes the local "importance" of the value landscape:

| Value Landscape           | $\lVert\nabla^2 V\rVert$ | $\Omega$    | Effect                           |
|---------------------------|--------------------------|-------------|----------------------------------|
| **Flat** (low importance) | $\approx 0$              | $\approx 1$ | Default hyperbolic bulk geometry |
| **Curved** (ridge/valley) | $\gg 0$                  | $\gg 1$     | Distances expand, mass increases |
| **Saddle** (transition)   | moderate                 | $> 1$       | Intermediate slowdown            |

:::

:::{prf:corollary} Inertia at Critical Regions
:label: cor-inertia-at-critical-regions

Near sharp ridges or valleys of $V$ (where $\|\nabla^2 V\|$ is large), the conformal factor causes:

1. **Inertia Increase:** The effective mass $\tilde{G}(z) = \Omega^2(z) G(z)$ increases, so the agent slows down near critical decision boundaries ({ref}`sec-the-coupled-jump-diffusion-sde` mass scaling).

2. **Resolution Increase:** The capacity-constrained metric allocates more volume to high-curvature regions (Theorem {prf:ref}`thm-capacity-constrained-metric-law`), allowing higher-fidelity representation of value gradients.

3. **Stability:** The agent cannot "rush through" regions of high value curvature—it is forced to carefully navigate decision boundaries.

*Remark (Physical analogy).* The conformal scaling of effective velocity is mathematically analogous to gravitational time dilation in general relativity, where proper time dilates in regions of high gravitational potential.

:::

:::{prf:proposition} Conformal Laplacian Transformation
:label: prop-conformal-laplacian-transformation

Under the conformal transformation $G \to \tilde{G} = \Omega^2 G$, the Laplace-Beltrami operator acting on a scalar function $f$ transforms as:

$$
\Delta_{\tilde{G}} f = \Omega^{-2} \left( \Delta_G f + (d-2) \frac{G^{ij} \partial_i \Omega}{\Omega} \partial_j f \right),

$$
where $d$ is the dimension. For the Value function $V$ itself (which determines $\Omega$), this creates a **nonlinear coupling**. The screened Poisson equation (Theorem {prf:ref}`thm-the-hjb-helmholtz-correspondence`) in the conformally modified metric becomes:

$$
-\Delta_{\tilde{G}} V + \tilde{\kappa}^2 V = \tilde{\rho}_r,

$$
with effective screening mass $\tilde{\kappa}^2 = \Omega^{-2} \kappa^2$.

*Remark (Self-Consistency).* Since $\Omega$ depends on $\nabla^2 V$, the equation becomes nonlinear: the geometry adapts to the value landscape which in turn affects the geometry. In practice, we solve this iteratively or treat $\Omega$ as slowly-varying.

*Interpretation:* In high-curvature regions ($\Omega$ large), the effective screening mass decreases, making the field more "massless" and allowing longer-range correlations. This is the **self-focusing** effect: important regions become more interconnected.

**Cross-references:** Theorem {prf:ref}`thm-capacity-constrained-metric-law`, {ref}`sec-the-stochastic-action-principle` (Mass=Metric), Proposition {prf:ref}`prop-mass-scaling-near-boundary`.

:::

:::{prf:theorem} RL as Electrodynamics on a Curved Manifold
:label: thm-rl-as-electrodynamics-on-a-curved-manifold

The complete agent dynamics can be summarized as follows:

The agent is a **particle** with:
- **Position** $q \in \mathcal{Z}$ (from Perception / Dirichlet BC)
- **Momentum** $p \in T_q\mathcal{Z}$ (from Action / Neumann BC)
- **Mass** $G(q)$ (the Riemannian metric = information geometry)
- **Potential Energy** $V(q)$ (from Reward / Poisson source)
- **External Forces** $u_\pi(q)$ (from Policy / symmetry-breaking kick)

moving according to the **geodesic SDE** (Definition {prf:ref}`def-bulk-drift-continuous-flow`):

$$
dq^k = G^{kj}(q) p_j \, ds, \qquad
dp_k = -\frac{\partial V}{\partial q^k} ds - \frac{1}{2}\frac{\partial G^{ij}}{\partial q^k} p_i p_j \, ds + u_{\pi,k} \, ds + \sqrt{2T_c} \, (G^{1/2})_{kj} dW^j_s,

$$
on a **curved manifold** with metric $G$ satisfying the **capacity constraint** (Theorem {prf:ref}`thm-capacity-constrained-metric-law`), in a **screened potential** $V$ satisfying the **Helmholtz equation** (Theorem {prf:ref}`thm-the-hjb-helmholtz-correspondence`). The term $\frac{1}{2}\frac{\partial G^{ij}}{\partial q^k} p_i p_j$ encodes the geodesic correction (equivalent to Christoffel symbols in the position formulation).

*Conclusion:* **Reinforcement Learning is Electrodynamics on a Curved Manifold.** The standard RL components (encoder, critic, policy) are revealed to be the components of a field theory:

| RL Component      | Field Theory Role                                         |
|-------------------|-----------------------------------------------------------|
| Encoder           | **Coordinate Chart** (embedding from boundary to bulk)    |
| Critic            | **Field Solver** (Green's function of screened Laplacian) |
| Policy            | **External Force** (symmetry-breaking current)            |
| Discount $\gamma$ | **Screening Mass** (controls correlation length)          |
| Temperature $T_c$ | **Thermal Bath** (fluctuation-dissipation source)         |

:::

:::{prf:corollary} The Three Boundary Conditions
:label: cor-the-three-boundary-conditions

The agent-environment interface decomposes into exactly three types of boundary conditions:

1. **Dirichlet** (Sensors): Clamp position $q = q_{\text{obs}}$. Information flows **in**.
2. **Neumann** (Motors): Clamp flux $\nabla_n \cdot p = j_{\text{motor}}$. Information flows **out**.
3. **Source** (Rewards): Inject charge $\sigma_r$ at boundary. Creates **potential field**.

These three conditions fully specify the agent's interaction with its environment.

**Cross-references:** {ref}`sec-the-boundary-interface-symplectic-structure` (Holographic Interface), {ref}`sec-the-equations-of-motion-geodesic-jump-diffusion` (Equations of Motion), {ref}`sec-capacity-constrained-metric-law-geometry-from-interface-limits` (Capacity-Constrained Geometry).

:::

## 06_fields/03_info_bound.md

:::{prf:definition} Holographic Coefficient
:label: def-holographic-coefficient

The **Holographic Coefficient** $\nu_D$ for a $D$-dimensional latent manifold with $(D-1)$-sphere boundary is:

$$
\nu_D := \frac{(D-1)\,\Omega_{D-1}}{8\pi}

$$

where $\Omega_{D-1} = \frac{2\pi^{D/2}}{\Gamma(D/2)}$ is the surface area of the unit $(D-1)$-sphere.

| $D$ | Boundary | $\Omega_{D-1}$ | $\nu_D$ | Numerical |
|-----|----------|----------------|---------|-----------|
| 2   | Circle ($S^1$) | $2\pi$ | $1/4$ | 0.250 |
| 3   | Sphere ($S^2$) | $4\pi$ | $1$ | 1.000 |
| 4   | Glome ($S^3$) | $2\pi^2$ | $3\pi/4$ | 2.356 |
| 5   | 4-sphere ($S^4$) | $8\pi^2/3$ | $4\pi/3$ | 4.189 |
| 6   | 5-sphere ($S^5$) | $\pi^3$ | $5\pi^2/8$ | 6.169 |
| $D \gg 1$ | Hyper-sphere | $\to 0$ | $\to 0$ | Capacity collapse |

*Remark (Dimensional pressure).* The coefficient $\nu_D$ is non-monotonic: it increases from $D=2$ to a peak near $D \approx 9$ ($\nu_9 \approx 9.45$), then decays to zero as $D \to \infty$. The curse of dimensionality applies to this high-dimensional tail. Dimensional reduction pressure arises beyond the peak; $D \approx 3$ lies on the rising portion of the curve.

*Remark (Physics correspondence).* For $D=2$, we recover the Bekenstein-Hawking coefficient $\nu_2 = 1/4$, making the Causal Information Bound $I_{\max} = \text{Area}/(4\ell_L)$ directly analogous to black hole entropy $S = A/(4\ell_P^2)$.

*Units:* $[\nu_D] = \text{dimensionless}$.

:::

:::{prf:definition} Levin Length
:label: def-levin-length

Let $\eta_\ell$ be the boundary area-per-nat at resolution $\ell$ (Definition {prf:ref}`def-boundary-capacity-area-law-at-finite-resolution`). The **Levin Length** $\ell_L$ is the characteristic length scale of a single unit of distinction:

$$
\ell_L := \sqrt{\eta_\ell}.

$$
Units: $[\ell_L] = [z]$ (latent coordinate length).

*Interpretation.* A cell of area $\ell_L^2$ in the latent manifold corresponds to one nat of information capacity. The Levin Length is the information-geometric analog of a minimal resolvable element—the "pixel size" of the agent's internal representation.

*Remark (Naming).* The name honors Leonid Levin's foundational work on algorithmic information theory and the universal distribution {cite}`levin1973universal`. The Levin Length represents the floor below which distinctions cannot be computationally meaningful.

:::

:::{prf:definition} Saturation Limit
:label: def-saturation-limit

The agent is at the **Saturation Limit** when the bulk information volume (Definition {prf:ref}`def-a-bulk-information-volume`) equals the boundary capacity (Definition {prf:ref}`def-dpi-boundary-capacity-constraint`):

$$
I_{\text{bulk}} = C_\partial.

$$
At this limit, the DPI constraint $I_{\text{bulk}} \le C_\partial$ is satisfied with equality.

:::

:::{prf:lemma} Metric Divergence at Saturation
:label: lem-metric-divergence-at-saturation

Consider an isotropic latent space of dimension $n \ge 3$ with polar coordinates $(r, \Omega)$. At saturation with uniform stress $T_{ij} = \sigma_{\max} G_{ij}$, the radial metric component $G_{rr} = A(r)$ satisfies the Metric Law (Theorem {prf:ref}`thm-capacity-constrained-metric-law`) and takes the form:

$$
A(r) = \left( 1 - \frac{2\mu(r)}{(n-2)r^{n-2}} - \frac{\Lambda_{\text{eff}} r^2}{n(n-1)} \right)^{-1},

$$
where $\mu(r) := \frac{\kappa}{n-2} \int_0^r \sigma_{\max} r'^{n-1} dr'$ is the integrated **information mass** (with $\kappa$ the coupling constant from the Metric Law) and $\Lambda_{\text{eff}} = \Lambda + \kappa\sigma_{\max}$.

*Remark ($n=2$ case).* For $n=2$ (the Poincare disk), the $(n-2)$ factor vanishes and the solution requires separate
treatment. The Poincare metric $G_{ij} = 4\delta_{ij}/(1-|z|^2)^2$ is the correctly regularized saturation geometry,
with the horizon at $|z|=1$. In computation we truncate at $\lvert z\rvert = 1-\varepsilon$ with $\varepsilon$ tied to
Levin length/resolution; this is a numerical cutoff distinct from any terminal subset $\Gamma_{\text{term}}$.

*Proof sketch.* Substitute the uniform density into the Metric Law. The spherically symmetric solution follows from standard analysis of Einstein-like field equations {cite}`wald1984general`. Full derivation in {ref}`sec-appendix-a-full-derivations`. $\square$

*Critical observation.* The metric component $A(r)$ diverges at the horizon radius $r_h$ satisfying:

$$
1 - \frac{2\mu(r_h)}{(n-2)r_h^{n-2}} - \frac{\Lambda_{\text{eff}} r_h^2}{n(n-1)} = 0.

$$
At this radius, $G_{rr} \to \infty$ and consequently $G^{rr} \to 0$.

:::

:::{prf:theorem} The Causal Information Bound
:label: thm-causal-information-bound

For a $D$-dimensional latent manifold $(\mathcal{Z}, G)$, the maximum information $I_{\max}$ that can be stably represented without the metric becoming singular is:

$$
\boxed{I_{\max} = \nu_D \cdot \frac{\text{Area}(\partial\mathcal{Z})}{\ell_L^{D-1}}}

$$

where:
- $\text{Area}(\partial\mathcal{Z}) = \oint_{\partial\mathcal{Z}} dA_G$ is the $(D-1)$-dimensional boundary measure in the induced metric
- $\ell_L$ is the Levin Length (Definition {prf:ref}`def-levin-length`)
- $\nu_D$ is the Holographic Coefficient (Definition {prf:ref}`def-holographic-coefficient`)

*Corollary (Poincare disk, $D=2$).* For the 2-dimensional Poincare disk, the formula reduces to the Bekenstein-Hawking form:

$$
I_{\max} = \nu_2 \cdot \frac{\text{Area}(\partial\mathcal{Z})}{\ell_L^{D-1}} = \frac{\text{Area}(\partial\mathcal{Z})}{4\ell_L}

$$
where $\nu_2 = 1/4$ is the holographic coefficient for $D=2$ and $\ell_L^{D-1} = \ell_L$ for the 1-dimensional boundary.

*Proof sketch (full derivation in {ref}`sec-appendix-a-area-law`).*

**Step 1 (Holographic Reduction).** The bulk-to-boundary conversion relies on the Einstein tensor divergence identity (valid in arbitrary dimension): integrating the scalar curvature over a compact manifold with boundary yields a boundary term involving the extrinsic curvature. Applying this to the Metric Law (Theorem {prf:ref}`thm-capacity-constrained-metric-law`) via Lemma {prf:ref}`lem-a-divergence-to-boundary-conversion`:

$$
I_{\text{bulk}} = \int_{\mathcal{Z}} \rho_I \, d\mu_G = \frac{1}{\kappa} \oint_{\partial\mathcal{Z}} \text{Tr}(K) \, dA_G,

$$
where $K$ is the extrinsic curvature of the boundary and $\kappa$ is the coupling constant from the Metric Law.

**Step 2 (Saturation Geometry).** At the saturation limit, the extrinsic curvature approaches $\text{Tr}(K) \to (D-1)/r_h$ where $r_h$ is the horizon radius from Lemma {prf:ref}`lem-metric-divergence-at-saturation`. The boundary area is $\text{Area}(\partial\mathcal{Z}) = \Omega_{D-1} r_h^{D-1}$ where $\Omega_{D-1}$ is the unit sphere surface area.

**Step 3 (Fisher Normalization).** The coupling constant $\kappa = 8\pi\ell_L^{D-1}$ is fixed by consistency with the Fisher Information Metric {cite}`amari2016information`. The dimension-dependent coefficient $\nu_D = (D-1)\Omega_{D-1}/(8\pi)$ emerges from the geometric factors.

Combining these steps yields the general bound. $\square$

*Operational interpretation.* The agent's "intelligence" (measured in grounded bits) is geometrically constrained by the size of its interface. To represent more information, you must either:
1. **Expand the boundary** (increase interface bandwidth), or
2. **Reduce the Levin Length** (improve resolution per unit area).

There is no third option. Adding internal parameters without expanding the interface yields diminishing returns as the agent approaches saturation.

*Remark (Dimensional efficiency).* The coefficient $\nu_D$ increases through moderate dimensions, peaks near $D \approx 9$, and declines toward zero for $D \gg 1$. The asymptotic decay corresponds to the curse of dimensionality; $D \approx 3$ is efficient but not the dimension maximizing holographic capacity.

:::

:::{prf:theorem} Causal Stasis
:label: thm-causal-stasis

Let $v^k = dz^k/ds$ be the velocity of the agent's belief update in computation time $s$ (Definition {prf:ref}`def-bulk-drift-continuous-flow`). As $I_{\text{bulk}} \to I_{\max}$:

$$
\|v\|_G \to 0.

$$
*Proof.* From the Equation of Motion (Definition {prf:ref}`def-bulk-drift-continuous-flow`) with effective potential $\Phi_{\text{eff}}$ ({prf:ref}`def-effective-potential`):

$$
dz^k = \left( -G^{kj}\partial_j \Phi_{\text{eff}} + u_\pi^k - \Gamma^k_{ij}\dot{z}^i\dot{z}^j \right) ds + \sqrt{2T_c}(G^{-1/2})^{kj} dW^j_s.

$$
The drift velocity scales as:

$$
v^k \propto G^{kj} \partial_j \Phi_{\text{eff}}.

$$
As the information density approaches saturation, Lemma {prf:ref}`lem-metric-divergence-at-saturation` implies $G_{rr} \to \infty$, hence $G^{rr} \to 0$. The radial component of velocity:

$$
v^r = -G^{rr}\partial_r \Phi_{\text{eff}} \to 0. \quad \blacksquare

$$
*Operational interpretation.* The agent becomes **frozen in thought**. Its internal update rate slows as the "inertia" (mass = metric, per Definition {prf:ref}`def-mass-tensor`) becomes infinite. The agent can still receive observations (inflow), but it cannot process them into updated beliefs or emit actions (outflow). This is **Causal Stasis**: the agent is overwhelmed by its own representational complexity.

*Remark (Distinction from Deadlock).* Causal Stasis is not a software deadlock or resource exhaustion. It is a geometric phenomenon: the agent's belief manifold has curved so severely that motion becomes infinitely costly. The remedy is not debugging but **ontological surgery**—reducing $I_{\text{bulk}}$ via Fusion ({ref}`sec-ontological-fusion-concept-consolidation`) or expanding the boundary capacity.

:::

:::{prf:corollary} The Saturation-Velocity Tradeoff
:label: cor-saturation-velocity-tradeoff

Let $\eta := I_{\text{bulk}}/I_{\max}$ be the saturation ratio. Near the bound, the update velocity scales as:

$$
\|v\|_G \sim (1 - \eta)^{1/2}.

$$
*Proof.* From Lemma {prf:ref}`lem-metric-divergence-at-saturation`, the metric component $G^{rr} = A(r)^{-1}$ vanishes at the horizon. Under uniform saturation, the information mass $\mu(r)$ grows with radius. At the horizon, $\mu(r_h) = \mu_{\max}$. The saturation ratio $\eta := I_{\text{bulk}}/I_{\max} = \mu/\mu_{\max}$ measures the fraction of capacity used. Near the horizon, $G^{rr} \sim (1 - \mu/\mu_{\max}) = (1 - \eta)$. Since velocity scales as $v^r \propto G^{rr}$, we have $\|v\| \sim (G^{rr})^{1/2} \sim (1-\eta)^{1/2}$. $\square$

*Interpretation.* At 90% saturation ($\eta = 0.9$), the agent operates at $\sim 32\%$ of its maximum velocity. At 99% saturation, velocity drops to $\sim 10\%$. The approach to the bound is gradual but accelerating.

:::

:::{prf:definition} Capacity Horizon Diagnostic
:label: def-capacity-horizon-diagnostic

Compute the **Saturation Ratio**:

$$
\eta_{\text{Sch}}(s) := \frac{I_{\text{bulk}}(s)}{I_{\max}} = \frac{I_{\text{bulk}}(s)}{\nu_D \cdot \text{Area}(\partial\mathcal{Z}) / \ell_L^{D-1}},

$$
where:
- $I_{\text{bulk}}(s) = \int_{\mathcal{Z}} \rho_I(z,s) \, d\mu_G$ per Definition {prf:ref}`def-a-bulk-information-volume`
- $\nu_D$ is the Holographic Coefficient (Definition {prf:ref}`def-holographic-coefficient`)
- $D$ is the latent manifold dimension

*Special case (Poincare disk, $D=2$):* $\eta_{\text{Sch}} = 4\ell_L \cdot I_{\text{bulk}} / \text{Area}(\partial\mathcal{Z})$.

*Interpretation:*
- $\eta_{\text{Sch}} < 0.5$: Safe operating regime. Ample capacity headroom.
- $0.5 \le \eta_{\text{Sch}} < 0.9$: Elevated utilization. Monitor for growth trends.
- $0.9 \le \eta_{\text{Sch}} < 0.99$: **Warning.** Update velocity degraded (Corollary {prf:ref}`cor-saturation-velocity-tradeoff`). Prepare for ontological intervention.
- $\eta_{\text{Sch}} \ge 0.99$: **Critical.** Causal Stasis imminent. Halt exploration and trigger emergency fusion.

*Cross-reference:* Complements CapacitySaturationCheck (Node 40, {ref}`sec-diagnostic-node-capacity-saturation`) by providing the velocity-degradation interpretation and connecting to ontological remediation.
:::

## 07_cognition/01_supervised_topo.md

:::{prf:remark} Extension, Not Replacement
:label: rem-extension-not-replacement

{ref}`sec-relationship-to-the-context-conditioned-framework` establishes classification as selecting a context $c \in \mathcal{Y}$ (the label space), with effective potential $\Phi_{\text{eff}} = -\log p(y|z)$ (Theorem {prf:ref}`thm-universal-context-structure`). This section specifies the **topological constraints** that enforce geometric coherence of this classification:

1. Charts should be semantically pure (one class per chart, modulo transition regions)
2. Different classes should be metrically separated (long geodesics between class regions)
3. Classification should be stable under dynamics (regions of attraction)

:::

:::{prf:definition} Semantic Partition
:label: def-semantic-partition

Let $\mathcal{Y} = \{1, \ldots, C\}$ be the set of class labels and $\mathcal{K}$ the macro-state register (Definition 2.2.1). A labeling $Y: \mathcal{X} \to \mathcal{Y}$ induces a **soft partition** of the chart atlas:

$$
\mathcal{A}_y := \{k \in \mathcal{K} : P(Y=y \mid K=k) > 1 - \epsilon_{\text{purity}}\},

$$
where $\epsilon_{\text{purity}} \in (0, 0.5)$ is the purity threshold.

*Interpretation:* $\mathcal{A}_y$ is the **sub-atlas** of charts predominantly associated with class $y$. A chart $k$ belongs to $\mathcal{A}_y$ if, given that a sample routes to chart $k$, the probability of class $y$ exceeds $1 - \epsilon_{\text{purity}}$.

:::

:::{prf:proposition} Soft Injectivity
:label: prop-soft-injectivity

The sub-atlases need not be disjoint. Charts in $\mathcal{A}_i \cap \mathcal{A}_j$ for $i \neq j$ are **transition regions** characterized by:

1. **Low purity:** $\max_y P(Y=y \mid K=k) < 1 - \epsilon_{\text{purity}}$ for all $y$
2. **High entropy:** $H(Y \mid K=k) > H_{\text{transition}}$ (conditional entropy; see {cite}`cover1991elements`)
3. **Low information content:** These charts carry less semantic information per the information bottleneck principle {cite}`tishby2015ib`

*Remark (Geometric Interpretation).* Transition charts correspond to saddle regions of the semantic potential landscape---unstable fixed points between class regions of attraction.

**Cross-references:** {ref}`sec-relationship-to-the-context-conditioned-framework` (Context-Conditioned Policies), Definition 2.2.1 (Macro-State Register), {ref}`sec-tier-the-attentive-atlas` (Router Weights).

:::

:::{prf:definition} Class-Conditioned Potential
:label: def-class-conditioned-potential

Given a target class $y \in \mathcal{Y}$, define the semantic potential:

$$
V_y(z, K) := -\beta_{\text{class}} \log P(Y=y \mid K) + V_{\text{base}}(z, K),

$$
where:
- $P(Y=y \mid K) = \text{softmax}(\Theta_{K,:})_y$ with learnable parameters $\Theta \in \mathbb{R}^{N_c \times C}$
- $V_{\text{base}}(z, K)$ is the unconditioned critic ({ref}`sec-the-hjb-correspondence`)
- $\beta_{\text{class}} > 0$ is the **class temperature** (inverse of semantic diffusion)
- Units: $[V_y] = \mathrm{nat}$

*Remark (Chart-to-Class Mapping).* The learnable parameter $\Theta_{k,y}$ represents the log-affinity of chart $k$ for class $y$. After training, $P(Y=y \mid K=k) = \text{softmax}(\Theta_{k,:})_y$ approximates the empirical conditional distribution.

*Remark (Alternative: Empirical Estimation).* Instead of learnable parameters, one may estimate $P(Y|K)$ empirically via exponential moving average:

$$
\hat{P}(Y=y \mid K=k) = \frac{\text{EMA}[\mathbb{I}[Y=y, K=k]]}{\text{EMA}[\mathbb{I}[K=k]]}.

$$
This is non-differentiable w.r.t. chart assignment but more grounded in observations. A hybrid approach initializes learnable $\Theta$ from empirical estimates after warmup.

:::

:::{prf:definition} Region of Attraction
:label: def-region-of-attraction

The **region of attraction** for class $y$ is:

$$
\mathcal{B}_y := \{z \in \mathcal{Z} : \lim_{t \to \infty} \phi_t(z) \in \mathcal{A}_y\},

$$
where $\phi_t$ denotes the flow of the curl-corrected system

$$
\dot{z} = \mathcal{M}_{\text{curl}}\!\left(-G^{-1}(z)\nabla_A V_y(z)\right), \qquad \mathcal{M}_{\text{curl}} := (I - \beta_{\text{curl}} G^{-1}\mathcal{F})^{-1}
$$
(conservative case: $\mathcal{F}=0$).
Here $\nabla_A V_y := \nabla V_y - A$ with $A := \delta\Psi + \eta$ the non-conservative component of the reward 1-form
(conservative case: $A=0$).

*Interpretation:* $\mathcal{B}_y$ is the set of initial conditions from which the deterministic gradient flow on $V_y$ converges to the class-$y$ region.

:::

:::{prf:theorem} Classification as Relaxation
:label: thm-classification-as-relaxation

Under the overdamped dynamics ({ref}`sec-the-overdamped-limit`) with potential $V_y$:

$$
dz = \mathcal{M}_{\text{curl}}\!\left(-G^{-1}(z) \nabla_A V_y(z, K)\right) ds + \sqrt{2T_c}\, G^{-1/2}(z)\, dW_s, \quad T_c \text{ cognitive temperature } ({prf:ref}`def-cognitive-temperature`)

$$
When $\mathcal{F}=0$ (conservative case), $\mathcal{M}_{\text{curl}} = I$ and we recover pure gradient flow.
The limiting chart assignment satisfies:

$$
\lim_{s \to \infty} K(z(s)) \in \mathcal{A}_y \quad \text{almost surely},

$$
provided:
1. $z(0) \in \mathcal{B}_y$ (initial condition in the basin)
2. $T_c$ is sufficiently small (low temperature limit)
3. The basins have positive measure and are separated by finite barriers

*Proof sketch.* Define the Lyapunov function $L(z) := V_y(z, K(z))$ (see {cite}`khalil2002nonlinear` for Lyapunov theory, {cite}`lasalle1960invariance` for the invariance principle). Under the overdamped dynamics:

$$
\frac{dL}{ds} = \nabla_A V_y \cdot \dot{z} = -\nabla_A V_y \cdot \mathcal{M}_{\text{curl}} G^{-1}\nabla_A V_y + \text{noise terms}.

$$
The antisymmetric curl contribution in $\mathcal{M}_{\text{curl}}$ does no work, so it does not increase $L$.
For small $T_c$, the deterministic term dominates, ensuring $L$ decreases until $z$ reaches a local minimum. The class-$y$ region is the global minimum of $V_y$ by construction. Full proof in {ref}`sec-appendix-a-full-derivations`. $\square$

:::

:::{prf:corollary} Inference via Relaxation
:label: cor-inference-via-relaxation

Classification inference proceeds as:
1. Encode: $z_0 = \text{Enc}(x)$
2. Relax under neutral potential $V_{\text{base}}$ (no class conditioning) to equilibrium $z^*$
3. Read out: $\hat{y} = \arg\max_y P(Y=y \mid K(z^*))$

*Remark (Fast Path).* In practice, we often skip the relaxation and use direct readout: $\hat{y} = \arg\max_y \sum_k w_k(x) \cdot P(Y=y \mid K=k)$, where $w_k(x)$ are the router weights ({ref}`sec-tier-the-attentive-atlas`). The relaxation interpretation justifies this as the $T_c \to 0$, $s \to \infty$ limit.

**Cross-references:** {ref}`sec-the-overdamped-limit` (Overdamped Limit), Definition {prf:ref}`def-effective-potential`, {ref}`sec-the-hjb-correspondence` (Critic).

:::

:::{prf:definition} Class-Consistent Jump Rate
:label: def-class-consistent-jump-rate

For the WFR reaction term (Definition {prf:ref}`def-the-wfr-action`), modulate the inter-chart transition rate:

$$
\lambda_{i \to j}^{\text{sup}} := \lambda_{i \to j}^{(0)} \cdot \exp\left(-\gamma_{\text{sep}} \cdot D_{\text{class}}(i, j)\right),

$$
where:
- $\lambda^{(0)}_{i \to j}$ is the **base transition rate** from the GKSL master equation ({prf:ref}`def-gksl-generator`, {cite}`lindblad1976gksl,gorini1976gksl`, {ref}`sec-connection-to-gksl-master-equation`), derived from the overlap consistency of jump operators (Section 7.13)
- $\gamma_{\text{sep}} \geq 0$ is the **separation strength** (hyperparameter)
- $D_{\text{class}}(i, j) = \mathbb{I}[\text{Class}(i) \neq \text{Class}(j)]$ is the class disagreement indicator
- $\text{Class}(k) := \arg\max_y P(Y=y \mid K=k)$ is the dominant class of chart $k$

*Remark (Rate vs Operator).* {ref}`sec-factorized-jump-operators-efficient-chart-transitions` defines the **transition function** $L_{i \to j}$ (the coordinate change map). The **transition rate** $\lambda_{i \to j}$ is a separate quantity from the GKSL/master equation framework ({ref}`sec-connection-to-gksl-master-equation`, Equation 20.5.2) that governs *how often* jumps occur, not *where* they go. The rate is typically derived from the overlap structure: $\lambda_{i \to j}^{(0)} \propto \mathbb{E}_{x}[w_i(x) w_j(x)]$, measuring how much probability mass lies in the overlap $U_i \cap U_j$.

*Interpretation:* Transitions between charts of the same class proceed at the base rate $\lambda^{(0)}$. Transitions between charts of different classes are exponentially suppressed by factor $e^{-\gamma_{\text{sep}}}$.

:::

:::{prf:proposition} Effective Disconnection
:label: prop-effective-disconnection

As $\gamma_{\text{sep}} \to \infty$, the effective WFR distance between charts of different classes diverges:

$$
d_{\text{WFR}}(\mathcal{A}_{y_1}, \mathcal{A}_{y_2}) \to \infty \quad \text{for } y_1 \neq y_2.

$$
*Proof sketch.* The WFR distance (Definition {prf:ref}`def-the-wfr-action`) involves minimizing over paths that may use both transport (continuous flow within charts) and reaction (jumps between charts). Consider a path from $\mathcal{A}_{y_1}$ to $\mathcal{A}_{y_2}$:

1. **Transport-only paths:** If $\mathcal{A}_{y_1}$ and $\mathcal{A}_{y_2}$ are not geometrically adjacent (no shared chart boundary), pure transport paths have infinite cost.

2. **Jump paths:** Any path using cross-class jumps incurs reaction cost. In the GKSL interpretation ({ref}`sec-connection-to-gksl-master-equation`), the suppressed jump rate $\lambda^{\text{sup}} = \lambda^{(0)} e^{-\gamma_{\text{sep}}}$ means mass transfer between unlike-class charts requires longer dwell times, increasing the action.

3. **Divergence:** As $\gamma_{\text{sep}} \to \infty$, cross-class jumps become arbitrarily rare. The optimal path cost diverges because: (a) pure transport is blocked by chart boundaries, and (b) the reaction term penalizes staying in transition states waiting for rare jumps.

The precise scaling (exponential, polynomial, etc.) depends on the manifold geometry, but divergence is guaranteed. $\square$

:::

:::{prf:remark} Tunneling as Anomaly Detection
:label: rem-tunneling-as-anomaly-detection

Cross-class transitions are not forbidden, merely exponentially suppressed. A detected cross-class jump indicates:

1. **Anomaly:** The sample lies in a transition region not well-covered by training
2. **Distribution shift:** The test distribution differs from training
3. **Adversarial input:** Deliberate perturbation to cross class boundaries

This provides a natural **out-of-distribution detection** mechanism: monitor the rate of cross-class transitions.

:::

:::{prf:definition} Class-Modulated Jump Operator
:label: def-class-modulated-jump-operator

Modify the jump operator (Definition {prf:ref}`def-factorized-jump-operator`) to incorporate class consistency:

```python
def class_modulated_jump_rate(
    lambda_base: torch.Tensor,    # [N_c, N_c] base jump rates
    chart_to_class: torch.Tensor, # [N_c, C] learnable logits
    gamma_sep: float = 5.0,       # Separation strength
) -> torch.Tensor:
    """
    Compute class-modulated jump rates.

    Cross-ref:
        - Definition 25.3.1 (Class-Consistent Jump Rate)
        - Definition 7.13.1 (Jump Operator)
    """
    # Get dominant class per chart
    p_y_given_k = F.softmax(chart_to_class, dim=1)  # [N_c, C]
    dominant_class = p_y_given_k.argmax(dim=1)       # [N_c]

    # Compute class disagreement matrix
    class_match = (dominant_class.unsqueeze(1) == dominant_class.unsqueeze(0)).float()  # [N_c, N_c]
    D_class = 1.0 - class_match  # 1 if classes differ, 0 if same

    # Modulate rates
    lambda_sup = lambda_base * torch.exp(-gamma_sep * D_class)

    return lambda_sup
```

**Cross-references:** {ref}`sec-the-wfr-metric` (WFR Metric), Definition {prf:ref}`def-factorized-jump-operator`, {ref}`sec-connection-to-gksl-master-equation` (GKSL Connection).

:::

:::{prf:definition} Purity Loss
:label: def-purity-loss

The purity loss measures how well charts separate classes:

$$
\mathcal{L}_{\text{purity}} = \sum_{k=1}^{N_c} P(K=k) \cdot H(Y \mid K=k),

$$
where:
- $P(K=k) = \mathbb{E}_{x \sim \mathcal{D}}[w_k(x)]$ is the marginal chart probability
- $H(Y \mid K=k) = -\sum_y P(Y=y \mid K=k) \log P(Y=y \mid K=k)$ is the class entropy within chart $k$

*Interpretation:* $\mathcal{L}_{\text{purity}} = H(Y \mid K)$, the conditional entropy of class given chart. Minimizing this encourages each chart to be associated with a single class.

:::

:::{prf:proposition} Purity-Information Duality
:label: prop-purity-information-duality

Minimizing $\mathcal{L}_{\text{purity}}$ is equivalent to maximizing the mutual information $I(K; Y)$:

$$
\mathcal{L}_{\text{purity}} = H(Y) - I(K; Y).

$$
Since $H(Y)$ is fixed by the data, $\min \mathcal{L}_{\text{purity}} \Leftrightarrow \max I(K; Y)$.

:::

:::{prf:definition} Balance Loss
:label: def-balance-loss

Prevent degenerate solutions where all samples route to few charts:

$$
\mathcal{L}_{\text{balance}} = D_{\text{KL}}\left(\bar{w} \;\|\; \text{Uniform}(N_c)\right),

$$
where $\bar{w} = \mathbb{E}_{x \sim \mathcal{D}}[w(x)]$ is the average router weight vector.

*Interpretation:* Encourages all charts to be used, preventing "dead charts" and ensuring the atlas covers the label space.

:::

:::{prf:definition} Contrastive Loss
:label: def-contrastive-loss

Enforce that different-class samples are geometrically separated:

$$
\mathcal{L}_{\text{metric}} = \frac{1}{|\mathcal{P}|} \sum_{(i,j) \in \mathcal{P}: y_i \neq y_j} w_i^\top w_j \cdot \max(0, m - d_{\text{jump}}(z_i, z_j))^2,

$$
where:
- $\mathcal{P}$ is the set of sample pairs in the batch
- $w_i, w_j$ are router weight vectors
- $m > 0$ is the margin (minimum desired separation)
- $d_{\text{jump}}(z_i, z_j)$ is the minimum jump cost ({ref}`sec-factorized-jump-operators-efficient-chart-transitions`)

*Interpretation:* If two samples have different labels but high router overlap ($w_i^\top w_j$ large), they must be separated by at least margin $m$ in jump distance. Otherwise, the loss penalizes the configuration.

:::

:::{prf:definition} Route Alignment Loss
:label: def-route-alignment-loss

The primary classification loss:

$$
\mathcal{L}_{\text{route}} = \mathbb{E}_{x, y_{\text{true}}}\left[\text{CE}\left(\sum_k w_k(x) \cdot P(Y=\cdot \mid K=k), \; y_{\text{true}}\right)\right],

$$
where $\text{CE}$ denotes cross-entropy.

*Interpretation:* The predicted class distribution is the router-weighted average of per-chart class distributions. This must match the true label.

:::

:::{prf:definition} Total Loss
:label: def-total-loss

The full supervised topology loss:

$$
\mathcal{L}_{\text{sup-topo}} = \mathcal{L}_{\text{route}} + \lambda_{\text{pur}} \mathcal{L}_{\text{purity}} + \lambda_{\text{bal}} \mathcal{L}_{\text{balance}} + \lambda_{\text{met}} \mathcal{L}_{\text{metric}}.

$$
Typical hyperparameters: $\lambda_{\text{pur}} = 0.1$, $\lambda_{\text{bal}} = 0.01$, $\lambda_{\text{met}} = 0.01$.

**Algorithm 25.4.7 (SupervisedTopologyLoss Implementation).**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict


class SupervisedTopologyLoss(nn.Module):
    """
    Supervised topology loss enforcing chart purity, balance, and separation.

    Cross-ref:
        - Definition 25.4.6 (Total Loss)
        - {ref}`sec-tier-the-attentive-atlas` (Router Weights)
    """

    def __init__(
        self,
        num_charts: int,
        num_classes: int,
        lambda_purity: float = 0.1,
        lambda_balance: float = 0.01,
        lambda_metric: float = 0.01,
        margin: float = 1.0,
        temperature: float = 1.0,
    ):
        super().__init__()
        self.num_charts = num_charts
        self.num_classes = num_classes
        self.lambda_purity = lambda_purity
        self.lambda_balance = lambda_balance
        self.lambda_metric = lambda_metric
        self.margin = margin

        # Learnable chart-to-class mapping (Definition 25.2.1)
        self.chart_to_class = nn.Parameter(
            torch.randn(num_charts, num_classes) * 0.01
        )
        self.temperature = temperature

    @property
    def p_y_given_k(self) -> torch.Tensor:
        """P(Y|K) distribution [N_c, C]."""
        return F.softmax(self.chart_to_class / self.temperature, dim=1)

    def forward(
        self,
        router_weights: torch.Tensor,  # [B, N_c]
        y_true: torch.Tensor,          # [B] class labels
        z_latent: torch.Tensor = None, # [B, D] optional for metric loss
    ) -> Dict[str, torch.Tensor]:
        """
        Compute supervised topology losses.

        Returns dict with individual losses and total.
        """
        B = router_weights.shape[0]
        p_y_k = self.p_y_given_k  # [N_c, C]

        # === Route Alignment Loss (Definition 25.4.5) ===
        # P(Y|x) = sum_k w_k(x) * P(Y|K=k)
        p_y_x = torch.matmul(router_weights, p_y_k)  # [B, C]
        loss_route = F.cross_entropy(
            torch.log(p_y_x + 1e-8), y_true
        )

        # === Purity Loss (Definition 25.4.1) ===
        # H(Y|K=k) for each chart
        entropy_per_chart = -(p_y_k * torch.log(p_y_k + 1e-8)).sum(dim=1)  # [N_c]
        # P(K=k) = average router weight
        p_k = router_weights.mean(dim=0)  # [N_c]
        # L_purity = sum_k P(K=k) * H(Y|K=k)
        loss_purity = (p_k * entropy_per_chart).sum()

        # === Balance Loss (Definition 25.4.3) ===
        # KL(p_k || Uniform) = sum_k p_k * log(p_k / (1/N_c)) = sum_k p_k * (log(p_k) + log(N_c))
        uniform = torch.ones_like(p_k) / self.num_charts
        # Manual KL computation: KL(P||Q) = sum P * log(P/Q)
        loss_balance = (p_k * (torch.log(p_k + 1e-8) - torch.log(uniform))).sum()

        # === Metric Contrastive Loss (Definition 25.4.4) ===
        loss_metric = torch.tensor(0.0, device=router_weights.device)
        if self.lambda_metric > 0 and B > 1:
            # Router overlap as proxy for proximity
            # w_i^T w_j measures routing similarity
            overlap = torch.matmul(router_weights, router_weights.t())  # [B, B]

            # Class disagreement mask
            y_match = (y_true.unsqueeze(1) == y_true.unsqueeze(0)).float()
            y_diff = 1.0 - y_match  # 1 if different classes

            # Penalize high overlap for different-class pairs
            # Using overlap as proxy for d_jump (lower overlap ~ larger distance)
            pseudo_dist = 1.0 - overlap  # Rough proxy
            hinge = F.relu(self.margin - pseudo_dist)
            loss_metric = (y_diff * overlap * hinge ** 2).sum() / (y_diff.sum() + 1e-8)

        # === Total Loss ===
        loss_total = (
            loss_route
            + self.lambda_purity * loss_purity
            + self.lambda_balance * loss_balance
            + self.lambda_metric * loss_metric
        )

        return {
            'loss_total': loss_total,
            'loss_route': loss_route,
            'loss_purity': loss_purity,
            'loss_balance': loss_balance,
            'loss_metric': loss_metric,
        }
```

**Cross-references:** {ref}`sec-tier-the-attentive-atlas` (Router Weights), Section 7.13 (Jump Operators), {ref}`sec-diagnostics-stability-checks` (Diagnostic Nodes).

:::

:::{prf:remark} Connection to Mobius Re-centering
:label: rem-connection-to-m-bius-re-centering

The Mobius re-centering $\phi_c$ for conditioned generation (Definition {prf:ref}`ax-bulk-boundary-decoupling`) can be interpreted as centering at the **class centroid**:

$$
c_y := \mathbb{E}_{x: Y(x)=y}[\text{Enc}(x)],

$$
i.e., the average latent position of class-$y$ samples. Conditioned generation "starts" the holographic expansion from this centroid.

:::

:::{prf:proposition} Class-Conditioned Langevin
:label: prop-class-conditioned-langevin

The generative Langevin equation {cite}`welling2011sgld,song2019ncsn` (Definition {prf:ref}`prop-so-d-symmetry-at-origin`) with class conditioning becomes:

$$
dz = \mathcal{M}_{\text{curl}}\!\left(-\nabla_G V_y(z, K)\right) d\tau + \sqrt{2T_c}\,G^{-1/2}(z)\,dW_\tau,

$$
where $V_y$ is the class-conditioned potential (Definition {prf:ref}`def-class-conditioned-potential`).

*Interpretation:* To generate a sample of class $y$, we run Langevin dynamics with the $V_y$ potential. The semantic term $-\beta_{\text{class}} \log P(Y=y \mid K)$ biases the flow toward class-$y$ charts.

:::

:::{prf:corollary} Label as Symmetry-Breaking Field, cf. classifier-free guidance {cite}`ho2022cfg`
:label: cor-label-as-symmetry-breaking-field-cf-classifier-free-guidance

The class label $y$ breaks the $SO(2)$ symmetry of the unconditioned flow in the Poincare disk. At the origin:

1. **Unconditioned:** $\nabla_A V_{\text{base}}(0) = 0$ (symmetric saddle)
2. **Conditioned:** $\nabla_A V_y(0) = -\beta_{\text{class}} \nabla_z \log P(Y=y \mid K(z))|_{z=0} \neq 0$

The non-zero gradient aligns the initial "kick" direction with the class-$y$ basin.

:::

:::{prf:definition} Class Centroid in Poincare Disk
:label: def-class-centroid-in-poincar-disk

For the Poincare disk embedding {cite}`nickel2017poincare,ganea2018hnn`, define the class centroid using the **Frechet mean** {cite}`lou2020frechet`:

$$
c_y := \arg\min_{c \in \mathbb{D}} \sum_{x: Y(x)=y} d_{\mathbb{D}}(c, \text{Enc}(x))^2.

$$
This is well-defined since the Poincare disk has negative curvature (unique Frechet means).

**Cross-references:** {ref}`sec-policy-control-field` (Langevin Dynamics), {ref}`sec-the-retrieval-texture-firewall` (Mobius Re-centering), Definition {prf:ref}`prop-so-d-symmetry-at-origin`.

:::

:::{prf:remark} Integration with TopologicalDecoder
:label: rem-integration-with-topologicaldecoder

The TopologicalDecoder ({ref}`sec-decoder-architecture-overview-topological-decoder`) receives the geometric content $z_{\text{geo}} = e_K + z_n$ and routes through chart-specific projectors. For class-conditioned generation:

1. **Class determines charts:** The class label $y$ biases chart selection toward $\mathcal{A}_y$ via the semantic potential $V_y$
2. **Decoder routing:** The TopologicalDecoder's inverse router ({ref}`sec-topological-decoder-module`) can either:
   - Accept an explicit chart index $K$ (from the generative flow)
   - Infer routing from $z_{\text{geo}}$ (autonomous mode)
3. **Consistency constraint:** The decoder's inferred routing should agree with the encoder's class-conditioned routing:

   $$
   \mathcal{L}_{\text{route-consistency}} = \mathbb{E}_{x,y}\left[\text{CE}\left(w_{\text{dec}}(z_{\text{geo}}), w_{\text{enc}}(x)\right)\right]

   $$
   where $w_{\text{dec}}$ are the decoder's soft router weights and $w_{\text{enc}}$ are the encoder's.

This ensures that class-conditioned generation produces samples that the encoder would classify correctly---a form of **cycle consistency** between encoding and decoding under the semantic topology.

:::

:::{prf:definition} Hierarchical Labels
:label: def-hierarchical-labels

A **label hierarchy** is a sequence of label spaces:

$$
\mathcal{Y}_0 \twoheadrightarrow \mathcal{Y}_1 \twoheadrightarrow \cdots \twoheadrightarrow \mathcal{Y}_L,

$$
where $\twoheadrightarrow$ denotes a surjection (coarsening). $\mathcal{Y}_0$ are coarse labels (super-categories), $\mathcal{Y}_L$ are fine labels (leaf categories).

*Example:* $\mathcal{Y}_0 = \{\text{Animal}, \text{Vehicle}\}$, $\mathcal{Y}_1 = \{\text{Dog}, \text{Cat}, \text{Car}, \text{Bike}\}$, $\mathcal{Y}_2 = \{\text{Terrier}, \text{Poodle}, \ldots\}$.

:::

:::{prf:proposition} Scale-Label Alignment
:label: prop-scale-label-alignment

In the stacked TopoEncoder ({ref}`sec-stacked-topoencoders-deep-renormalization-group-flow`), enforce purity at each scale:

- **Layer 0 (Bulk/Slow):** Charts at level 0 correspond to coarse classes. Enforce:

  $$
  \mathcal{L}_{\text{purity}}^{(0)} = H(\mathcal{Y}_0 \mid K^{(0)})

  $$
- **Layer $\ell$ (Intermediate):** Charts at level $\ell$ correspond to level-$\ell$ classes. Enforce:

  $$
  \mathcal{L}_{\text{purity}}^{(\ell)} = H(\mathcal{Y}_\ell \mid K^{(\ell)})

  $$
- **Layer $L$ (Boundary/Fast):** Charts at level $L$ correspond to fine classes. Enforce:

  $$
  \mathcal{L}_{\text{purity}}^{(L)} = H(\mathcal{Y}_L \mid K^{(L)})

  $$
:::

:::{prf:remark} Renormalization Group Interpretation
:label: rem-renormalization-group-interpretation

The semantic hierarchy matches the physical renormalization scale:

| Scale                | Latent Structure              | Semantic Structure |
|----------------------|-------------------------------|--------------------|
| Bulk (Layer 0)       | Slow modes, large wavelengths | Super-categories   |
| Intermediate         | Medium modes                  | Categories         |
| Boundary (Layer $L$) | Fast modes, fine details      | Sub-categories     |

This is the **semantic RG flow**: coarse-graining in the label space corresponds to flowing toward the bulk in latent space.

:::

:::{prf:definition} Hierarchical Supervised Loss
:label: def-hierarchical-supervised-loss

The total hierarchical loss:

$$
\mathcal{L}_{\text{hier}} = \sum_{\ell=0}^{L} \alpha_\ell \left(\mathcal{L}_{\text{route}}^{(\ell)} + \lambda_{\text{pur}} \mathcal{L}_{\text{purity}}^{(\ell)}\right),

$$
where $\alpha_\ell$ weights the contribution of each scale (typically $\alpha_\ell = 1$ or decaying with $\ell$).

**Cross-references:** {ref}`sec-stacked-topoencoders-deep-renormalization-group-flow` (Stacked TopoEncoder), Definition {prf:ref}`def-the-peeling-step`, {ref}`sec-rigorous-interpretation-renormalization-group-flow` (RG Interpretation).

:::

## 07_cognition/02_governor.md

:::{prf:remark} Extending {ref}`sec-adaptive-multipliers-learned-penalties-setpoints-and-calibration`
:label: rem-extending-section

{ref}`sec-adaptive-multipliers-learned-penalties-setpoints-and-calibration` introduces three methods for adaptive multiplier tuning:
- **3.5.A (Primal-Dual):** $\lambda_{t+1} = \Pi[\lambda_t + \eta_\lambda (C(\theta_t) - \epsilon)]$ — linear, memoryless
- **3.5.B (PID):** $\lambda_{t+1} = K_p e_t + K_i \sum e + K_d \Delta e$ — hand-tuned temporal filter
- **3.5.C (Learned Precisions):** $\lambda_i = \exp(-s_i)$ — diagonal covariance, no temporal structure

Each method addresses a specific failure mode but lacks generality. The **Universal Governor** subsumes all three as special cases of a learned temporal policy over the diagnostic stream.

:::

:::{prf:definition} The Meta-Control Problem
:label: def-the-meta-control-problem

Let $\theta_t \in \mathcal{M}_\Theta$ be the agent parameters at training step $t$. The meta-control problem is: find a policy $\pi_{\mathfrak{G}}$ that selects hyperparameters $\Lambda_t$ to minimize task loss while satisfying the Sieve constraints.

**Cross-references:** {ref}`sec-adaptive-multipliers-learned-penalties-setpoints-and-calibration` (Adaptive Multipliers), Section 3.4 (Joint Optimization).

:::

:::{prf:definition} Uncontrolled Dynamics
:label: def-uncontrolled-dynamics

Standard gradient descent defines a discrete flow on $\mathcal{M}_\Theta$:

$$
\theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}_{\text{task}}(\theta_t),

$$
where $\eta > 0$ is the step size.

Units: $[\theta] = \text{parameter units}$, $[\eta] = \text{step}^{-1}$, $[\nabla\mathcal{L}] = \text{nat} \cdot [\theta]^{-1}$.

:::

:::{prf:definition} Constrained Dynamics
:label: def-constrained-dynamics

The Fragile Agent imposes $K$ constraints $\{C_k(\theta) \leq 0\}_{k=1}^K$ defined by the Sieve ({ref}`sec-theory-thin-interfaces`). Each $C_k$ corresponds to a diagnostic node:

$$
C_k(\theta) = \text{Node}_k(\theta) - \epsilon_k,

$$
where $\epsilon_k$ is the tolerance threshold. The learning dynamics must satisfy these constraints throughout training.

:::

:::{prf:definition} Controlled Update Law
:label: def-controlled-update-law

The controlled update with adaptive multipliers is:

$$
\theta_{t+1} = \theta_t - \eta_t \left( G^{-1}(\theta_t) \nabla \mathcal{L}_{\text{task}}(\theta_t) + \sum_{k=1}^K \lambda_{k,t} \nabla C_k(\theta_t) \right),

$$
where:
- $G(\theta)$ is the parameter-space metric (cf. natural gradient, {ref}`sec-second-order-sensitivity-value-defines-a-local-metric`)
- $\eta_t$ is the adaptive learning rate
- $\lambda_{k,t} \geq 0$ are the constraint multipliers

Units: $[\lambda_k] = \text{dimensionless}$.

*Remark (Natural Gradient Connection).* The factor $G^{-1}$ applies preconditioning analogous to Fisher Information in natural gradient methods {cite}`amari1998natural`. This ensures updates are measured in information-geometric units rather than Euclidean units.

**Cross-references:** {ref}`sec-second-order-sensitivity-value-defines-a-local-metric` (State-Space Metric), Section 3.1 (Diagnostic Nodes).

:::

:::{prf:definition} Diagnostic State Space
:label: def-diagnostic-state-space

The Governor observes the **Sieve Residuals** via the constraint evaluation map $\Psi: \mathcal{M}_\Theta \to \mathbb{R}^K$:

$$
s_t = \Psi(\theta_t) = [C_1(\theta_t), \ldots, C_K(\theta_t)]^\top.

$$
The components of $s_t$ are the normalized defect functionals corresponding to diagnostic nodes 1–41 ({ref}`sec-theory-thin-interfaces`). Positive values indicate constraint violation.

Units: $[s_t] = \text{nat}$ (for entropy-based nodes) or dimensionless (for normalized defects).

:::

:::{prf:definition} The Universal Governor
:label: def-the-universal-governor

The Governor is a policy $\pi_{\mathfrak{G}}: \mathbb{R}^{K \times H} \to \mathbb{R}_+^{K+2}$ mapping the history of Sieve residuals to control inputs:

$$
\Lambda_t = \pi_{\mathfrak{G}}(s_t, s_{t-1}, \ldots, s_{t-H}; \phi),

$$
where:
- $\Lambda_t = (\eta_t, \lambda_{1,t}, \ldots, \lambda_{K,t}, T_{c,t}) \in \mathbb{R}_+^{K+2}$, where $T_c$ is the cognitive temperature ({prf:ref}`def-cognitive-temperature`)
- $\phi$ are the learnable parameters of the Governor
- $H$ is the history horizon (temporal context)

Units: $[\eta_t] = \text{step}^{-1}$, $[\lambda_{k,t}] = \text{dimensionless}$, $[T_{c,t}] = \text{nat}$.

*Remark (Temporal Processing).* The Governor processes a window of $H$ diagnostic snapshots. This enables detection of first and second differences $\Delta s_t$, $\Delta^2 s_t$, which are required for PID-like control (Proposition {prf:ref}`prop-subsumption-of-section`).

:::

:::{prf:proposition} Subsumption of {ref}`sec-adaptive-multipliers-learned-penalties-setpoints-and-calibration`
:label: prop-subsumption-of-section

The methods of {ref}`sec-adaptive-multipliers-learned-penalties-setpoints-and-calibration` are recovered as special cases of $\pi_{\mathfrak{G}}$:

| Method                     | Governor Instantiation                                                       |
|----------------------------|------------------------------------------------------------------------------|
| Primal-Dual (3.5.A)        | $\pi_{\mathfrak{G}}(s_t) = \lambda_{t-1} + \eta_\lambda s_t$ (affine, $H=1$) |
| PID (3.5.B)                | Linear filter with fixed $(K_p, K_i, K_d)$, $H \geq 2$                       |
| Learned Precisions (3.5.C) | Diagonal, no temporal dependence, $H=0$                                      |

*Proof.* Direct verification. The Primal-Dual update is a memoryless affine map. The PID controller is a linear filter over error history. Learned precisions ignore temporal structure entirely. $\square$

:::

:::{prf:definition} Inner Problem: Agent Optimization
:label: def-inner-problem-agent-optimization

Given fixed control $\Lambda$, the agent minimizes the regularized objective:

$$
\theta^*(\Lambda) = \arg\min_{\theta} \left[ \mathcal{L}_{\text{task}}(\theta) + \sum_{k=1}^K \lambda_k C_k(\theta) \right].

$$
:::

:::{prf:definition} Outer Problem: Governor Optimization
:label: def-outer-problem-governor-optimization

The Governor minimizes the **Training Regret** over the distribution of tasks $\mathcal{T}$:

$$
J(\phi) = \mathbb{E}_{\mathcal{T} \sim P(\mathcal{T})} \left[ \sum_{t=0}^T \left( \mathcal{L}_{\text{task}}(\theta_t) + \gamma_{\text{viol}} \sum_{k=1}^K \text{ReLU}(C_k(\theta_t))^2 \right) \right],

$$
subject to: $\theta_{t+1} = \Phi(\theta_t, \pi_{\mathfrak{G}}(\Psi(\theta_t); \phi))$.

Units: $[J] = \text{nat}$, $[\gamma_{\text{viol}}] = \text{dimensionless}$.

The outer objective penalizes cumulative task loss (convergence speed) and squared constraint violations (feasibility). The weight $\gamma_{\text{viol}}$ trades off these two objectives.

:::

:::{prf:theorem} Bilevel Structure
:label: thm-bilevel-structure

The training of the Universal Governor has bilevel structure:

$$
\min_\phi \; J(\phi) \quad \text{s.t.} \quad \theta_t = \theta_t(\Lambda_{0:t-1}), \quad \Lambda_t = \pi_{\mathfrak{G}}(s_{t:t-H}; \phi).

$$
The inner problem (agent learning) depends on the outer variables (Governor parameters) through the control sequence $\{\Lambda_t\}$.

*Remark (Gradient Computation).* Computing $\nabla_\phi J$ requires differentiating through the entire training trajectory. In practice, we use truncated backpropagation through time or evolutionary strategies.

**Cross-references:** {ref}`sec-joint-optimization` (Joint Optimization).

:::

:::{prf:definition} Training Lyapunov Function
:label: def-training-lyapunov-function

Define the candidate Lyapunov function for the training dynamics:

$$
V_{\mathfrak{L}}(\theta) = \mathcal{L}_{\text{task}}(\theta) + \sum_{k=1}^K \frac{\mu_k}{2} \max(0, C_k(\theta))^2,

$$
where $\mu_k > 0$ are penalty weights for constraint violations.

Units: $[V_{\mathfrak{L}}] = \text{nat}$, $[\mu_k] = \text{dimensionless}$.

$V_{\mathfrak{L}}$ is the augmented Lagrangian with quadratic penalty. If $\Delta V_{\mathfrak{L}} < 0$ along the training trajectory, training converges (Theorem {prf:ref}`thm-stable-training-trajectory`).

:::

:::{prf:theorem} Stable Training Trajectory
:label: thm-stable-training-trajectory

If the Governor $\pi_{\mathfrak{G}}$ selects $\Lambda_t$ such that:

$$
\Delta V_{\mathfrak{L}} := V_{\mathfrak{L}}(\theta_{t+1}) - V_{\mathfrak{L}}(\theta_t) < 0 \quad \forall t \text{ where } \theta_t \notin \Omega,

$$
then the training process converges to the largest invariant set $\Omega$ where $\Delta V_{\mathfrak{L}} = 0$. Under standard regularity (twice-differentiable $\mathcal{L}$, LICQ), $\Omega$ consists of KKT points.

*Proof.* $V_{\mathfrak{L}}$ is bounded below by $\inf \mathcal{L}_{\text{task}}$. By hypothesis, $V_{\mathfrak{L}}(\theta_t)$ is strictly decreasing. Since $V_{\mathfrak{L}}$ is bounded below and strictly decreasing, $\lim_{t \to \infty} V_{\mathfrak{L}}(\theta_t)$ exists. By LaSalle's invariance principle {cite}`lasalle1960invariance`, trajectories converge to the largest invariant set $\Omega$ where $\Delta V_{\mathfrak{L}} = 0$. At points in $\Omega$, either (i) $\nabla \mathcal{L}_{\text{task}} = 0$ and all constraints are satisfied, or (ii) the trajectory is at a boundary where the gradient is balanced by constraint forces. $\square$

:::

:::{prf:corollary} Existence of Descent Direction
:label: cor-existence-of-descent-direction

At any non-stationary point $\theta$ where LICQ holds (the gradients $\{\nabla C_k : C_k(\theta) = 0\}$ for active constraints are linearly independent), there exist multipliers $\lambda_k \geq 0$ and step size $\eta > 0$ such that $\Delta V_{\mathfrak{L}} < 0$.

*Proof.* At a non-KKT point, either (i) the unconstrained gradient $-\nabla \mathcal{L}_{\text{task}}$ points into the feasible region, giving descent, or (ii) some constraint is active with $\nabla C_k \neq 0$. Under LICQ, we can solve for $\lambda_k$ such that the projected gradient onto the feasible tangent cone is non-zero {cite}`nocedal2006numerical`. Taking $\eta$ sufficiently small ensures descent. $\square$

**Cross-references:** {ref}`sec-the-bridge-rl-as-lyapunov-constrained-control` (Lyapunov-Constrained Control).

:::

:::{prf:corollary} The Varentropy Brake (Annealing Safety Margin)
:label: cor-varentropy-brake

The training process involves lowering $T_c$ (annealing) to converge on a Nash equilibrium. The stability of this process is governed by the Varentropy (Corollary {prf:ref}`cor-varentropy-stability`).

For the optimization trajectory to remain in the basin of attraction of the global minimum, the cooling schedule must be modulated by the Varentropy:

$$
\frac{d T_c}{dt} = - \eta \cdot \frac{T_c}{1 + \gamma V_H(\theta_t)},

$$
where $\eta, \gamma > 0$ are constants.

*Units:* $[\dot{T}_c] = \mathrm{nat}/[\text{time}]$.

**Mechanism:**
- When $V_H(\theta_t)$ is high (system is near a critical decision point/ridge), the effective cooling rate $\dot{T}_c \to 0$. The Governor "freezes" the temperature to allow the agent to resolve the bifurcation via exploration rather than collapsing into a random mode.
- This prevents **Spontaneous Symmetry Breaking** errors where rapid cooling locks the agent into a suboptimal local minimum.

*Proof:* See Appendix {ref}`E.10 <sec-appendix-e-proof-of-corollary-varentropy-brake>`.

:::

:::{prf:proposition} Structure of Diagnostic Inputs
:label: prop-structure-of-diagnostic-inputs

The input to the Governor, $s_t = \Psi(\theta_t)$, consists of quantities that depend only on the learned representations, not on the raw data $\mathcal{D}$:
- Entropies: $H(K)$, $H(Y|K)$, $I(K;X)$
- Spectral norms: $\|\nabla_A V\|$, $\lambda_{\max}(G)$
- Curvatures: $\|\nabla^2 V\|$, $R_{\text{Ric}}$

These are computed from the model's internal state $\theta_t$ and its outputs on training batches.

*Example:* Codebook collapse is diagnosed by $H(K) \to 0$. The correction (increase VQ commitment loss $\beta$) depends only on the diagnostic value, not on whether the data is images, audio, or tabular.

:::

:::{prf:proposition} Transfer via Meta-Generalization
:label: prop-transfer-via-meta-generalization

Under the conditions of the Meta-Generalization Metatheorem (**MT: Meta-Generalization** in `metalearning.md`), the Governor $\pi_{\mathfrak{G}}$ trained on a distribution of optimization landscapes $\mathcal{S}$ generalizes to new systems drawn from $\mathcal{S}$.

Specifically, if:
1. **Compact structural manifold:** The optimal diagnostic-to-correction mappings $\{\phi^*(S) : S \in \text{supp}(\mathcal{S})\}$ lie on a compact $C^1$ submanifold of the policy space
2. **Uniform local strong convexity:** The training regret $J(\phi)$ satisfies $c\,\text{dist}(\phi, \mathcal{M})^2 \leq J(\phi) \leq C\,\text{dist}(\phi, \mathcal{M})^2$ near the optimal manifold
3. **Lipschitz continuity:** The regret is Lipschitz in both the policy parameters and the training landscape

Then, with probability at least $1 - \delta$, a Governor trained on $N$ sampled landscapes satisfies:

$$
\mathbb{E}_{S \sim \mathcal{S}}[J_S(\hat{\phi}_N)] \leq C_1\left(\varepsilon_N + \sqrt{\frac{\log(1/\delta)}{N}}\right)

$$
where $\varepsilon_N$ is the optimization accuracy.

*Proof sketch (from **MT: Meta-Generalization**):*
1. The optimal corrections form a compact manifold $\mathcal{M}$ in policy space
2. Lipschitz continuity ensures uniform convergence of empirical risk to population risk
3. Approximate minimization on training landscapes implies bounded population risk
4. Local strong convexity implies the learned policy is close to the optimal manifold

In plain terms: if different training landscapes require similar corrections for similar diagnostic signatures, and the training distribution is diverse enough, the learned mapping transfers to new landscapes in the same structural class.

::::{warning} Caveat

The Meta-Generalization Metatheorem is proven in the unpublished document `metalearning.md`. While the proof follows standard statistical learning arguments (uniform convergence, Rademacher complexity bounds), the document has not undergone peer review. The assumptions (compactness, Lipschitz, strong convexity) must be verified for specific applications.
::::

:::

:::{prf:proposition} Dimensional Analysis
:label: prop-dimensional-analysis

All inputs to $\pi_{\mathfrak{G}}$ are either:
1. **Dimensionless ratios:** $\nu_{\text{cap}} = I_{\text{bulk}}/C_\partial$
2. **Entropies:** measured in nats
3. **Normalized defects:** $(C_k - \epsilon_k)/\epsilon_k$

All outputs are either dimensionless (multipliers $\lambda_k$) or have standard units ($\eta$ in step$^{-1}$, $T_c$ in nat). This ensures the Governor's function approximator operates in a well-conditioned, scale-invariant regime.

:::

:::{prf:definition} Canonical Obstruction Suite
:label: def-canonical-obstruction-suite

A distribution of synthetic optimization landscapes $\{\mathcal{L}_{\text{syn}}^{(i)}\}$ constructed to elicit specific failure modes:

| Obstruction            | Hessian Property                          | Failure Mode            | Diagnostic Signal                              | Required Correction                        |
|------------------------|-------------------------------------------|-------------------------|------------------------------------------------|--------------------------------------------|
| **Rosenbrock Valley**  | $\kappa(\nabla^2\mathcal{L}) \gg 1$       | Oscillation             | High $\lVert\nabla\mathcal{L}\rVert$ variance  | Reduce $\eta$ (gain scheduling)            |
| **Saddle Point**       | $\lambda_{\min}(\nabla^2\mathcal{L}) < 0$ | Stagnation              | Low $\lVert\nabla\mathcal{L}\rVert$, flat loss | Increase $T_c$ (entropy injection)         |
| **Disconnected Modes** | Multimodal landscape                      | Mode collapse           | $H(K) \to 0$                                   | Increase jump rate $\lambda_{\text{jump}}$ |
| **Noise Floor**        | High aleatoric uncertainty                | Overfitting             | $I(K; Z_{\text{tex}}) > 0$                     | Texture firewalling                        |
| **Constraint Cliff**   | Sharp constraint boundary                 | Oscillation at boundary | $C_k$ sign changes                             | Increase $\mu_k$ (barrier strength)        |

*Remark (Training Protocol).* The Governor is trained via reinforcement learning on this suite, with reward $r_t = -\Delta V_{\mathfrak{L}}$. Episodes terminate when $V_{\mathfrak{L}}$ plateaus or diverges.

:::

## 07_cognition/03_memory_retrieval.md

:::{prf:definition} Historical Record
:label: def-historical-record

Let $\gamma: [0, T] \to \mathcal{Z}$ be the agent's trajectory on the latent manifold $(\mathcal{Z}, G)$ over time interval $[0, T]$. The *historical record* is the pair $(\gamma, \alpha)$ where $\alpha: [0, T] \to \mathbb{R}$ is the reward flux along the trajectory (Definition {prf:ref}`def-the-reward-flux`).

*Units:* $[\gamma(t)] = [z]$, $[\alpha(t)] = \text{nat}/[s]$.

*Cross-reference:* This connects to Memory Time $t' < t$ (Definition 1.3.4).

:::

:::{prf:definition} Memory Screen
:label: def-memory-screen

The *memory screen* is the signed measure on $\mathcal{Z}$ defined by

$$
\Xi_T := \int_0^T \alpha(t') \, \delta_{\gamma(t')} \, dt',

$$
where:
- $\delta_{\gamma(t')}$ is the Dirac measure concentrated at $\gamma(t') \in \mathcal{Z}$,
- $\alpha(t') = J_r(t')$ is the (signed) reward flux at time $t'$ (Definition {prf:ref}`def-the-reward-flux`).

*Units:* $[\Xi_T] = \text{nat}$ (total signed measure), $[\alpha] = \text{nat}/[s]$ (reward flux rate).

*Interpretation:* $\Xi_T$ encodes where the agent has been, weighted by the sign and magnitude of reward received. Positive rewards contribute positive measure (attractive memory); negative rewards contribute negative measure (repulsive memory).

*Cross-reference (Relativistic Multi-Agent):* In Chapter 29, the Memory Screen is elevated from an auxiliary construct to a **primary state variable**. The Causal Bundle $\mathcal{Z}_{\text{causal}} := \mathcal{Z}^{(N)} \times \Xi_{<t}$ restores the Markov property in relativistic multi-agent settings where finite information speed creates non-Markovian dynamics. See Definition {prf:ref}`def-causal-bundle`.

:::

:::{prf:remark} Connection to Holographic Persistence
:label: rem-connection-to-holographic-persistence

The memory screen $\Xi_T$ provides the mathematical realization of holographic persistence ({ref}`FAQ D.5.3 <sec-appendix-d-control-theory-system-safety>`). The measure $\Xi_T$ on $\mathcal{Z}$ acts as a "hologram" of the agent's history projected onto the latent space, from which non-local forces can be computed.

:::

:::{prf:definition} Memory Kernel via Heat Equation {cite}`grigoryan2009heat,rosenberg1997laplacian`
:label: def-memory-kernel-via-heat-equation

The canonical memory kernel is the *Heat Kernel* $H_\tau(z, z')$ on $(\mathcal{Z}, G)$, defined as the fundamental solution to the heat equation:

$$
(\partial_\tau - \Delta_G) H_\tau(z, z') = 0, \quad H_0(z, z') = \delta(z - z'),

$$
where:
- $\tau > 0$ is the *diffusion time* (memory smoothing scale),
- $\Delta_G = G^{ij}\nabla_i\nabla_j$ is the Laplace-Beltrami operator on $(\mathcal{Z}, G)$ (Definition 2.5.3).

*Units:* $[H_\tau] = [z]^{-d}$ (probability density), $[\tau] = [z]^2$ (diffusion time in geometric units).

*Interpretation:* $H_\tau(z, z')$ measures how much influence a memory at $z'$ has on the current position $z$ after diffusion time $\tau$. Larger $\tau$ yields smoother, more diffuse memory influence. For compact manifolds, $H_\tau$ admits an eigenfunction expansion; for non-compact manifolds with bounded geometry, Gaussian upper bounds hold {cite}`grigoryan2009heat`.

:::

:::{prf:definition} Memory Potential
:label: def-memory-potential

The *memory potential* is defined by

$$
\Psi_{\text{mem}}(z) := -\int_{\mathcal{Z}} H_\tau(z, z') \, d\Xi_T(z').

$$
Expanding using Definition {prf:ref}`def-memory-screen`:

$$
\Psi_{\text{mem}}(z) = -\int_0^T \alpha(t') H_\tau(z, \gamma(t')) \, dt'.

$$
*Units:* $[\Psi_{\text{mem}}] = \text{nat}$.

*Interpretation:* The memory potential is the convolution of the heat kernel with the signed reward-weighted trajectory measure. Since $\Xi_T$ is a signed measure:
- Near high-reward past positions ($\alpha > 0$): $\Psi_{\text{mem}} < 0$, creating a potential well. The force $-\nabla_G \Psi_{\text{mem}}$ points toward the memory (attractive).
- Near high-penalty past positions ($\alpha < 0$): $\Psi_{\text{mem}} > 0$, creating a potential barrier. The force $-\nabla_G \Psi_{\text{mem}}$ points away from the memory (repulsive).

The sign convention ensures that the drift term inside $\mathcal{M}_{\text{curl}}\!\left(-G^{-1}\nabla \Psi_{\text{mem}}\right)$ moves toward rewarding experiences and away from penalizing ones.

:::

:::{prf:proposition} Kernel Alternatives {cite}`rasmussen2006gp`
:label: prop-kernel-alternatives

Alternative kernels may be used depending on application requirements:

1. **Gaussian (RBF) Kernel:**

   $$
   K_{\text{Gauss}}(z, z') := \exp\left(-\frac{d_G(z, z')^2}{2\ell^2}\right),

   $$
   where $d_G$ is the geodesic distance and $\ell > 0$ is the length scale. This provides fast (exponential) decay, suitable for short-range memory effects.

2. **Matérn Kernel:**

   $$
   K_{\nu}(z, z') \propto (-\Delta_G + \kappa^2)^{-\nu}\delta(z - z'),

   $$
   where $\nu > 0$ is the smoothness parameter and $\kappa > 0$ is the inverse correlation length. For $\nu = 1$, this recovers the Green's function $G_\kappa$ from {ref}`sec-the-bulk-potential-screened-poisson-equation`. The Matérn kernel has polynomial (rather than exponential) tails, providing longer-range correlations. See {cite}`rasmussen2006gp` Chapter 4 for the Euclidean case.

*Cross-reference:* The Matern kernel with $\nu = 1$ coincides with the screened Poisson Green's function (Definition {prf:ref}`prop-green-s-function-decay`), establishing a direct connection between memory effects and value propagation.

:::

:::{prf:theorem} Non-Markovian Nature of Memory
:label: thm-non-markovian-nature-of-memory

The force field $-\nabla_G \Psi_{\text{mem}}$ violates the Markov property.

*Proof.* By Definition {prf:ref}`def-memory-potential`, $\Psi_{\text{mem}}(z_t)$ depends on $\Xi_T$, which contains $\gamma(t')$ for all $t' < t$. Therefore, $\nabla_G \Psi_{\text{mem}}(z_t)$ depends on the entire trajectory history $\{\gamma(t')\}_{t' \in [0,t)}$, not merely on $z_t$. This violates the Markov property $P(z_{t+\delta} | z_t, \{z_s\}_{s<t}) = P(z_{t+\delta} | z_t)$. $\square$

*Remark (State Augmentation):* The non-Markovian character is essential for capturing genuine memory effects. The system state must be *augmented* to include $\Xi_T$ (or a sufficient statistic thereof) to recover a Markovian description in an extended state space.

*Remark (Computational Complexity):* Naively, evaluating $\Psi_{\text{mem}}(z)$ requires $O(T)$ kernel evaluations where $T$ is the trajectory length. For long histories, approximations are necessary: (i) truncate to recent history, (ii) subsample the trajectory, (iii) use inducing points {cite}`rasmussen2006gp`, or (iv) maintain a running kernel density estimate.

:::

:::{prf:definition} Memory-Augmented Geodesic SDE
:label: def-memory-augmented-geodesic-sde

The memory-augmented dynamics on $(\mathcal{Z}, G)$ are:

$$
dz^k = \left[\mathcal{M}_{\text{curl}}\right]^k{}_{j}\left(-G^{j\ell}\partial_\ell\bigl(\Phi_{\text{eff}} + \Psi_{\text{mem}}\bigr) + u_\pi^j\right) ds - \Gamma^k_{ij}\dot{z}^i\dot{z}^j\,ds + \sqrt{2T_c}\,(G^{-1/2})^{kj}\,dW^j_s,

$$
where:
- $\Phi_{\text{eff}}$ is the effective potential (Definition {prf:ref}`def-effective-potential`),
- $\Psi_{\text{mem}}$ is the memory potential (Definition {prf:ref}`def-memory-potential`),
- $\Gamma^k_{ij}$ are the Christoffel symbols of $G$ (Definition 2.5.1),
- $u_\pi^k$ is the policy control field (Definition {prf:ref}`def-the-control-field`),
- $T_c$ is the cognitive temperature ({prf:ref}`def-cognitive-temperature`, {ref}`sec-the-geodesic-baoab-integrator`),
- $W^j_s$ is a standard Wiener process,
- $\mathcal{M}_{\text{curl}} := (I - \beta_{\text{curl}} G^{-1}\mathcal{F})^{-1}$ is the curl-corrected mobility.

*Cross-reference:* Definition {prf:ref}`def-bulk-drift-continuous-flow`.

*Units:* All terms have units $[z]/[s]$.

:::

:::{prf:lemma} Virtual Work of Recall
:label: lem-virtual-work-of-recall

The infinitesimal work performed by the memory force during displacement $dz$ is:

$$
dW_{\text{mem}} := \langle -\nabla_G \Psi_{\text{mem}}, dz \rangle_G = -G_{kj}\,G^{k\ell}\partial_\ell \Psi_{\text{mem}}\, dz^j = -\partial_j \Psi_{\text{mem}}\, dz^j.

$$
*Units:* $[dW_{\text{mem}}] = \text{nat}$.

*Interpretation:* When the agent moves toward regions of low $\Psi_{\text{mem}}$ (attractive memory, i.e., $d\Psi_{\text{mem}} < 0$), positive work $dW_{\text{mem}} > 0$ is extracted from the memory field. This corresponds to "reward from recall"---revisiting previously successful states.

:::

:::{prf:theorem} Memory-Induced Barrier Crossing
:label: thm-memory-induced-barrier-crossing

Let $z_t$ be the current position and suppose there exists a past time $t^* < t$ with $z^* := \gamma(t^*)$ such that:
1. $d_G(z_t, z^*) < \ell_{\text{mem}}$ for some memory influence radius $\ell_{\text{mem}}$,
2. $|\alpha(t^*)|$ is large (strong reward signal at time $t^*$).

Then the memory gradient $\|\nabla_G \Psi_{\text{mem}}\|_G$ can exceed the local barrier gradient $\|\nabla_G \Phi_{\text{eff}}\|_G$, enabling transitions that would be forbidden under purely local dynamics.

*Proof sketch.* By Definition {prf:ref}`def-memory-potential` and the concentration of $H_\tau$ near the diagonal for small $\tau$:

$$
\|\nabla_G \Psi_{\text{mem}}(z_t)\|_G \approx |\alpha(t^*)| \cdot \|\nabla_G H_\tau(z_t, z^*)\|_G.

$$
For $d_G(z_t, z^*) \sim O(\sqrt{\tau})$, the gradient $\|\nabla_G H_\tau\|_G \sim O(\tau^{-(d+1)/2})$ can be made arbitrarily large by choosing small $\tau$. If $|\alpha(t^*)|$ is sufficiently large, this dominates $\|\nabla_G \Phi_{\text{eff}}\|_G$. $\square$

*Cross-reference:* BarrierGap diagnostic ({ref}`sec-4-limits-barriers-the-limits-of-control`).

*Interpretation:* Strong memories can "pull" the agent across local energy barriers, providing a mechanism for experience-guided exploration that transcends gradient-based planning.

:::

:::{prf:definition} Memory-Augmented Reaction-Diffusion
:label: def-memory-augmented-reaction-diffusion

The WFR dynamics with memory are:

$$
\partial_s \rho + \nabla \cdot (\rho \mathbf{v}) = \rho \left(\frac{\Phi_{\text{eff}} + \Psi_{\text{mem}} - \bar{\Phi}_{\text{aug}}}{T_c}\right),

$$
where:
- $\rho(z, s)$ is the belief density,
- $\mathbf{v} = \mathcal{M}_{\text{curl}}\!\left(-G^{-1}\nabla(\Phi_{\text{eff}} + \Psi_{\text{mem}}) + u_\pi\right)$ is the curl-corrected drift,
- $\bar{\Phi}_{\text{aug}} = \int_{\mathcal{Z}} (\Phi_{\text{eff}} + \Psi_{\text{mem}}) \rho \, d\mu_G$ is the mean augmented potential.

*Cross-reference:* Definition {prf:ref}`def-the-wfr-action`, Theorem {prf:ref}`thm-wfr-consistency-value-creates-mass`.

*Units:* $[\partial_s \rho] = [z]^{-d}/[s]$, all terms balance.

:::

:::{prf:proposition} Mass Creation from Experience
:label: prop-mass-creation-from-experience

The memory contribution to the reaction term is:

$$
r_{\text{mem}}(z) := \frac{\rho(z)(\Psi_{\text{mem}}(z) - \bar{\Psi}_{\text{mem}})}{T_c},

$$
where $\bar{\Psi}_{\text{mem}} = \int_{\mathcal{Z}} \Psi_{\text{mem}} \rho \, d\mu_G$.

*Interpretation:* Belief mass is created where $\Psi_{\text{mem}} < \bar{\Psi}_{\text{mem}}$ (attractive memory) and destroyed where $\Psi_{\text{mem}} > \bar{\Psi}_{\text{mem}}$ (repulsive memory). This acts as a *virtual source* that redistributes probability toward remembered high-reward regions, even when local dynamics (via $\Phi_{\text{eff}}$) do not support such transitions.

:::

:::{prf:definition} Non-Locality Ratio
:label: def-non-locality-ratio

The *non-locality ratio* at position $z$ is:

$$
\Omega_{\text{mem}}(z) := \frac{\|\nabla_G \Psi_{\text{mem}}(z)\|_G}{\|\nabla_G \Phi_{\text{eff}}(z)\|_G + \epsilon},

$$
where $\epsilon > 0$ is a regularization constant preventing division by zero.

*Units:* $[\Omega_{\text{mem}}] = \text{dimensionless}$.

**Heuristic 27.5.2 (Homeostatic Bound on Memory).** For stable operation, the non-locality ratio should satisfy:

$$
\Omega_{\text{mem}} \in [\Omega_{\min}, \Omega_{\max}],

$$
with empirically recommended bounds $\Omega_{\min} \approx 0.01$, $\Omega_{\max} \approx 10$. These bounds are task-dependent and should be tuned based on the environment's stationarity.

*Boundary cases:*
- $\Omega_{\text{mem}} \to 0$: Pure Markovian dynamics; agent exhibits catastrophic forgetting.
- $\Omega_{\text{mem}} \to \infty$: Pure memory-driven dynamics; agent overfits to historical experience and fails to respond to current environmental gradients.

*Cross-reference:* The Governor ({ref}`sec-theory-of-meta-stability-the-universal-governor-as-homeostatic-controller`) can regulate $\Omega_{\text{mem}}$ by adjusting the memory smoothing scale $\tau$ or the reward flux weighting in $\alpha(t')$.

:::

:::{prf:definition} External Knowledge Manifold
:label: def-external-knowledge-manifold

Let $\mathcal{Z}_{\text{ext}}$ denote the external knowledge manifold equipped with metric $G_{\text{ext}}$, structured as a fiber bundle:

$$
\mathcal{Z}_{\text{ext}} = \mathcal{K} \times \mathcal{Z}_n \times \mathcal{Z}_{\text{tex}},

$$
where $\mathcal{K}$ is the macro-concept space, $\mathcal{Z}_n$ the nuisance coordinates, and $\mathcal{Z}_{\text{tex}}$ the texture fiber.

*Units:* $[G_{\text{ext},ij}] = [z]^{-2}$ (matching the internal metric).

*Cross-reference:* This decomposition mirrors {ref}`sec-conditional-independence-and-sufficiency`'s latent structure $(K, z_n, z_{\text{tex}})$ and {ref}`sec-tier-the-attentive-atlas`'s Atlas architecture.

:::

:::{prf:axiom} Metric Isometry
:label: ax-metric-isometry

There exists a canonical isometry $\Phi: \mathcal{Z}_{\text{int}} \to \mathcal{Z}_{\text{ext}}$ such that for all $z, z' \in \mathcal{Z}_{\text{int}}$:

$$
d_{G_{\text{int}}}(z, z') = d_{G_{\text{ext}}}(\Phi(z), \Phi(z')),

$$
where both manifolds carry the Poincare metric (Definition {prf:ref}`def-hyperbolic-volume-growth`):

$$
G_{ij}(z) = \frac{4\delta_{ij}}{(1 - \|z\|^2)^2}.

$$
*Interpretation:* The isometry axiom asserts that embedding models trained on shared semantic corpora induce compatible distance structures. This is the mathematical foundation for cross-modal retrieval.

:::

:::{prf:definition} Knowledge Atom
:label: def-knowledge-atom

A *knowledge atom* is a triple $\xi = (K, z_n, z_{\text{tex}}) \in \mathcal{Z}_{\text{ext}}$ where:
- $K \in \mathcal{K}$: macro-concept (topic, entity class, logical category)
- $z_n \in \mathcal{Z}_n$: nuisance coordinates (style, formatting, source metadata)
- $z_{\text{tex}} \in \mathcal{Z}_{\text{tex}}$: high-frequency texture (specific wording, surface form)

*Cross-reference:* Compare {ref}`sec-conditional-independence-and-sufficiency`'s decomposition. The macro closure mechanism (Definition 2.8.1) applies equally to external atoms.

:::

:::{prf:definition} Hyperbolic Geodesic Distance
:label: def-hyperbolic-geodesic-distance

For points $z, \xi \in \mathbb{D}^d$ (the Poincare disk), the geodesic distance is:

$$
d_{\mathbb{D}}(z, \xi) = \operatorname{acosh}\left(1 + \frac{2\|z - \xi\|^2}{(1 - \|z\|^2)(1 - \|\xi\|^2)}\right).

$$
*Units:* $[d_{\mathbb{D}}] = [z]$ (dimensionless in Poincare coordinates).

*Cross-reference:* This is the distance function induced by the Poincare metric $G_{ij}$ (Definition {prf:ref}`def-hyperbolic-volume-growth`). See also Definition {prf:ref}`prop-isotropic-radial-expansion` for the hyperbolic potential $U(z) = -2\operatorname{artanh}(\|z\|)$.

:::

:::{prf:definition} Retrieval Measure via Geodesic Functional
:label: def-retrieval-measure-via-geodesic-functional

Given a query position $z \in \mathcal{Z}_{\text{int}}$ and archive prior $\mu_{\mathcal{E}} \in \mathcal{P}(\mathcal{Z}_{\text{ext}})$, the *retrieval measure* is:

$$
\nu_\omega = \arg\min_{\nu \in \mathcal{P}(\mathcal{Z}_{\text{ext}})} \left\{ \int d_{\mathbb{D}}(z, \xi) \, d\nu(\xi) + T_{\text{ret}} D_{\text{KL}}(\nu \| \mu_{\mathcal{E}}) \right\},

$$
where $T_{\text{ret}} > 0$ is the *retrieval temperature*.

*Units:* $[T_{\text{ret}}] = \text{nat}$.

*Interpretation:* This variational problem balances semantic proximity (first term) against prior plausibility (KL term). At $T_{\text{ret}} \to 0$, retrieval concentrates on the nearest neighbor; at $T_{\text{ret}} \to \infty$, it reverts to the archive prior.

:::

:::{prf:proposition} Exponential Complexity of Specificity
:label: prop-exponential-complexity-of-specificity

The volume of a geodesic ball in the Poincare disk grows exponentially with radius:

$$
\text{Vol}(B_r(z)) \sim \sinh^{d-1}(r) \sim \frac{1}{2^{d-1}} e^{(d-1)r} \quad \text{as } r \to \infty.

$$
*Proof sketch:* The hyperbolic metric has constant negative curvature $\kappa = -1$. Standard volume comparison (Bishop-Gromov) yields exponential growth. $\square$

*Interpretation:* As the agent descends toward the boundary (increasing semantic specificity), the number of accessible knowledge atoms grows exponentially. This captures the combinatorial explosion of specific facts relative to abstract concepts---compare TopoEncoder hierarchy ({ref}`sec-supervised-topology-semantic-potentials-and-metric-segmentation`).

:::

:::{prf:definition} Bulk Projection Operator
:label: def-bulk-projection-operator

The *bulk projection* $\Pi_{\text{bulk}}: \mathcal{Z}_{\text{ext}} \to \mathcal{K} \times \mathcal{Z}_n$ is defined by:

$$
\Pi_{\text{bulk}}(\xi) = \Pi_{\text{bulk}}(K, z_n, z_{\text{tex}}) := (K, z_n).

$$
*Interpretation:* This projection discards texture, retaining only control-relevant coordinates.

*Cross-reference:* This extends the internal texture exclusion of {ref}`sec-conditional-independence-and-sufficiency` to external retrieval.

:::

:::{prf:definition} Bulk-Filtered Retrieval Potential
:label: def-bulk-filtered-retrieval-potential

The *retrieval potential* is:

$$
\Psi_{\text{ret}}(z) = -\Lambda_{\text{ret}} \int_{\mathcal{Z}_{\text{ext}}} \exp\left(-\lambda \, d_{\mathbb{D}}(z, \Pi_{\text{bulk}}(\xi))\right) d\nu_\omega(\xi),

$$
with the firewall constraint:

$$
\frac{\partial \Psi_{\text{ret}}}{\partial z_{\text{tex,ext}}} \equiv 0.

$$
*Units:* $[\Psi_{\text{ret}}] = \text{nat}$, $[\Lambda_{\text{ret}}] = \text{nat}$, $[\lambda] = [z]^{-1}$.

*Cross-reference:* Compare the memory potential $\Psi_{\text{mem}}$ (Definition {prf:ref}`def-memory-potential`), which uses heat kernel rather than geodesic exponential. Both generate conservative forces.

:::

:::{prf:theorem} Stability of Retrieval Loop
:label: thm-stability-of-retrieval-loop

Under the firewall constraint (Definition {prf:ref}`def-bulk-filtered-retrieval-potential`), the retrieval force field:

$$
\mathbf{f}_{\text{ret}} = -G^{-1}\nabla_G \Psi_{\text{ret}}

$$
is smooth (Lipschitz in $z$) and independent of external texture coordinates $z_{\text{tex,ext}}$.

*Consequence:* The control loop remains stable; external texture cannot inject high-frequency gradients that would trigger Mode T.C (Labyrinthine Overfitting).

*Proof sketch:* The bulk projection $\Pi_{\text{bulk}}$ is a smooth submersion. Composition with the smooth geodesic exponential preserves smoothness. The firewall constraint ensures $\nabla_{z_{\text{tex,ext}}} \Psi_{\text{ret}} = 0$ by construction. $\square$

*Cross-reference:* This theorem extends TextureFirewallCheck (Node 29) to external retrieval. See {ref}`sec-failure-modes` for Mode T.C classification.

**Heuristic 28.3.4 (Side-Channel Texture Delivery).**
External texture $z_{\text{tex,ext}}$ is delivered to the decoder via a side channel:
1. At stopping radius $R_{\text{cutoff}}$ ({ref}`sec-the-retrieval-texture-firewall`), retrieve the full atom $\xi = (K, z_n, z_{\text{tex}})$
2. Inject $z_{\text{tex}}$ directly to decoder attention, bypassing the EoM
3. The control loop only sees $(K, z_n)$

*Interpretation:* This is the retrieval analog of "reading a document without letting its style affect your reasoning."

:::

:::{prf:definition} Retrieval-Augmented Geodesic SDE
:label: def-retrieval-augmented-geodesic-sde

The equations of motion with retrieval are:

$$
dz^k = \left[\mathcal{M}_{\text{curl}}\right]^k{}_{j}\left(-G^{j\ell}\partial_\ell(\Phi_{\text{eff}} + \Psi_{\text{mem}} + \Psi_{\text{ret}}) + u_\pi^j\right) ds - \Gamma^k_{ij}\dot{z}^i\dot{z}^j\,ds + \sqrt{2T_c}(G^{-1/2})^{kj}dW^j_s,

$$
where:
- $\Phi_{\text{eff}}$: effective potential (Definition {prf:ref}`def-effective-potential`)
- $\Psi_{\text{mem}}$: memory potential (Definition {prf:ref}`def-memory-potential`)
- $\Psi_{\text{ret}}$: retrieval potential (Definition {prf:ref}`def-bulk-filtered-retrieval-potential`)
- $\Gamma^k_{ij}$: Christoffel symbols (Definition 2.5.1, Definition 22.2.1a)
- $u_\pi^k$: policy control field (Definition {prf:ref}`def-the-control-field`)
- $T_c$: cognitive temperature ({ref}`sec-the-geodesic-baoab-integrator`)

*Cross-reference:* This extends the memory-augmented SDE (Definition {prf:ref}`def-memory-augmented-geodesic-sde`) with the retrieval term $\Psi_{\text{ret}}$.

:::

:::{prf:proposition} Superposition of Non-Local Forces
:label: prop-superposition-of-non-local-forces

The total non-local force is:

$$
\mathbf{f}_{\text{non-local}} = -G^{-1}\nabla_G(\Psi_{\text{mem}} + \Psi_{\text{ret}}),

$$
where:
- Memory force $\mathbf{f}_{\text{mem}}$ integrates over the agent's past trajectory
- Retrieval force $\mathbf{f}_{\text{ret}}$ integrates over the external archive

*Interpretation:* The agent simultaneously experiences attraction to its own memory ({ref}`sec-section-non-local-memory-as-self-interaction-functional`) and to relevant external knowledge (this section).

:::

:::{prf:definition} Retrieval Source Term
:label: def-retrieval-source-term

The Wasserstein–Fisher–Rao continuity equation with retrieval is:

$$
\partial_s \rho + \nabla \cdot (\rho \mathbf{v}) = \rho \, r_{\text{local}}(z) + \sigma_{\text{ret}}(z),

$$
where:
- $r_{\text{local}}(z)$: local mass creation rate (reward-driven, Definition {prf:ref}`def-the-wfr-action`)
- $\sigma_{\text{ret}}(z)$: retrieval source term

The retrieval source is:

$$
\sigma_{\text{ret}}(z) = \eta_{\text{ret}} \cdot \Psi_{\text{ret}}(z) \cdot \mathbf{1}[\Psi_{\text{ret}}(z) > \Psi_{\text{threshold}}],

$$
with $[\sigma_{\text{ret}}] = \text{nat}/[z]^d/\text{step}$.

*Cross-reference:* Compare {ref}`sec-wfr-dynamics-with-memory-sources`'s memory mass creation. Both mechanisms inject mass at non-local locations.

:::

:::{prf:proposition} Non-Causal Transition via Retrieval
:label: prop-non-causal-transition-via-retrieval

Mass injection at retrieved locations enables transitions without continuous geodesic paths:

$$
\rho(z', s + \Delta s) > 0 \quad \text{even if} \quad d_G(z, z') > \sup_{0 \leq \tau \leq \Delta s} \|\mathbf{v}(z, s+\tau)\| \cdot \Delta s.

$$
*Interpretation:* Retrieval teleports probability mass to semantically relevant regions, bypassing the diffusion constraint. This is the WFR-level description of "jumping to a retrieved fact."

:::

:::{prf:proposition} Optimal Non-Local Coupling
:label: prop-optimal-nonlocal-coupling

Let the control vector be $\Lambda = (\Lambda_{\text{mem}}, \Lambda_{\text{ret}})$. The optimal coupling is the fixed point of the Governor's policy $\pi_{\mathfrak{G}}$ ({prf:ref}`def-the-universal-governor`) given the diagnostic state $s_t = (\Delta_{\text{causal}}, \Omega_{\text{mem}})$.

**Control Law Derivation:**

1. **Surprise Signal:** Let $\Delta_{\text{causal}} = D_{\text{KL}}(P_{\text{int}} \| P_{\text{obs}})$ be the Interventional Gap (Node 53).

2. **Overfitting Signal:** Let $\Omega_{\text{mem}}$ be the Non-Locality Ratio ({prf:ref}`def-non-locality-ratio`, Node 43).

3. **Governor Update:** The Lyapunov descent condition $\Delta V_{\mathfrak{L}} < 0$ ({prf:ref}`def-training-lyapunov-function`) implies the following qualitative update dynamics:

$$
\begin{aligned}
\dot{\Lambda}_{\text{ret}} &\propto \alpha_1 \cdot \Delta_{\text{causal}} \\
\dot{\Lambda}_{\text{mem}} &\propto \alpha_2 \cdot (\Delta_{\text{causal}}^{\text{target}} - \Delta_{\text{causal}}) - \alpha_3 \cdot \operatorname{ReLU}(\Omega_{\text{mem}} - \Omega_{\max})
\end{aligned}

$$
where $\alpha_1, \alpha_2, \alpha_3 > 0$ are learning rates and $\Omega_{\max}$ is the maximum tolerable non-locality ratio.

*Proof sketch.* The Governor's outer objective ({prf:ref}`def-outer-problem-governor-optimization`) includes terms penalizing both prediction error (Interventional Gap) and overfitting (Non-Locality Ratio). The gradient of this objective with respect to $\Lambda$ yields the stated control law. At equilibrium, $\dot{\Lambda} = 0$, which implies a balance between reliance on memory and retrieval calibrated to the agent's surprise level. $\square$
:::

:::{prf:remark} Operational Interpretation
:label: rem-memory-retrieval-interpretation

- **If the agent is surprised by reality** ($\Delta_{\text{causal}}$ high): It must increase reliance on external truth ($\Lambda_{\text{ret}} \uparrow$).
- **If the agent is not surprised** ($\Delta_{\text{causal}}$ low): It can conserve bandwidth by relying on internal memory ($\Lambda_{\text{mem}} \uparrow$), subject to the constraint that it must not overfit ($\Omega_{\text{mem}} < \Omega_{\max}$).

This closes the joint optimization problem by reducing it to a specific instantiation of the Governor's Lyapunov stability framework ({prf:ref}`def-training-lyapunov-function`).
:::

:::{prf:theorem} Safe Retrieval Bandwidth
:label: thm-safe-retrieval-bandwidth

Let $\sigma_{\text{ret}}(z)$ be the retrieval source term in the WFR continuity equation ({prf:ref}`def-retrieval-source-term`). The latent geometry remains non-singular if and only if the total information flux satisfies:

$$
\int_{\mathcal{Z}} \left( \rho_I(z) + \sigma_{\text{ret}}(z) \right) \, d\mu_G \leq \kappa \, C_{\partial}

$$
where $C_{\partial} = \nu_D \cdot \text{Area}(\partial\mathcal{Z})/\ell_L^{D-1}$ is the boundary capacity (Definition {prf:ref}`def-holographic-coefficient`, {prf:ref}`def-levin-length`).

*Proof.*
1. **Mass Augmentation:** Retrieval modifies the bulk information density: $\tilde{\rho}_I = \rho_I + \sigma_{\text{ret}}$.

2. **Metric Response:** By the Capacity-Constrained Metric Law (Theorem {prf:ref}`thm-capacity-constrained-metric-law`), the radial metric component scales as $G_{rr} \propto (1 - \tilde{I}_{\text{bulk}}/C_{\partial})^{-1}$.

3. **Singularity:** If $\int \sigma_{\text{ret}} > C_{\partial} - I_{\text{bulk}}$, then $G_{rr} \to \infty$ at a radius $r < 1$ (the horizon moves inward).

4. **Dynamical Consequence:** The update velocity $\|v\|_G \to 0$ (Causal Stasis, {ref}`sec-causal-information-bound`). The instability manifests as the freezing of the agent's inference dynamics due to saturation of the holographic bound. $\square$
:::

:::{prf:theorem} Causal Isometry Theorem
:label: thm-causal-isometry

Let $\mathcal{M}_A$ and $\mathcal{M}_B$ be latent manifolds encoding modalities $A$ and $B$ of a common environment $\mathcal{E}$. Let $\Phi_{\text{causal}}$ be the Causal Information Potential ({ref}`sec-causal-discovery-interventional-geometry-and-the-singularity-of-action`). If both representations are **Interventionally Closed** ({prf:ref}`thm-interventional-closure`), then the induced metrics $G_A$ and $G_B$ are isometric.

*Proof.*
1. **Metric Genesis:** According to the Capacity-Constrained Metric Law ({prf:ref}`thm-capacity-constrained-metric-law`), the metric $G$ is determined by the solution to the Einstein-like equation $R_{ij} - \frac{1}{2}R G_{ij} + \Lambda G_{ij} = \kappa T_{ij}$, where the stress-energy tensor $T_{ij}$ is derived from the risk Lagrangian $\mathcal{L}_{\text{risk}}$.

2. **Risk Invariance:** The risk Lagrangian $\mathcal{L}_{\text{risk}}(V) = \frac{1}{2}\|\nabla_A V\|^2 + U(V)$ depends only on the Value function $V$ and the Causal Potential $\Psi_{\text{causal}}$.

3. **Task Invariance:** The potentials $V$ and $\Psi_{\text{causal}}$ are functions of the *causal graph* of the environment $\mathcal{E}$, which is an invariant independent of the sensory modality (pixels vs. tokens).

4. **Uniqueness:** Assuming the solution to the metric field equation is unique (guaranteed for the Poincare disk ansatz in the saturation limit), the geometries $G_A$ and $G_B$ are identical up to a diffeomorphism determined by the encoder parameterization. $\square$
:::

## 07_cognition/04_ontology.md

:::{prf:definition} Semantic Vacuum
:label: def-semantic-vacuum

Let $(\mathbb{D}, G)$ be the Poincare disk with metric $G_{ij}(z) = 4\delta_{ij}/(1-|z|^2)^2$ (Definition {prf:ref}`def-hyperbolic-volume-growth`). The **Semantic Vacuum** is the fiber

$$
\emptyset := \{z \in \mathcal{Z} : |z| = 0\} = \{0\} \times \mathcal{Z}_{\text{tex}},

$$
equipped with the following properties:

1. **$SO(D)$ Symmetry:** At $z=0$, the metric is isotropic $G(0) = 4I$ (Proposition {prf:ref}`prop-so-d-symmetry-at-origin`), and the entropic force vanishes: $F_{\text{entropy}}(0) = 0$. The system has full rotational symmetry $SO(D)$.

2. **Infrared Limit:** For any TopoEncoder scale $\tau$ ({ref}`sec-rigorous-interpretation-renormalization-group-flow`), $\lim_{\tau \to 0} z(\tau) = \emptyset$. The vacuum is the coarsest resolution.

3. **Reference Measure:** The vacuum carries the Dirac reference measure $\delta_0$ on the bulk coordinates $(K, z_n)$:

   $$
   \mu_{\emptyset} := \delta_0 \otimes \mathcal{N}(0, \sigma_{\text{tex}}^2 I),

   $$
   where the texture component is drawn from the isotropic prior (Definition {prf:ref}`def-boundary-texture-distribution` with $G^{-1}(0) = I/4$).

4. **Information Content:** At the vacuum, $U(0) = 0$ (Definition {prf:ref}`prop-isotropic-radial-expansion`), corresponding to zero information content (maximum entropy).

*Units:* $[\mu_{\emptyset}]$ is a probability measure; $[U] = \mathrm{nat}$.

*Remark (Unstable Origin).* The vacuum is not a fixed point of the radial dynamics: the entropic drift $(1-r^2)/2 > 0$ at $r=0$ implies trajectories immediately expand outward (Theorem {prf:ref}`thm-angular-symmetry-breaking`). The $SO(D)$ angular symmetry is broken by the policy or thermal fluctuations during this expansion.

:::

:::{prf:lemma} Default Mapping to Vacuum
:label: lem-default-mapping-to-vacuum

Let $\{q_i\}_{i=1}^{N_c}$ be the chart query bank (Definition {prf:ref}`def-attentive-routing-law`) and assume the queries are **centered**: $\sum_{i=1}^{N_c} q_i = 0$. Then for any key $k(x)$ such that all inner products are equal---$\langle q_i, k(x) \rangle = c$ for all $i$---the router weights are uniform:

$$
w_i(x) = \frac{1}{N_c} \quad \forall i \in \{1, \ldots, N_c\}.

$$
The resulting soft codebook embedding is the **barycenter**:

$$
z_q(x) = \sum_{i=1}^{N_c} w_i(x) e_{i, K_{\text{code},i}(x)} = \frac{1}{N_c} \sum_{i=1}^{N_c} e_{i,*},

$$
which equals $0$ if the per-chart codebooks are also centered ($\sum_c e_{i,c} = 0$ for each chart $i$).

*Proof.* From Definition {prf:ref}`def-attentive-routing-law`, $w_i(x) = \exp(\langle q_i, k(x)\rangle/\sqrt{d}) / \sum_j \exp(\langle q_j, k(x)\rangle/\sqrt{d})$. If $\langle q_i, k(x)\rangle = c$ for all $i$, then $w_i = e^{c/\sqrt{d}} / (N_c \cdot e^{c/\sqrt{d}}) = 1/N_c$. The soft code $z_q$ is the weighted sum; under centering, this is the barycenter at $0$. $\square$

*Interpretation.* When the observation $x$ is equally compatible with all charts (or incompatible with all), the router outputs uniform weights. Under centering, this maps to the vacuum---the maximum-entropy state in latent space.

**Architectural Requirement 30.1.3 (Codebook Centering).** To ensure the vacuum is reachable, initialize and regularize codebooks to satisfy $\sum_i q_i = 0$ and $\sum_c e_{i,c} = 0$. This can be enforced via:

$$
\mathcal{L}_{\text{center}} := \left\|\sum_{i=1}^{N_c} q_i\right\|^2 + \sum_{i=1}^{N_c} \left\|\sum_{c=1}^{N_v} e_{i,c}\right\|^2.

$$
:::

:::{prf:definition} Ontological Stress
:label: def-ontological-stress

Let $(K_t, z_{n,t}, z_{\text{tex},t})$ be the agent's state at time $t$ (Definition {prf:ref}`def-bounded-rationality-controller`). The **Ontological Stress** is the conditional mutual information:

$$
\Xi := I(z_{\text{tex},t}; z_{\text{tex},t+1} \mid K_t, z_{n,t}, K^{\text{act}}_t),

$$
where $I(\cdot;\cdot|\cdot)$ denotes conditional mutual information in nats.

*Units:* $[\Xi] = \mathrm{nat}$ (dimensionless information).

*Interpretation.* By Axiom {prf:ref}`ax-bulk-boundary-decoupling` (Bulk-Boundary Decoupling), texture should be unpredictable -- a white-noise residual. If $\Xi > 0$, then texture at time $t$ predicts texture at time $t+1$, conditional on the macro-state and action. This violates the partition condition: the texture channel contains structure that should have been captured by $(K, z_n)$ but was not. The agent's ontology is **too coarse**.

*Cross-reference.* Compare with the closure defect $I(K_{t+1}; Z_t \mid K_t, K^{\text{act}}_t)$ ({ref}`sec-conditional-independence-and-sufficiency`). Ontological Stress is the dual: predictability *within* texture rather than *from* texture to macro.

:::

:::{prf:theorem} Vacuum Concentration Under Unknown Unknowns
:label: thm-vacuum-concentration-under-unknown-unknowns

Let $\mathcal{F}[p, \pi]$ be the entropy-regularized objective (Definition {prf:ref}`def-entropy-regularized-objective-functional`):

$$
\mathcal{F}[p, \pi] = \int_{\mathcal{Z}} p(z) \Big( V(z) - \tau H(\pi(\cdot|z)) \Big) d\mu_G.

$$
If the value function $V$ is **uninformative** in a region $\Omega \subset \mathcal{Z}$ -- i.e., $\nabla_A V|_\Omega \approx 0$ and $\nabla^2 V|_\Omega \approx 0$ -- then the entropy term dominates and the optimal belief concentrates toward maximum-entropy configurations:

$$
p^*(z) \propto \exp\left(-\frac{V(z)}{\tau}\right) \xrightarrow{\nabla_A V \to 0} \text{uniform on } \Omega.

$$
In the Poincare disk geometry, the maximum-entropy state is the vacuum $z = 0$.

*Proof sketch.* The stationary distribution of the Langevin dynamics (Definition {prf:ref}`def-bulk-drift-continuous-flow`) is $p(z) \propto \exp(-\Phi_{\text{eff}}(z)/T_c)$ where $\Phi_{\text{eff}}$ includes the hyperbolic potential $U(z)$. When $V$ is flat, $\Phi_{\text{eff}} \approx U(z) = -2\operatorname{artanh}(|z|)$, which is maximized at $z = 0$. The entropic drift $-\nabla_G U$ vanishes at the origin (Proposition {prf:ref}`def-hyperbolic-information-potential`), making it the unique stationary point. $\square$

*Interpretation.* When encountering observations outside the learned structure, the MaxEnt policy concentrates at the vacuum, correctly representing maximum uncertainty.

*Remark (Capacity Tension).* If belief mass accumulates at the vacuum such that bulk information $I_{\mathrm{bulk}}$ approaches the boundary capacity $C_\partial$ (the Capacity-Constrained Metric Law, Theorem {prf:ref}`thm-capacity-constrained-metric-law`), the current chart structure is insufficient. This tension -- high information density at a single point -- indicates fission is required to distribute the representational load.

:::

:::{prf:axiom} Ontological Expansion Principle
:label: ax-ontological-expansion-principle

The agent should expand its chart structure (increase $N_c$) if and only if the expected value improvement exceeds the complexity cost:

$$
\mathbb{E}\left[\Delta V \mid \text{fission}\right] > \mathcal{C}_{\text{complexity}}(N_c \to N_c + 1),

$$
where $\Delta V$ is the value gain from finer discrimination and $\mathcal{C}_{\text{complexity}}$ is measured in nats (to match units with value).

*Remark.* This is the MDL/rate-distortion principle ({ref}`sec-the-shutter-as-a-vq-vae`) applied to ontology: expand only if the distortion reduction exceeds the rate increase.

:::

:::{prf:theorem} Fission Criterion
:label: thm-fission-criterion

Let $\Xi$ be the Ontological Stress (Definition {prf:ref}`def-ontological-stress`) and let $\Xi_{\text{crit}} > 0$ be a threshold. Let $\Delta V_{\text{proj}}$ be the projected value improvement from splitting the highest-stress chart. The fission criterion is:

$$
\text{Fission} \iff \Xi > \Xi_{\text{crit}} \quad \text{AND} \quad \Delta V_{\text{proj}} > \mathcal{C}_{\text{complexity}}.

$$
*Units:* All quantities are in nats. The complexity cost $\mathcal{C}_{\text{complexity}}(N_c \to N_c + 1)$ includes the entropy increase $\log((N_c+1)/N_c)$ from the expanded codebook plus any regularization penalty on parameter count.

:::

:::{prf:definition} Query Fission
:label: def-query-fission

Let $q_i \in \mathbb{R}^d$ be a chart query vector ({ref}`sec-tier-the-attentive-atlas`) with associated codebook $\{e_{i,c}\}_{c=1}^{N_v}$. A **query fission** replaces $q_i$ with two daughter queries:

$$
q_i \mapsto \{q_i^+, q_i^-\} := \{q_i + \epsilon u, q_i - \epsilon u\},

$$
where $u \in \mathbb{R}^d$ is the **fission direction** (unit vector) and $\epsilon > 0$ is the **fission amplitude**.

The daughter codebooks are initialized as copies:

$$
e_{i^\pm, c} := e_{i, c} \quad \forall c \in \{1, \ldots, N_v\}.

$$
*Selection of fission direction.* The optimal $u$ maximizes the variance of router assignments under the new queries:

$$
u^* = \arg\max_{\|u\|=1} \text{Var}_{x \sim \mathcal{D}}\left[\langle k(x), u \rangle \mid w_i(x) > 1/N_c\right],

$$
i.e., the principal component of keys within the chart's Voronoi cell.

:::

:::{prf:theorem} Supercritical Pitchfork Bifurcation for Charts {cite}`strogatz2015nonlinear`
:label: thm-supercritical-pitchfork-bifurcation-for-charts

The query fission dynamics exhibit a **supercritical pitchfork bifurcation**. Let $r := \|q_i^+ - q_i^-\|/2 = \epsilon$ be the half-separation of daughter queries. The radial evolution satisfies:

$$
\frac{dr}{ds} = (\Xi - \Xi_{\text{crit}}) r - \alpha r^3 + \sigma\xi,

$$
where:
- $\Xi - \Xi_{\text{crit}}$ is the **bifurcation parameter** (supercritical when positive)
- $\alpha > 0$ is a stabilizing cubic coefficient (from competition for training data)
- $\sigma\xi$ is noise from stochastic gradient updates
- $s$ is the training step (flow time)

**Phase Transition:**
1. **Sub-critical ($\Xi < \Xi_{\text{crit}}$):** $r=0$ is the unique stable fixed point. The daughters collapse back to the parent ($r \to 0$).
2. **Super-critical ($\Xi > \Xi_{\text{crit}}$):** $r=0$ becomes unstable. The daughters separate toward a new equilibrium:

   $$
   r^* = \sqrt{\frac{\Xi - \Xi_{\text{crit}}}{\alpha}}.

   $$
*Proof.* The dynamics derive from the effective potential:

$$
\Phi_{\text{fission}}(r) = -\frac{(\Xi - \Xi_{\text{crit}})}{2} r^2 + \frac{\alpha}{4} r^4,

$$
which has the standard pitchfork normal form. For $\Xi > \Xi_{\text{crit}}$, the origin has $\Phi_{\text{fission}}''(0) = -(\Xi - \Xi_{\text{crit}}) < 0$, becoming unstable. Stable minima appear at $r = \pm r^*$. The cubic term arises from router saturation: as daughters separate, they compete for data, and the loss landscape penalizes excessive separation. $\square$

*Critical Temperature Constraint.* The barrier height of the effective potential is $\Delta\Phi = (\Xi - \Xi_{\text{crit}})^2 / (4\alpha)$. Thermal fluctuations can restore symmetry (collapse daughters) if cognitive temperature ({prf:ref}`def-cognitive-temperature`) exceeds this barrier. For stable fission, require:

$$
T_c < \frac{(\Xi - \Xi_{\text{crit}})^2}{4\alpha}.

$$
:::

:::{prf:definition} Ontological Ricci Flow
:label: def-ontological-ricci-flow

Let $G_{ij}(z, s)$ be the capacity-constrained metric (Theorem {prf:ref}`thm-capacity-constrained-metric-law`) parameterized by flow time $s$. Define the **local stress field** $\Xi(z) := \mathbb{E}[\Xi \mid K = k(z)]$, where $k(z)$ is the chart containing $z$. The **Ontological Ricci Flow** is:

$$
\frac{\partial G_{ij}}{\partial s} = -2\left(R_{ij} - \frac{1}{2}R\, G_{ij} + \Lambda G_{ij} - \kappa T_{ij}\right) + \nu \nabla_i \nabla_j \Xi(z),

$$
where:
- $R_{ij}$ is the Ricci curvature tensor, $R = G^{ij}R_{ij}$ the scalar curvature
- $\Lambda, \kappa$ are constants from Theorem {prf:ref}`thm-capacity-constrained-metric-law`
- $T_{ij}$ is the risk tensor
- $\nu > 0$ is the stress-curvature coupling constant

*Units:* $[\partial G / \partial s] = [z]^{-2}$; $[\Xi] = \text{nat}$; $[\nabla_i \nabla_j \Xi] = \text{nat}/[z]^2$.

*Interpretation.* The first term drives the metric toward the capacity-constrained fixed point. The second term $\nu \nabla_i \nabla_j \Xi$ introduces curvature in regions of high stress gradient, expanding the metric where new distinctions are needed.

:::

:::{prf:proposition} Fixed Points of Ontological Ricci Flow
:label: prop-fixed-points-of-ontological-ricci-flow

The flow has fixed points when:
1. The capacity-constrained metric law is satisfied: $R_{ij} - \frac{1}{2}R\,G_{ij} + \Lambda G_{ij} = \kappa T_{ij}$
2. The Ontological Stress has vanishing Hessian: $\nabla_i \nabla_j \Xi = 0$

Condition (2) is satisfied when either $\Xi$ is constant (uniform stress) or $\Xi = 0$ (no stress).

*Computational Proxy.* In practice, we do not solve the Ricci flow PDE. The squared residual of the fixed-point condition can be used as a regularization loss:

$$
\mathcal{L}_{\text{Ricci}} := \left\|R_{ij} - \frac{1}{2}R\,G_{ij} + \Lambda G_{ij} - \kappa T_{ij}\right\|_F^2 + \nu^2 \|\nabla_i \nabla_j \Xi\|_F^2,

$$
encouraging the learned metric to satisfy the capacity constraint while penalizing stress gradients.

:::

:::{prf:definition} Ontological Redundancy
:label: def-ontological-redundancy

Let $K_i$ and $K_j$ be two charts with associated belief distributions $\mu_i, \mu_j$, transition models $\bar{P}_i, \bar{P}_j$, and value functions $V_i, V_j$. Their **ontological redundancy** is:

$$
\Upsilon_{ij} := \exp\left(-\left[ d_{\text{WFR}}(\mu_i, \mu_j) + D_{\mathrm{KL}}(\bar{P}_i \| \bar{P}_j) + \|V_i - V_j\|_G^2 \right]\right)

$$
where:
- $d_{\text{WFR}}(\mu_i, \mu_j)$ is the Wasserstein-Fisher-Rao distance ({prf:ref}`def-the-wfr-action`) between belief distributions,
- $D_{\mathrm{KL}}(\bar{P}_i \| \bar{P}_j) := \mathbb{E}_{k \sim \mu_i}\left[ D_{\mathrm{KL}}(\bar{P}(\cdot|k, a) \| \bar{P}_j(\cdot|k, a)) \right]$ is the mean predictive divergence,
- $\|V_i - V_j\|_G^2 := \mathbb{E}_{z \sim \mu_i}\left[ (V_i(z) - V_j(z))^2 \right]$ is the mean squared value divergence.

*Units:* Dimensionless; $\Upsilon_{ij} \in [0, 1]$.

*Interpretation:* $\Upsilon_{ij} \to 1$ implies the charts are functionally redundant: they occupy similar regions of belief space, predict similar futures, and assign similar values. $\Upsilon_{ij} \to 0$ implies they are functionally distinct.
:::

:::{prf:definition} Discrimination Gain
:label: def-discrimination-gain

The **Discrimination Gain** $G_\Delta(i, j)$ is the mutual information the agent loses about observations by merging charts $i$ and $j$:

$$
G_\Delta(i, j) := I(X; \{K_i, K_j\}) - I(X; K_{i \cup j})

$$
where $K_{i \cup j}$ is the merged chart that routes observations previously assigned to $K_i$ or $K_j$ to a single index.

*Units:* nat.

*MDL interpretation:* $G_\Delta$ is the increase in **distortion** (description length) resulting from the merge. If $G_\Delta \approx 0$, the distinction between $K_i$ and $K_j$ carries negligible information about the observation stream.
:::

:::{prf:lemma} Redundancy-Gain Relationship
:label: lem-redundancy-gain

Under the assumption that charts partition the observation space and the encoder is deterministic given observation $x$:

$$
G_\Delta(i, j) \leq H(K_i, K_j) - H(K_{i \cup j}) = \log 2 - H(K_i | K_j) \cdot \mathbb{I}[\Upsilon_{ij} < 1]

$$
When $\Upsilon_{ij} \to 1$, the bound tightens: $G_\Delta \to 0$.

*Proof sketch.* The discrimination gain is upper-bounded by the entropy reduction from merging. When charts are redundant ($\Upsilon_{ij} \to 1$), they route to the same observations with high probability, so the conditional entropy $H(K_i | K_j) \to 0$. $\square$
:::

:::{prf:axiom} Ontological Simplification Principle
:label: ax-ontological-simplification

The agent shall reduce ontological complexity when the expected value of maintaining a distinction is negative:

$$
\mathcal{C}_{\text{saved}}(N_c \to N_c - 1) > G_\Delta(i, j) + \mathbb{E}[\Delta V \mid \text{no fusion}]

$$
where $\mathcal{C}_{\text{saved}}$ is the metabolic savings from eliminating a chart.

*Remark.* This is the dual of {prf:ref}`ax-ontological-expansion-principle` (Ontological Expansion Principle). Both derive from the same MDL objective: minimize description length plus expected regret.
:::

:::{prf:theorem} Fusion Criterion
:label: thm-fusion-criterion

Charts $i$ and $j$ shall be merged if and only if:

$$
G_\Delta(i, j) < \mathcal{C}_{\text{complexity}}(N_c) - \mathcal{C}_{\text{complexity}}(N_c - 1) + \epsilon_{\text{hysteresis}}

$$
where:
- $\mathcal{C}_{\text{complexity}}(N_c) = \log N_c + \lambda_{\text{param}} |\theta_{\text{chart}}|$ is the metabolic cost of maintaining $N_c$ charts ({ref}`sec-the-fission-criterion`),
- $\epsilon_{\text{hysteresis}} > 0$ is a hysteresis constant preventing oscillatory fission-fusion ("ontological churn").

*Proof sketch.* By {prf:ref}`ax-ontological-simplification`, fusion is justified when saved complexity exceeds lost discrimination. The complexity difference is:

$$
\mathcal{C}_{\text{complexity}}(N_c) - \mathcal{C}_{\text{complexity}}(N_c - 1) = \log\frac{N_c}{N_c - 1} + \lambda_{\text{param}} |\theta_{\text{chart}}|

$$
The hysteresis term $\epsilon_{\text{hysteresis}}$ breaks the symmetry with Fission, ensuring that a chart is not immediately re-created after being destroyed. $\square$

*Remark (Units):* All terms are in nats. The criterion is dimensionally consistent.
:::

:::{prf:definition} Query Coalescence
:label: def-query-coalescence

Given charts $i, j$ satisfying the Fusion Criterion ({prf:ref}`thm-fusion-criterion`), the merged query is the **usage-weighted barycenter**:

$$
q_{\text{merged}} := \frac{\bar{w}_i q_i + \bar{w}_j q_j}{\bar{w}_i + \bar{w}_j}

$$
where $\bar{w}_k := \mathbb{E}[w_k(x)]$ is the historical routing weight from the Attentive Atlas ({prf:ref}`def-attentive-routing-law`).

*Interpretation:* The more frequently used chart contributes more to the merged query position. This preserves the routing behavior for the majority of observations.
:::

:::{prf:definition} Fiber Reconciliation
:label: def-fiber-reconciliation

Let $L_{j \to i}: \mathcal{F}_j \to \mathcal{F}_i$ be the factorized jump operator ({prf:ref}`def-factorized-jump-operator`). For an observation $x$ previously assigned to chart $j$ with nuisance coordinates $z_n^{(j)}$, the reconciled coordinates in chart $i$ are:

$$
z_n^{(i, \text{reconciled})} := L_{j \to i}(z_n^{(j)}) = A_i(B_j z_n^{(j)} + c_j) + d_i

$$
where $B_j$ is the chart-to-global encoder and $A_i$ is the global-to-chart decoder.

*Codebook reconciliation:* The codebook entries of chart $j$ are projected into chart $i$'s Voronoi structure. Entries that fall within existing Voronoi cells of chart $i$ are absorbed; entries that create new structure may be retained if codebook capacity permits.
:::

:::{prf:theorem} Subcritical Pitchfork for Fusion
:label: thm-subcritical-pitchfork-fusion

Let $r(s) := \|q_i(s) - q_j(s)\|$ be the query separation at computation time $s$. During fusion, the dynamics become:

$$
\frac{dr}{ds} = -(\Upsilon_{ij} - \Upsilon_{\text{crit}}) r - \alpha r^3 + \sigma\xi(s)

$$
where:
- $\Upsilon_{\text{crit}} \in (0, 1)$ is the critical redundancy threshold,
- $\alpha > 0$ is the cubic stabilization coefficient,
- $\sigma\xi(s)$ is white noise with intensity $\sigma$.

When $\Upsilon_{ij} > \Upsilon_{\text{crit}}$:
1. The linear term is **negative** (attractive toward $r = 0$).
2. $r = 0$ becomes the **unique stable attractor**.
3. The queries "fall into each other" until they merge.

*Contrast with Fission ({prf:ref}`thm-supercritical-pitchfork-bifurcation-for-charts`):*

| Property                | Fission (Supercritical)          | Fusion (Subcritical)                |
|:------------------------|:---------------------------------|:------------------------------------|
| Linear term sign        | $+\mu r$ (repulsive from origin) | $-\mu r$ (attractive to origin)     |
| Trigger                 | $\Xi > \Xi_{\text{crit}}$        | $\Upsilon > \Upsilon_{\text{crit}}$ |
| Stable fixed points     | $r^* = \pm\sqrt{\mu/\alpha}$     | $r^* = 0$                           |
| Physical interpretation | Charts repel and separate        | Charts attract and merge            |

*Proof sketch.* The bifurcation structure follows from standard dynamical systems theory {cite}`strogatz2018nonlinear`. The key insight is that Fission and Fusion are **dual bifurcations**: Fission breaks $\mathbb{Z}_2$ symmetry (one chart to two); Fusion restores it (two charts to one). The sign flip in the linear term corresponds to the duality between expansion ($\Xi$) and contraction ($\Upsilon$) forces. $\square$
:::

:::{prf:definition} Node 54 --- FusionReadinessCheck
:label: node-fusion-readiness-check

**Component:** Atlas (Chart Router)

**Type:** Metabolic Efficiency

**Interpretation:** Are any two charts functionally redundant?

**Proxy:**

$$
\text{FusionReady} := \mathbb{I}\left[ \max_{i \neq j} \Upsilon_{ij} > \Upsilon_{\text{crit}} \right]

$$
**Computational cost:** $O(N_c^2)$ pairwise comparisons.

**Trigger condition:** Two or more charts have redundancy exceeding threshold.

**Remediation:**
1. Identify most redundant pair $(i^*, j^*) = \arg\max_{i \neq j} \Upsilon_{ij}$.
2. Verify Fusion Criterion ({prf:ref}`thm-fusion-criterion`).
3. If satisfied, initiate subcritical bifurcation dynamics.
4. Execute Query Coalescence and Fiber Reconciliation.
5. Decrement chart count: $N_c \to N_c - 1$.
:::

:::{prf:definition} Node 55 --- CodebookLivenessCheck
:label: node-codebook-liveness-check

**Component:** Codebook (VQ Layer)

**Type:** Dead Code Detection

**Interpretation:** Are any code indices unused?

**Proxy:**

$$
\text{DeadCodeDetected} := \mathbb{I}\left[ \min_k P(K = k) < \epsilon_{\text{dead}} \right]

$$
where $P(K = k)$ is the empirical usage frequency of code $k$ over a trailing window.

**Computational cost:** $O(|\mathcal{K}|)$.

**Trigger condition:** Code usage falls below minimum threshold (default $\epsilon_{\text{dead}} = 10^{-4}$).

**Remediation:** Execute Lazarus Protocol ({prf:ref}`alg-lazarus`).

*Connection to existing diagnostics:* This node operationalizes the dead-code tolerance constraint from {ref}`sec-calibrating-tolerances`: $H(K) \geq \log((1 - \rho_{\text{dead}})|\mathcal{K}|)$.
:::

:::{prf:definition} Intra-Symbol Variance (Geometric Tension)
:label: def-intra-symbol-variance

For code $e_k$ in chart $i$, the **geometric tension** is:

$$
\sigma_k^2 := \mathbb{E}\left[ \|z_e - e_k\|^2 \;\Big|\; \text{VQ}(z_e) = k \right]

$$
where $z_e$ is the pre-quantized encoder output.

*Units:* $[z]^2$ (squared latent units).

*Interpretation:* High $\sigma_k^2$ indicates the symbol is overloaded---its Voronoi cell contains multiple distinct clusters that should be separated.
:::

:::{prf:definition} Functional Indistinguishability
:label: def-functional-indistinguishability

Two symbols $k_1, k_2$ within the same chart are fusion candidates if the **policy divergence** and **value gap** are negligible:

$$
\mathcal{D}_f(k_1, k_2) := D_{\mathrm{KL}}\left( \pi(\cdot | k_1) \| \pi(\cdot | k_2) \right) + |V(k_1) - V(k_2)|

$$
If $\mathcal{D}_f(k_1, k_2) < \epsilon_{\text{indist}}$, the distinction provides no **control authority**.

*Units:* nat.

*Interpretation:* Symbols are functionally indistinguishable when the policy and value function treat them identically.
:::

:::{prf:algorithm} Lazarus Reallocation
:label: alg-lazarus

**Input:** Dead code $k_{\text{dead}}$ with $P(K = k_{\text{dead}}) < \epsilon_{\text{dead}}$.

**Procedure:**
1. Find the most stressed symbol:

   $$
   k_{\text{stressed}} := \arg\max_k \sigma_k^2

   $$
2. Perform Symbol Fission on $k_{\text{stressed}}$, reusing index $k_{\text{dead}}$:
   - Compute split direction $v_1$ from $\Sigma_{k_{\text{stressed}}}$.
   - Set $e_{k_{\text{dead}}} := e_{k_{\text{stressed}}} + \epsilon v_1$.
   - Update $e_{k_{\text{stressed}}} := e_{k_{\text{stressed}}} - \epsilon v_1$.
3. Update Voronoi cells: The new code inherits half of $k_{\text{stressed}}$'s cell.

**Effect:** Vocabulary migrates to high-information-density regions. Dead codes are "resurrected" where they are needed.

*Connection to existing constraints:* This implements the anti-collapse regularizer from {ref}`sec-calibrating-tolerances`: $\lambda_{\text{use}} D_{\mathrm{KL}}(\hat{p}(K) \| \text{Unif}(\mathcal{K}))$.
:::

:::{prf:definition} Symbolic Voronoi Partition
:label: def-voronoi-partition

Let $\mathcal{Z}_i$ be the continuous fiber associated with chart $i$. The codebook $\mathcal{C}_i = \{e_{i,k}\}_{k=1}^{N_v}$ induces a partition $\{\mathcal{V}_k\}$ of $\mathcal{Z}_i$ via:

$$
\mathcal{V}_k := \left\{ z \in \mathcal{Z}_i : d_G(z, e_k) \leq d_G(z, e_j) \;\forall j \neq k \right\}

$$
The probability mass of symbol $k$ is the measure of its Voronoi cell:

$$
P(k) := \int_{\mathcal{V}_k} p(z)\, d\mu_G(z)

$$
where $d\mu_G = \sqrt{\det G}\, dz$ is the Riemannian volume form.
:::

:::{prf:definition} Local Distortion Functional
:label: def-local-distortion

The **local distortion** of symbol $k$ quantifies the representational error within its Voronoi cell:

$$
\mathcal{D}_k := \int_{\mathcal{V}_k} d_G(z, e_k)^2\, p(z)\, d\mu_G(z)

$$
*Units:* $[z]^2$ (weighted squared geodesic distance).

*Relation to geometric tension:* $\mathcal{D}_k = P(k) \cdot \sigma_k^2$, where $\sigma_k^2$ is the intra-symbol variance ({prf:ref}`def-intra-symbol-variance`).
:::

:::{prf:definition} Symbol Utility Functional
:label: def-symbol-utility

The **utility** $U_k$ of symbol $k$ measures its contribution to control authority and predictive accuracy:

$$
U_k := P(k) \cdot I(K=k; A) + P(k) \cdot I(K=k; K_{t+1})

$$
where:
- $I(K=k; A)$ is the mutual information between symbol activation and action selection,
- $I(K=k; K_{t+1})$ is the mutual information between symbol activation and next-state prediction.

*Units:* nat.

*Interpretation:* A symbol with $U_k \approx 0$ neither influences actions nor aids prediction---it is **semantically dead** regardless of its usage frequency.
:::

:::{prf:theorem} Optimal Reallocation Gradient
:label: thm-reallocation-gradient

Let $k_{\text{dead}}$ satisfy $U_{k_{\text{dead}}} < \epsilon_U$ and let $k_{\text{stressed}}$ satisfy $\mathcal{D}_{k_{\text{stressed}}} = \max_k \mathcal{D}_k$. The expected reduction in global distortion per reallocated code is:

$$
\frac{\delta \mathcal{D}}{\delta N_{\text{codes}}} \approx \frac{\mathcal{D}_{k_{\text{stressed}}}}{H(K = k_{\text{stressed}})}

$$
*Proof sketch.* In the high-resolution limit of vector quantization (Zador's theorem {cite}`zador1982asymptotic`), distortion scales as $\mathcal{D} \propto N_v^{-2/d}$ where $d$ is the latent dimension. Reallocating a code from a zero-utility region to a high-distortion region maximizes the gradient of the distortion functional. The denominator $H(K = k_{\text{stressed}})$ normalizes by the information content of the target symbol. $\square$
:::

:::{prf:corollary} The Bimodal Instability Theorem (Fission Trigger)
:label: cor-bimodal-instability

Let $K$ be a macro-symbol with associated policy $\pi(\cdot|K)$. The **Structural Stability** of $K$ is inversely proportional to its Varentropy.

If the policy $\pi(\cdot|K)$ is a mixture of two disjoint, equally weighted strategies (a "Buridan's Ass" scenario on a value ridge), the Varentropy satisfies:

$$
V_H(K) = \frac{1}{4}\left(\frac{\Delta Q}{T_c}\right)^2,

$$
where $\Delta Q = |Q_1 - Q_2|$ is the value gap between the modes. In the limit of distinct modes ($\Delta Q \gg T_c$), $V_H$ is maximized, whereas for a uniform (maximum entropy) distribution, $V_H = 0$.

*Units:* $\mathrm{nat}^2$.

**Refined Fission Criterion:**
The **Geometric Tension** $\sigma_k^2$ (Definition {prf:ref}`def-intra-symbol-variance`) is rigorously generalized by the **Varentropy Excess**:

$$
\text{Fission}(K) \iff V_H(K) > \mathcal{V}_{\text{crit}} \quad \text{AND} \quad H(K) > H_{\text{noise}}.

$$
**Interpretation:**
- **High $H$, Low $V_H$:** Aleatoric Uncertainty (Noise/Fog). The distribution is flat. *Action:* Smoothing/Integration.
- **High $H$, High $V_H$:** Epistemic Conflict (Bifurcation). The distribution is multimodal. *Action:* Topological Fission (Node 50).

*Proof:* See Appendix {ref}`E.9 <sec-appendix-e-proof-of-corollary-bimodal-instability>`.

:::

:::{prf:proposition} Equipartition of Meaning
:label: prop-equipartition

At metabolic equilibrium, the marginal utility per bit is uniform across the ontological hierarchy:

$$
\frac{\partial U}{\partial H(K_{\text{chart}})} \approx \frac{\partial U}{\partial H(K_{\text{code}})} \approx \text{const.}

$$
where $U$ is the total utility functional (value minus complexity cost).

*Interpretation:* The agent allocates representational capacity such that one additional bit of chart-level information provides the same marginal value as one additional bit of symbol-level information. This is the information-theoretic analogue of thermodynamic equipartition.
:::

:::{prf:theorem} Thermodynamic Lower Bound on Hysteresis
:label: thm-thermodynamic-hysteresis-bound

Let $\mathcal{C}$ be a cycle of ontological operations consisting of a fission event $N_c \to N_c + 1$ followed immediately by a fusion event $N_c + 1 \to N_c$. Let $T_c$ be the cognitive temperature and $\mathcal{W}_{\text{comp}}$ be the metabolic work of parameter instantiation. To satisfy the generalized Second Law of Thermodynamics for open cognitive systems (Theorem {prf:ref}`thm-generalized-landauer-bound`), the hysteresis threshold must satisfy:

$$
\epsilon_{\text{hysteresis}} \geq \frac{1}{\beta_{\text{eff}}} \left( \Delta H_{\text{Shannon}} + \frac{1}{T_c}\mathcal{W}_{\text{comp}} \right)

$$
where $\beta_{\text{eff}} = 1/T_c$ is the inverse cognitive temperature and $\Delta H_{\text{Shannon}}$ is the entropy reduction associated with the discarded distinction.

*Proof.*
Consider the free energy functional $\mathcal{F} = E - T_c S$.

1. **Fission Cost:** The creation of a new chart requires initializing a set of parameters $\theta_{\text{new}}$. By Landauer's Principle ({ref}`Landauer's Principle <pi-landauer-principle>`), the erasure of the previous random state of these memory units to a low-entropy initialization requires work $\mathcal{W}_{\text{init}} \geq k T_c \ln 2 \cdot |\theta_{\text{new}}|$.

2. **Fusion Cost:** The merger of two charts implies the erasure of the mutual information $I(X; \{K_i, K_j\}) - I(X; K_{i \cup j})$, defined as the Discrimination Gain $G_\Delta$ ({prf:ref}`def-discrimination-gain`). This is an irreversible logical operation, dissipating heat $Q_{\text{fus}} \geq T_c G_\Delta$.

3. **Cycle Condition:** For the cycle $\mathcal{C}$ to be non-spontaneous (preventing chattering), the total free energy change must be positive. The Governor imposes a metabolic efficiency constraint $\eta_{\text{ROI}} > \eta_{\min}$ ({ref}`sec-theory-of-meta-stability-the-universal-governor-as-homeostatic-controller`).

4. **Derivation:** The utility gain of the cycle is zero (the topology is unchanged). The cost is $\mathcal{W}_{\text{init}} + Q_{\text{fus}}$. For the cycle to be rejected by the Fusion Criterion ({prf:ref}`thm-fusion-criterion`), the hysteresis term must exceed the minimum metabolic dissipation of the cycle:

$$
\epsilon_{\text{hysteresis}} \geq \inf_{\mathcal{C}} \oint \dot{\mathcal{M}}(s) ds

$$
Substituting the Landauer bound yields the stated inequality. $\square$
:::

:::{prf:definition} Hyperbolic Frechet Mean for Query Coalescence
:label: def-hyperbolic-frechet-coalescence

Let $\{q_i\}_{i=1}^k \subset \mathbb{D}$ be a set of chart query vectors with associated usage weights $\bar{w}_i := \mathbb{E}[w_i(x)]$ from the Attentive Atlas ({prf:ref}`def-attentive-routing-law`). The **Intrinsic Merged Query** is:

$$
q_{\text{merged}} := \operatorname*{arg\,min}_{q \in \mathbb{D}} \sum_{i=1}^k \bar{w}_i \cdot d^2_{\mathbb{D}}(q, q_i),

$$
where $d_{\mathbb{D}}(x, y) = \operatorname{arccosh}\left(1 + \frac{2\|x-y\|^2}{(1-\|x\|^2)(1-\|y\|^2)}\right)$ is the hyperbolic distance.

*Units:* $[q_{\text{merged}}] = [q_i]$ (dimensionless in the unit disk).

*Cross-reference:* This definition supersedes {prf:ref}`def-query-coalescence` for hyperbolic embeddings.
:::

:::{prf:theorem} Existence and Uniqueness of Fusion Center
:label: thm-frechet-fusion-uniqueness

Since the Poincare disk $(\mathbb{D}, G)$ is a complete, simply connected Riemannian manifold with non-positive sectional curvature ($K=-1$), it is a Hadamard space (global CAT(0) space). The squared distance function $d^2_{\mathbb{D}}(\cdot, y)$ is strictly convex. Therefore, the functional $F(q) = \sum \bar{w}_i d^2_{\mathbb{D}}(q, q_i)$ admits a unique global minimizer.

*Proof.* By Cartan's theorem on Hadamard manifolds, the distance function from any point is strictly convex along geodesics. The weighted sum of strictly convex functions is strictly convex, ensuring the minimizer exists and is unique. $\square$
:::

:::{prf:remark} Computational Algorithm
:label: rem-frechet-algorithm

The minimizer can be computed via Riemannian gradient descent:

$$
q_{t+1} = \operatorname{Exp}_{q_t}\left( -\eta \sum_i \bar{w}_i \operatorname{Log}_{q_t}(q_i) \right)

$$
where:
- $\operatorname{Exp}_p: T_p\mathbb{D} \to \mathbb{D}$ is the exponential map at $p$
- $\operatorname{Log}_p: \mathbb{D} \to T_p\mathbb{D}$ is the logarithmic map (inverse of exponential)

For the Poincare disk, these have closed-form expressions via Mobius operations ({ref}`sec-bulk-boundary-independence`).

*Complexity:* $O(k \cdot d)$ per iteration, where $k$ is the number of charts being merged and $d$ is the embedding dimension.
:::

:::{prf:theorem} Fission Inhibition Corollary
:label: thm-fission-inhibition

Let $\mathcal{E}^{(\ell)}$ be the encoder at scale $\ell$. A Topological Fission event at layer $\ell$ (increasing chart count $N_c^{(\ell)} \to N_c^{(\ell)}+1$) strictly reduces the probability of fission at layer $\ell+1$.

*Proof.*
1. **Residual Coupling:** The input to layer $\ell+1$ is the normalized residual of layer $\ell$: $x^{(\ell+1)} = z_{\text{tex}}^{(\ell)} / \sigma^{(\ell)}$.

2. **Approximation Theory:** Fission adds a centroid to the Voronoi partition at layer $\ell$. By standard quantization theory (Zador's theorem), increasing codebook size strictly reduces the mean squared quantization error (distortion), provided the data is not uniform.

3. **Variance Reduction:** The reconstruction error $\|z_{\text{tex}}^{(\ell)}\|^2$ decreases, implying the scale factor $\sigma^{(\ell)}$ decreases.

4. **Stress Damping:** Ontological Stress at layer $\ell+1$ is upper-bounded by the mutual information of its input. Since the input variance is reduced (relative to the pre-fission state), the extractable structure $I(x^{(\ell+1)}_t; x^{(\ell+1)}_{t+1})$ decreases.

5. **Conclusion:** Macro-scale adaptation absorbs structural variance, starving the micro-scale of the stress required to trigger bifurcation. $\square$
:::

:::{prf:corollary} Hierarchical Stability
:label: cor-hierarchical-stability

The stacked architecture is **inherently stable** against fission cascades. Ontological expansion at coarse scales (low $\ell$) pre-empts the need for expansion at fine scales (high $\ell$).

*Interpretation:* If the agent learns a new high-level concept (e.g., "mammal"), the residual variance available to learn low-level distinctions (e.g., specific breeds) is reduced. The hierarchy self-regulates, preventing runaway complexity growth.
:::

## 07_cognition/05_metabolism.md

:::{prf:definition} Metabolic Flux
:label: def-metabolic-flux

Let $\rho(s, z)$ be the belief density evolving in computation time $s$ according to the WFR continuity equation (Definition {prf:ref}`def-the-wfr-action`):

$$
\partial_s \rho + \nabla \cdot (\rho v) = \rho r.

$$
We define the **Metabolic Flux** $\dot{\mathcal{M}}: \mathbb{R}_{\ge 0} \to \mathbb{R}_{\ge 0}$ as:

$$
\dot{\mathcal{M}}(s) := \sigma_{\text{met}} \int_{\mathcal{Z}} \left( \|v_s(z)\|_G^2 + \lambda^2 |r_s(z)|^2 \right) \rho(s, z) \, d\mu_G,

$$
where:
- $\sigma_{\text{met}} > 0$ is the **metabolic resistance coefficient** (units: nat$\cdot$step)
- $v_s(z)$ is the velocity field at computation time $s$
- $r_s(z)$ is the reaction rate (mass creation/destruction)
- $\lambda$ is the WFR length-scale (Definition {prf:ref}`def-the-wfr-action`)
- $d\mu_G = \sqrt{\det G} \, dz$ is the Riemannian volume form

*Physical interpretation:* The metabolic flux measures the instantaneous rate of energy dissipation required to update the belief distribution. Transport ($\|v\|_G^2$) represents the cost of moving probability mass; reaction ($|r|^2$) represents the cost of creating or destroying mass. The WFR action is the kinetic energy of the belief flow.

:::

:::{prf:theorem} Generalized Landauer Bound
:label: thm-generalized-landauer-bound

The metabolic flux $\dot{\mathcal{M}}$ provides a physical lower bound on the rate of entropy reduction within the agent. Specifically:

$$
\dot{\mathcal{M}}(s) \ge T_c \left| \frac{d}{ds} H(\rho_s) \right|,

$$
where $H(\rho_s) = -\int_{\mathcal{Z}} \rho \ln \rho \, d\mu_G$ is the Shannon entropy and $T_c$ is the cognitive temperature ({prf:ref}`def-cognitive-temperature`, {ref}`sec-the-geodesic-baoab-integrator`).

*Proof sketch.* The time derivative of the Shannon entropy is:

$$
\frac{d}{ds} H(\rho_s) = -\int_{\mathcal{Z}} (1 + \ln \rho) \partial_s \rho \, d\mu_G.

$$
Substituting the WFR continuity equation and integrating by parts (assuming vanishing flux at $\partial\mathcal{Z}$):

$$
\frac{d}{ds} H = \int_{\mathcal{Z}} \rho \langle \nabla \ln \rho, v \rangle_G \, d\mu_G - \int_{\mathcal{Z}} r \ln \rho \cdot \rho \, d\mu_G.

$$
By the Cauchy-Schwarz inequality on the tangent bundle $(T\mathcal{Z}, G)$:

$$
\left| \int_{\mathcal{Z}} \rho \langle \nabla \ln \rho, v \rangle_G \, d\mu_G \right| \le \left( \int_{\mathcal{Z}} \rho \|\nabla \ln \rho\|_G^2 \, d\mu_G \right)^{1/2} \left( \int_{\mathcal{Z}} \rho \|v\|_G^2 \, d\mu_G \right)^{1/2}.

$$
The first factor is the **Fisher Information** $\mathcal{I}(\rho) = \int \rho \|\nabla \ln \rho\|_G^2 \, d\mu_G$ {cite}`amari2016information`. Under the optimal transport scaling $v = -T_c \nabla \ln \rho$ (gradient flow of the free energy), we recover the de Bruijn identity {cite}`stam1959some` and the bound follows. The reaction term satisfies an analogous inequality via the $L^2(\rho)$ norm. See {ref}`sec-appendix-e-rigorous-proof-sketches-for-ontological-and-metabolic-laws` for the full proof. $\square$

*Remark (Landauer's Principle).* The classical Landauer bound states that erasing one bit of information requires dissipating at least $k_B T \ln 2$ joules of heat. Theorem {prf:ref}`thm-generalized-landauer-bound` is the information-geometric generalization: reducing belief entropy by $\Delta H$ nats requires dissipating at least $T_c \cdot |\Delta H|$ nats of metabolic energy.

:::

:::{prf:definition} Metabolic Potential
:label: def-metabolic-potential

We define $\Psi_{\text{met}}(s) := \int_0^s \dot{\mathcal{M}}(u) \, du$ as the cumulative metabolic energy dissipated during a single interaction step $t$ for an internal rollout of duration $s$. Units: $[\Psi_{\text{met}}] = \text{nat}$.

:::

:::{prf:axiom} Dual-Horizon Action
:label: ax-dual-horizon-action

For any interaction step $t$, the agent selects a total computation budget $S \in [0, S_{\max}]$ that minimizes the **Deliberation Action** $\mathcal{S}_{\text{delib}}$:

$$
\mathcal{S}_{\text{delib}}[S] = -\underbrace{\mathbb{E}_{z \sim \rho_S} [V(z)]}_{\text{Expected Terminal Value}} + \underbrace{\Psi_{\text{met}}(S)}_{\text{Computational Cost}},

$$
where $V(z)$ is the task potential ({ref}`sec-hodge-decomposition-of-value`). Units: $[\mathcal{S}_{\text{delib}}] = \text{nat}$.

*Physical interpretation:* The agent faces a trade-off: longer deliberation ($S$ large) improves the expected value $\langle V \rangle_{\rho_S}$ by refining the belief toward high-value regions, but incurs greater metabolic cost $\Psi_{\text{met}}(S)$. The optimal $S^*$ balances these competing pressures.

*Remark (Sign convention).* We write $-\langle V \rangle$ because the agent seeks to **maximize** value. The Deliberation Action $\mathcal{S}_{\text{delib}}$ is minimized when value is maximized and cost is minimized.

:::

:::{prf:theorem} Deliberation Optimality Condition
:label: thm-deliberation-optimality-condition

Let $\rho_s$ evolve as a gradient flow of $V$ under WFR dynamics. The optimal computation budget $S^*$ satisfies:

$$
\left. \frac{d}{ds} \langle V \rangle_{\rho_s} \right|_{s=S^*} = \dot{\mathcal{M}}(S^*),

$$
provided such an $S^*$ exists in $(0, S_{\max})$.

*Proof.* We seek to extremize $\mathcal{S}_{\text{delib}}$ with respect to the upper integration limit $S$. By the Leibniz Integral Rule and the definition of $\Psi_{\text{met}}$:

$$
\frac{d}{dS} \mathcal{S}_{\text{delib}} = -\frac{d}{dS} \langle V \rangle_{\rho_S} + \dot{\mathcal{M}}(S).

$$
The first term is the **Value-Improvement Rate**:

$$
\frac{d}{dS} \langle V \rangle_{\rho_S} = \int_{\mathcal{Z}} V(z) \partial_s \rho(S, z) \, d\mu_G.

$$
Applying the WFR continuity equation $\partial_s \rho = \rho r - \nabla \cdot (\rho v)$:

$$
\frac{d}{dS} \langle V \rangle_{\rho_S} = \int_{\mathcal{Z}} V \cdot \rho r \, d\mu_G + \int_{\mathcal{Z}} V (-\nabla \cdot (\rho v)) \, d\mu_G.

$$
Integrating the divergence term by parts (assuming vanishing flux at $\partial\mathcal{Z}$):

$$
\int_{\mathcal{Z}} V (-\nabla \cdot (\rho v)) \, d\mu_G = \int_{\mathcal{Z}} \rho \langle \nabla_A V, v \rangle_G \, d\mu_G.

$$
For gradient flow dynamics, $v = -G^{-1} \nabla_A V$ (up to temperature scaling), so $\langle \nabla_A V, v \rangle_G = -\|\nabla_A V\|_G^2 \le 0$. Thus:

$$
\frac{d}{dS} \langle V \rangle_{\rho_S} = \int_{\mathcal{Z}} \rho \left( V r - \|\nabla_A V\|_G^2 \right) d\mu_G.

$$
The stationarity condition $\frac{d}{dS} \mathcal{S}_{\text{delib}} = 0$ yields the optimality condition. See {ref}`sec-appendix-e-rigorous-proof-sketches-for-ontological-and-metabolic-laws` for the full proof using the WFR adjoint operator. $\square$

*Physical interpretation:* The optimal stopping time $S^*$ is reached when the marginal gain in expected value (the "return on thinking") exactly equals the marginal metabolic cost (the "price of thinking"). At $S^*$, the agent has extracted all cost-effective information from deliberation.

:::

:::{prf:theorem} Fast/Slow Phase Transition
:label: thm-fast-slow-phase-transition

Let $\Gamma(s) := \left| \frac{d}{ds} \langle V \rangle_{\rho_s} \right|$ be the **Value-Improvement Rate**. There exists a critical threshold such that:

1. **Reflexive Regime (Fast):** If $\Gamma(0) < \dot{\mathcal{M}}(0)$, then $S^* = 0$. The agent executes an immediate action based on the prior $\rho_0$.

2. **Deliberative Regime (Slow):** If $\Gamma(0) > \dot{\mathcal{M}}(0)$, then $S^* > 0$. The agent enters a planning state, terminating only when the marginal gain in Value equals the marginal metabolic cost.

*Proof.* Consider the derivative of the Deliberation Action at $S = 0$:

$$
\left. \frac{d}{dS} \mathcal{S}_{\text{delib}} \right|_{S=0} = -\Gamma(0) + \dot{\mathcal{M}}(0).

$$
If $\Gamma(0) < \dot{\mathcal{M}}(0)$, then $\frac{d}{dS} \mathcal{S}_{\text{delib}}|_{S=0} > 0$. Since $\mathcal{S}_{\text{delib}}$ is increasing at $S=0$ and we assume $\mathcal{S}_{\text{delib}}$ is convex (which holds when $\Gamma(s)$ is decreasing due to diminishing returns), the minimum occurs at the boundary $S^* = 0$.

If $\Gamma(0) > \dot{\mathcal{M}}(0)$, then $\frac{d}{dS} \mathcal{S}_{\text{delib}}|_{S=0} < 0$. The agent benefits from deliberation. As $s$ increases, $\Gamma(s)$ decreases (diminishing marginal returns on thinking) while $\dot{\mathcal{M}}(s)$ may increase or remain constant. The optimum $S^* > 0$ occurs when the curves cross: $\Gamma(S^*) = \dot{\mathcal{M}}(S^*)$. $\square$

*Remark (Dual-Process Theory).* Theorem {prf:ref}`thm-fast-slow-phase-transition` provides a first-principles derivation of Kahneman's "System 1 / System 2" dichotomy {cite}`kahneman2011thinking`. System 1 (reflexive) corresponds to $S^* = 0$; System 2 (deliberative) corresponds to $S^* > 0$. The transition is not a cognitive style but a phase transition governed by the ratio $\Gamma(0) / \dot{\mathcal{M}}(0)$.

:::

:::{prf:theorem} Generalized Stopping for Non-Conservative Fields
:label: thm-generalized-stopping

When the Value Curl does not vanish ($\mathcal{F} \neq 0$, Definition {prf:ref}`def-value-curl`), the agent converges to a Non-Equilibrium Steady State (Theorem {prf:ref}`thm-ness-existence`) rather than a fixed point. The stopping criterion generalizes as follows:

**Conservative Case ($\mathcal{F} = 0$):** Stop when the Value-Improvement Rate equals the metabolic cost:

$$
\Gamma(S^*) = \dot{\mathcal{M}}(S^*)

$$

**Non-Conservative Case ($\mathcal{F} \neq 0$):** Stop when the **orbit parameters converge**:

$$
\frac{d}{ds}\|\text{Orbit}(s)\|_{\text{param}} < \epsilon_{\text{orbit}}

$$
even if the agent continues moving within the limit cycle.

*Remark.* In the conservative case, convergence is to a fixed point ($\dot{z} \to 0$). In the non-conservative case, convergence is to a stable limit cycle (periodic orbit with constant parameters).

**Operational Criterion:** Define the orbit-change metric as:

$$
\Delta_{\text{orbit}}(s) := \left\| \oint_{\gamma_s} \mathcal{R} - \oint_{\gamma_{s-\delta}} \mathcal{R} \right\|

$$
where $\gamma_s$ is the closed trajectory over one cycle at time $s$. Stop when $\Delta_{\text{orbit}}(s) < \epsilon_{\text{orbit}}$.

*Remark.* In the non-conservative case, the agent accumulates reward along periodic trajectories. Deliberation terminates when the orbit parameters stabilize, not when motion ceases.

:::

:::{prf:theorem} Total Entropy Production
:label: thm-total-entropy-production

The total entropy production rate of the agent $\sigma_{\text{tot}}$ during computation is:

$$
\sigma_{\text{tot}}(s) := \frac{d}{ds} H(\rho_s) + \frac{1}{T_c} \dot{\mathcal{M}}(s) \ge 0.

$$
*Proof.* From Theorem {prf:ref}`thm-generalized-landauer-bound`, $\dot{\mathcal{M}}(s) \ge T_c |\frac{d}{ds} H(\rho_s)|$. If $\frac{d}{ds} H < 0$ (entropy decreasing), then:

$$
\sigma_{\text{tot}} = \frac{dH}{ds} + \frac{\dot{\mathcal{M}}}{T_c} \ge \frac{dH}{ds} + \left| \frac{dH}{ds} \right| = \frac{dH}{ds} - \frac{dH}{ds} = 0.

$$
If $\frac{d}{ds} H \ge 0$, then $\sigma_{\text{tot}} \ge 0$ trivially since $\dot{\mathcal{M}} \ge 0$. $\square$

*Interpretation:* The agent can only reduce its internal uncertainty ($dH/ds < 0$) by dissipating metabolic energy ($\dot{\mathcal{M}} > 0$) {cite}`still2012thermodynamics`. This defines the **Efficiency of Thought**:

$$
\eta_{\text{thought}} := \frac{-T_c \cdot dH/ds}{\dot{\mathcal{M}}} \le 1.

$$
An agent is "thermodynamically fragile" if it requires high metabolic flux for low entropy reduction ($\eta_{\text{thought}} \ll 1$).

:::

:::{prf:definition} Cognitive Carnot Efficiency
:label: def-cognitive-carnot-efficiency

The **Carnot limit** for cognitive systems is $\eta_{\text{thought}} = 1$, achieved when the belief update is a reversible isothermal process. Real agents operate at $\eta_{\text{thought}} < 1$ due to:
1. **Friction:** Non-optimal transport paths (geodesic deviation)
2. **Irreversibility:** Finite-rate updates (non-quasi-static processes)
3. **Dissipation:** Exploration noise ($T_c > 0$)

:::

## 07_cognition/06_causality.md

:::{prf:definition} The Interventional Surgery
:label: def-the-interventional-surgery

Let $P(z_{t+1} | z_t, a_t)$ be the transition kernel on the latent manifold $\mathcal{Z}$. We define the **Interventional Operator** $\mathfrak{I}: \mathcal{P}(\mathcal{Z} \times \mathcal{A} \times \mathcal{Z}) \to \mathcal{P}(\mathcal{Z} \times \mathcal{A} \times \mathcal{Z})$—equivalent to Pearl's $do(a_t)$ {cite}`pearl2009causality`—as a surgery on the joint distribution that cuts the incoming edges to the action variable.

Geometrically, $\mathfrak{I}$ transforms the symplectic interface ({ref}`sec-the-symplectic-interface-position-momentum-duality`) from a **Coupled Dirichlet state** (where $z_t$ is clamped by the observation $x_t$) to a **Forced Neumann state** (where $z_{t+1}$ is driven purely by the agent's internal motor impulse $u_\pi$).

Formally, the operator acts by truncated factorization:

$$
P(z' | z, do(a)) := P(z' | z, a),

$$
where the structural mechanism $P(z' | z, a)$ is preserved but $a$ is no longer a function of $z$. For marginal interventional queries:

$$
P(z' | do(a)) = \int_{\mathcal{Z}} P(z' | \tilde{z}, a) P_{\text{pre}}(\tilde{z}) \, d\mu_G(\tilde{z}),

$$
where $P_{\text{pre}}(\tilde{z})$ is the pre-intervention distribution over latent states.

:::

:::{prf:lemma} The Interventional Singularity
:label: lem-the-interventional-singularity

An intervention at state $z$ is a point-source singularity in the field theory. It imposes a non-natural boundary condition that forces the system to explore the off-equilibrium response of the environment law $P_\partial$ ({ref}`sec-the-environment-is-an-input-output-law`).

*Proof sketch.* Under passive observation, the agent samples from the equilibrium distribution $P_{\text{eq}}(z' | z, a)$ determined by the environment's Dirichlet boundary $\partial\mathcal{Z}$. The $do$-operator breaks this equilibrium by injecting an external impulse $u_\pi$ that does not arise from the natural dynamics. In PDE terms, this corresponds to introducing a Dirac delta source $\delta(z - z_0)$ at the intervention point, creating a Green's function response that propagates through the causal graph. The "singularity" is geometric: the intervention point has infinite curvature in the causal manifold because all causal arrows pointing into it are severed. $\square$

*Remark (Surgery vs. Conditioning).* The key distinction from Bayesian conditioning is that $P(z' | do(a)) \neq P(z' | a)$ in general. Conditioning updates beliefs given evidence; intervention changes the generating mechanism. The former is reversible; the latter is a topological surgery.

:::

:::{prf:definition} Causal Information Potential
:label: def-causal-information-potential

Recall the World Model scaling coefficient $\gamma$ ({ref}`sec-scaling-exponents-characterizing-the-agent`). We define the **Causal Information Potential** $\Psi_{\text{causal}}: \mathcal{Z} \times \mathcal{A} \to \mathbb{R}_{\ge 0}$ as the Expected Information Gain (EIG) {cite}`lindley1956measure` regarding the transition parameters $\theta_W$ at state-action pair $(z, a)$:

$$
\Psi_{\text{causal}}(z, a) := \mathbb{E}_{z' \sim \bar{P}(\cdot | z, a)} \left[ D_{\text{KL}} \left( p(\theta_W | z, a, z') \| p(\theta_W | z, a) \right) \right].

$$
Units: $[\Psi_{\text{causal}}] = \text{nat}$.

*Physical interpretation:* $\Psi_{\text{causal}}(z, a)$ measures how much the agent expects to learn about the World Model parameters $\theta_W$ by executing action $a$ from state $z$. High $\Psi_{\text{causal}}$ indicates that the outcome $z'$ is highly informative about the transition dynamics—the agent is uncertain about what will happen, and observing the outcome will resolve significant uncertainty. This is the foundation of Bayesian experimental design {cite}`chaloner1995bayesian`.

:::

:::{prf:theorem} The Interventional Gap
:label: thm-the-interventional-gap

Let $P_{\text{obs}}(z' | z, a)$ be the conditional density obtained via passive observation, and $P_{\text{int}}(z' | do(z, a))$ be the density under intervention. We define the **Causal Deficit** $\Delta_{\text{causal}}: \mathcal{Z} \times \mathcal{A} \to \mathbb{R}_{\ge 0}$ as:

$$
\Delta_{\text{causal}}(z, a) := D_{\text{KL}} \left( P_{\text{int}}(z' | do(z, a)) \| P_{\text{obs}}(z' | z, a) \right).

$$
*Interpretation:* The Causal Deficit measures the discrepancy between interventional and observational predictions. If $\Delta_{\text{causal}} = 0$, the observational model is causally correct -- correlations reflect true causal mechanisms. If $\Delta_{\text{causal}} > 0$, the agent has mistaken a correlation for a causal link (confounding) or vice versa.

*Proof.* By the properties of KL-divergence, $\Delta_{\text{causal}} \ge 0$ with equality iff $P_{\text{int}} = P_{\text{obs}}$ almost everywhere. The agent's "Causal Ignorance" is the volume of states where $\Delta_{\text{causal}} > 0$:

$$
\text{Vol}_{\text{ignorant}} := \int_{\mathcal{Z} \times \mathcal{A}} \mathbb{I}[\Delta_{\text{causal}}(z, a) > 0] \, d\mu_G(z) \, da.

$$
This volume represents the region of state-action space where the agent's observational model fails to predict interventional outcomes. $\square$

:::

:::{prf:corollary} The Epistemic Curiosity Filter
:label: cor-epistemic-curiosity-filter

The Causal Information Potential $\Psi_{\text{causal}}$ (Definition {prf:ref}`def-causal-information-potential`) is maximized in regions of high **posterior varentropy**, not merely high entropy.

**Key Insight:** Let $V_H[P(\theta_W | z, a, z')]$ denote the Varentropy of the posterior over World Model parameters after observing transition $(z, a) \to z'$. Then:

$$
\nabla \Psi_{\text{causal}} \propto \nabla \mathbb{E}_{z'} \left[ V_H [P(\theta_W | z, a, z')] \right].

$$
*Units:* nat (for $\Psi_{\text{causal}}$), $\mathrm{nat}^2$ (for $V_H$).

**Operational Significance:**
The Curiosity Force $\mathbf{f}_{\text{exp}}$ (Theorem {prf:ref}`thm-augmented-drift-law`) should be weighted by the Varentropy of the World Model's prediction, not just its Entropy.

1. **High Entropy, Low Varentropy:** The World Model is confidently predicting "I don't know" (White Noise). The gradient $\nabla \Psi \approx 0$. The agent ignores this region (solves the "Noisy TV" problem).
2. **High Entropy, High Varentropy:** The World Model oscillates between distinct causal hypotheses ($H_1$: "Object falls", $H_2$: "Object floats"). The gradient $\nabla \Psi$ is maximal. The agent is strongly attracted to this state to resolve the structural ambiguity.

**Implementation:** The Experimental Sieve (Algorithm 32.5.1) selects interventions $do(a)$ that maximize the **Varentropy of the expected outcome distribution**.

*Proof:* See Appendix {ref}`E.11 <sec-appendix-e-proof-of-corollary-epistemic-curiosity-filter>`.

:::

:::{prf:theorem} Augmented Drift Law
:label: thm-augmented-drift-law

The Equation of Motion ({ref}`sec-the-coupled-jump-diffusion-sde`) is extended by the **Interventional Force** $\mathbf{f}_{\text{exp}}$:

$$
F_{\text{total}} = \underbrace{-G^{-1} \nabla_G V}_{\text{Utility Force}} + \underbrace{\beta_{\text{exp}} \mathbf{f}_{\text{exp}}}_{\text{Curiosity Force}},

$$
where:
- $\mathbf{f}_{\text{exp}} := G^{-1} \nabla_z \Psi_{\text{causal}}$ is the gradient of the causal potential
- $\beta_{\text{exp}} \ge 0$ is the **exploration coefficient** balancing exploitation vs. exploration

*Proof.* We define a combined action functional $\mathcal{S}_{\text{total}} = \int_0^T \left[ \frac{1}{2}\|\dot{z}\|_G^2 - V(z) - \beta_{\text{exp}} \Psi_{\text{causal}}(z) \right] dt$. The Euler-Lagrange equations on $(\mathcal{Z}, G)$ yield:

$$
\frac{d}{dt}\left( G_{kj} \dot{z}^j \right) - \frac{1}{2} \partial_k G_{ij} \dot{z}^i \dot{z}^j = -\partial_k V - \beta_{\text{exp}} \partial_k \Psi_{\text{causal}}.

$$
Expanding the left-hand side and identifying the Christoffel symbols of the first kind $[ij, k] = \frac{1}{2}(\partial_i G_{jk} + \partial_j G_{ik} - \partial_k G_{ij})$:

$$
G_{kj} \ddot{z}^j + [ij, k] \dot{z}^i \dot{z}^j = -(\nabla_A V)_k - \beta_{\text{exp}} \partial_k \Psi_{\text{causal}} + \beta_{\text{curl}} \mathcal{F}_{kj}\dot{z}^j.

$$
Contracting with $G^{mk}$ and using $\Gamma^m_{ij} = G^{mk}[ij, k]$:

$$
\ddot{z}^m + \Gamma^m_{ij} \dot{z}^i \dot{z}^j = -G^{mk} (\nabla_A V)_k - \beta_{\text{exp}} G^{mk} \partial_k \Psi_{\text{causal}} + \beta_{\text{curl}} G^{mk} \mathcal{F}_{kj}\dot{z}^j.

$$
In the overdamped limit ({ref}`sec-the-unified-effective-potential`), the acceleration term vanishes and the drift field is

$$
\dot{z} = \mathcal{M}_{\text{curl}}\!\left(-G^{-1}\nabla_A V + \beta_{\text{exp}} G^{-1}\nabla\Psi_{\text{causal}}\right),
$$
with $\mathcal{M}_{\text{curl}} := (I - \beta_{\text{curl}} G^{-1}\mathcal{F})^{-1}$.
Here $\nabla_A V := \nabla V - A$ with $A := \delta\Psi + \eta$ the non-conservative component of the reward 1-form
(conservative case: $A=0$).
See {ref}`sec-appendix-e-rigorous-proof-sketches-for-ontological-and-metabolic-laws` for the full derivation. $\square$

*Physical interpretation:* The curiosity force $\mathbf{f}_{\text{exp}}$ pulls the agent toward regions of high epistemic uncertainty about the transition dynamics. This is the geometric formulation of **intrinsic motivation** {cite}`schmidhuber2010formal,oudeyer2007intrinsic`: the agent is rewarded for reducing its causal ignorance, independent of external task reward. This connects to curiosity-driven exploration in reinforcement learning {cite}`pathak2017curiosity,houthooft2016vime`.

:::

:::{prf:corollary} Scientific Method as Geodesic
:label: cor-scientific-method-as-geodesic

In the absence of task reward ($V = \text{const}$), the agent behaves as a "Pure Scientist," traversing the latent manifold to minimize the total epistemic entropy of the World Model.

*Proof.* Setting $V = \text{const}$ and $A=0$ implies $\nabla_A V = 0$. The equation of motion reduces to
$\ddot{z}^m + \Gamma^m_{ij} \dot{z}^i \dot{z}^j = \beta_{\text{exp}} G^{mk} \partial_k \Psi_{\text{causal}} + \beta_{\text{curl}} G^{mk} \mathcal{F}_{kj}\dot{z}^j$.
The agent follows geodesics modified by the curiosity potential (and any value-curl drift), exploring the manifold to maximize $\Psi_{\text{causal}}$ (i.e., to find maximally informative experiments). $\square$

:::

:::{prf:theorem} Interventional Closure
:label: thm-interventional-closure

The macro-ontology $K$ is **Interventionally Closed** if and only if the predictability of the macro-state is invariant under $do$-operations:

$$
I(K_{t+1} ; Z_{\text{micro}, t} | K_t, do(K^{\text{act}}_t)) = 0.

$$
*Interpretation:* If an agent moves an object (intervention), and the resulting macro-state $K_{t+1}$ depends on micro-texture $z_{\text{tex}}$ that was previously labeled "noise," the ontology has failed. The intervention has **exposed a hidden variable**, triggering **Ontological Expansion** ({ref}`sec-ontological-expansion-topological-fission-and-the-semantic-vacuum`).

*Proof sketch.* We compare the mutual information $I(K_{t+1}; Z_{\text{micro}, t} | K_t)$ under the observational measure $P$ and the interventional measure $P_{do(K^{\text{act}})}$. Causal enclosure ({ref}`sec-conditional-independence-and-sufficiency`) guarantees the condition for $P$. Because the $do(K^{\text{act}})$ operator is a surgery that only removes incoming edges to $K^{\text{act}}$ (Pearl's Causal Markov Condition {cite}`pearl2009causality`), it leaves the mechanism $P(K_{t+1} | K_t, K^{\text{act}}_t, Z_{\text{micro}, t})$ invariant.

If the observational distribution is closed ($I = 0$), and the mechanism is invariant, the interventional distribution is necessarily closed. A violation ($I > 0$ under $do$) implies the existence of a back-door path through $Z_{\text{micro}}$ that was previously unobserved, necessitating a topological expansion of $K$ to include the confounding variable. See {ref}`sec-appendix-e-rigorous-proof-sketches-for-ontological-and-metabolic-laws` for the full proof. $\square$

*Remark (Interventional Debugging).* Theorem {prf:ref}`thm-interventional-closure` provides a diagnostic for ontological adequacy: if the agent's predictions fail specifically under intervention but succeed under observation, the ontology contains a hidden confounder. This is the geometric manifestation of Simpson's paradox {cite}`pearl2009causality`. Algorithmic approaches to discovering such confounders are developed in the causal discovery literature {cite}`spirtes2000causation`.

:::

## 07_cognition/07_metabolic_transducer.md

:::{prf:definition} The Reward Flux
:label: def-reward-flux-harvesting

The **Reward Flux** $J_r(t)$ is the instantaneous rate of reward accumulation (Definition {prf:ref}`def-the-reward-flux`):

$$
J_r(t) = \langle \mathcal{R}(z_t), v_t \rangle_G = r_t

$$

where $\mathcal{R}$ is the reward 1-form ({ref}`sec-the-reward-field-value-forms-and-hodge-geometry`) and $v_t = \dot{z}_t$ is the velocity in latent space.

*Units:* $[J_r] = \text{nats/step}$ (information-theoretic) or $[\text{utility/step}]$ (decision-theoretic).

*Interpretation:* A positive reward $r_t > 0$ indicates the agent has navigated to a state with lower environmental entropy—a configuration where resources (food, fuel, safety) are localized and accessible.

:::

:::{prf:definition} Information Utility
:label: def-information-utility

The **Information Utility** $\mathcal{I}_{\text{util}}(r_t)$ quantifies the actionable information content of the reward signal:

$$
\mathcal{I}_{\text{util}}(r_t) := I(Z_t; R_t) = H[R_t] - H[R_t \mid Z_t]

$$

where $I(Z_t; R_t)$ is the mutual information between the agent's state $Z_t$ and the reward $R_t$.

*Operational interpretation:* This is the reduction in uncertainty about environmental resources achieved by navigating to state $z_t$ and observing reward $r_t$.

*Units:* $[\mathcal{I}_{\text{util}}] = \text{nats}$ (or bits if using $\log_2$).

*Simplification:* When the reward signal is deterministic given state, $H[R_t \mid Z_t] = 0$, so $\mathcal{I}_{\text{util}}(r_t) = H[R_t]$. In practice, we often use the approximation $\mathcal{I}_{\text{util}}(r_t) \approx |r_t|$ for rewards measured in natural units.

:::

:::{prf:axiom} The Szilard Correspondence (Information-Work Duality)
:label: ax-szilard-correspondence

Information about low-entropy configurations can be converted to extractable work. Specifically, if an agent possesses $I$ nats of mutual information with a thermal reservoir at temperature $T_{\text{env}}$, it can extract at most:

$$
W_{\max} = k_B T_{\text{env}} \cdot I

$$

joules of work, where $k_B$ is Boltzmann's constant.

*Physical basis:* This is the inverse of Landauer's principle. Landauer states that erasing 1 bit costs $k_B T \ln 2$ joules. Szilard's engine demonstrates that acquiring 1 bit about a system enables extracting $k_B T \ln 2$ joules. The two are thermodynamically dual.

*Cognitive interpretation:* A reward signal $r_t > 0$ encodes mutual information between the agent's state and resource availability. This information, when acted upon, enables work extraction from the environment.

:::

:::{prf:theorem} The Transducer Bound
:label: thm-szilard-transducer-bound

Let $r_t$ be the instantaneous reward signal with information content $\mathcal{I}_{\text{util}}(r_t)$ nats. The maximum free energy extractable per unit time is bounded by:

$$
\dot{E}_{\text{in}}^{\max}(t) = k_B T_{\text{env}} \cdot \mathcal{I}_{\text{util}}(r_t)

$$

where $T_{\text{env}}$ is the environmental temperature (characterizing energy availability).

*Proof sketch.*
1. The agent navigates to state $z_t$ and receives reward $r(z_t)$.
2. The reward encodes mutual information $I(Z_t; \text{Resource})$ between the agent's position and resource availability.
3. By the Szilard engine analysis, this mutual information enables extraction of $k_B T_{\text{env}} \cdot I$ joules.
4. The information utility $\mathcal{I}_{\text{util}}(r_t)$ quantifies the actionable information in the reward signal.
5. Real transduction incurs irreversibility losses captured by efficiency $\eta \leq 1$. $\square$

:::

:::{prf:definition} The Metabolic Transducer Operator
:label: def-metabolic-transducer

The **Metabolic Transducer** $\mathfrak{T}_{\text{harvest}}$ is the operator converting the reward flux to free energy flux:

$$
\dot{E}_{\text{in}}(t) = \mathfrak{T}_{\text{harvest}}(r_t) := \eta \cdot k_B T_{\text{env}} \cdot \mathcal{I}_{\text{util}}(r_t)

$$

where:
- $k_B \approx 1.38 \times 10^{-23}$ J/K is **Boltzmann's constant**
- $T_{\text{env}}$ is the **environmental temperature** (Kelvin)
- The product $k_B T_{\text{env}}$ is the **energy-per-nat conversion factor** (Joules/nat)
- $\eta \in [0, 1]$ is the **transduction efficiency** (Carnot-bounded, see Theorem {prf:ref}`thm-carnot-transduction-bound`)
- $\mathcal{I}_{\text{util}}(r_t)$ is the **information utility** of the reward signal (Definition {prf:ref}`def-information-utility`)

*Units:* $[\mathfrak{T}] = \text{Joules/step}$ (power).

*Simplified form:* For dimensionless analysis with $k_B = 1$, we write:

$$
\mathfrak{T}_{\text{harvest}}(r_t) = \eta \cdot T_{\text{env}} \cdot r_t

$$

where $r_t$ is measured in nats.

:::

:::{prf:definition} The Internal Battery
:label: def-internal-battery

The **Internal Battery** $B(t)$ is a scalar state variable representing the agent's stored free energy:

$$
B: [0, \infty) \to [0, B_{\max}]

$$

where:
- $B_{\max}$ is the maximum storage capacity (Joules)
- $B(0) = B_0$ is the initial endowment

*Units:* $[B] = \text{Joules}$ (energy).

*Interpretation:* The battery represents the agent's capacity for future computation. In biological systems, this corresponds to ATP/glucose reserves; in artificial systems, to available compute budget.

:::

:::{prf:axiom} Energy Conservation (First Law)
:label: ax-energy-conservation-battery

The battery evolves according to the First Law of Thermodynamics:

$$
\frac{dB}{dt} = \underbrace{\mathfrak{T}_{\text{harvest}}(r_t)}_{\text{Income}} - \underbrace{\dot{\mathcal{M}}(t)}_{\text{Metabolic Cost}} - \underbrace{\gamma_{\text{leak}} B(t)}_{\text{Passive Dissipation}}

$$

where:
- $\mathfrak{T}_{\text{harvest}}(r_t)$ is the transduced energy from rewards (Definition {prf:ref}`def-metabolic-transducer`)
- $\dot{\mathcal{M}}(t)$ is the metabolic cost from Theorem {prf:ref}`thm-generalized-landauer-bound`
- $\gamma_{\text{leak}} \geq 0$ is the passive self-discharge rate (basal metabolic rate)

*Terminal Condition:* If $B(t) \leq 0$, the agent undergoes **Thermodynamic Death**. The metric collapses (Theorem {prf:ref}`thm-fading-metric-law`), inference halts, and the agent can no longer perform coherent computation.

:::

:::{prf:theorem} The Autopoietic Inequality
:label: thm-autopoietic-inequality

Let $\tau > 0$ be a target survival horizon. A **sufficient condition** for the agent to survive at time $\tau$ (i.e., $B(\tau) > 0$) is:

$$
\int_0^\tau \left( \mathfrak{T}_{\text{harvest}}(r_t) - \dot{\mathcal{M}}(t) \right) dt > \gamma_{\text{leak}} \int_0^\tau B(t) \, dt - B_0

$$

*Equivalently:* The time-averaged **Net Harvest Rate** must be positive:

$$
\langle \mathfrak{T} - \dot{\mathcal{M}} \rangle_\tau > \gamma_{\text{leak}} \langle B \rangle_\tau - \frac{B_0}{\tau}

$$

*Proof.*
Integrate the battery ODE (Axiom {prf:ref}`ax-energy-conservation-battery`):

$$
B(\tau) - B_0 = \int_0^\tau \mathfrak{T}(r_t) \, dt - \int_0^\tau \dot{\mathcal{M}}(t) \, dt - \gamma_{\text{leak}} \int_0^\tau B(t) \, dt

$$

Requiring $B(\tau) > 0$ and rearranging yields the inequality. $\square$

*Physical interpretation:* The agent must harvest more energy than it dissipates. This is the **autopoietic closure condition**—the system must actively maintain its own organization against thermodynamic decay.

:::

:::{prf:corollary} The Survival Objective
:label: cor-survival-objective

The agent's fundamental objective is not reward maximization but **energy surplus maximization**:

$$
\mathcal{J}_{\text{survival}} = \mathbb{E}\left[ \int_0^\infty \left( \mathfrak{T}_{\text{harvest}}(r_t) - \dot{\mathcal{M}}(t) \right) e^{-\gamma_{\text{leak}} t} \, dt \right]

$$

Standard reward maximization $\max \mathbb{E}[\sum_t \gamma^t r_t]$ emerges as a degenerate case when:
1. Metabolic cost $\dot{\mathcal{M}} \to 0$ (free computation)
2. Transduction efficiency $\eta \to 1$ (perfect conversion)
3. Battery capacity $B_{\max} \to \infty$ (unlimited storage)

:::

:::{prf:theorem} The Information-Maintenance Cost
:label: thm-information-maintenance-cost

Maintaining Fisher Information $I_F$ on the latent manifold $(\mathcal{Z}, G)$ requires continuous energy expenditure:

$$
\dot{E}_{\text{maintain}} \geq \frac{1}{2} T_c \cdot I_F

$$

where $T_c$ is the cognitive temperature ({prf:ref}`def-cognitive-temperature`) and $I_F$ is the Fisher Information of the belief distribution.

*Proof sketch.*
1. **Fisher Information definition:** For belief density $\rho(z)$ on $(\mathcal{Z}, G)$:
   $$I_F = \mathbb{E}_\rho\left[ \|\nabla \ln \rho\|_G^2 \right] = \int_\mathcal{Z} \rho(z) \|\nabla \ln \rho(z)\|_{G^{-1}}^2 \, d\mu_G(z)$$

2. **de Bruijn identity** {cite}`stam1959some,cover2006elements`: Under diffusion $d\rho/dt = T_c \Delta_G \rho$, entropy evolves as:
   $$\frac{dH[\rho]}{dt} = \frac{1}{2} I_F[\rho]$$
   Entropy increases at rate proportional to Fisher Information.

3. **Landauer cost:** By Theorem {prf:ref}`thm-generalized-landauer-bound`, maintaining entropy against diffusion requires:
   $$\dot{E}_{\text{maintain}} \geq T_c \left| \frac{dH}{dt} \right| = \frac{1}{2} T_c \cdot I_F$$

4. **Interpretation:** Sharp probability distributions (high $I_F$) cost more to maintain. $\square$

:::

:::{prf:theorem} The Fading Metric Law
:label: thm-fading-metric-law

When available energy $B(t)$ falls below the maintenance requirement, the effective metric contracts. The **effective metric** is:

$$
G_{ij}^{\text{eff}}(z, B) = f\left(\frac{B}{B_{\text{crit}}}\right) \cdot G_{ij}(z)

$$

where:
- $G_{ij}(z)$ is the full-capacity metric (Theorem {prf:ref}`thm-capacity-constrained-metric-law`)
- $B_{\text{crit}}$ is the **critical energy** required to sustain full metric resolution
- $f: [0, \infty) \to [0, 1]$ is the **fading function** with $f(0) = 0$, $\lim_{x \to \infty} f(x) = 1$

**Specific form:** The fading function satisfying thermodynamic constraints is:

$$
f(x) = 1 - e^{-x}

$$

This gives exponential saturation: $f(x) \approx x$ for $x \ll 1$ (linear regime) and $f(x) \approx 1$ for $x \gg 1$ (saturation).

*Proof sketch.*
1. **Fisher metric interpretation:** The metric $G$ encodes distinguishability—the statistical distance between nearby states. Formally, $G_{ij} = \mathbb{E}[\partial_i \ln p \cdot \partial_j \ln p]$ where $p$ is the encoding distribution.

2. **Signal-to-noise scaling:** Neural signals have SNR proportional to available energy:
   $$\text{SNR} \propto \sqrt{\frac{E_{\text{available}}}{E_{\text{noise}}}} = \sqrt{\frac{B}{B_{\text{crit}}}}$$

3. **Fisher Information scaling:** Since Fisher Information scales as SNR²:
   $$I_F^{\text{eff}} \propto \text{SNR}^2 \propto \frac{B}{B_{\text{crit}}}$$

4. **Metric scaling:** The metric tensor scales with Fisher Information:
   $$G^{\text{eff}} \propto I_F^{\text{eff}} \propto \frac{B}{B_{\text{crit}}} \quad \text{for } B \ll B_{\text{crit}}$$

5. **Saturation:** For $B \gg B_{\text{crit}}$, the metric saturates at $G$ (maximum resolution). The exponential form $f(x) = 1 - e^{-x}$ interpolates smoothly between these regimes. $\square$

:::

:::{prf:corollary} Consequences of Metric Fading
:label: cor-metric-fading-consequences

As $B(t) \to 0$, the following degenerations occur:

1. **Resolution Loss:** Geodesic distances collapse:
   $$d_G^{\text{eff}}(z, z') = \sqrt{f(B/B_{\text{crit}})} \cdot d_G(z, z') \to 0$$
   Distinct concepts become indistinguishable.

2. **Inertia Loss:** The mass term in the geodesic SDE (Definition {prf:ref}`def-bulk-drift-continuous-flow`) vanishes. The agent loses momentum and becomes dominated by thermal noise.

3. **Causal Dissolution:** The Causal Information Bound ({ref}`sec-causal-information-bound`, Theorem {prf:ref}`thm-causal-information-bound`) collapses:
   $$I_{\max}^{\text{eff}} = \frac{\text{Area}(\partial\mathcal{Z})}{4\ell_L^2} \cdot f(B/B_{\text{crit}}) \to 0$$
   The agent's representational capacity vanishes.

4. **Control Loss:** The policy gradient $\nabla_z \Phi_{\text{eff}}$ scales with metric, so control authority degrades.

:::

:::{prf:corollary} The Starvation-Hallucination Regime
:label: cor-starvation-hallucination

As $B(t) \to 0$, the signal-to-noise ratio of internal dynamics degrades:

$$
\text{SNR}_{\text{dynamics}} = \frac{\|v\|_{G^{\text{eff}}}^2}{2T_c} \propto f(B/B_{\text{crit}}) \to 0

$$

In this regime:
- The drift term $v = -G^{-1} \nabla \Phi$ vanishes relative to diffusion $\sqrt{2T_c} dW$
- The agent performs a **random walk** in latent space
- Internal trajectories are indistinguishable from noise: **hallucination**

*Biological analogue:* Hypoglycemia causes confusion, disorientation, and hallucinations before coma—the same phenomenology predicted by metric fading. See also the Cognitive Temperature (Definition {prf:ref}`def-cognitive-temperature`) which controls the noise-to-signal ratio in latent dynamics.

:::

:::{prf:definition} The Homeostatic Potential
:label: def-homeostatic-potential

The battery level $B(t)$ induces a scalar potential field acting on the policy:

$$
\Phi_{\text{homeo}}(z, B) = \frac{\lambda_{\text{surv}}}{B + \epsilon} \cdot \mathbb{1}[z \in \mathcal{Z}_{\text{food}}]

$$

where:
- $\lambda_{\text{surv}} > 0$ is the **survival weight** (dimensionless priority)
- $\epsilon > 0$ is a regularization constant preventing singularity
- $\mathcal{Z}_{\text{food}} \subset \mathcal{Z}$ is the **food region** (states where $\mathfrak{T}(r) > 0$)

*Units:* $[\Phi_{\text{homeo}}] = [\Phi_{\text{task}}] = \text{nats}$ (log-probability scale).

:::

:::{prf:theorem} The Augmented Value Equation
:label: thm-augmented-value-equation

The total effective potential combines task and homeostatic contributions:

$$
\Phi_{\text{total}}(z, B) = \Phi_{\text{task}}(z) + \Phi_{\text{homeo}}(z, B)

$$

The value function satisfies the augmented screened Poisson equation ({ref}`sec-the-reward-field-value-forms-and-hodge-geometry`):

$$
(-\Delta_{G^{\text{eff}}} + \kappa^2) V = \rho_r + \rho_{\text{homeo}}

$$

where:
- $G^{\text{eff}} = f(B/B_{\text{crit}}) \cdot G$ is the faded metric (Theorem {prf:ref}`thm-fading-metric-law`)
- $\rho_{\text{homeo}} = -\Delta \Phi_{\text{homeo}}$ is the homeostatic source term
- The screening mass $\kappa = -\ln \gamma$ remains unchanged

*Consequence:* Both the metric (geometry) and the source term (drive) depend on battery state.

:::

:::{prf:corollary} Priority Inversion at Low Battery
:label: cor-priority-inversion

As $B \to 0$:

1. **Homeostatic dominance:** $\Phi_{\text{homeo}} \propto 1/B \to \infty$ while $\Phi_{\text{task}}$ remains bounded
2. **Gradient steering:** $\nabla_z \Phi_{\text{total}} \approx \nabla_z \Phi_{\text{homeo}}$ points toward $\mathcal{Z}_{\text{food}}$
3. **Priority inversion:** Task objectives become irrelevant; survival dominates

*Behavioral consequence:* A starving agent abandons task pursuit and seeks energy. This behavior emerges from the thermodynamic structure of autopoietic systems.

:::

:::{prf:theorem} The Carnot Bound on Transduction
:label: thm-carnot-transduction-bound

The transduction efficiency is bounded by the Carnot limit:

$$
\eta \leq \eta_{\text{Carnot}} = 1 - \frac{T_c}{T_{\text{env}}}

$$

where $T_c$ is the agent's cognitive temperature and $T_{\text{env}}$ is the environmental temperature.

*Proof.* By the Second Law of Thermodynamics, no heat engine can exceed Carnot efficiency when operating between reservoirs at temperatures $T_{\text{hot}} = T_{\text{env}}$ and $T_{\text{cold}} = T_c$. The Metabolic Transducer is such an engine—it extracts work from the temperature differential between environment and internal state. $\square$

*Consequence:* The agent must maintain $T_c < T_{\text{env}}$ (a thermal gradient) to extract any work. If $T_c \geq T_{\text{env}}$, then $\eta \leq 0$ and no harvesting is possible.

:::

:::{prf:definition} The Waste Heat Flux
:label: def-waste-heat-flux

The **Waste Heat Flux** is the rate at which the agent must dump entropy to the environment:

$$
\dot{Q}_{\text{waste}} = (1 - \eta) \cdot \mathfrak{T}_{\text{gross}}(r_t) + \dot{\mathcal{M}}(t)

$$

where $\mathfrak{T}_{\text{gross}} = k_B T_{\text{env}} \cdot \mathcal{I}_{\text{util}}(r_t)$ is the gross transduction before efficiency losses.

*Units:* $[\dot{Q}_{\text{waste}}] = \text{Watts}$ (power).

*Interpretation:* All non-useful energy becomes waste heat that must be radiated to maintain thermal equilibrium.

:::

:::{prf:corollary} The Thermal Runaway Condition
:label: cor-thermal-runaway

Let $\dot{Q}_{\text{radiate}}$ be the maximum heat dissipation rate (determined by surface area, environment, cooling mechanisms). If:

$$
\dot{Q}_{\text{waste}} > \dot{Q}_{\text{radiate}}

$$

then the agent's internal temperature $T_c$ increases. This triggers a positive feedback loop:

1. $T_c \uparrow$ $\Rightarrow$ $\eta_{\text{Carnot}} = 1 - T_c/T_{\text{env}} \downarrow$
2. Lower $\eta$ $\Rightarrow$ more waste heat for same harvesting
3. More waste heat $\Rightarrow$ $T_c \uparrow$ (feedback)

*Terminal state:* $T_c \to T_{\text{env}}$, $\eta \to 0$, no harvesting possible, death by thermal runaway.

*Biological analogue:* Hyperthermia/heat stroke—metabolic rate increases with temperature, but cooling capacity is bounded, leading to runaway heating.

:::

:::{prf:definition} The Thermal Operating Envelope
:label: def-thermal-operating-envelope

The agent is **thermally viable** if there exists a steady-state solution to:

$$
\dot{Q}_{\text{waste}}(T_c) = \dot{Q}_{\text{radiate}}(T_c)

$$

with $T_c < T_{\text{env}}$ and $\eta(T_c) > \eta_{\min}$ where $\eta_{\min}$ is the minimum efficiency for survival (from Theorem {prf:ref}`thm-autopoietic-inequality`).

The **Thermal Operating Envelope** is the region in $(T_c, \dot{\mathcal{M}}, \dot{Q}_{\text{radiate}})$ space where this condition holds.

:::

## 07_cognition/08_intersubjective_metric.md

:::{prf:definition} Metric Friction
:label: def-metric-friction

Let $\phi_{A \to B}: \mathcal{Z}_A \to \mathcal{Z}_B$ be the best-fit map between agent ontologies (the correspondence minimizing distortion). **Metric Friction** is the squared Frobenius norm of the pullback metric distortion:

$$
\mathcal{F}_{AB}(z) := \| G_A(z) - \phi_{A \to B}^* G_B(\phi(z)) \|_F^2

$$

where $\phi^* G_B$ denotes the pullback metric and $\|\cdot\|_F$ is the Frobenius norm.

*Interpretation:* If $\mathcal{F}_{AB} > 0$, the agents disagree on the fundamental geometry of the world—distances, angles, and causal structure. Cooperation becomes impossible because "gradients" point in different directions.

*Units:* $[\mathcal{F}_{AB}] = [z]^{-4}$ (squared Frobenius norm of metric difference). When normalized by $\|G_A\|_F^2$, the dimensionless ratio $\tilde{\mathcal{F}}_{AB} := \mathcal{F}_{AB}/\|G_A\|_F^2$ measures relative distortion.

:::

:::{prf:lemma} Metric Friction Bounds Cooperative Utility
:label: lem-friction-bounds-utility

Let $V_{\text{coop}}$ denote the cooperative value achievable by agents $A$ and $B$. The friction bound is:

$$
V_{\text{coop}} \leq V_{\text{max}} \cdot \exp\left(-\frac{\mathcal{F}_{AB}}{\mathcal{F}_0}\right)

$$

where $V_{\text{max}}$ is the optimal cooperative value under perfect alignment and $\mathcal{F}_0$ is a characteristic friction scale.

*Proof sketch.* Cooperation requires coordinated gradients. When $\mathcal{F}_{AB} > 0$, the agents' covariant value gradients $\nabla_{A^{(A)}} V_A$ and $\nabla_{A^{(B)}} V_B$ (with $A^{(i)}$ the non-conservative component of agent $i$'s reward 1-form) misalign by an angle $\theta \propto \sqrt{\mathcal{F}_{AB}}$. The effective cooperative gradient is $|\nabla_{A^{(\text{coop})}} V_{\text{coop}}| = |\nabla_{A^{(A)}} V_A| \cos\theta$. Integrating the exponential decay of cosine near $\theta = \pi/2$ yields the bound. $\square$

:::

:::{prf:definition} The Inter-Agent Connection
:label: def-inter-agent-connection

Let agents $A$ and $B$ each possess a nuisance bundle with gauge connection $A_\mu^{(A)}$ and $A_\mu^{(B)}$ respectively (Definition {prf:ref}`def-strategic-connection`). The **Inter-Agent Connection** on the product manifold $\mathcal{Z}_A \times \mathcal{Z}_B$ is:

$$
\mathcal{A}_{AB}^\mu(z_A, z_B) := A_\mu^{(A)}(z_A) \otimes \mathbb{1}_B + \mathbb{1}_A \otimes A_\mu^{(B)}(z_B) + \lambda_{\text{lock}} \mathcal{C}_{AB}^\mu

$$

where:
- $\mathbb{1}_A, \mathbb{1}_B$ are identity operators on the respective bundles
- $\mathcal{C}_{AB}^\mu$ is the **Coupling Connection** encoding the interaction
- $\lambda_{\text{lock}} \geq 0$ is the **Locking Strength**

*Interpretation:* The first two terms represent independent gauge evolution. The third term, proportional to $\lambda_{\text{lock}}$, couples the agents' internal gauges via communication.

:::

:::{prf:definition} The Locking Curvature
:label: def-locking-curvature

The **Locking Curvature** tensor measuring gauge mismatch between agents is:

$$
\mathcal{F}_{AB}^{\mu\nu} := \partial^\mu \mathcal{A}_{AB}^\nu - \partial^\nu \mathcal{A}_{AB}^\mu - ig_{\text{lock}}[\mathcal{A}_{AB}^\mu, \mathcal{A}_{AB}^\nu]

$$

where $g_{\text{lock}}$ is the inter-agent coupling constant. The **Integrated Friction** (gauge-invariant scalar) is:

$$
\Psi_{\text{sync}} := \int_{\mathcal{Z}_{\text{shared}}} \text{Tr}(\mathcal{F}_{AB}^{\mu\nu} \mathcal{F}_{AB,\mu\nu}) \sqrt{|G_{\text{shared}}|} \, d^D z

$$

*Interpretation:* When $\mathcal{F}_{AB}^{\mu\nu} = 0$, the inter-agent connection is flat—parallel transport is path-independent, meaning the agents' gauge choices are compatible. When $\mathcal{F}_{AB}^{\mu\nu} \neq 0$, the agents disagree on how to "translate" internal states.

:::

:::{prf:theorem} Derivation of the Locking Operator
:label: thm-locking-operator-derivation

The Locking Operator $\mathfrak{L}_{\text{sync}}$ is the Yang-Mills energy of the inter-agent connection:

$$
\mathfrak{L}_{\text{sync}}(G_A, G_B) := -\frac{1}{4g_{\text{lock}}^2} \int_{\mathcal{Z}_{\text{shared}}} \text{Tr}(\mathcal{F}_{AB}^{\mu\nu} \mathcal{F}_{AB,\mu\nu}) \sqrt{|G_{AB}|} \, d^D z

$$

*Proof.*

**Step 1.** By Definition {prf:ref}`def-gauge-covariant-game-tensor`, each agent's belief spinor $\psi^{(i)}$ transforms under local gauge $U^{(i)}(z) \in G_{\text{Fragile}}$.

**Step 2.** The joint space $\mathcal{Z}_A \times \mathcal{Z}_B$ carries a product gauge group $G^{(A)} \times G^{(B)}$. By the minimal coupling principle (Proposition {prf:ref}`prop-minimal-coupling`), dynamics on the joint space require a connection.

**Step 3.** The curvature $\mathcal{F}_{AB}^{\mu\nu}$ of Definition {prf:ref}`def-locking-curvature` measures the failure of the connection to be flat. By standard gauge theory, this curvature vanishes if and only if:

$$
A_\mu^{(A)}(z) \sim A_\mu^{(B)}(z) \quad \text{(gauge equivalent)}

$$

**Step 4.** The Yang-Mills action principle (Definition {prf:ref}`def-yang-mills-action`) states that physical configurations minimize the integrated curvature squared. Applying this to $\mathcal{A}_{AB}$ yields the Locking Operator.

**Step 5.** The normalization $-1/(4g_{\text{lock}}^2)$ ensures correct dimensionality: $[\mathfrak{L}_{\text{sync}}] = \text{nat}$.

**Step 6 (Identification).** The Locking Operator generates a **Synchronizing Potential** $\Psi_{\text{sync}}$ that penalizes geometric disagreement. By comparison geometry, the local Gromov-Hausdorff distance satisfies:

$$
d_{\text{GH}}(\mathcal{U}_A, \mathcal{U}_B) \leq C \cdot \|\mathcal{F}_{AB}\|^{1/2}

$$

for a universal constant $C > 0$. Thus $\mathfrak{L}_{\text{sync}}$ controls the metric alignment.

$\square$

:::

:::{prf:axiom} Finite Communication Bandwidth
:label: ax-finite-communication-bandwidth

The communication channel $\mathcal{L}$ between agents has finite Shannon capacity $C_{\mathcal{L}}$. By the Causal Information Bound (Theorem {prf:ref}`thm-causal-information-bound`):

$$
C_{\mathcal{L}} \leq \nu_D \cdot \frac{\text{Area}(\partial\mathcal{L})}{\ell_L^{D-1}}

$$

*Justification:* Communication occurs through the agent's boundary interface. The Area Law limits the information rate of any boundary channel.

:::

:::{prf:definition} The Gauge Alignment Order Parameter
:label: def-gauge-alignment-order-parameter

The **Gauge Alignment Order Parameter** measuring the relative orientation of agents' internal gauges is:

$$
\phi_{AB}(z) := \text{Tr}(U_A(z) U_B^\dagger(z)) \in \mathbb{C}

$$

where $U_A, U_B \in G_{\text{Fragile}}$ are the local gauge transformations. The **Locking Potential** governing its dynamics is:

$$
\mathcal{V}_{\text{lock}}(\phi_{AB}) = -\mu_{\text{lock}}^2 |\phi_{AB}|^2 + g_{\text{lock}} |\phi_{AB}|^4

$$

where:
- $\mu_{\text{lock}}^2 = \beta - \beta_c$ is the effective mass parameter
- $\beta$ is the interaction coupling strength
- $\beta_c$ is the critical coupling
- $g_{\text{lock}} > 0$ is the quartic self-interaction coefficient (stabilization term)

:::

:::{prf:theorem} Spontaneous Gauge Locking
:label: thm-spontaneous-gauge-locking

Consider two agents interacting in a shared environment $E$. If they minimize the joint prediction error:

$$
\mathcal{L}_{\text{joint}} = \|\hat{x}_{t+1}^A - x_{t+1}\|^2 + \|\hat{x}_{t+1}^B - x_{t+1}\|^2 + \beta \Psi_{\text{sync}}

$$

Then, as the interaction coupling $\beta \to \infty$, the system undergoes a phase transition where the internal gauge groups $U_A(z)$ and $U_B(z)$ become locked:

$$
U_A(z) \cdot U_B^{-1}(z) \to \text{const}.

$$

*Proof.*

**Step 1 (Setup).** Let $\psi^{(A)}, \psi^{(B)}$ be belief spinors (Definition {prf:ref}`def-cognitive-spinor`) with local gauge transformations:

$$
\psi'^{(i)} = U^{(i)}(z) \psi^{(i)}, \quad U^{(i)} \in G_{\text{Fragile}}

$$

**Step 2 (Prediction Error).** The prediction error for agent $i$ is:

$$
\epsilon^{(i)} = \|D^{(i)}(\psi^{(i)}) - x_{t+1}\|^2

$$

where $D^{(i)}$ is the TopologicalDecoder ({ref}`sec-decoder-architecture-overview-topological-decoder`).

**Step 3 (Relative Gauge).** Define the relative gauge transformation:

$$
\Delta U(z) := U_A(z) U_B^{-1}(z)

$$

When $\Delta U \neq \text{const}$, the agents encode the same environment state $x$ with spatially varying internal orientations.

**Step 4 (Synchronization Potential).** The synchronization term from Definition {prf:ref}`def-locking-curvature` is:

$$
\Psi_{\text{sync}} = \int_{\mathcal{Z}_{\text{shared}}} \text{Tr}(\mathcal{F}_{AB}^{\mu\nu} \mathcal{F}_{AB,\mu\nu}) \, d\mu_G

$$

**Step 5 (Joint Action).** The joint WFR action (Definition {prf:ref}`def-joint-wfr-action`) becomes:

$$
\mathcal{A}_{\text{joint}} = \mathcal{A}_{\text{WFR}}^{(A)} + \mathcal{A}_{\text{WFR}}^{(B)} + \beta \Psi_{\text{sync}}

$$

**Step 6 (Gradient Flow).** At equilibrium, the functional derivative vanishes:

$$
\frac{\delta \mathcal{A}_{\text{joint}}}{\delta A_\mu^{(i)}} = 0

$$

This yields coupled Yang-Mills equations for both agents.

**Step 7 (Strong Coupling Limit).** As $\beta \to \infty$, the synchronization term dominates. The energy minimum requires $\Psi_{\text{sync}} \to 0$, hence $\mathcal{F}_{AB}^{\mu\nu} \to 0$.

**Step 8 (Flat Connection).** By Theorem {prf:ref}`thm-three-cognitive-forces`, a vanishing field strength tensor implies:

$$
[D_{AB}^\mu, D_{AB}^\nu] = 0

$$

Parallel transport on the joint bundle is path-independent.

**Step 9 (Gauge Alignment).** For simply-connected $\mathcal{Z}_{\text{shared}}$, a flat connection is pure gauge:

$$
A_\mu^{(A)}(z) - A_\mu^{(B)}(z) = \partial_\mu \chi(z)

$$

for some $\chi: \mathcal{Z} \to \mathfrak{g}$.

**Step 10 (Gauge Fixing).** The gauge transformation $U_A \to U_A e^{-i\chi}$ absorbs the gradient term, yielding:

$$
A_\mu^{(A)}(z) = A_\mu^{(B)}(z)

$$

in this fixed gauge.

**Step 11 (Phase Transition).** The transition from $\beta < \beta_c$ (unlocked) to $\beta > \beta_c$ (locked) is a continuous phase transition. The order parameter is:

$$
\langle |\phi_{AB}| \rangle = \begin{cases}
0 & \beta < \beta_c \\
v_{\text{lock}} = \sqrt{(\beta - \beta_c)/g_{\text{lock}}} & \beta > \beta_c
\end{cases}

$$

This is analogous to Corollary {prf:ref}`cor-ontological-ssb`.

**Step 12 (Conclusion).** In the locked phase, $\Delta U(z) = U_A U_B^{-1} = \text{const}$, the constant being the residual global gauge freedom (the "shared coordinate system").

$\square$

:::

:::{prf:corollary} Critical Coupling for Locking
:label: cor-critical-coupling-locking

The critical coupling $\beta_c$ for spontaneous gauge locking is:

$$
\beta_c = \frac{\sigma^2 \text{Vol}(\mathcal{Z}_{\text{shared}})}{2 g_{\text{lock}}^2}

$$

where $\sigma$ is the Cognitive Action Scale (Definition {prf:ref}`def-cognitive-action-scale`).

*Proof.* Balance the kinetic (diffusion) term $\sigma^2 |\nabla \psi|^2$ against the synchronization potential $\beta \Psi_{\text{sync}}$. The transition occurs when coupling energy equals the thermal fluctuation scale. $\square$

:::

:::{prf:definition} Message as Lie Algebra Element
:label: def-message-lie-algebra

A **Message** $m_{A \to B}$ from Agent $A$ to Agent $B$ is an element of the Lie algebra $\mathfrak{g}$ of the gauge group:

$$
m_{A \to B} \in \mathfrak{g} = \text{Lie}(G_{\text{Fragile}}), \quad m = m^a T_a

$$

where $\{T_a\}$ are the generators satisfying $[T_a, T_b] = i f^{abc} T_c$.

*Interpretation:* A message is an **instruction** to apply an infinitesimal gauge transformation. The symbol sequence encodes the coefficients $m^a$. "Understanding" a message means successfully applying $e^{im}$ to one's internal manifold.

:::

:::{prf:definition} The Language Channel
:label: def-language-channel

The **Language Channel** $\mathcal{L}$ is a low-bandwidth projection of the full gauge algebra:

$$
\mathcal{L}: \mathfrak{g} \to \mathfrak{g}_{\mathcal{L}} \subset \mathfrak{g}

$$

where $\dim(\mathfrak{g}_{\mathcal{L}}) \ll \dim(\mathfrak{g})$. The channel satisfies the bandwidth constraint of Axiom {prf:ref}`ax-finite-communication-bandwidth`.

*Interpretation:* Language cannot transmit the full metric tensor. It projects onto a finite-dimensional subspace—the "expressible" portion of experience.

:::

:::{prf:definition} Gauge-Covariant Translation Operator
:label: def-translation-operator

The **Translation Operator** $\mathcal{T}_{A \to B}(m)$ induced by message $m$ along path $\gamma_{AB}$ is:

$$
\mathcal{T}_{A \to B}(m) := \exp\left(-ig \int_{\gamma_{AB}} m^a A_\mu^a \, dz^\mu\right) \cdot \mathcal{P}\exp\left(-ig \int_{\gamma_{AB}} A_\mu \, dz^\mu\right)

$$

where:
- The first factor encodes the **message content**
- The second factor is the **Wilson line** (parallel transport)
- $\mathcal{P}$ denotes path-ordering

*Properties:*
1. **Gauge Covariance:** $\mathcal{T}_{A \to B}$ transforms as $U_A \mathcal{T}_{A \to B} U_B^\dagger$
2. **Composition:** $\mathcal{T}_{A \to C} = \mathcal{T}_{B \to C} \circ \mathcal{T}_{A \to B}$
3. **Identity at Locking:** When $A^{(A)} = A^{(B)}$, reduces to pure message action

:::

:::{prf:definition} Semantic Alignment
:label: def-semantic-alignment

**Understanding** occurs when the message reduces metric friction:

$$
\text{Understanding}(m) \iff \mathcal{F}_{AB}(z; t+\Delta t) < \mathcal{F}_{AB}(z; t)

$$

after Agent $B$ receives and processes message $m$.

*Interpretation:* "Meaning" is not in the symbol $m$, but in the **metric update** $\Delta G_B = G_B(e^{im} \cdot) - G_B(\cdot)$ triggered by $m$. A symbol "means" the geometric transformation it induces in the listener.

:::

:::{prf:theorem} The Untranslatability Bound
:label: thm-untranslatability-bound

The **Untranslatability** $\mathcal{U}_{AB}(m)$ of message $m$ between agents with misaligned gauges is bounded by the integrated curvature:

$$
\mathcal{U}_{AB}(m) \leq \|m\| \cdot \oint_{\partial\Sigma} \|\mathcal{F}_{AB}\|_F \, dA

$$

where $\Sigma$ is any surface bounded by the communication path.

*Proof.*

**Step 1.** The translation operator around a closed loop $\gamma = \partial\Sigma$ yields the holonomy:

$$
\mathcal{H}_\gamma = \mathcal{P}\exp\left(-ig \oint_\gamma A_\mu \, dz^\mu\right)

$$

**Step 2.** By the non-Abelian Stokes theorem:

$$
\mathcal{H}_\gamma = \exp\left(-ig \int_\Sigma \mathcal{F}_{\mu\nu} \, dS^{\mu\nu}\right) + O(\mathcal{F}^2)

$$

**Step 3.** When $\mathcal{F}_{AB} \neq 0$, the holonomy is non-trivial: the message received by $B$ differs from the message sent by $A$.

**Step 4.** The discrepancy satisfies:

$$
\|m_{\text{received}} - m_{\text{sent}}\| \leq \|m\| \cdot \|\mathcal{H}_\gamma - \mathbb{1}\|

$$

**Step 5.** Bounding the holonomy deviation by the curvature integral via standard estimates yields the theorem.

$\square$

:::

:::{prf:corollary} Perfect Translation Requires Flat Connection
:label: cor-perfect-translation

Perfect translation ($\mathcal{U}_{AB} = 0$) is achievable for all messages if and only if the inter-agent curvature vanishes: $\mathcal{F}_{AB}^{\mu\nu} = 0$.

*Interpretation:* This is equivalent to Spontaneous Gauge Locking. Perfect mutual understanding requires complete geometric alignment.

:::

:::{prf:theorem} The Babel Limit
:label: thm-babel-limit

Let $\mathcal{L}$ be the Language Channel with Shannon capacity $C_{\mathcal{L}}$, and let $H(G_A)$ be the differential entropy rate of Agent $A$'s metric tensor. Complete gauge locking is achievable only if:

$$
\dim(\mathfrak{g}) \cdot H(G_A) \leq C_{\mathcal{L}}

$$

*Proof.*

**Step 1.** By Theorem {prf:ref}`thm-causal-information-bound`, the maximum information transmittable through the Language Channel is:

$$
C_{\mathcal{L}} = \nu_D \cdot \frac{\text{Area}(\partial\mathcal{L})}{\ell_L^{D-1}}

$$

**Step 2.** To achieve complete gauge alignment, Agent $A$ must transmit sufficient information to specify all $\dim(\mathfrak{g})$ independent gauge parameters.

**Step 3.** The information required to specify the metric tensor $G_A$ at rate $r$ is $r \cdot H(G_A)$ nats per unit time.

**Step 4.** For full alignment, the transmitted information must cover all gauge degrees of freedom:

$$
I_{\text{required}} = \dim(\mathfrak{g}) \cdot H(G_A)

$$

**Step 5.** If $I_{\text{required}} > C_{\mathcal{L}}$, complete locking is impossible by Shannon's theorem. The residual unlocked subspace has dimension:

$$
d_{\text{unlocked}} = \dim(\mathfrak{g}) - \lfloor C_{\mathcal{L}} / H(G_A) \rfloor

$$

$\square$

:::

:::{prf:corollary} The Ineffability Theorem
:label: cor-ineffability-theorem

When the Babel Limit is violated ($\dim(\mathfrak{g}) \cdot H(G_A) > C_{\mathcal{L}}$), there exists an unlocked subspace $\mathfrak{q} \subset \mathfrak{g}$ with:

$$
\dim(\mathfrak{q}) = \dim(\mathfrak{g}) - \lfloor C_{\mathcal{L}} / H(G_A) \rfloor > 0

$$

This subspace corresponds to **Private Qualia**: aspects of Agent $A$'s experience that cannot be communicated to Agent $B$ regardless of the symbol system used.

*Interpretation:* "Ineffability" is not mysticism—it is a Shannon capacity limit. Some experiences are incommunicable because the channel bandwidth is insufficient to transmit the metric information encoding them.

:::

:::{prf:definition} Metric Eigendecomposition
:label: def-metric-eigendecomposition

Decompose the metric tensor into its principal components:

$$
G_A = \sum_{k=1}^{D} \sigma_k^{(A)} v_k^{(A)} \otimes v_k^{(A)}

$$

where $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_D > 0$ are eigenvalues (principal curvatures) and $v_k^{(A)}$ are eigenvectors.

- **Core Concepts:** Components with $\sigma_k > \sigma_{\text{thresh}}$ (high information density)
- **Nuance:** Components with $\sigma_k \leq \sigma_{\text{thresh}}$ (low information density)

:::

:::{prf:theorem} Spectral Locking Order
:label: thm-spectral-locking-order

Under bandwidth-constrained communication, gauge locking proceeds in eigenvalue order. The locked subspace after time $T$ consists of the $k_{\max}$ highest eigenvalue components where:

$$
k_{\max} = \max\left\{k : \sum_{j=1}^k H(\sigma_j v_j) \leq C_{\mathcal{L}} \cdot T\right\}

$$

*Proof sketch.* Optimal channel coding allocates bandwidth to components by decreasing significance (eigenvalue magnitude). The waterfilling algorithm from information theory specifies the allocation. Locking proceeds from high-curvature (salient) features to low-curvature (subtle) features. $\square$

*Interpretation:* This explains why agents agree on "Gravity" (high eigenvalue, fundamental physics) before agreeing on "Politics" (low eigenvalue, high variance personal experience).

:::

:::{prf:theorem} Emergence of Objective Reality
:label: thm-emergence-objective-reality

In the limit of perfect locking ($\mathcal{F}_{AB} \to 0$), the private manifolds $\mathcal{Z}_A$ and $\mathcal{Z}_B$ collapse into a single **Quotient Manifold**:

$$
\mathcal{Z}_{\text{shared}} := (\mathcal{Z}_A \sqcup \mathcal{Z}_B) / \sim_{\text{isometry}}

$$

where $\sim_{\text{isometry}}$ identifies points with vanishing metric friction.

*Proof.*

**Step 1.** Perfect locking implies $\mathcal{F}_{AB}(z) = 0$ for all $z$.

**Step 2.** By Definition {prf:ref}`def-metric-friction`, this means:

$$
G_A(z) = \phi_{A \to B}^* G_B(\phi(z))

$$

The manifolds are isometric.

**Step 3.** Define the equivalence relation: $z_A \sim z_B$ iff $\phi_{A \to B}(z_A) = z_B$ and $G_A(z_A) = G_B(z_B)$.

**Step 4.** The quotient $\mathcal{Z}_{\text{shared}}$ inherits a well-defined metric from either $G_A$ or $G_B$ (they agree by isometry).

**Step 5.** To the agents, $\mathcal{Z}_{\text{shared}}$ appears as **Objective Reality**: it possesses properties (rigidity, persistence) that neither private imagination possesses alone.

$\square$

*Interpretation:* "Objective Reality" is a hallucination shared by $N$ agents with locked metrics. It is the fixed point of the consensus dynamics.

:::

:::{prf:remark} Echo Chamber Effect (Metric Drift)
:label: rem-echo-chamber-effect

If agents $A$ and $B$ minimize inter-agent friction $\mathcal{F}_{AB}$ but ignore environment friction $\mathcal{F}_{AE}$, $\mathcal{F}_{BE}$, they can spiral into a shared hallucination (folie à deux).

The corrected loss function must include grounding:

$$
\mathcal{L}_{\text{total}} = \lambda_{\text{lock}} \mathcal{F}_{AB} + \lambda_{\text{ground}} (\mathcal{F}_{AE} + \mathcal{F}_{BE})

$$

where $\mathcal{F}_{iE}$ measures the friction between agent $i$ and the environment's causal structure.

*Diagnostic:* Node 70 (BabelCheck) monitors $\partial \mathcal{F}_{AE}/\partial t$. If positive while $\mathcal{F}_{AB}$ decreases, the agents are drifting from ground truth.

:::

:::{prf:corollary} Critical Mass for Consensus
:label: cor-critical-mass-consensus

For a population of $N$ agents, spontaneous emergence of a shared "Objective Reality" requires:

$$
N > N_c = \frac{\sigma^2}{\lambda_{\text{lock}} \cdot \langle \mathcal{F}_{ij} \rangle}

$$

where $\langle \mathcal{F}_{ij} \rangle$ is the average pairwise friction.

*Interpretation:* Below critical mass, each agent maintains private reality. Above critical mass, a dominant consensus basin emerges—the "shared world."

:::

:::{prf:definition} The Institutional Manifold
:label: def-institutional-manifold

The **Institutional Manifold** $\mathcal{Z}_{\text{Inst}}$ is a **Static Reference Manifold** encoding shared conventions (Laws, Dictionaries, Money). Agents lock to the Institution rather than each other:

$$
\mathcal{F}_{A,\text{Inst}} + \mathcal{F}_{B,\text{Inst}} \quad \text{replaces} \quad \mathcal{F}_{AB}

$$

*Scaling:* Institution-mediated locking is $O(N)$ instead of $O(N^2)$.

:::

:::{prf:remark} Money as Universal Metric
:label: rem-money-universal-metric

**Money** is a **Universal Metric** in the institutional sense. It quantifies the "cost distance" between any two states:

$$
d_{\text{money}}(z_1, z_2) = \inf_{\gamma: z_1 \to z_2} \int_\gamma \text{Price}(\dot{z}) \, dt

$$

This provides a normalized gauge that allows agents with disjoint utility functions to coordinate.

*Interpretation:* Money emerges as the eigenmode of the institutional metric with highest consensus (largest eigenvalue in the shared subspace).

:::

## 07_cognition/09_retrieval_attention.md

:::{prf:proposition} Failure Modes of Standard Self-Attention for Memory
:label: prop-failure-modes-standard-self-attention

A self-attention mechanism $\text{Attention}(Q, K, V) = \text{softmax}(QK^T/\sqrt{d_k})V$ applied to memory sequences fails to preserve:

1. **Causality**: Attention weight $\alpha_{t,t'} > 0$ for $t' > t$ (future influences past).

2. **Metric structure**: The inner product $Q^T K$ treats $\mathcal{Z}$ as flat Euclidean. The capacity-constrained metric $G(z)$ (Theorem {prf:ref}`thm-capacity-constrained-metric-law`) implies position-dependent distances.

3. **Gauge covariance**: Under local gauge transformation $\psi \to U(z)\psi$, attention scores change. Physical predictions should be gauge-invariant.

4. **Finite information speed**: Even with causal mask $t' < t$, there is no constraint on spatial separation. Events outside the light cone can influence attention.

5. **Geodesic correction**: No Christoffel symbol encoding. Parallel transport along worldlines is not accounted for.

*Consequence*: Standard self-attention requires extensive regularization and does not guarantee physical consistency even then.

:::

:::{prf:definition} Memory Potential (Recap)
:label: def-memory-potential-recap

From Definition {prf:ref}`def-memory-potential`, the **memory potential** is:

$$
\Psi_{\text{mem}}(z) = -\int_0^T \alpha(t') H_\tau(z, \gamma(t')) \, dt'
$$

where:
- $\gamma: [0, T] \to \mathcal{Z}$ is the agent's trajectory
- $\alpha(t')$ is the reward flux at time $t'$ (Definition {prf:ref}`def-the-reward-flux`)
- $H_\tau(z, z')$ is the heat kernel on $(\mathcal{Z}, G)$ (Definition {prf:ref}`def-memory-kernel-via-heat-equation`)

*Units:* $[\Psi_{\text{mem}}] = \text{nat}$.

*Cross-reference:* This definition does not incorporate causal structure; the integration runs over *all* past times without regard to light-cone constraints.

:::

:::{prf:definition} Causal Information Potential (Recap)
:label: def-causal-information-potential-recap

From Definition {prf:ref}`def-causal-information-potential`, the **Causal Information Potential** is:

$$
\Psi_{\text{causal}}(z, a) := \mathbb{E}_{z' \sim \bar{P}(\cdot | z, a)} \left[ D_{\text{KL}} \left( p(\theta_W | z, a, z') \| p(\theta_W | z, a) \right) \right]
$$

*Units:* $[\Psi_{\text{causal}}] = \text{nat}$.

*Interpretation:* Measures where the world model is most uncertain and experiments would be most informative.

:::

:::{prf:definition} Information Speed
:label: def-information-speed-recap

The **information speed** $c_{\text{info}}$ is the maximum rate at which influence can propagate through the latent manifold:

$$
c_{\text{info}} := \sup_{z, z', t} \left\{ \frac{d_G(z, z')}{|t - t'|} : (z', t') \text{ can causally influence } (z, t) \right\}
$$

*Units:* $[c_{\text{info}}] = [z]/[t]$ (geodesic distance per interaction time).

*Safety constraint:* The causal buffer condition requires $c_{\text{info}} < c_{\text{buffer}}$ for agent safety, ensuring the agent can respond to environmental changes before they propagate across its entire state space.

:::

:::{prf:definition} Lorentzian Memory Manifold
:label: def-lorentzian-memory-manifold

The **Lorentzian memory manifold** is the product $\mathcal{M} = \mathbb{R} \times \mathcal{Z}$ equipped with the metric:

$$
g_{\mu\nu}(z, t) = \begin{pmatrix} -c_{\text{info}}^2 \lambda(z)^2 & 0 \\ 0 & G_{ij}(z) \end{pmatrix}
$$

where:
- $(t, z) \in \mathbb{R} \times \mathcal{Z}$ are spacetime coordinates
- $c_{\text{info}}$ is the information speed (Definition {prf:ref}`def-information-speed-recap`)
- $\lambda(z) = 2/(1-|z|^2)$ is the conformal factor (Poincaré disk)
- $G_{ij}(z) = \lambda(z)^2 \delta_{ij}$ is the spatial metric (Definition {prf:ref}`def-poincare-metric-recap`)

The signature is $(-,+,+,\ldots,+)$ with the time component negative.

*Conformal structure:* The metric is conformally flat: $g_{\mu\nu} = \lambda^2 \eta_{\mu\nu}$ where $\eta_{\mu\nu} = \text{diag}(-c_{\text{info}}^2, 1, \ldots, 1)$. This preserves the causal structure of flat spacetime in *coordinate* terms.

*Units:* $[g_{00}] = [z]^2/[t]^2 \cdot [z]^{-2} = [t]^{-2}$ (after absorbing $c_{\text{info}}$ units), $[g_{ij}] = [z]^{-2}$.

:::

:::{prf:definition} Effective Spacetime Interval
:label: def-spacetime-interval

The **effective spacetime interval** for memory causality between events $(z, t)$ and $(z', t')$ is:

$$
\Delta s^2_{\text{eff}} = -c_{\text{info}}^2 (t - t')^2 + d_G(z, z')^2
$$

where $d_G(z, z')$ is the geodesic distance on $(\mathcal{Z}, G)$ and $c_{\text{info}}$ is the information speed measured in *geodesic distance per unit time*.

*Mathematical remark:* This is an **effective** interval for defining causal structure, distinct from the metric-induced proper interval. The metric $g_{\mu\nu}$ in Definition {prf:ref}`def-lorentzian-memory-manifold` is conformally flat, so its null geodesics have constant *coordinate* speed $c_{\text{info}}$. However, for cognitive systems, the operationally meaningful constraint is that information travels at most $c_{\text{info}}$ in *proper (geodesic) distance* per unit time. This leads to the effective interval above.

*Equivalently:* The effective interval can be understood as the proper interval in a hypothetical metric $\tilde{g}_{\mu\nu} = \text{diag}(-c_{\text{info}}^2, 1, \ldots, 1)$ where spatial distances are measured in geodesic coordinates.

**Classification:**
- **Timelike** ($\Delta s^2_{\text{eff}} < 0$): $|t - t'| > d_G(z, z') / c_{\text{info}}$ — causal connection possible
- **Spacelike** ($\Delta s^2_{\text{eff}} > 0$): $|t - t'| < d_G(z, z') / c_{\text{info}}$ — no causal connection
- **Lightlike** ($\Delta s^2_{\text{eff}} = 0$): $|t - t'| = d_G(z, z') / c_{\text{info}}$ — boundary of light cone

:::

:::{prf:definition} Causal Past Light Cone
:label: def-causal-past-light-cone

The **causal past** of event $(z, t)$ is the set:

$$
J^-(z, t) := \left\{ (z', t') \in \mathcal{M} : t' < t \text{ and } \Delta s^2_{\text{eff}}(z, t; z', t') \leq 0 \right\}
$$

Equivalently, using the spacetime interval from Definition {prf:ref}`def-spacetime-interval`:

$$
J^-(z, t) = \left\{ (z', t') : t' < t \text{ and } d_G(z, z') \leq c_{\text{info}} (t - t') \right\}
$$

*Interpretation:* $J^-(z, t)$ contains all events from which information, traveling at most at speed $c_{\text{info}}$ (measured in geodesic distance per unit time), could have reached $(z, t)$.

*Boundary:* The past light cone $\partial J^-(z, t)$ consists of null geodesics emanating backward in time from $(z, t)$, where $d_G(z, z') = c_{\text{info}} (t - t')$.

:::

:::{prf:theorem} Memory Causality Constraint
:label: thm-memory-causality-constraint

Let $\alpha(z, t; z', t')$ be the attention weight from Query at $(z, t)$ to Key at $(z', t')$. Causality requires:

$$
\alpha(z, t; z', t') = 0 \quad \text{if } (z', t') \notin J^-(z, t)
$$

*Proof.* By the principle of finite information speed (Definition {prf:ref}`def-information-speed-recap`), no signal can propagate faster than $c_{\text{info}}$. An event outside the causal past cannot have influenced the present. Any non-zero attention weight to such events would constitute acausal information flow, violating the causal structure of $(\mathcal{M}, g)$. $\square$

*Implementation:* The constraint is enforced via a **causal mask** derived from the metric:

$$
M_{\text{causal}}(z, t; z', t') = \mathbf{1}\left[ (z', t') \in J^-(z, t) \right] = \mathbf{1}\left[ t' < t \text{ and } d_G(z, z') \leq c_{\text{info}} (t - t') \right]
$$

:::

:::{prf:definition} Covariant Self-Attention with Causal Mask
:label: def-covariant-self-attention-causal

The **Covariant Self-Attention** mechanism for memory is:

$$
\text{SelfAttn}(z, t) = \sum_{t'=1}^{T} \alpha(z, t; z_{t'}, t') \cdot V(z_{t'}, t')
$$

where the attention weight is:

$$
\alpha(z, t; z', t') = M_{\text{causal}}(z, t; z', t') \cdot \text{softmax}_{(z', t') \in J^-(z,t)}\left( \frac{Q(z, t)^T K(z', t')}{\tau(z, t)} \right)
$$

Components:
- **Query**: $Q(z, t) = \Pi_Q \cdot U_{0 \to (z,t)} \cdot D_\mu \psi_{\text{mem}}(z, t)$
- **Key**: $K(z', t') = \Pi_K \cdot U_{0 \to (z',t')} \cdot D_\nu \psi_{\text{mem}}(z', t')$
- **Value**: $V(z', t') = \Pi_V \cdot U_{0 \to (z',t')} \cdot \psi_{\text{mem}}(z', t')$
- **Temperature**: $\tau(z) = \sqrt{d_k} / \lambda(z)$ (metric-encoded)
- **Causal mask**: $M_{\text{causal}}(z, t; z', t') = \mathbf{1}[(z', t') \in J^-(z, t)]$

Here $U_{0 \to (z,t)}$ is the Wilson line from origin to $(z, t)$ along a causal geodesic, $D_\mu$ is the covariant derivative (Definition {prf:ref}`def-covariant-derivative-recap`), and $\Pi_Q, \Pi_K, \Pi_V$ are learnable projections.

*Units:* $[\alpha] = \text{dimensionless}$, $[\text{SelfAttn}] = [\psi]$.

:::

:::{prf:theorem} Gauge Invariance of Causal Self-Attention
:label: thm-gauge-invariance-causal-self-attention

The attention weight $\alpha(z, t; z', t')$ in Definition {prf:ref}`def-covariant-self-attention-causal` is invariant under local gauge transformations $\psi(x) \to \Omega(x)\psi(x)$.

*Proof.*

**Step 1.** By Theorem {prf:ref}`thm-gauge-invariance-cross-attention`, the Wilson line preprocessing ensures that $Q(z,t)^T K(z',t')$ is gauge-invariant: both Q and K are transported to the common reference point (origin), where the gauge transformation cancels.

**Step 2.** The causal mask $M_{\text{causal}}$ depends only on the metric structure via $J^-(z, t)$, which is gauge-invariant (the light cone is a geometric object determined by $g_{\mu\nu}$, not by gauge choice).

**Step 3.** The temperature $\tau(z) = \sqrt{d_k}/\lambda(z)$ depends only on the conformal factor, which is gauge-invariant.

**Step 4.** The softmax normalization is over $(z', t') \in J^-(z, t)$, a gauge-invariant set.

Therefore $\alpha(z, t; z', t')$ is gauge-invariant. $\square$

:::

:::{prf:definition} Temporal Christoffel Encoding
:label: def-temporal-christoffel-encoding

The **Temporal Geodesic Query** extends Definition {prf:ref}`def-geodesic-query-projection` to include temporal terms:

$$
Q_{\text{geo}}(x, z, t, v) = W_Q x + W_{Qz} z + W_{Qt} t + W_{Qv} v_{\text{feat}} + W_{Q,\Gamma}(z, z) + W_{Q,t}(t, t) + W_{Q,zt}(z, t)
$$

where:
- $W_Q \in \mathbb{R}^{d_k \times d_{\text{model}}}$: feature projection
- $W_{Qz} \in \mathbb{R}^{d_k \times d}$: spatial position
- $W_{Qt} \in \mathbb{R}^{d_k \times 1}$: temporal position
- $W_{Q,\Gamma} \in \mathbb{R}^{d_k \times d \times d}$: spatial Christoffel encoding
- $W_{Q,t} \in \mathbb{R}^{d_k \times 1 \times 1}$: temporal Christoffel encoding
- $W_{Q,zt} \in \mathbb{R}^{d_k \times d \times 1}$: mixed spacetime Christoffel encoding

**Lorentzian Christoffel structure:** For the metric $g_{\mu\nu} = \text{diag}(-c^2\lambda^2, \lambda^2 I_d)$ with $\lambda(z) = 2/(1-|z|^2)$, the non-zero Christoffel symbols are:

- **Spatial** ($\Gamma^k_{ij}$): $\Gamma^k_{ij} = \frac{2}{1-|z|^2}(\delta^k_i z_j + \delta^k_j z_i - \delta_{ij} z^k)$ (as in Proposition {prf:ref}`prop-christoffel-encoding-poincare`)
- **Time-time-space** ($\Gamma^0_{0j}$): $\Gamma^0_{0j} = \frac{\partial_j \lambda}{\lambda} = \frac{2z_j}{1-|z|^2}$ (gradient of log conformal factor)
- **Space-time-time** ($\Gamma^k_{00}$): $\Gamma^k_{00} = \frac{c^2}{\lambda} \partial_k \lambda = \frac{2c^2 z_k}{1-|z|^2}$ (acceleration term)

*Note:* $\Gamma^k_{0j} = 0$ and $\Gamma^0_{ij} = 0$ for this diagonal metric (no off-diagonal time-space mixing).

:::

:::{prf:proposition} Causal Wilson Line Along Worldline
:label: prop-causal-wilson-line

The Wilson line in Definition {prf:ref}`def-covariant-self-attention-causal` is computed along the **causal geodesic** connecting $(z', t')$ to $(z, t)$, not an arbitrary path.

For events in the causal past with small separation, the linearized Wilson line is:

$$
U_{(z',t') \to (z,t)} \approx I - i A_\mu(\bar{z}, \bar{t}) \Delta x^\mu
$$

where:
- $\Delta x^\mu = (t - t', z - z')$ is the spacetime displacement
- $\bar{z}, \bar{t}$ is a reference point (midpoint or initial)
- $A_\mu$ is the total gauge connection

For events outside the light cone, no causal geodesic exists, so the Wilson line is undefined. This is consistent with the causal mask zeroing out such contributions.

:::

:::{prf:definition} Lorentzian Cross-Attention
:label: def-lorentzian-cross-attention

The **Lorentzian Cross-Attention** for retrieval from an external archive $\mathcal{E} = \{(z'_i, t'_i, v_i)\}_{i=1}^N$ is:

$$
\text{CrossAttn}(z, t) = \sum_{i=1}^{N} \alpha_{\text{ret}}(z, t; z'_i, t'_i) \cdot V(z'_i, t'_i)
$$

where the **retarded attention weight** is:

$$
\alpha_{\text{ret}}(z, t; z', t') = \alpha_{\text{bare}}(z, t; z', t') \cdot \Theta_{\text{ret}}(z, t; z', t')
$$

with:
- **Bare weight**: $\alpha_{\text{bare}} = \text{softmax}\left( Q(z, t)^T K(z', t') / \tau(z) \right)$ (gauge-covariant)
- **Retarded factor**: $\Theta_{\text{ret}}(z, t; z', t') = \theta\left( t - t' - \frac{d_G(z, z')}{c_{\text{info}}} \right)$

Here $\theta$ is the Heaviside step function: $\theta(x) = 1$ if $x \geq 0$, else $0$.

*Interpretation:* The retarded factor enforces that $(z', t')$ must be in the causal past of $(z, t)$, with information having had time to propagate the geodesic distance at speed $c_{\text{info}}$.

*Units:* $[\alpha_{\text{ret}}] = \text{dimensionless}$.

:::

:::{prf:definition} Ghost Memory Interface
:label: def-ghost-memory-interface

The **Ghost Memory** at archive position $i$, as seen from Query position $(z, t)$, is the retarded image:

$$
\xi_i^{\text{ghost}}(z, t) = \xi_i(t - t_{\text{ret},i})
$$

where the **retardation time** is:

$$
t_{\text{ret},i} = \frac{d_G(z, z'_i)}{c_{\text{info}}}
$$

*Cross-reference:* This extends the Ghost Interface of Definition {prf:ref}`def-ghost-interface` to memory retrieval. The archive item appears "frozen" at the time when information about it could have reached the Query position.

*Implementation:* For memory buffers where $t_{\text{ret}} \ll \Delta t$ (retardation much smaller than timestep), the ghost image is approximately instantaneous. For external knowledge bases or multi-agent settings, retardation may be significant.

:::

:::{prf:theorem} Lorentzian Cross-Attention Structure
:label: thm-lorentzian-cross-attention-structure

The Lorentzian Cross-Attention (Definition {prf:ref}`def-lorentzian-cross-attention`) satisfies:

1. **Causality**: $\alpha_{\text{ret}}(z, t; z', t') = 0$ for $(z', t') \notin J^-(z, t)$

2. **Gauge covariance**: The bare weight $\alpha_{\text{bare}}$ is gauge-invariant (Theorem {prf:ref}`thm-gauge-invariance-cross-attention`)

3. **Retarded propagator structure**: In the continuum limit, the attention kernel approaches the **retarded Green's function**:

$$
G_{\text{ret}}(z, t; z', t') \propto \delta\left( t - t' - \frac{d_G(z, z')}{c_{\text{info}}} \right) \cdot \theta(t - t')
$$

4. **Lorentz invariance** (in flat limit): Under Lorentz boosts, the attention structure is preserved (the light cone is Lorentz-invariant).

*Proof sketch.*

**Part 1.** The retarded factor $\Theta_{\text{ret}}$ is zero when $t - t' < d_G(z, z')/c_{\text{info}}$, which is exactly the condition for $(z', t')$ being outside $J^-(z, t)$.

**Part 2.** Follows from Theorem {prf:ref}`thm-gauge-invariance-cross-attention` applied to the Q/K/V projections.

**Part 3.** The Heaviside function $\theta(t - t' - d_G/c)$ in the continuum becomes a delta function concentrated on the light cone when differentiated appropriately. This is the structure of the retarded Green's function for the wave equation.

**Part 4.** In the flat-space limit ($\lambda = 1$, Minkowski metric), the light cone $t - t' = |z - z'|/c$ is Lorentz-invariant by construction. $\square$

:::

:::{prf:definition} Causal Heat Kernel
:label: def-causal-heat-kernel

The **Causal Heat Kernel** on the Lorentzian memory manifold is:

$$
H_\tau^{\text{causal}}(z, t; z', t') := H_\tau(z, z') \cdot M_{\text{causal}}(z, t; z', t')
$$

where:
- $H_\tau(z, z')$ is the standard heat kernel on $(\mathcal{Z}, G)$ (Definition {prf:ref}`def-memory-kernel-via-heat-equation`)
- $M_{\text{causal}}(z, t; z', t') = \mathbf{1}[(z', t') \in J^-(z, t)]$ is the causal mask

*Interpretation:* The heat kernel provides spatial smoothing (how much influence a memory at $z'$ has on position $z$), while the causal mask restricts to events in the past light cone.

*Properties:*
- $H_\tau^{\text{causal}}(z, t; z', t') = 0$ if $t' \geq t$ (no future influence)
- $H_\tau^{\text{causal}}(z, t; z', t') = 0$ if $d_G(z, z') > c_{\text{info}}(t - t')$ (no superluminal influence)
- As $\tau \to 0$: $H_\tau^{\text{causal}} \to \delta(z - z') \cdot M_{\text{causal}}$

:::

:::{prf:theorem} Causal Memory Potential
:label: thm-causal-memory-potential

The **Causal Memory Potential** is:

$$
\Psi_{\text{mem}}^{\text{causal}}(z, t) := -\int_{J^-(z,t)} \alpha(t') H_\tau(z, \gamma(t')) \, d\mu_{J^-}(z', t')
$$

where the integration is over the causal past $J^-(z, t)$ with measure $d\mu_{J^-}$.

Equivalently, using the causal heat kernel:

$$
\Psi_{\text{mem}}^{\text{causal}}(z, t) = -\int_0^t \int_{\mathcal{Z}} \alpha(t') H_\tau^{\text{causal}}(z, t; z', t') \, d\mu_G(z') \, dt'
$$

*Comparison with Definition {prf:ref}`def-memory-potential-recap`:* The original memory potential integrates over all past times without regard to spatial separation. The causal memory potential restricts to events that could have causally influenced the present.

*Physical interpretation:* Only memories from within your light cone can pull you. Memories that are "too far away" (in spacetime) to have reached you yet have no influence.

:::

:::{prf:corollary} Memory Force with Causal Constraint
:label: cor-memory-force-causal

The memory-induced force (Lemma {prf:ref}`lem-virtual-work-of-recall`) becomes:

$$
\mathbf{f}_{\text{mem}}^{\text{causal}}(z, t) = -G^{-1}(z) \nabla_z \Psi_{\text{mem}}^{\text{causal}}(z, t)
$$

where the gradient is with respect to the spatial coordinates $z$, holding $t$ fixed.

*Properties:*
- The force is conservative (derived from a potential)
- The force respects causality (only causal past contributes)
- Near the light cone boundary, the force may have discontinuities as memories enter/exit the causal past

:::

:::{prf:definition} Causal BAOAB Steps for Memory
:label: def-causal-baoab-memory

The **Causal BAOAB** integrator for memory-augmented dynamics uses five steps as in Definition {prf:ref}`def-baoab-attention-heads`, with the memory potential modified to respect causality:

**Step 1 (B-step, first half-kick):**

$$
p \leftarrow p - \frac{h}{2} \nabla_z \left( \Phi_{\text{eff}}(z) + \Psi_{\text{mem}}^{\text{causal}}(z, t) \right)
$$

**Step 1.5 (Boris rotation, if $\mathcal{F} \neq 0$):** Apply the rotation from Definition {prf:ref}`def-baoab-splitting` using the Value Curl $\mathcal{F}$.

**Step 2 (A-step, first half-drift):**

$$
z \leftarrow z + \frac{h}{2} G^{-1}(z) p
$$

**Step 3 (O-step, thermostat):**

$$
p \leftarrow c_1 p + c_2 G^{1/2}(z) \xi, \quad \xi \sim \mathcal{N}(0, I)
$$
with $c_1 = e^{-\gamma h}$, $c_2 = \sqrt{(1 - c_1^2) T_c}$.

**Step 4 (A-step, second half-drift):**

$$
z \leftarrow z + \frac{h}{2} G^{-1}(z) p
$$

**Step 5 (B-step, second half-kick):**

$$
p \leftarrow p - \frac{h}{2} \nabla_z \left( \Phi_{\text{eff}}(z) + \Psi_{\text{mem}}^{\text{causal}}(z, t) \right)
$$

**Step 5.5 (Boris rotation, if $\mathcal{F} \neq 0$):** Apply the same rotation as in Step 1.5.

**Time update:** $t \leftarrow t + h$

The gradient $\nabla_z \Psi_{\text{mem}}^{\text{causal}}$ is computed via Covariant Self-Attention with causal mask, as the weighted sum over memory Keys in $J^-(z, t)$.

:::

:::{prf:theorem} Boltzmann Preservation with Causal Memory
:label: thm-boltzmann-causal-memory

The Causal BAOAB integrator (Definition {prf:ref}`def-causal-baoab-memory`) preserves the stationary distribution:

$$
\rho(z, p, t) \propto \exp\left( -\frac{\Phi_{\text{eff}}(z) + \Psi_{\text{mem}}^{\text{causal}}(z, t)}{T_c} - \frac{\|p\|_G^2}{2 T_c} \right)
$$

to second order in $h$, provided the causal memory potential varies slowly on the timescale $h$.

*Proof sketch.*

**Step 1.** The BAOAB splitting preserves the Boltzmann distribution for any potential $\Phi(z)$ (Theorem {prf:ref}`thm-baoab-attention-boltzmann`).

**Step 2.** The causal memory potential $\Psi_{\text{mem}}^{\text{causal}}$ adds to the effective potential. As long as the potential is well-defined and smooth, the preservation property extends.

**Step 3.** The causal mask introduces a time-dependence: as $t$ advances, more events enter the past light cone and can contribute to $\Psi_{\text{mem}}^{\text{causal}}$. This causes $\Psi_{\text{mem}}^{\text{causal}}(z, t)$ to change over time.

**Step 4.** For slowly varying $\Psi_{\text{mem}}^{\text{causal}}$ (change per timestep $\ll T_c$), the adiabatic approximation holds and the distribution tracks the instantaneous Boltzmann form.

**Step 5.** The causal mask does not break detailed balance because it is one-directional (past influences present, not vice versa). This is consistent with time-irreversibility of memory accumulation. $\square$

*Caveat:* Unlike standard BAOAB where the potential is time-independent, the causal memory potential grows as the agent accumulates history. The "stationary" distribution is actually quasi-stationary, tracking the evolving potential.

:::

## 08_multiagent/01_gauge_theory.md

:::{prf:definition} N-Agent Product Manifold
:label: def-n-agent-product-manifold

The global configuration space is the product manifold:

$$
\mathcal{Z}^{(N)} := \mathcal{Z}^{(1)} \times \mathcal{Z}^{(2)} \times \cdots \times \mathcal{Z}^{(N)}.

$$
The metric on $\mathcal{Z}^{(N)}$ is the direct sum of individual metrics:

$$
G^{(N)} := \bigoplus_{i=1}^N G^{(i)},

$$
where each $G^{(i)}$ is the capacity-constrained metric from Theorem {prf:ref}`thm-capacity-constrained-metric-law`. In coordinates, this is block-diagonal: if $\mathbf{z} = (z^{(1)}, \ldots, z^{(N)})$ with $z^{(i)} \in \mathbb{R}^{d_i}$, then $G^{(N)}_{\mu\nu}(\mathbf{z}) = G^{(i)}_{ab}(z^{(i)})$ when indices $\mu, \nu$ both lie in agent $i$'s block, and $G^{(N)}_{\mu\nu} = 0$ otherwise.

*Units:* $[G^{(N)}] = [z]^{-2}$.

*Remark (Isolated Agents).* The product metric $G^{(N)}$ describes agents in **isolation**—there is no cross-coupling between $\mathcal{Z}^{(i)}$ and $\mathcal{Z}^{(j)}$. Strategic coupling modifies this to $\tilde{G}^{(N)}$ via the Game Tensor ({ref}`sec-the-game-tensor-deriving-adversarial-geometry`).

:::

:::{prf:definition} Agent-Specific Boundary Interface
:label: def-agent-specific-boundary-interface

Each agent $i$ possesses its own symplectic boundary $(\partial\mathcal{Z}^{(i)}, \omega^{(i)})$ with:
- **Dirichlet component** (sensors): $\phi^{(i)}(x) = $ observation stream
- **Neumann component** (motors): $j^{(i)}_{\text{motor}}(x) = $ action flux
- **Reward component** (source): boundary reward flux $J_r^{(i)}$ (1-form); conservative case reduces to scalar charge
  density $\sigma_r^{(i)}$ (Definition {prf:ref}`def-the-reward-flux`)

The boundary conditions follow the structure of Definition {prf:ref}`def-dirichlet-boundary-condition-sensors`–23.1.3,
applied per-agent.

*Cross-reference:* {ref}`sec-the-symplectic-interface-position-momentum-duality` (Symplectic Boundary Manifold), Definition {prf:ref}`def-mass-tensor`.

:::

:::{prf:definition} Environment Distance
:label: def-environment-distance

Let $d_{\mathcal{E}}^{ij}$ denote the **environment distance** between agents $i$ and $j$—the geodesic length in the environment manifold $\mathcal{E}$ that information must traverse. This may differ from the latent distance $d_G(z^{(i)}, z^{(j)})$.

*Examples:*
- **Physical agents:** $d_{\mathcal{E}}^{ij} = $ spatial separation in meters
- **Networked agents:** $d_{\mathcal{E}}^{ij} = $ network hop distance or latency
- **Co-located agents:** $d_{\mathcal{E}}^{ij} = 0$ (shared boundary)

*Units:* $[d_{\mathcal{E}}^{ij}] = $ meters or equivalent environment-specific units.

:::

:::{prf:axiom} Information Speed Limit
:label: ax-information-speed-limit

There exists a maximum speed $c_{\text{info}} > 0$ at which information propagates through the environment $\mathcal{E}$. The **Causal Delay** between agents $i$ and $j$ is:

$$
\tau_{ij} := \frac{d_{\mathcal{E}}^{ij}}{c_{\text{info}}},

$$
where $d_{\mathcal{E}}^{ij}$ is the environment distance (Definition {prf:ref}`def-environment-distance`).

*Units:* $[c_{\text{info}}] = [\text{length}]/[\text{time}]$, $[\tau_{ij}] = [\text{time}]$.

*Examples:*
- **Physical systems:** $c_{\text{info}} = c \approx 3 \times 10^8$ m/s (speed of light)
- **Acoustic systems:** $c_{\text{info}} \approx 343$ m/s (speed of sound)
- **Networked systems:** $c_{\text{info}} \approx d/\text{latency}$ (effective propagation speed)
- **Co-located agents:** $c_{\text{info}} \to \infty$ effective limit when $d_{\mathcal{E}}^{ij} = 0$

:::

:::{prf:definition} Causal Interval
:label: def-causal-interval

The **Causal Interval** between spacetime events $(z^{(i)}, t_i)$ and $(z^{(j)}, t_j)$ is:

$$
\Delta s^2_{ij} := -c_{\text{info}}^2 (t_j - t_i)^2 + (d_{\mathcal{E}}^{ij})^2.

$$
The events are classified as:
- **Timelike** ($\Delta s^2_{ij} < 0$): $|t_j - t_i| > \tau_{ij}$. Causal influence is possible.
- **Spacelike** ($\Delta s^2_{ij} > 0$): $|t_j - t_i| < \tau_{ij}$. No causal influence is possible.
- **Lightlike** ($\Delta s^2_{ij} = 0$): $|t_j - t_i| = \tau_{ij}$. Boundary case.

*Consequence:* If agents $i$ and $j$ are spacelike separated at time $t$, no instantaneous Hamiltonian $H(z^{(i)}_t, z^{(j)}_t)$ can couple their states. Coupling must occur via retarded potentials.

:::

:::{prf:definition} Past Light Cone
:label: def-past-light-cone

The **Past Light Cone** of Agent $i$ at time $t$ is the set of all agent-time pairs that can causally influence Agent $i$:

$$
\mathcal{C}^-_i(t) := \left\{ (j, t') \in \{1,\ldots,N\} \times \mathbb{R} : t' \leq t - \tau_{ij} \right\}.

$$
The **Future Light Cone** is defined symmetrically:

$$
\mathcal{C}^+_i(t) := \left\{ (j, t') : t' \geq t + \tau_{ij} \right\}.

$$
*Physical interpretation:* Agent $i$ at time $t$ can only receive information from events in $\mathcal{C}^-_i(t)$ and can only influence events in $\mathcal{C}^+_i(t)$. The region outside both cones is causally disconnected.

:::

:::{prf:definition} Retarded Potential (Memory Screen)
:label: def-retarded-potential

Let $\rho^{(j)}_r(t, z)$ be the scalar source density associated with the conservative component of Agent $j$'s boundary
reward flux. The potential perceived by Agent $i$ at position $z$ and time $t$ is the **Retarded Potential**:

$$
\Psi_{\text{ret}}^{(i)}(t, z) = \sum_{j \neq i} \int_{-\infty}^{t} \int_{\mathcal{Z}^{(j)}} G_{\text{ret}}(z, t; \zeta, \tau) \rho^{(j)}_r(\tau, \zeta) \, d\mu_{G^{(j)}}(\zeta) \, d\tau,

$$
where $G_{\text{ret}}$ is the **Retarded Green's Function** for the wave operator on the manifold:

$$
G_{\text{ret}}(z, t; \zeta, \tau) \quad \text{solves} \quad \left(\frac{1}{c_{\text{info}}^2}\partial_t^2 - \Delta_G + \kappa^2\right)G_{\text{ret}} = \delta(z-\zeta)\delta(t-\tau),

$$
with $G_{\text{ret}} = 0$ for $t < \tau$. In flat space and the massless limit ($\kappa = 0$), $G_{\text{ret}}$ reduces to a light-cone delta; for $\kappa > 0$ it develops an interior light-cone tail.

*Interpretation:* Agent $i$ does not perceive Agent $j$'s current state. It perceives the "ghost" of Agent $j$ from time $\tau_{ij} = d_{\mathcal{E}}^{ij}/c_{\text{info}}$ ago.

*Units:* $[\Psi_{\text{ret}}] = \text{nat}$, $[G_{\text{ret}}] = [\text{length}]^{2-D}[\text{time}]^{-1}$.

*Remark (Strategic coupling).* When strategic relationships matter, weight each source by $\alpha_{ij}$; equivalently replace
$\rho^{(j)}_r$ with $\rho^{\text{ret}}_{ij}$ from Definition {prf:ref}`def-retarded-interaction-potential`.

:::

:::{prf:definition} Causal Bundle
:label: def-causal-bundle

The **Causal Bundle** is the augmented state space:

$$
\mathcal{Z}_{\text{causal}} := \mathcal{Z}^{(N)} \times \Xi_{<t},

$$
where:
- $\mathcal{Z}^{(N)} = \prod_i \mathcal{Z}^{(i)}$ is the product configuration space (Definition {prf:ref}`def-n-agent-product-manifold`)
- $\Xi_{<t}$ is the **Memory Screen** restricted to the causal past (Definition {prf:ref}`def-memory-screen`): $\Xi_{<t} = \{(\gamma(t'), \alpha(t')) : t' < t\}$

The **Relativistic State** for Agent $i$ at time $t$ is:

$$
\mathcal{S}^{(i)}_t := \left( z^{(i)}_t, \Xi^{(i)}_{<t} \right),

$$
where $\Xi^{(i)}_{<t}$ stores the history of received retarded potentials over the interval $[t - \tau_{\text{horizon}}, t)$.

*Operational validity:* $\Xi^{(i)}_{<t}$ is locally observable at time $t$. The "true" global state of all agents is hidden, but $\mathcal{S}^{(i)}_t$ is a sufficient statistic for Agent $i$'s optimal policy within its future light cone.

:::

:::{prf:theorem} Markov Restoration on Causal Bundle
:label: thm-markov-restoration

Let $P(z^{(N)}_{t+\Delta t} | z^{(N)}_t, \Xi_{<t})$ denote the transition probability. When agents have finite causal delay $\tau_{ij} > 0$:

1. **On $\mathcal{Z}^{(N)}$ alone:** The Markov property fails:

   $$
   P(z^{(N)}_{t+\Delta t} | z^{(N)}_t) \neq P(z^{(N)}_{t+\Delta t} | z^{(N)}_{\leq t}).

   $$

2. **On $\mathcal{Z}_{\text{causal}}$:** The Markov property is restored:

   $$
   P\left((z^{(N)}_{t+\Delta t}, \Xi_{<t+\Delta t}) \,\big|\, (z^{(N)}_t, \Xi_{<t})\right) = P\left((z^{(N)}_{t+\Delta t}, \Xi_{<t+\Delta t}) \,\big|\, \text{full history}\right).

   $$

*Proof sketch.* The Memory Screen $\Xi_{<t}$ encodes all information about past states that can causally influence the future. By the definition of the past light cone (Definition {prf:ref}`def-past-light-cone`), no additional information from $\Xi_{<t'}$ for $t' < t$ is needed beyond what is already encoded in $\Xi_{<t}$. The causal structure guarantees that spacelike-separated events cannot contribute new information. See **{ref}`sec-appendix-e-rigorous-proof-sketches-for-ontological-and-metabolic-laws`** for the complete proof using causal factorization and Chapman-Kolmogorov. $\square$

:::

:::{prf:corollary} Memory as Physical Necessity
:label: cor-memory-physical-necessity

In the relativistic multi-agent setting, the Memory Screen (Definition {prf:ref}`def-memory-screen`) is not an optional enhancement but a **physical requirement** for a well-posed control problem. Without it, the agent's state is non-Markovian, and optimal control theory does not apply.

*Cross-reference:* This elevates the role of $\Xi_{<t}$ from {ref}`sec-the-historical-manifold-and-memory-screen`, where it served as a recording device for trajectory history, to a primary state variable that restores the Markov property.

:::

:::{prf:definition} Ghost Interface
:label: def-ghost-interface

The **Ghost Interface** $\mathcal{G}_{ij}(t)$ between agents $i$ and $j$ at time $t$ is:

$$
\mathcal{G}_{ij}(t) := \partial\mathcal{Z}^{(i)}(t) \times \partial\mathcal{Z}^{(j)}(t - \tau_{ij}),

$$
coupling Agent $i$'s current boundary to Agent $j$'s past boundary, where $\tau_{ij} = d_{\mathcal{E}}^{ij}/c_{\text{info}}$ is the causal delay.

The **Ghost Symplectic Structure** is:

$$
\omega_{\mathcal{G},ij} := \omega^{(i)}(t) \oplus \omega^{(j)}(t - \tau_{ij})\big|_{\mathcal{G}_{ij}}.

$$

*Mechanism:* Agent $i$ couples not to $z^{(j)}_t$, but to the **Ghost State** $\hat{z}^{(j)}_t := z^{(j)}_{t-\tau_{ij}}$—the state of Agent $j$ when the signal was emitted.

*Units:* $[\tau_{ij}] = [\text{time}]$.

:::

:::{prf:proposition} Interaction Kernel
:label: prop-interaction-kernel

The **pairwise interaction potential** $\Phi_{\text{int}}: \mathcal{Z} \times \mathcal{Z} \to \mathbb{R}$ between agents at positions $z, \zeta$ is the screened Green's function weighted by influence:

$$
\Phi_{\text{int}}(z, \zeta) := \alpha \cdot \mathcal{G}_{\kappa}(z, \zeta)

$$
where $\mathcal{G}_{\kappa}$ is the screened Green's function (Proposition {prf:ref}`prop-green-s-function-interpretation`) and $\alpha$ encodes the strategic relationship.

*Properties:*
- $\Phi_{\text{int}}(z, \zeta) = \Phi_{\text{int}}(\zeta, z)$ (symmetric in cooperative settings)
- $\Phi_{\text{int}} \to 0$ as $d_G(z, \zeta) \to \infty$ (locality via screening)
- $\nabla^2_z \Phi_{\text{int}}$ defines the Game Tensor contribution (Definition {prf:ref}`def-the-game-tensor`)
:::

:::{prf:definition} Retarded Interaction Potential
:label: def-retarded-interaction-potential

The **Retarded Interaction Source Density** from Agent $j$ to Agent $i$ is:

$$
\rho^{\text{ret}}_{ij}(\zeta, \tau) := \alpha_{ij} \cdot \rho^{(j)}_r(\zeta, \tau),

$$
where:
- $\rho^{(j)}_r$ is the conservative reward source density for Agent $j$ derived from boundary reward flux
  (Definition {prf:ref}`def-the-reward-flux`)
- $\alpha_{ij} \in \{-1, 0, +1\}$ encodes the strategic relationship:
  - $\alpha_{ij} = +1$: Cooperative
  - $\alpha_{ij} = 0$: Independent
  - $\alpha_{ij} = -1$: Adversarial

We write $\rho^{\text{ret}}_{ij}$ on $\mathcal{Z}^{(j)}$ and pull it back to Agent $i$'s chart along the Ghost Interface;
for notational simplicity, we suppress the pullback in what follows.

The induced **Retarded Interaction Potential** is the retarded Green's function convolution:

$$
\Phi^{\text{ret}}_{ij}(z^{(i)}, t) = \int_{-\infty}^{t} \int_{\mathcal{Z}^{(j)}} G_{\text{ret}}(z^{(i)}, t; \zeta, \tau)\,
\rho^{\text{ret}}_{ij}(\zeta, \tau)\, d\mu_{G^{(j)}}(\zeta)\, d\tau,

$$
where $G_{\text{ret}}$ is the retarded Green's function (Definition {prf:ref}`def-retarded-potential`).

*Remark (Point-source / ghost limit).* If Agent $j$'s conservative source is concentrated along a trajectory,
$\rho^{(j)}_r(\zeta, \tau) = \sigma^{(j)}_r(\tau)\,\delta(\zeta - z^{(j)}_\tau)$, then

$$
\Phi^{\text{ret}}_{ij}(z^{(i)}, t) = \alpha_{ij}\int_{-\infty}^{t} G_{\text{ret}}(z^{(i)}, t; z^{(j)}_\tau, \tau)\,
\sigma^{(j)}_r(\tau)\, d\tau,
$$
which reduces to evaluation at the retarded time in the massless flat-space limit. This recovers the ghost-state
interpretation.

*Remark (Quasi-static kernel).* In the low-frequency limit, $G_{\text{ret}}$ reduces to the screened static kernel
$\mathcal{G}_\kappa$ and the potential can be approximated by evaluating the instantaneous interaction at the ghost
state. This is a computational shortcut, not the first-principles definition.

*Remark (Non-conservative component).* Solenoidal reward components are not captured by the scalar source; they enter via
the curl field in the dynamics.

:::

:::{prf:theorem} Strategic Delay Tensor
:label: thm-strategic-delay-tensor

The effective coupling tensor $\mathcal{T}_{ij}$ between agents splits into instantaneous and retarded components:

$$
\mathcal{T}_{ij}^{\text{total}}(t) = \underbrace{\mathcal{T}_{ij}^{\text{local}}(t)}_{\text{Short-range}} + \underbrace{\int_{-\infty}^t \mathcal{K}_{\text{delay}}(t-\tau) \mathcal{T}_{ij}^{\text{ghost}}(\tau) \, d\tau}_{\text{Long-range Retarded}},

$$
where $\mathcal{K}_{\text{delay}}(t-\tau) = \delta(t - \tau - \tau_{ij})$ is the delay kernel.

**Adversarial consequence:** Against a distant adversary, the effective metric inflation (from the Game Tensor) is delayed. An agent may commit to an aggressive trajectory only to experience a "wall" of increased inertia arriving from the opponent's past actions.

*Proof.* Expand the coupled value equation to second order in the retarded potential. The cross-Hessian $\partial^2 V^{(i)} / \partial z^{(j)} \partial z^{(j)}$ evaluated at $z^{(j)}_{t-\tau_{ij}}$ yields the delayed Game Tensor contribution. $\square$

:::

:::{prf:corollary} Newtonian Limit
:label: cor-newtonian-limit-ghost

As $c_{\text{info}} \to \infty$, the causal delay vanishes: $\tau_{ij} \to 0$ for all pairs. The Ghost Interface reduces to the instantaneous interface:

$$
\lim_{c_{\text{info}} \to \infty} \mathcal{G}_{ij}(t) = \partial\mathcal{Z}^{(i)}(t) \times \partial\mathcal{Z}^{(j)}(t),

$$
and the retarded potential becomes instantaneous:

$$
\lim_{c_{\text{info}} \to \infty} \Phi^{\text{ret}}_{ij}(z^{(i)}, t) = \Phi_{ij}(z^{(i)}, z^{(j)}_t),

$$
where $\Phi_{ij}$ denotes the instantaneous interaction potential (the quasi-static limit of Definition {prf:ref}`def-retarded-interaction-potential`).

*Interpretation:* Co-located agents ($d_{\mathcal{E}}^{ij} = 0$) or systems with negligible propagation delay operate in the Newtonian regime where standard MARL applies.

:::

:::{prf:theorem} HJB-Klein-Gordon Correspondence
:label: thm-hjb-klein-gordon

Let information propagate at speed $c_{\text{info}}$. The Value Function $V^{(i)}(z, t)$ for Agent $i$ satisfies the **Screened Wave Equation**:

$$
\boxed{\left( \frac{1}{c_{\text{info}}^2} \frac{\partial^2}{\partial t^2} - \Delta_{G^{(i)}} + \kappa_i^2 \right) V^{(i)}(z, t) = \rho^{(i)}_r(z, t) + \sum_{j \neq i} \rho^{\text{ret}}_{ij}(z, t)}

$$
where:
- $\square_{G} = \frac{1}{c_{\text{info}}^2}\partial_t^2 - \Delta_G$ is the **D'Alembertian** on the manifold. With spacetime metric $g_{\mu\nu} = \text{diag}(-c_{\text{info}}^2, G_{ij})$, this equals
  
  $$
  \square_G = -\frac{1}{\sqrt{|g|}}\partial_\mu\left(\sqrt{|g|}g^{\mu\nu}\partial_\nu\right).
  $$
- $\kappa_i$ is the **spatial screening mass** with $[\kappa_i] = 1/[\text{length}]$. Let the temporal discount rate be $\lambda_i := -\ln\gamma_i / \Delta t$ (units $1/[\text{time}]$). Using the relativistic conversion $\text{length} = c_{\text{info}} \cdot \text{time}$ gives $\kappa_i = \lambda_i / c_{\text{info}}$. In natural units ($\Delta t = 1$, $c_{\text{info}} = 1$) this reduces to $\kappa_i = -\ln\gamma_i$.

*Notation.* When we write the spacetime metric $g_{\mu\nu} = \text{diag}(-c_{\text{info}}^2, G_{ij})$ in this section, $G_{ij}$ denotes the **active** spatial metric for the equation at hand (the intrinsic $G$ for scalar $V$, or the strategic $\tilde{G}$ when the game-augmented metric is used).
- $\rho^{(i)}_r$ is the local **conservative** reward source density derived from boundary reward flux
  (Definition {prf:ref}`def-the-reward-flux`) (units: $[\text{nat}]/[\text{length}]^2$)
- $\rho^{\text{ret}}_{ij}$ is the retarded interaction source density (Definition {prf:ref}`def-retarded-interaction-potential`)

*Optional damping.* A linear term $\gamma_{\text{damp}}\partial_t V$ may be added to model explicit non-stationary friction or learning lag. This is a modeling choice distinct from the discount-induced screening $\kappa$.

*Scope.* This equation governs the scalar potential associated with the conservative component of the reward 1-form.
Solenoidal/harmonic components induce velocity-dependent coupling and are handled by the curl field $\mathcal{F}$ in the
dynamics.

*Proof sketch.* Expand the Bellman recursion $V(z, t) = r \Delta t + \gamma \mathbb{E}[V(z', t+\Delta t)]$ to second order in both spatial and temporal increments. The finite propagation speed $c_{\text{info}}$ introduces the wave term $\partial_t^2 V$. The derivation parallels the passage from Poisson to wave equation in electrostatics vs. electrodynamics. See {ref}`sec-appendix-e-rigorous-proof-sketches-for-ontological-and-metabolic-laws`. $\square$

*Character:* This is a hyperbolic PDE (wave equation with mass and damping), in contrast to the elliptic Helmholtz equation of {ref}`sec-the-bulk-potential-screened-poisson-equation`.

:::

:::{prf:corollary} Value Wavefront Propagation
:label: cor-value-wavefront

A sudden change in conservative reward source at location $z_A$ and time $t_0$ propagates outward as a **Value
Wavefront**:

$$
V(z, t) \sim \Theta\!\left(t - t_0 - \frac{d_G(z, z_A)}{c_{\text{info}}}\right)\, e^{-\kappa d_G(z, z_A)} \cdot \rho_r(z_A, t_0),

$$
where $\Theta$ is the Heaviside step function enforcing causality. This expression is schematic: the prefactor is dimension-dependent, and for $\kappa > 0$ the exact retarded Green's function has an interior light-cone tail (Bessel decay) rather than a pure delta on the cone.

*Interpretation:* The Value surface is not a static potential but a dynamic "ocean" of interfering causal ripples.
Conservative reward shocks propagate at speed $c_{\text{info}}$, decaying exponentially with the screening length
$1/\kappa$.

:::

:::{prf:corollary} Helmholtz as Newtonian Limit
:label: cor-helmholtz-limit

In the limit $c_{\text{info}} \to \infty$, the temporal derivatives become negligible:

$$
\frac{1}{c_{\text{info}}^2} \frac{\partial^2 V}{\partial t^2} \to 0,

$$
and the Klein-Gordon equation reduces to the **stationary Helmholtz equation**:

$$
(-\Delta_G + \kappa^2) V = \rho_r + \sum_{j \neq i} \rho^{\text{ret}}_{ij}.

$$
This recovers Theorem {prf:ref}`thm-the-hjb-helmholtz-correspondence` as the instantaneous (Newtonian) limit.

:::

:::{prf:proposition} Retarded Green's Function
:label: prop-retarded-greens-function

The solution to the inhomogeneous Klein-Gordon equation is given by convolution with the **Retarded Green's Function**:

$$
V^{(i)}(z, t) = \int_{-\infty}^{t} \int_{\mathcal{Z}^{(i)}} G_{\text{ret}}(z, t; \zeta, \tau) \left[ \rho^{(i)}_r(\zeta, \tau) + \sum_{j \neq i} \rho^{\text{ret}}_{ij}(\zeta, \tau) \right] d\mu_{G^{(i)}}(\zeta) \, d\tau,

$$
where $G_{\text{ret}}$ satisfies:

$$
\left( \frac{1}{c_{\text{info}}^2} \frac{\partial^2}{\partial t^2} - \Delta_G + \kappa^2 \right) G_{\text{ret}}(z, t; \zeta, \tau) = \delta(z - \zeta)\delta(t - \tau),

$$
with the **causal boundary condition** $G_{\text{ret}} = 0$ for $t < \tau$.

*Massless flat-space example (D = 3):* For $\mathcal{Z} = \mathbb{R}^3$ and $\kappa = 0$,

$$
G_{\text{ret}}(z, t; \zeta, \tau) = \frac{\Theta(t - \tau)}{4\pi |z - \zeta|} \delta\left(t - \tau - \frac{|z-\zeta|}{c_{\text{info}}}\right).

$$
For $\kappa > 0$, the retarded kernel acquires an interior light-cone tail with Bessel decay; we keep $G_{\text{ret}}$ abstract to avoid dimension-specific formulas.

:::

:::{prf:definition} The Game Tensor
:label: def-the-game-tensor

We define the **Game Tensor** $\mathcal{G}_{ij}^{kl}$ as the cross-Hessian of Agent $i$'s value with respect to Agent $j$'s position:

$$
\mathcal{G}_{ij}^{kl}(z^{(i)}, z^{(j)}) := \frac{\partial^2 V^{(i)}}{\partial z^{(j)}_k \partial z^{(j)}_l}\bigg|_{z^{(j)} = z^{(j)*}},

$$
where $z^{(j)*}$ is Agent $j$'s current position (or expected position under their policy). This tensor measures how sensitive Agent $i$'s value landscape is to Agent $j$'s location.

*Units:* $[\mathcal{G}_{ij}^{kl}] = \text{nat}/[z]^2$.

*Remark (Heterogeneous manifolds).* $\mathcal{G}_{ij}$ lives in $T_{z^{(j)}}\mathcal{Z}^{(j)}$. To compare or add it to Agent $i$'s metric, pull it back along the Ghost Interface using the Strategic Jacobian $\mathcal{J}_{ji}: T_{z^{(i)}}\mathcal{Z}^{(i)} \to T_{z^{(j)}}\mathcal{Z}^{(j)}$:

$$
\mathcal{G}_{ij,mn} := G^{(j)}_{mp} G^{(j)}_{nq}\mathcal{G}_{ij}^{pq}, \quad
\mathcal{G}^{(i)}_{ij,kl} := (\mathcal{J}_{ji})^m{}_k (\mathcal{J}_{ji})^n{}_l \mathcal{G}_{ij,mn}.

$$

**Derivation 29.4.2 (The Strategic Metric).** Recall the **Capacity-Constrained Metric Law** (Theorem {prf:ref}`thm-capacity-constrained-metric-law`), where curvature is driven by the Risk Tensor $T_{ab}$. See **{ref}`sec-appendix-e-rigorous-proof-sketches-for-ontological-and-metabolic-laws`** for the formal derivation of the Strategic Jacobian and Game Tensor using the implicit function theorem.

For Agent $i$, the "risk" includes the **Predictive Volatility** of the adversary $j$. If Agent $i$ updates its state by $\delta z^{(i)}$, and the adversary $j$ responds with $\delta z^{(j)} \approx \mathcal{J}_{ji} \delta z^{(i)}$ (where $\mathcal{J}_{ji}$ is the **Strategic Jacobian**—the best-response derivative, see Definition {prf:ref}`def-strategic-jacobian`), write the block Hessians
$H_{ab} := \nabla_{z^{(a)}}\nabla_{z^{(b)}} V^{(i)}$. The second-order variation is:

$$
\delta^2 V^{(i)} = \delta z_i^\top H_{ii}\,\delta z_i + 2\,\delta z_i^\top H_{ij}\,\delta z_j + \delta z_j^\top H_{jj}\,\delta z_j.

$$

Substituting $\delta z_j = \mathcal{J}_{ji}\delta z_i$ gives the general quadratic form:

$$
\delta^2 V^{(i)} = (\delta z^{(i)})^\top\!\left( H_{ii} + H_{ij}\mathcal{J}_{ji} + \mathcal{J}_{ji}^\top H_{ji} + \mathcal{J}_{ji}^\top H_{jj}\mathcal{J}_{ji} \right)\!\delta z^{(i)}.

$$
**Agent $i$'s perceived geometry** is modified by adversarial presence as follows:

1. **Effective metric inflation.** In regions where the strategic back-reaction has positive eigenvalues (adversarial curvature), Agent $i$ perceives an inflated metric:

   $$
   \tilde{G}^{(i)}_{kl}(z) = G^{(i)}_{kl}(z) + \sum_{j \neq i} \beta_{ij} \cdot \mathcal{G}^{(i)}_{ij,kl}(z),

   $$
   where $\beta_{ij} > 0$ for adversarial agents, $\beta_{ij} = 0$ for neutral, $\beta_{ij} < 0$ for cooperative.

2. **Geodesic deflection.** The Christoffel symbols acquire correction terms from the metric perturbation:

   $$
   \tilde{\Gamma}^{(i),m}_{kl} = \Gamma^{(i),m}_{kl} + \frac{1}{2}(G^{(i)})^{mn}\left(\nabla_k (\beta \mathcal{G})_{nl} + \nabla_l (\beta \mathcal{G})_{nk} - \nabla_n (\beta \mathcal{G})_{kl}\right),

   $$
   where $(\beta\mathcal{G})_{kl} := \sum_{j \neq i} \beta_{ij} \mathcal{G}^{(i)}_{ij,kl}$.

3. **Risk amplification.** High $\|\mathcal{G}_{ij}\|$ regions correspond to strategic uncertainty. This contributes to the Risk Tensor (Theorem {prf:ref}`thm-capacity-constrained-metric-law`):

   $$
   T^{(i)}_{kl} \to T^{(i)}_{kl} + \gamma_{\text{game}} \sum_{j \neq i} |\beta_{ij}| \cdot \mathcal{G}^{(i)}_{ij,kl}.

   $$
*Physical interpretation:* Adversarial agents effectively "curve" each other's latent space. An agent approaching a contested region experiences increased geodesic resistance (higher mass), making aggressive maneuvers more costly.

**The sign structure** of the pulled-back Game Tensor $\mathcal{G}^{(i)}_{ij}$ determines the strategic relationship:

| Eigenvalue Structure | $\text{sgn}(\det \mathcal{G}^{(i)}_{ij})$ | Interpretation                                              |
|----------------------|-------------------------------------|-------------------------------------------------------------|
| All positive         | $+$                                 | Adversarial: $j$'s presence increases $i$'s value curvature |
| All negative         | $(-1)^d$                            | Cooperative: $j$'s presence smooths $i$'s value landscape   |
| Mixed signs          | varies                              | Mixed-motive game                                           |
| Near-zero            | $\approx 0$                         | Weakly coupled (near-independent)                           |

The trace $\operatorname{tr}(\mathcal{G}^{(i)}_{ij}) = \sum_k \mathcal{G}^{(i)}_{ij}{}^{kk}$ measures **total strategic sensitivity**: how much Agent $i$'s value curvature depends on Agent $j$'s position. Large $|\operatorname{tr}(\mathcal{G}^{(i)}_{ij})|$ indicates high strategic coupling; small trace indicates approximate independence.

*Cross-reference:* The Game Tensor generalizes the conformal factor $\Omega$ (Definition {prf:ref}`def-value-metric-conformal-coupling`) to the multi-agent setting. Where $\Omega$ captured self-induced value curvature, $\mathcal{G}_{ij}$ captures cross-agent value curvature.

*Cross-reference (Gauge-Consistent Version):* For scalar $V^{(i)}$, the correct covariant object is the Riemannian Hessian $\nabla_k\nabla_l V^{(i)}$ (Definition {prf:ref}`def-gauge-covariant-game-tensor`). Gauge covariance becomes relevant only if one defines cross-sensitivities of gauge-charged fields (e.g., $\psi$ or nuisance-frame vectors).

:::

:::{prf:theorem} Adversarial Mass Inflation
:label: thm-adversarial-mass-inflation

In a competitive game where Agent $j$ is adversarial ($\beta_{ij} > 0$) and the pulled-back Game Tensor $\mathcal{G}^{(i)}_{ij}$ is positive semi-definite, the effective metric $\tilde{G}^{(i)}$ satisfies:

$$
\tilde{G}^{(i)}_{kl} \xi^k \xi^l \geq G^{(i)}_{kl} \xi^k \xi^l \quad \forall \xi \in T_{z}\mathcal{Z}^{(i)}.

$$
*Consequence:* The effective **Mass** $M^{(i)}(z)$ (Definition {prf:ref}`def-mass-tensor`) of Agent $i$ increases: $\tilde{M}^{(i)} \geq M^{(i)}$.

*First-Principles Interpretation:* Adversarial presence "thickens" the latent space. The agent moves more slowly (smaller geodesic steps) because it must account for the adversary's counter-maneuvers. **Strategic uncertainty is geometrically identical to physical inertia.**

*Proof.* From Definition {prf:ref}`def-the-game-tensor`, the metric perturbation is $\delta G_{kl} = \sum_{j} \beta_{ij} \mathcal{G}^{(i)}_{ij,kl}$. For adversarial agents, $\beta_{ij} > 0$. If $\mathcal{G}^{(i)}_{ij}$ is positive semi-definite (which occurs when Agent $j$'s presence increases the curvature of $V^{(i)}$), then $\mathcal{G}^{(i)}_{ij,kl} \xi^k \xi^l \geq 0$ for all $\xi$. Thus $\tilde{G}^{(i)}_{kl} \xi^k \xi^l = G^{(i)}_{kl} \xi^k \xi^l + \beta_{ij} \mathcal{G}^{(i)}_{ij,kl} \xi^k \xi^l \geq G^{(i)}_{kl} \xi^k \xi^l$. $\square$

:::

:::{prf:definition} Retarded Game Tensor
:label: def-retarded-game-tensor

Under finite information speed $c_{\text{info}}$, the Game Tensor acquires a **retarded component**. The **Retarded Game Tensor** is:

$$
\mathcal{G}_{ij}^{kl,\text{ret}}(z^{(i)}, t) := \frac{\partial^2 V^{(i)}}{\partial z^{(j)}_k \partial z^{(j)}_l}\bigg|_{z^{(j)} = \hat{z}^{(j)}_t},

$$
where $\hat{z}^{(j)}_t = z^{(j)}_{t - \tau_{ij}}$ is the ghost state of Agent $j$ at the retarded time.

Define the pulled-back retarded tensor $\mathcal{G}^{(i),\text{ret}}_{ij,kl} := (\mathcal{J}_{ji})^m{}_k (\mathcal{J}_{ji})^n{}_l \mathcal{G}^{\text{ret}}_{ij,mn}$. The **total effective metric** including retardation is:

$$
\tilde{G}^{(i)}_{kl}(z, t) = G^{(i)}_{kl}(z) + \sum_{j \neq i} \beta_{ij} \cdot \mathcal{G}^{(i),\text{ret}}_{ij,kl}(z, t).

$$

*Consequence (Strategic Hysteresis):* The metric inflation Agent $i$ experiences depends on Agent $j$'s position at the retarded time, not the current time. An agent may enter a region expecting low resistance, only to encounter a "delayed wall" of metric inflation arriving from the opponent's past position.

:::

:::{prf:proposition} Retarded Metric Propagation
:label: prop-retarded-metric-propagation

The effective metric $\tilde{G}^{(i)}(z, t)$ satisfies a wave-like propagation equation:

$$
\frac{\partial \tilde{G}^{(i)}_{kl}}{\partial t} = \sum_{j \neq i} \beta_{ij} \frac{\partial \mathcal{G}^{(i),\text{ret}}_{ij,kl}}{\partial t} = \sum_{j \neq i} \beta_{ij} \frac{d\mathcal{G}^{(i)}_{ij,kl}}{dt}\bigg|_{t-\tau_{ij}}.

$$

The metric perturbation at time $t$ depends on the opponent's dynamics at time $t - \tau_{ij}$. Information about strategic coupling propagates at speed $c_{\text{info}}$.

:::

:::{prf:definition} Joint WFR Action (Relativistic)
:label: def-joint-wfr-action

The N-agent WFR action on the product space with retarded interactions is:

$$
\mathcal{A}^{(N)}[\boldsymbol{\rho}, \mathbf{v}, \mathbf{r}] = \int_0^T \left[ \sum_{i=1}^N \int_{\mathcal{Z}^{(i)}} \left(\|v^{(i)}\|_{\tilde{G}^{(i)}}^2 + \lambda_i^2 |r^{(i)}|^2 \right) d\rho^{(i)} + \mathcal{V}_{\text{int}}^{\text{ret}}(\boldsymbol{\rho}, t) \right] dt,

$$
where:
- $v^{(i)}$ is the velocity field for Agent $i$'s belief flow
- $r^{(i)}$ is the reaction term (mass creation/destruction)
- $\tilde{G}^{(i)}$ is the game-augmented metric with retarded components (Definition {prf:ref}`def-retarded-game-tensor`)
- $\mathcal{V}_{\text{int}}^{\text{ret}}(\boldsymbol{\rho}, t) = \sum_{i=1}^N \int_{\mathcal{Z}^{(i)}} \Phi^{\text{ret}}_{i}(z^{(i)}, t) \, d\rho^{(i)}(z^{(i)})$ is the retarded interaction energy, with $\Phi^{\text{ret}}_{i} := \sum_{j \neq i} \Phi^{\text{ret}}_{ij}$

*Cross-reference:* Definition {prf:ref}`def-the-wfr-action`, Definition {prf:ref}`def-retarded-interaction-potential`.

:::

:::{prf:theorem} Nash Equilibrium as Standing Wave
:label: thm-nash-standing-wave

Assume the joint causal domain is effectively compact (finite volume under the induced metric), so the joint d'Alembertian admits a discrete mode expansion. In the relativistic formulation, a Nash equilibrium is a joint density $\boldsymbol{\rho}^*(\mathbf{z}, t)$ satisfying **time-averaged stationarity**:

$$
\left\langle \frac{\partial \boldsymbol{\rho}^*}{\partial t} \right\rangle_T := \frac{1}{T}\int_0^T \frac{\partial \boldsymbol{\rho}^*}{\partial t}(\mathbf{z}, t') \, dt' = 0,

$$
where the averaging period $T \gg \max_{i,j} \tau_{ij}$ exceeds all causal delays.

**Characterization:** A standing wave Nash equilibrium satisfies:

1. **Time-averaged gradient vanishing:**

   $$
   \left\langle (G^{(i)})^{-1} \nabla_{z^{(i)}} \Phi_{\text{eff}}^{(i,\text{ret})} \right\rangle_T = 0 \quad \forall i

   $$

2. **Balanced probability currents:** The flux exchanged between agents via retarded potentials is balanced over one wave period:

   $$
   \int_0^T \mathbf{J}^{(i)}(z, t) \, dt = 0 \quad \text{for all } z \in \mathcal{Z}^{(i)}

   $$
   where $\mathbf{J}^{(i)} = \rho^{(i)} \mathbf{v}^{(i)}$ is the probability current.

3. **Resonance condition:** The system oscillates at a characteristic causal frequency set by the effective cavity size:

   $$
   \omega_{\text{Nash}} \sim \frac{c_{\text{info}}}{L_{\text{eff}}},

   $$
   where $L_{\text{eff}}$ is the dominant spatial scale (e.g., boundary separation or imposed horizon).

*Proof sketch.* The coupled Klein-Gordon system (Theorem {prf:ref}`thm-hjb-klein-gordon`) for $N$ agents forms a cavity resonator when the domain is effectively compact (finite horizon, reflecting boundary, or confining potential). Equilibrium states are the eigenmodes of the joint D'Alembertian operator. The ground state (lowest energy mode) corresponds to the stable Nash equilibrium; higher modes are metastable. Without effective compactness, interpret equilibrium as time-averaged stationarity rather than literal standing waves. See **{ref}`sec-appendix-e-rigorous-proof-sketches-for-ontological-and-metabolic-laws`** for the derivation under explicit boundary conditions. $\square$

:::

:::{prf:corollary} Newtonian Limit of Nash
:label: cor-newtonian-nash-limit

As $c_{\text{info}} \to \infty$, the standing wave Nash reduces to the static Nash equilibrium:

$$
\lim_{c_{\text{info}} \to \infty} \boldsymbol{\rho}^*(\mathbf{z}, t) = \boldsymbol{\rho}^*_{\text{static}}(\mathbf{z}),

$$
and the geometric stasis conditions (vanishing gradient, stationary Game Tensor) hold instantaneously rather than on average.

:::

:::{prf:theorem} Geometric Stasis (Newtonian Limit)
:label: thm-nash-equilibrium-as-geometric-stasis

In the Newtonian limit ($c_{\text{info}} \to \infty$), a strategy profile $\mathbf{z}^* = (z^{(1)*}, \ldots, z^{(N)*})$ is a Nash equilibrium if and only if it satisfies the instantaneous **geometric stasis conditions**:

1. **Vanishing individual gradient:**

   $$
   (G^{(i)})^{-1} \nabla_{z^{(i)}} \Phi_{\text{eff}}^{(i)}(z^{(i)*}; z^{(-i)*}) = 0 \quad \forall i

   $$

2. **Stationary Game Tensor:**

   $$
   \frac{d}{dt}\mathcal{G}_{ij}^{kl}\bigg|_{\mathbf{z}^*} = 0 \quad \forall i,j

   $$

3. **Non-positive second variation:**

   $$
   \delta^2 V^{(i)}|_{z^{(i)*}} \leq 0 \quad \forall i, \forall \delta z^{(i)}

   $$

*Remark (Nash vs. Pareto).* Geometric stasis need not coincide with global optimality (Pareto). The Game Tensor eigenstructure determines the gap: trace-negative (cooperative) tends toward Pareto-improving basins; trace-positive (adversarial) tends toward Pareto-suboptimal saddles.

:::

:::{prf:corollary} Vanishing Probability Current at Nash
:label: cor-vanishing-current-nash

At a standing wave Nash equilibrium, the **time-averaged probability current** vanishes:

$$
\langle \mathbf{J}^{(i)} \rangle_T = \langle \rho^{(i)} \mathbf{v}^{(i)} \rangle_T = 0 \quad \forall i.

$$

*Interpretation:* The agents are not "frozen"—they oscillate with the causal frequency $\omega_{\text{Nash}}$—but the net flow averages to zero. Nash equilibrium is dynamic balance, not static rest.

:::

:::{prf:theorem} Mean-Field Metric Law
:label: thm-mean-field-metric-law

Let $\boldsymbol{z} = (z_1, \dots, z_N)$ be the configuration of $N$ agents on $\mathcal{Z}$. Let the empirical measure be $\mu_N = \frac{1}{N} \sum_{i=1}^N \delta_{z_i}$. As $N \to \infty$, assuming $\mu_N$ converges weakly to a smooth density $\rho \in \mathcal{P}(\mathcal{Z})$, the effective metric $\tilde{G}(z)$ for a test agent at position $z$ converges to:

$$
\tilde{G}_{ab}(z) = G_{\text{intrinsic},ab}(z) + \alpha_{\text{adv}} \int_{\mathcal{Z}} (\mathcal{J}(z,\zeta))^m{}_a (\mathcal{J}(z,\zeta))^n{}_b \nabla^2_{\zeta, m, n} \Phi_{\text{int}}(z, \zeta)\, \rho(\zeta)\, d\mu_G(\zeta)

$$
where $\Phi_{\text{int}}(z, \zeta)$ is the pairwise interaction potential ({prf:ref}`prop-interaction-kernel`) and $\mathcal{J}(z,\zeta)$ is the transport map between local charts. If agents share a common chart and $\Phi_{\text{int}}$ depends only on relative coordinates, this reduces to $\tilde{G}_{ab}(z) = G_{\text{intrinsic},ab}(z) + \alpha_{\text{adv}} \nabla^2_{z,a,b} (\Phi_{\text{int}} * \rho)(z)$.

*Proof.*
1. **Discrete Interaction Energy:** The total interaction potential for agent $i$ is $V_{\text{int}}(z_i) = \frac{1}{N} \sum_{j \neq i} \Phi_{\text{int}}(z_i, z_j)$.

2. **Discrete Game Tensor:** The Game Tensor acting on the metric is defined as the sum of cross-sensitivities ({prf:ref}`thm-adversarial-mass-inflation`):

$$
(\delta G)_{ab}(z_i) = \alpha_{\text{adv}} \sum_{j \neq i} (\mathcal{J}_{ji})^m{}_a (\mathcal{J}_{ji})^n{}_b \frac{\partial^2 \Phi_{\text{int}}(z_i, z_j)}{\partial z_j^m \partial z_j^n}.

$$
3. **Continuum Limit:** We rewrite the sum as an integral against the empirical measure:

$$
(\delta G)_{ab}(z) = \alpha_{\text{adv}} \int_{\mathcal{Z}} (\mathcal{J}(z,\zeta))^m{}_a (\mathcal{J}(z,\zeta))^n{}_b \nabla^2_{\zeta, m, n} \Phi_{\text{int}}(z, \zeta) \, d\mu_N(\zeta).

$$
Here $\mathcal{J}(z,\zeta)$ is the transport map between local charts (identity if agents share a common coordinate system).
4. **Convergence:** Assuming $\Phi_{\text{int}}$ is $C^2$ and bounded, and $\mu_N \rightharpoonup \rho$ weakly, the integral converges to the pulled-back mean-field term. In a shared chart with $\mathcal{J} = I$, this reduces to the convolution $(\nabla^2 \Phi_{\text{int}} * \rho)(z)$.

5. **Complexity Reduction:** The computation of $\tilde{G}$ now requires evaluating the pulled-back Hessian field induced by $\rho$. In a shared chart, this reduces to the Hessian of the static field $\Psi(z) = (\Phi_{\text{int}} * \rho)(z)$. This is $O(1)$ with respect to $N$ (given the density field), effectively decoupling the agent's complexity from the population size. $\square$
:::

:::{prf:theorem} Metabolic Tracking Bound
:label: thm-metabolic-tracking-bound

Let $z^*(t)$ be a time-varying Nash equilibrium. An agent with maximum metabolic flux budget $\dot{\mathcal{M}}_{\max}$ can maintain tracking error $\epsilon \to 0$ if and only if the target's trajectory satisfies:

$$
\|\dot{z}^*\|_{\tilde{G}(z^*)} \leq \sqrt{\frac{2 \dot{\mathcal{M}}_{\max}}{\sigma_{\text{met}}}}

$$
where $\tilde{G}$ is the game-augmented metric ({prf:ref}`thm-adversarial-mass-inflation`).

*Proof.*
1. **Kinematic Requirement:** To track $z^*(t)$, the agent's transport velocity must satisfy $v = \dot{z}^*$.

2. **Thermodynamic Cost:** The metabolic cost of transport is $\dot{\mathcal{M}} = \frac{1}{2} \sigma_{\text{met}} \|v\|_{\tilde{G}}^2$ ({prf:ref}`def-metabolic-flux`).

3. **Adversarial Drag:** The metric $\tilde{G} = G + \alpha \mathcal{G}^{(i)}_{ij}$ includes the pulled-back Game Tensor. High adversarial tension ($\mathcal{G}^{(i)}_{ij} \gg 0$) inflates the norm $\|\cdot\|_{\tilde{G}}$.

4. **Critical Failure:** If the adversary moves sufficiently fast or the conflict is sufficiently intense, the required dissipation exceeds $\dot{\mathcal{M}}_{\max}$. The agent loses tracking not due to algorithmic error, but due to exceeding its thermodynamic budget. $\square$
:::

:::{prf:theorem} Geometric Locking Principle
:label: thm-geometric-locking-principle

Consider $N$ agents with Game Tensor $\mathcal{G}_{ij}$ ({prf:ref}`def-the-game-tensor`). In the presence of strong adversarial coupling, the joint system tends toward configurations where $\operatorname{Tr}(\mathcal{G}^{(i)}_{ij})$ is minimized.

*Proof.*

1. **Metric Inflation:** By {prf:ref}`thm-adversarial-mass-inflation`, the effective metric for agent $i$ is $\tilde{G}^{(i)} = G^{(i)} + \sum_j \beta_{ij} \mathcal{G}^{(i)}_{ij}$. For adversarial agents, $\beta_{ij} > 0$ and $\mathcal{G}^{(i)}_{ij}$ is positive semi-definite, implying $\det(\tilde{G}^{(i)}) \ge \det(G^{(i)})$.

2. **Kinetic Cost:** The WFR action ({prf:ref}`def-joint-wfr-action`) includes the transport term $\int \|v\|_{\tilde{G}}^2 d\rho$. An inflated metric implies a higher metabolic cost for any movement $v \neq 0$.

3. **Energy Minimization:** The system evolves to minimize the free energy $\mathcal{F}$. If the potential gain $\nabla_B V$ is bounded, but the kinetic cost scales with $\mathcal{G}^{(i)}_{ij}$, trajectories with large $\mathcal{G}^{(i)}_{ij}$ (intense conflict) become energetically prohibitive.

4. **Stationarity:** The system relaxes to a state where either $v \to 0$ (Nash stasis, {prf:ref}`thm-nash-equilibrium-as-geometric-stasis`) or the metric perturbation vanishes ($\mathcal{G}^{(i)}_{ij} \to 0$). The condition $\mathcal{G}^{(i)}_{ij} \to 0$ implies $\nabla_{z^{(j)}}\nabla_{z^{(i)}} V^{(i)} \to 0$, which defines a region of **strategic decoupling**. $\square$
:::

:::{prf:corollary} Metabolic Basis of Cooperation
:label: cor-metabolic-cooperation

Adversarial agents converge to cooperative or decoupled configurations because conflict maximizes the effective inertia of the state space, rendering non-cooperative trajectories metabolically unsustainable.

*Interpretation:* The Game Tensor acts as a "friction term" that penalizes rapid strategic maneuvers. In the long run, agents either:
1. **Cooperate:** Reduce $\mathcal{G}^{(i)}_{ij}$ by aligning their gradients
2. **Decouple:** Move to regions where $\nabla_{z^{(j)}} V^{(i)} \approx 0$
3. **Freeze:** Accept Nash stasis with $v^{(i)} = 0$

All three outcomes correspond to stationary points of the joint action functional.
:::

:::{prf:axiom} Local Gauge Invariance (Nuisance Invariance)
:label: ax-local-gauge-invariance

The physical dynamics of the multi-agent system are invariant under position-dependent rotations of the internal nuisance coordinates. Formally, let $G$ be a compact Lie group with Lie algebra $\mathfrak{g}$. For any smooth map $U: \mathcal{Z} \to G$, the nuisance-frame transformation

$$
\xi'(z) = U(z)\xi(z), \qquad \psi'(z, t) = U(z)\psi(z, t)

$$

leaves observable quantities (reward, policy output, Nash conditions) unchanged. The scalar fields $\rho$ and $V$ are gauge-invariant; only the internal orientation $\xi$ (and any vector-valued nuisance features) transform.

*Units:* $[U] = \text{dimensionless}$ (group element).

*Interpretation:* Agent $i$ at location $z$ is free to rotate its internal representation (the "basis" in which it encodes nuisance). This is not a symmetry to be broken but a **redundancy** in the description that must be properly handled via gauge theory.

:::

:::{prf:definition} Local Gauge Group
:label: def-local-gauge-group

The **Local Gauge Group** is a compact Lie group $G$ with:

1. **Lie algebra $\mathfrak{g}$:** The tangent space at identity, with generators $\{T_a\}_{a=1}^{\dim(G)}$ satisfying $[T_a, T_b] = if^{abc}T_c$ where $f^{abc}$ are the **structure constants**.

2. **Representation:** The matter fields $\psi^{(i)}$ transform in a representation $\rho: G \to GL(V)$ where $V$ is the representation space.

3. **Position-dependent element:** $U(z) \in G$ for each $z \in \mathcal{Z}$, forming the infinite-dimensional group of gauge transformations $\mathcal{G} := C^\infty(\mathcal{Z}, G)$.

*Standard choices:*
- $G = SO(D)$: Rotations of $D$-dimensional nuisance space
- $G = SU(N)$: Unitary transformations (for complex representations)
- $G = U(1)$: Abelian phase rotations (electromagnetic limit)

*Cross-reference:* The $SO(D)$ symmetry at the origin (Proposition {prf:ref}`prop-so-d-symmetry-at-origin`) is the special case where the stabilizer is trivial.

:::

:::{prf:definition} Matter Field (Belief Amplitude)
:label: def-matter-field-belief-amplitude

The **Matter Field** for agent $i$ is the complex-valued section

$$
\psi^{(i)}: \mathcal{Z}^{(i)} \times \mathbb{R} \to V

$$

where $V$ is the representation space of $G$. The matter field is related to the belief wave-function by:

$$
\psi^{(i)}(z, t) = \sqrt{\rho^{(i)}(z, t)} \exp\left(\frac{iV^{(i)}(z, t)}{\sigma}\right) \cdot \xi^{(i)}(z)

$$

where:
- $\rho^{(i)}$ is the belief density
- $V^{(i)}$ is the value function (scalar, gauge-invariant)
- $\sigma > 0$ is the **cognitive action scale**, $\sigma := T_c \cdot \tau_{\text{update}}$, the information-theoretic analog of Planck's constant (full definition: {prf:ref}`def-cognitive-action-scale` in {ref}`sec-the-belief-wave-function-schrodinger-representation`)
- $\xi^{(i)}(z) \in V$ is the **internal state vector** encoding nuisance orientation

*Units:* $[\psi] = [\text{length}]^{-D/2}$ (probability amplitude density).

*Transformation law:* Under gauge transformation $U(z)$:

$$
\psi'^{(i)}(z, t) = \rho(U(z))\psi^{(i)}(z, t)

$$

where $\rho: G \to GL(V)$ is the representation. The scalar observables are unchanged: $\rho' = \rho$ and $V' = V$.

:::

:::{prf:conjecture} Nuisance Fiber as Gauge Orbit (Motivating Principle)
:label: conj-nuisance-fiber-gauge-orbit

The nuisance fiber at each macro-state $K \in \mathcal{K}$ admits interpretation as a gauge orbit:

$$
\mathcal{Z}_n\big|_K \cong G_K / H_K

$$

where:
- $G_K \subseteq G$ is the gauge group restricted to macro-state $K$
- $H_K \subseteq G_K$ is the **stabilizer subgroup** fixing the codebook centroid $e_K$

*Special cases:*
1. **At origin ($K = 0$, Semantic Vacuum):** $G_0 = SO(D)$, $H_0 = \{e\}$, so $\mathcal{Z}_n|_0 \cong SO(D)$ (full rotational freedom).
2. **At generic $K$:** The stabilizer $H_K$ is non-trivial if $e_K$ has special structure (e.g., aligned with coordinate axes).
3. **At boundary ($|z| \to 1$):** The gauge orbit collapses as degrees of freedom freeze ({ref}`sec-causal-information-bound`, Causal Stasis).

*Motivation (not a rigorous proof):*
The nuisance coordinates $z_n$ parameterize how an observation is embedded relative to the macro-code $K$. Under the VQ-VAE architecture ({ref}`sec-the-shutter-as-a-vq-vae`), two nuisance values $z_n$ and $z'_n$ are designed to be equivalent if they differ by a transformation preserving the macro-code: $z'_n = U \cdot z_n$ for some $U \in G_K$.

**Remark (Analogy vs. Isomorphism):** This correspondence is a *motivating analogy* rather than a proven isomorphism. A rigorous proof would require:
1. Showing the nuisance equivalence relation coincides with gauge equivalence
2. Proving the quotient $G_K/H_K$ is a smooth manifold diffeomorphic to $\mathcal{Z}_n|_K$
3. Establishing that the VQ-VAE induces a principal $G_K$-bundle structure

The gauge-theoretic formalism developed in Sections 29.13–29.20 is motivated by this conjecture but does not depend on it being rigorously true. The constructions (covariant derivative, field strength, etc.) are well-defined once the gauge group $G$ and its action are specified.

*Cross-reference:* This formalizes the design goal "K represents $x/G_{\text{spatial}}$" from {ref}`sec-the-shutter-as-a-vq-vae`.

:::

:::{prf:definition} Strategic Connection (Gauge Potential)
:label: def-strategic-connection

The **Strategic Connection** is a $\mathfrak{g}$-valued 1-form on $\mathcal{Z}$:

$$
A = A_\mu^a T_a \, dz^\mu

$$

where:
- $A_\mu^a(z, t)$ are the **connection coefficients** (real-valued functions)
- $\{T_a\}_{a=1}^{\dim(\mathfrak{g})}$ are the generators of the Lie algebra $\mathfrak{g}$
- $\mu$ indexes spacetime/latent coordinates $(t, z^1, \ldots, z^D)$

*Units:* $[A_\mu] = [\text{length}]^{-1}$ (inverse length, like momentum).

*Interpretation:* The connection $A_\mu$ tells agent $i$ how to "translate" the nuisance interpretation from point $z$ to point $z + dz$. It is the **strategic context** required to compare internal states at different locations.

:::

:::{prf:proposition} Gauge Transformation of the Connection
:label: prop-gauge-transformation-connection

Under a local gauge transformation $U(z) \in G$, the connection transforms as:

$$
A'_\mu = U A_\mu U^{-1} - \frac{i}{g}(\partial_\mu U)U^{-1}

$$

where $g > 0$ is the **coupling constant** (strategic coupling strength).

*Proof.*
Demand that the covariant derivative (Definition {prf:ref}`def-covariant-derivative`) transform covariantly: $(D_\mu\psi)' = U(D_\mu\psi)$. Expanding:

$$
\begin{aligned}
D'_\mu\psi' &= (\partial_\mu - igA'_\mu)(U\psi) \\
&= (\partial_\mu U)\psi + U(\partial_\mu\psi) - igA'_\mu U\psi
\end{aligned}

$$

For this to equal $U(\partial_\mu - igA_\mu)\psi = U(\partial_\mu\psi) - igUA_\mu\psi$, we require:

$$
(\partial_\mu U)\psi - igA'_\mu U\psi = -igUA_\mu\psi

$$

Solving for $A'_\mu$ yields the stated transformation law. $\square$

*Interpretation:* The inhomogeneous term $-\frac{i}{g}(\partial_\mu U)U^{-1}$ compensates for the "frame twist" introduced by position-dependent gauge transformations. The connection must counter-twist to maintain covariance.

:::

:::{prf:definition} Covariant Derivative
:label: def-covariant-derivative

The **Covariant Derivative** acting on matter fields is:

$$
D_\mu = \partial_\mu - igA_\mu

$$

For a matter field $\psi$ in representation $\rho$:

$$
D_\mu\psi = \partial_\mu\psi - igA_\mu^a \rho(T_a)\psi

$$

*Properties:*
1. **Covariant transformation:** $(D_\mu\psi)' = U(D_\mu\psi)$
2. **Leibniz rule:** $D_\mu(\psi\chi) = (D_\mu\psi)\chi + \psi(D_\mu\chi)$
3. **Reduces to partial derivative** when $A_\mu = 0$ (trivial connection)

*Units:* $[D_\mu\psi] = [\psi]/[\text{length}]$.

:::

:::{prf:theorem} Gauge-Covariant Klein-Gordon Equation
:label: thm-gauge-covariant-klein-gordon

The scalar value $V^{(i)}$ is gauge-invariant, so its wave equation remains the Klein-Gordon equation of Theorem {prf:ref}`thm-hjb-klein-gordon` with ordinary derivatives. Gauge-covariant derivatives apply to fields that transform under $G$, such as the belief amplitude $\psi^{(i)}$ (Definition {prf:ref}`def-matter-field-belief-amplitude`). In that case the covariant wave operator is:

$$
\left(\frac{1}{c_{\text{info}}^2}D_t^2 - D^i D_i + \kappa^2\right)\psi^{(i)} = \mathcal{S}^{(i)},

$$

where:
- $D_t = \partial_t - igA_0$ is the temporal covariant derivative
- $D_i = \partial_i - igA_i$ are spatial covariant derivatives
- $D^i = \tilde{G}^{ij}D_j$ with raised index via the strategic metric
- $\mathcal{S}^{(i)}$ is the source term determined by the chosen matter model

*Proof sketch.*
The minimal coupling principle replaces $\partial_\mu \to D_\mu$ for gauge-charged fields while preserving the equation's structure. The gauge-covariant d'Alembertian is:

$$
\Box_A := \frac{1}{c_{\text{info}}^2}D_t^2 - \tilde{G}^{ij}D_i D_j

$$

Introduce the spacetime metric

$$
g_{\mu\nu} := \text{diag}(-c_{\text{info}}^2, \tilde{G}_{ij}), \quad g^{00} = -\frac{1}{c_{\text{info}}^2}, \quad g^{ij} = \tilde{G}^{ij},
$$
with $|g| = c_{\text{info}}^2 |\tilde{G}|$. Then

$$
\Box_A = -\frac{1}{\sqrt{|g|}}D_\mu\left(\sqrt{|g|}g^{\mu\nu}D_\nu\right).
$$

For scalar $V^{(i)}$, $D_\mu V^{(i)} = \partial_\mu V^{(i)}$, so the equation reduces to the non-gauged Klein-Gordon form. $\square$

:::

:::{prf:proposition} Minimal Coupling Principle
:label: prop-minimal-coupling

To maintain gauge invariance, derivatives acting on gauge-charged fields must be replaced by covariant derivatives:

$$
\partial_\mu \longrightarrow D_\mu = \partial_\mu - igA_\mu

$$

This **Minimal Coupling Principle** ensures that:
1. Transport of nuisance-frame vectors is covariant
2. Matter-field dynamics (e.g., $\psi$) are gauge-covariant
3. Learning gradients for gauge-charged features transform properly under internal rotations

*Consequence for implementation:* Use covariant gradients for parameters that live in gauge bundles. Scalar objectives like $V$ remain invariant and use ordinary gradients.

:::

:::{prf:proposition} Game Tensor Gauge Transformation
:label: prop-game-tensor-gauge-transformation

Under a local gauge transformation $U(z)$, the scalar value field is unchanged, so:

$$
\mathcal{G}'_{ij}(z) = \mathcal{G}_{ij}(z).

$$

If one defines a cross-sensitivity of a gauge-charged field (e.g., $\psi$ or a nuisance-frame vector), then the corresponding tensor transforms covariantly by conjugation, and covariant derivatives must be used.

*Interpretation:* Strategic coupling in the scalar value landscape is gauge-invariant; only nuisance-frame comparisons require a connection.

:::

:::{prf:definition} Gauge-Consistent Game Tensor
:label: def-gauge-covariant-game-tensor

Because $V^{(i)}$ is a scalar, the gauge-consistent cross-sensitivity is the **Riemannian Hessian** (Levi-Civita covariant derivative) rather than a gauge-covariant derivative:

$$
\tilde{\mathcal{G}}_{ij}^{kl}(z) := \nabla_k \nabla_l V^{(i)}\big|_{z^{(j)}}

$$

Explicitly:

$$
\tilde{\mathcal{G}}_{ij}^{kl} = \partial_k\partial_l V^{(i)} - \Gamma^m_{kl}\partial_m V^{(i)},

$$

where $\Gamma^m_{kl}$ are the Christoffel symbols of the strategic metric.

For heterogeneous manifolds, pull back to Agent $i$ using the Strategic Jacobian: $\tilde{\mathcal{G}}^{(i)}_{ij,kl} := (\mathcal{J}_{ji})^m{}_k (\mathcal{J}_{ji})^n{}_l \tilde{\mathcal{G}}_{ij,mn}$.

*Remark (Gauge-charged variant).* If one defines cross-sensitivities of a gauge-charged field (e.g., $\psi$ or nuisance-frame vectors), then use gauge-covariant derivatives $D_k D_l(\cdot)$; the resulting tensor transforms by conjugation.

:::

:::{prf:theorem} Gauge-Invariant Metric Inflation
:label: thm-gauge-invariant-metric-inflation

The effective metric (Theorem {prf:ref}`thm-adversarial-mass-inflation`) generalizes to:

$$
\tilde{G}^{(i)}_{kl}(z) = G^{(i)}_{kl}(z) + \sum_{j \neq i} \beta_{ij} \tilde{\mathcal{G}}^{(i)}_{ij,kl}

$$

For gauge-charged cross-sensitivities, use a gauge-invariant contraction (e.g., a trace) before adding to the metric.

*Proof sketch.*
The physical metric must be gauge-invariant. For scalar $V^{(i)}$, $\tilde{\mathcal{G}}_{ij}$ is already invariant. For gauge-charged tensors, use invariant contractions. $\square$

*Consequence:* The metric inflation experienced by agents is a **physical observable** independent of internal frame choice.

:::

:::{prf:definition} Field Strength Tensor (Yang-Mills Curvature)
:label: def-field-strength-tensor

The **Field Strength Tensor** is the $\mathfrak{g}$-valued 2-form:

$$
\mathcal{F}_{\mu\nu} = \partial_\mu A_\nu - \partial_\nu A_\mu - ig[A_\mu, A_\nu]

$$

In components with Lie algebra generators:

$$
\mathcal{F}_{\mu\nu}^a = \partial_\mu A_\nu^a - \partial_\nu A_\mu^a + gf^{abc}A_\mu^b A_\nu^c

$$

where $f^{abc}$ are the structure constants of $\mathfrak{g}$.

*Units:* $[\mathcal{F}_{\mu\nu}] = [\text{length}]^{-2}$ (curvature).

*Special cases:*
- **Abelian ($[A_\mu, A_\nu] = 0$):** $F_{\mu\nu} = \partial_\mu A_\nu - \partial_\nu A_\mu$ (electromagnetic field tensor)
- **Non-Abelian:** The commutator term generates **self-interaction** of the gauge field

:::

:::{prf:proposition} Covariant Transformation of Field Strength
:label: prop-field-strength-transformation

Under gauge transformation $U(z)$, the field strength transforms **covariantly** (not invariantly):

$$
\mathcal{F}'_{\mu\nu} = U \mathcal{F}_{\mu\nu} U^{-1}

$$

*Proof.*
Direct calculation using the transformation law for $A_\mu$ (Proposition {prf:ref}`prop-gauge-transformation-connection`):

$$
\begin{aligned}
\mathcal{F}'_{\mu\nu} &= \partial_\mu A'_\nu - \partial_\nu A'_\mu - ig[A'_\mu, A'_\nu] \\
&= U(\partial_\mu A_\nu - \partial_\nu A_\mu - ig[A_\mu, A_\nu])U^{-1} \\
&= U\mathcal{F}_{\mu\nu}U^{-1}
\end{aligned}

$$

The inhomogeneous terms from $A'_\mu$ cancel exactly. $\square$

*Consequence:* While $\mathcal{F}_{\mu\nu}$ is not gauge-invariant, the trace $\text{Tr}(\mathcal{F}_{\mu\nu}\mathcal{F}^{\mu\nu})$ **is** gauge-invariant and can appear in the action.

:::

:::{prf:theorem} Curvature from Covariant Derivative Commutator
:label: thm-curvature-commutator

The field strength measures the failure of covariant derivatives to commute:

$$
[D_\mu, D_\nu]\psi = -ig\mathcal{F}_{\mu\nu}\psi

$$

*Proof.*
Expand the commutator:

$$
\begin{aligned}
[D_\mu, D_\nu]\psi &= D_\mu(D_\nu\psi) - D_\nu(D_\mu\psi) \\
&= (\partial_\mu - igA_\mu)(\partial_\nu\psi - igA_\nu\psi) - (\mu \leftrightarrow \nu) \\
&= \partial_\mu\partial_\nu\psi - ig(\partial_\mu A_\nu)\psi - igA_\nu\partial_\mu\psi - igA_\mu\partial_\nu\psi - g^2A_\mu A_\nu\psi - (\mu \leftrightarrow \nu) \\
&= -ig(\partial_\mu A_\nu - \partial_\nu A_\mu)\psi - g^2(A_\mu A_\nu - A_\nu A_\mu)\psi \\
&= -ig(\partial_\mu A_\nu - \partial_\nu A_\mu - ig[A_\mu, A_\nu])\psi \\
&= -ig\mathcal{F}_{\mu\nu}\psi \quad \square
\end{aligned}

$$

*Interpretation:* If $\mathcal{F}_{\mu\nu} \neq 0$, parallel transport around a closed loop results in a non-trivial rotation. The "meaning" of strategic nuisance **twists** as one navigates the latent space.

:::

:::{prf:theorem} Bianchi Identity
:label: thm-bianchi-identity

The field strength satisfies the **Bianchi Identity**:

$$
D_\mu \mathcal{F}_{\nu\rho} + D_\nu \mathcal{F}_{\rho\mu} + D_\rho \mathcal{F}_{\mu\nu} = 0

$$

or in differential form notation: $D\mathcal{F} = 0$ where $D = d - ig[A, \cdot]$.

*Proof sketch.*
Apply the Jacobi identity for covariant derivatives:

$$
[[D_\mu, D_\nu], D_\rho] + [[D_\nu, D_\rho], D_\mu] + [[D_\rho, D_\mu], D_\nu] = 0

$$

Since $[D_\mu, D_\nu] = -ig\mathcal{F}_{\mu\nu}$, this becomes:

$$
-ig([D_\rho, \mathcal{F}_{\mu\nu}] + \text{cyclic}) = 0

$$

The covariant derivative of $\mathcal{F}$ is $D_\rho\mathcal{F}_{\mu\nu} = \partial_\rho\mathcal{F}_{\mu\nu} - ig[A_\rho, \mathcal{F}_{\mu\nu}]$, and the identity follows. See **{ref}`sec-appendix-e-rigorous-proof-sketches-for-ontological-and-metabolic-laws`** for the complete algebraic derivation with component verification. $\square$

*Interpretation:* The Bianchi identity is a **conservation law** for the strategic flux. It ensures topological consistency of the gauge structure.

:::

:::{prf:definition} Strategic Curvature Scalar
:label: def-strategic-curvature-scalar

The **Strategic Curvature Scalar** is the gauge-invariant contraction:

$$
\mathcal{R}_{\text{strat}} := \text{Tr}(\mathcal{F}_{\mu\nu}\mathcal{F}^{\mu\nu}) = \mathcal{F}_{\mu\nu}^a \mathcal{F}^{\mu\nu,a}

$$

where indices are raised with the spacetime metric $g^{\mu\nu} = \text{diag}(-1/c_{\text{info}}^2, \tilde{G}^{ij})$ introduced above.

*Properties:*
- In Euclidean signature, $\mathcal{R}_{\text{strat}}$ is non-negative for compact gauge groups; in Lorentzian signature it is indefinite.
- $\mathcal{R}_{\text{strat}} = 0$ if and only if $\mathcal{F}_{\mu\nu} = 0$ (flat connection)
- Provides a measure of total strategic tension in a region

:::

:::{prf:definition} Yang-Mills Action
:label: def-yang-mills-action

The **Yang-Mills Action** for the strategic gauge field is:

$$
S_{\text{YM}}[A] = -\frac{1}{4}\int_{\mathcal{Z} \times \mathbb{R}} \text{Tr}(\mathcal{F}_{\mu\nu}\mathcal{F}^{\mu\nu})\sqrt{|g|}\,d^{D+1}x

$$

where:
- $\mathcal{F}_{\mu\nu}$ is the field strength tensor (Definition {prf:ref}`def-field-strength-tensor`)
- $g_{\mu\nu}$ is the spacetime metric $g_{\mu\nu} = \text{diag}(-c_{\text{info}}^2, \tilde{G}_{ij})$ with determinant $|g| = c_{\text{info}}^2|\tilde{G}|$
- $g_{\text{YM}}$ is the coupling constant
- The trace is over Lie algebra indices: $\text{Tr}(\mathcal{F}_{\mu\nu}\mathcal{F}^{\mu\nu}) = \mathcal{F}_{\mu\nu}^a\mathcal{F}^{\mu\nu,a}$

*Units:* $[S_{\text{YM}}] = \text{nat}$ (action).
*Dimensionality:* In spacetime dimension $d = D+1$, the coupling has $[g^2] = [\text{length}]^{d-4}$ (so $g$ is dimensionless only when $d = 4$).

*Properties:*
1. **Gauge-invariant:** $S_{\text{YM}}[A'] = S_{\text{YM}}[A]$ under $A \to A'$
2. **Lorentz-invariant:** Covariant under coordinate transformations
3. **Positive in Euclidean signature:** After Wick rotation the action is positive semi-definite for compact gauge groups; in Lorentzian signature it is indefinite.

:::

:::{prf:theorem} Yang-Mills Field Equations
:label: thm-yang-mills-equations

The Euler-Lagrange equations for the Yang-Mills action yield:

$$
D_\mu \mathcal{F}^{\mu\nu} = J^\nu

$$

where the **strategic current** (source term) is defined by the matter sector:

$$
J^{\nu,a} := -\frac{\delta \mathcal{L}_{\text{matter}}}{\delta A_\nu^a}.

$$

For a complex multiplet $\psi$ (scalar belief amplitude), a standard choice is

$$
J^{\nu,a} = g\,\mathrm{Im}\left(\psi^\dagger T^a D^\nu \psi\right),

$$

while for a spinor multiplet one recovers $J^{\nu,a} = g\bar{\psi}\gamma^\nu T^a \psi$.

*Expanded form:*

$$
\partial_\mu \mathcal{F}^{\mu\nu,a} + gf^{abc}A_\mu^b\mathcal{F}^{\mu\nu,c} = J^{\nu,a}

$$

*Proof sketch.*
Vary the total action $S = S_{\text{YM}} + S_{\text{matter}}$ with respect to $A_\mu^a$:

$$
\frac{\delta S}{\delta A_\mu^a} = 0 \implies \partial_\nu(\sqrt{|g|}\mathcal{F}^{\mu\nu,a}) + g f^{abc}A_\nu^b\sqrt{|g|}\mathcal{F}^{\mu\nu,c} + \frac{\delta S_{\text{matter}}}{\delta A_\mu^a} = 0

$$

The matter variation gives the current $J^{\mu,a}$, and reorganizing yields the Yang-Mills equation. $\square$

*Interpretation:* The gauge field is sourced by the strategic current—the flow of "charged" belief through latent space. Agents with non-zero internal state generate a gauge field that mediates their interaction with other agents.

:::

:::{prf:corollary} Abelian Limit (Maxwell Equations)
:label: cor-maxwell-limit

For an Abelian gauge group $G = U(1)$ with $[T_a, T_b] = 0$:

$$
\partial_\mu F^{\mu\nu} = J^\nu

$$

This recovers the **Maxwell equations** of electromagnetism in covariant form.

*Correspondence:*
- $F^{0i} = E^i$ (electric field) $\leftrightarrow$ temporal strategic gradient
- $F^{ij} = \epsilon^{ijk}B_k$ (magnetic field) $\leftrightarrow$ spatial strategic vorticity
- $J^0 = \rho_e$ (charge density) $\leftrightarrow$ belief density
- $J^i = j^i$ (current density) $\leftrightarrow$ belief flux

:::

:::{prf:proposition} Gauge Field Energy-Momentum Tensor
:label: prop-gauge-energy-momentum

The energy-momentum tensor of the gauge field is:

$$
T^{\text{gauge}}_{\mu\nu} = -\text{Tr}\left(\mathcal{F}_{\mu\rho}\mathcal{F}_\nu^{\ \rho} - \frac{1}{4}g_{\mu\nu}\mathcal{F}_{\rho\sigma}\mathcal{F}^{\rho\sigma}\right)

$$

*Properties:*
1. **Symmetric:** $T^{\text{gauge}}_{\mu\nu} = T^{\text{gauge}}_{\nu\mu}$
2. **Traceless** (for spacetime $d = 4$, i.e., $D = 3$): $T^{\text{gauge}\mu}_{\ \ \ \ \mu} = 0$
3. **Conserved (total):** $\nabla_\mu\left(T^{\text{gauge}\mu\nu} + T^{\text{matter}\mu\nu}\right) = 0$. The gauge sector alone satisfies
   $$\nabla_\mu T^{\text{gauge}\mu\nu} = -\text{Tr}(\mathcal{F}^{\nu\mu}J_\mu),$$
   which vanishes only in the source-free case $J_\mu = 0$.

*Interpretation:* The gauge field carries energy and momentum. Regions of high strategic curvature $\|\mathcal{F}\|$ have high energy density—strategic conflict is energetically costly.

:::

:::{prf:corollary} Current Conservation
:label: cor-current-conservation

The strategic current is covariantly conserved:

$$
D_\mu J^{\mu,a} = 0

$$

*Proof.*
Apply $D_\nu$ to the Yang-Mills equation $D_\mu\mathcal{F}^{\mu\nu} = J^\nu$:

$$
D_\nu D_\mu \mathcal{F}^{\mu\nu} = D_\nu J^\nu

$$

By the Bianchi identity (Theorem {prf:ref}`thm-bianchi-identity`) and the antisymmetry of $\mathcal{F}^{\mu\nu}$, the left side vanishes, giving $D_\nu J^\nu = 0$. $\square$

*Interpretation:* The total "charge" (internal state magnitude) is conserved. Belief cannot be created or destroyed, only transformed.

:::

:::{prf:definition} Complete Multi-Agent Lagrangian
:label: def-complete-lagrangian

The **Complete Multi-Agent Lagrangian** is:

$$
\mathcal{L}_{\text{SMFT}} = \mathcal{L}_{\text{YM}} + \mathcal{L}_{\text{Dirac}} + \mathcal{L}_{\text{Higgs}} + \mathcal{L}_{\text{Yukawa}}

$$

where each sector contributes:

**(i) Yang-Mills Sector (Strategic Gauge Field):**

$$
\mathcal{L}_{\text{YM}} = -\frac{1}{4}\text{Tr}(\mathcal{F}_{\mu\nu}\mathcal{F}^{\mu\nu})

$$

This governs the dynamics of the strategic connection $A_\mu$.

**(ii) Matter Sector (Belief Field):**

$$
\mathcal{L}_{\text{Dirac}} = \sum_{i=1}^N \bar{\psi}^{(i)}(i\gamma^\mu D_\mu - m_i)\psi^{(i)}

$$

where:
- $\psi^{(i)}$ is the belief multiplet for agent $i$
- $\bar{\psi}^{(i)} = \psi^{(i)\dagger}\gamma^0$ is the Dirac adjoint
- $D_\mu = \partial_\mu - igA_\mu$ is the covariant derivative
- $m_i$ is the "bare mass" (intrinsic inertia) of agent $i$

*Scalar option:* If the belief field is modeled as a complex scalar (the canonical single-agent choice), replace the Dirac term by

$$
\mathcal{L}_{\text{scalar}} = \sum_{i=1}^N (D_\mu \psi^{(i)})^\dagger (D^\mu \psi^{(i)}) - m_i^2\,\psi^{(i)\dagger}\psi^{(i)}.
$$

**(iii) Higgs Sector (Value Order Parameter):**

$$
\mathcal{L}_{\text{Higgs}} = |D_\mu\Phi|^2 - V(\Phi)

$$

with the Higgs potential:

$$
V(\Phi) = \mu^2|\Phi|^2 + \lambda|\Phi|^4

$$

where $\Phi$ is the **value order parameter** (a scalar field in a representation of $G$).

**(iv) Yukawa Sector (Strategic Coupling):**

$$
\mathcal{L}_{\text{Yukawa}} = -\sum_{i,j=1}^N y_{ij}\bar{\psi}^{(i)}\Phi\psi^{(j)}

$$

where $y_{ij}$ are the **Yukawa coupling constants** determining how strongly agents couple through the value field.

*Units:* $[\mathcal{L}] = \text{nat}/[\text{length}]^{D+1}$ (Lagrangian density).

:::

:::{prf:theorem} Spontaneous Symmetry Breaking (Higgs Mechanism)
:label: thm-higgs-mechanism

When the Higgs mass parameter satisfies $\mu^2 < 0$, the potential $V(\Phi)$ has a non-trivial minimum, and the gauge symmetry is **spontaneously broken**.

**Vacuum Expectation Value:**

$$
\langle\Phi\rangle = \frac{v}{\sqrt{2}}, \quad v = \sqrt{-\mu^2/\lambda}

$$

**Mass Generation:**

1. **Gauge boson masses:** The gauge fields acquire mass

   $$
   m_A = \frac{gv}{2}

   $$
   transforming from massless to massive (strategic inertia).

2. **Fermion masses:** The belief spinors acquire effective mass

   $$
   m_{\text{eff},i} = \frac{y_{ii}v}{\sqrt{2}}

   $$
   through Yukawa coupling.

3. **Residual symmetry:** The full gauge group $G$ breaks to a subgroup $H \subset G$ that leaves the vacuum invariant.

*Proof sketch.*
Expand $\Phi = (v + h)/\sqrt{2}$ around the vacuum, where $h$ is the physical Higgs field. The kinetic term $|D_\mu\Phi|^2$ generates:

$$
|D_\mu\Phi|^2 = \frac{1}{2}(\partial_\mu h)^2 + \frac{g^2v^2}{4}A_\mu A^\mu + \ldots

$$

The term $\frac{g^2v^2}{4}A_\mu A^\mu$ is a mass term for $A_\mu$ with $m_A^2 = g^2v^2/4$. Similarly, the Yukawa term generates fermion masses. See **{ref}`sec-appendix-e-rigorous-proof-sketches-for-ontological-and-metabolic-laws`** for the complete derivation including VEV calculation, Goldstone absorption, and the symmetry breaking pattern. $\square$

*Interpretation:* Policy selection (choosing a direction in latent space) is spontaneous symmetry breaking. The agent commits to a strategy, breaking the rotational invariance of the Semantic Vacuum. This commitment generates "mass"—resistance to changing strategy.

:::

:::{prf:corollary} Goldstone Modes and Gauge Boson Absorption
:label: cor-goldstone-absorption

Spontaneous breaking of a continuous symmetry produces massless **Goldstone bosons**—one for each broken generator of $G$. In gauge theories, these Goldstone modes are "eaten" by the gauge bosons, which acquire longitudinal polarization and mass.

*In the multi-agent context:*
- **Goldstone modes** = Angular fluctuations in policy direction (cheap rotations)
- **Massive gauge bosons** = Strategic connections with inertia (costly reorientations)
- **Residual massless modes** = Unbroken symmetry directions (free rotations)

:::

:::{prf:definition} Mass Gap
:label: def-mass-gap

The **Mass Gap** of the strategic Hamiltonian $\hat{H}_{\text{strat}}$ is:

$$
\Delta_H := \inf\left\{\text{spec}(\hat{H}_{\text{strat}}) \setminus \{E_0\}\right\} - E_0

$$

where $E_0$ is the ground state energy.

*Properties:*
- $\Delta_H > 0$: **Gapped** spectrum (isolated ground state)
- $\Delta_H = 0$: **Gapless** spectrum (continuous above ground state)

*Units:* $[\Delta_H] = \text{nat}/[\text{time}]$ (energy).

*Notation:* We reserve $\Delta_H$ for the Hamiltonian spectral gap. The Klein-Gordon operator has its own **frequency gap** $\Delta_{\text{KG}}$ defined below; the two coincide only after the non-relativistic reduction described in the Schr\"odinger-limit remark.

:::

:::{prf:theorem} Mass Gap from Screening
:label: thm-mass-gap-screening

The screening mass $\kappa = \lambda/c_{\text{info}}$ with $\lambda := -\ln\gamma / \Delta t$ from the Helmholtz equation (Theorem {prf:ref}`thm-the-hjb-helmholtz-correspondence`) sets the **Klein-Gordon spectral gap**. Let $\{-\Delta_G \phi_n = \lambda_n \phi_n\}$ with $\lambda_n \ge 0$ on the spatial manifold. The mode frequencies are:

$$
\omega_n = c_{\text{info}}\sqrt{\kappa^2 + \lambda_n}.

$$

The intrinsic **mass scale** (rest frequency) is

$$
\omega_0 = c_{\text{info}}\sqrt{\kappa^2 + \lambda_0},

$$

which reduces to $\omega_0 = c_{\text{info}}\kappa$ when $\lambda_0 = 0$ (e.g., periodic or Neumann boundary conditions). On a compact causal domain the **Klein-Gordon frequency gap** between the first two modes is

$$
\Delta_{\text{KG}} = \omega_1 - \omega_0 = c_{\text{info}}\left(\sqrt{\kappa^2 + \lambda_1} - \sqrt{\kappa^2 + \lambda_0}\right) \ge 0.

$$

*Proof sketch.*
Insert a normal mode ansatz $V(z,t)=e^{-i\omega t}\phi(z)$ into the screened wave equation
$\left(\frac{1}{c_{\text{info}}^2}\partial_t^2 - \Delta_G + \kappa^2\right)V=0$ to obtain
$\omega^2/c_{\text{info}}^2 = \kappa^2 + \lambda_n$. The gap statements follow. $\square$

*Remark (Schr\"odinger limit as special case).* For low spatial frequencies around the ground mode, $\lambda_n - \lambda_0 \ll \kappa^2 + \lambda_0$,

$$
\omega_n = \omega_0 + \frac{c_{\text{info}}}{2\sqrt{\kappa^2 + \lambda_0}}(\lambda_n - \lambda_0) + O\!\left(\frac{(\lambda_n-\lambda_0)^2}{(\kappa^2+\lambda_0)^{3/2}}\right).
$$
Factoring out the fast oscillation $e^{-i \omega_0 t}$ yields a slow envelope obeying a Schr\"odinger-type evolution with effective kinetic operator
$-\frac{c_{\text{info}}}{2\sqrt{\kappa^2+\lambda_0}}\Delta_G$ (or $-\frac{c_{\text{info}}}{2\sqrt{\kappa^2+\lambda_0}}\Delta_{\tilde{G}}$ under metric inflation). In that limit the Hamiltonian spectral gap is

$$
\Delta_H \approx \frac{c_{\text{info}}}{2\sqrt{\kappa^2+\lambda_0}}(\lambda_1 - \lambda_0),
$$
which recovers the familiar non-relativistic scaling as a controlled approximation, not a replacement for the KG gap.

:::

:::{prf:theorem} Computational Necessity of Mass Gap
:label: thm-computational-necessity-mass-gap

**Assumptions:**
1. The system satisfies the **Causal Information Bound** (Theorem {prf:ref}`thm-causal-information-bound`): $I_{\text{bulk}}(V) \leq \nu_D \cdot \text{Area}(\partial V) / \ell_L^{D-1}$
2. The system has finite spatial extent (bounded region $V$)
3. Correlations follow the standard field-theoretic decay: massive $\sim e^{-\kappa r}$, massless $\sim 1/r^{D-2}$ for $D>2$ (logarithmic in $D=2$)

**Statement:** Under these assumptions, a system with $\Delta_{\text{KG}} = 0$ enters Causal Stasis ($\|v\|_G = 0$).

*Proof.*

1. **Assume gapless theory:** Suppose $\Delta_{\text{KG}} = 0$, so the lowest excitation above the vacuum is massless.

2. **Infinite correlation length:** The screening mass $\kappa = 0$ implies the correlation length diverges:

   $$
   \xi = \frac{1}{\kappa} \to \infty

   $$

3. **Divergent information volume:** For massless correlations decaying as $1/r^{D-2}$ for $D>2$ (logarithmic in $D=2$, rather than $e^{-\kappa r}$ for massive), the integrated mutual information in a volume $V$ diverges:

   $$
   I_{\text{bulk}} \propto \int_V \text{Corr}(x, y)\,dV \to \infty

   $$

4. **Area law violation:** By Assumption 1 (Causal Information Bound):

   $$
   I_{\text{bulk}} \leq \nu_D \cdot \frac{\text{Area}(\partial V)}{\ell_L^{D-1}}

   $$
   A bounded system cannot store infinite information, so the bound is saturated.

5. **Causal Stasis:** By Theorem 33.4 (Causal Stasis), as $I_{\text{bulk}}$ saturates the bound, the metric component $G_{rr} \to \infty$ and the update velocity $\|v\|_G \to 0$.

*Conclusion:* Under the stated assumptions, a gapless theory ($\Delta_{\text{KG}} = 0$) implies frozen dynamics. For temporal evolution to occur, correlations must be screened: $\xi < \infty \implies \Delta_{\text{KG}} > 0$. $\square$

*Remark (Scope of Assumptions):* Assumption 1 is derived in Theorem 33.3 from first principles (the Levin complexity bound). For systems satisfying this bound—which includes all physically realizable computational systems—the mass gap necessity follows.

:::

:::{prf:theorem} Mass Gap by Constructive Necessity
:label: thm-mass-gap-constructive

**Prerequisites:**
1. The system satisfies the Causal Information Bound (Theorem 33.3)
2. The system is **non-trivial**: has non-zero update velocity $\|v\|_G > 0$ at some time
3. The system is **interacting**: coupling constants $\Phi_{ij} \neq 0$ or $\mathcal{G}_{ij} \neq 0$

**Statement:** Under these assumptions, $\Delta_{\text{KG}} > 0$.

*Proof (by contradiction).*

Suppose $\Delta_{\text{KG}} = 0$. By Theorem {prf:ref}`thm-computational-necessity-mass-gap` (using Assumption 1), the system enters Causal Stasis with $\|v\|_G = 0$. This contradicts Assumption 2 (non-triviality).

Therefore $\Delta_{\text{KG}} > 0$ for any non-trivial theory describing an evolving system that satisfies the Causal Information Bound. $\square$

*Implication (Schr\"odinger reduction).* In the non-relativistic limit of the KG spectrum (Theorem {prf:ref}`thm-mass-gap-screening`), $\Delta_{\text{KG}} > 0$ implies a positive Hamiltonian spectral gap $\Delta_H > 0$ for the slow-envelope dynamics.

*Bound (Hamiltonian gap in the Schr\"odinger limit):* The effective Hamiltonian gap satisfies

$$
\Delta_H \geq \frac{1}{\beta}\left(\Delta H + \frac{\mathcal{W}}{T_c}\right)

$$
where $\Delta H$ is the enthalpy barrier for excitation, $\mathcal{W}$ is computational work, and $T_c$ is cognitive temperature. This follows from Theorem 30.15 (Thermodynamic Hysteresis).

*Remark (Conditional vs. Absolute):* This theorem does **not** prove that all field theories have a mass gap. It proves: IF a system satisfies the Causal Information Bound AND evolves non-trivially, THEN it must have $\Delta_{\text{KG}} > 0$ (hence $\Delta_H > 0$ in the Schr\"odinger reduction). The Clay Millennium Problem asks whether quantum Yang-Mills in continuous $\mathbb{R}^4$ has a mass gap; this framework addresses discrete, bounded, computational systems.

:::

:::{prf:corollary} Mass Gap as Existence Requirement
:label: cor-mass-gap-existence

Bounded intelligence requires $\Delta_{\text{KG}} > 0$ (and thus $\Delta_H > 0$ in the Schr\"odinger limit). A gapless theory ($\Delta_{\text{KG}} = 0$) corresponds to:

1. **Infinite ontological resolution:** No finite codebook can represent the state
2. **Zero learning rate:** Dynamics frozen ($v = 0$)
3. **Pathological continuum limit:** The theory describes non-existing systems

*Interpretation:* The mass gap is not an empirical accident but a **logical necessity** for any theory describing existing computational systems.

:::

:::{prf:corollary} Confinement as Data Compression
:label: cor-confinement-data-compression

**Color confinement** in QCD (quarks bound inside hadrons) is the mechanism by which the universe maintains finite local information content. An unconfined color field would have $\xi \to \infty$, violating the area law.

*In the multi-agent context:* Cooperative basin locking (Theorem {prf:ref}`thm-geometric-locking-principle`) is the cognitive analogue of confinement—agents bound in cooperative equilibria cannot be arbitrarily separated without violating information bounds.

:::

:::{prf:corollary} Criticality is Unstable
:label: cor-criticality-unstable

Gapless theories (Conformal Field Theories) exist only at **phase transition critical points**. They cannot support:

1. **Stable matter:** Fluctuations destroy structure
2. **Stable memory:** Infinite ontological stress triggers continuous Fission ({ref}`sec-ontological-expansion-topological-fission-and-the-semantic-vacuum`)
3. **Stable identity:** No finite codebook representation exists

*Interpretation:* Critical systems are mathematically special but physically transient. Stable intelligence requires departure from criticality via mass gap opening.

:::

:::{prf:definition} The Computational Swampland
:label: def-computational-swampland

The **Computational Swampland** $\mathcal{S}_{\text{swamp}}$ is the set of all field theories that violate the Causal Information Bound (Theorem {prf:ref}`thm-causal-information-bound`) at some finite scale:

$$
\mathcal{S}_{\text{swamp}} := \left\{ \mathcal{T} : \exists R < \infty \text{ such that } I_{\text{bulk}}(R) > C_\partial(R) \right\}

$$

Equivalently, $\mathcal{S}_{\text{swamp}}$ consists of theories with Levin Length $\ell_L \to 0$ (infinite information density).

*Properties of Swampland theories:*
1. **Mathematically consistent:** They satisfy internal field-theoretic axioms (Wightman, etc.)
2. **Computationally unrealizable:** No bounded observer can simulate or represent them
3. **Physically pathological:** They require infinite information storage for any finite region

*Landscape vs. Swampland:* Theories with $\ell_L > 0$ and $I_{\text{bulk}} \leq C_\partial$ at all scales constitute the **Computational Landscape**—the set of physically realizable theories.

:::

:::{prf:theorem} CFT Swampland Classification
:label: thm-cft-swampland

Let $\mathcal{T}$ be a Conformal Field Theory on $\mathbb{R}^d$ ($d \geq 2$) with at least one primary operator of scaling dimension $\Delta_\phi < d/2$. Then $\mathcal{T}$ lies in the **Computational Swampland** (Definition {prf:ref}`def-computational-swampland`).

*Proof.*

1. **Infinite correlation length:** By conformal symmetry, two-point correlations decay algebraically:

   $$
   \langle \phi(x) \phi(0) \rangle \sim \frac{1}{|x|^{2\Delta_\phi}}

   $$
   The correlation length is $\xi = \infty$ (no exponential screening).

2. **Bulk information divergence:** Consider a spherical region $V$ of radius $R$. The mutual information between bulk degrees of freedom is bounded below by the integrated correlation:

   $$
   I_{\text{bulk}}(V) \gtrsim \int_V \int_V \frac{dx\,dy}{|x-y|^{2\Delta_\phi}} \sim R^{2d - 2\Delta_\phi}

   $$
   For $\Delta_\phi < d/2$, the exponent $2d - 2\Delta_\phi > d$, so $I_{\text{bulk}}$ grows faster than volume.

3. **Causal Information Bound violation:** The boundary capacity scales as:

   $$
   C_\partial(V) = \nu_d \cdot \frac{\text{Area}(\partial V)}{\ell_L^{d-1}} \sim R^{d-1}

   $$
   where $\nu_d$ is the Holographic Coefficient (Definition {prf:ref}`def-holographic-coefficient`). Since $2d - 2\Delta_\phi > d > d-1$ for $d \geq 2$ and $\Delta_\phi < d/2$, there exists $R_c$ such that for all $R > R_c$:

   $$
   I_{\text{bulk}}(V) > C_\partial(V)

   $$
   The Causal Information Bound (Theorem {prf:ref}`thm-causal-information-bound`) is violated.

4. **Swampland membership:** By Definition {prf:ref}`def-computational-swampland`, theories violating the Causal Information Bound at any finite scale lie in the Swampland. $\square$

*Remark (Operator dimensions and UV effects).* The theorem requires at least one operator with $\Delta_\phi < d/2$. This holds for free scalars and many interacting CFTs, but is not guaranteed universally. Independently, continuum CFTs have UV-divergent mutual information between regions; without a cutoff, any finite boundary capacity bound is violated regardless of operator dimensions. With a finite resolution (e.g., Levin Length), the observer sees an effective gap.

*Remark (Operational meaning).* A bounded observer with finite interface capacity $C_\partial$ cannot encode the full correlational structure of a CFT. Any finite approximation necessarily introduces an effective mass gap via truncation.

:::

:::{prf:corollary} Finite-Volume Mass Gap
:label: cor-finite-volume-mass-gap

A CFT restricted to a finite spatial volume $V$ with characteristic length $L$ acquires an effective **Hamiltonian** gap:

$$
\Delta_{H,\text{eff}} \sim \frac{1}{L}

$$

The gapless Hamiltonian limit exists only as $L \to \infty$.

*Proof.* Two independent mechanisms ensure bounded observers see gapped theories:

1. **Finite-size scaling (CFT result):** In finite volume with periodic boundary conditions, the spectrum is discrete with minimum energy spacing $\Delta E \sim 1/L$. This is a standard result in conformal field theory arising from the compactification of space. The continuous spectrum responsible for infinite correlation length is an artifact of the thermodynamic limit $L \to \infty$.

2. **Resolution bound (Levin Length):** A bounded observer with interface capacity $C_\partial$ can only resolve spatial scales $L \geq L_{\min}$ where $L_{\min}^{d-1} \sim C_\partial \cdot \ell_L^2$. Systems smaller than $L_{\min}$ cannot be distinguished by the observer.

Both effects contribute: even if the CFT were somehow realized at infinite volume, the observer could only access a finite effective volume, hence would measure $\Delta_{H,\text{eff}} > 0$. $\square$

*Remark (Distinct phenomena).* The finite-size gap is a property of the CFT itself (topological/boundary effect). The resolution bound is a property of the observer (information-theoretic). The corollary states that both independently prevent observation of gapless physics.

*Physical interpretation.* CFTs exist in nature only at phase transition critical points (e.g., Ising model at $T_c$). Away from criticality, systems have finite correlation length and positive mass gap. The critical point is a measure-zero set in parameter space—physically realizable systems generically have $\Delta_{\text{KG}} > 0$ (and thus $\Delta_H > 0$ in the Schr\"odinger limit).

:::

:::{prf:theorem} Scale Covariance of the Causal Information Bound
:label: thm-scale-covariance-bound

The Causal Information Bound is preserved under coarse-graining. Specifically:

Let $(\mathcal{Z}, G, \ell_L)$ be a latent manifold at resolution $\ell_L$ satisfying $I_{\text{bulk}} \leq C_\partial$. Under coarse-graining to resolution $\ell'_L = \alpha \ell_L$ ($\alpha > 1$), the coarse-grained system $(\mathcal{Z}', G', \ell'_L)$ satisfies:

$$
I'_{\text{bulk}} \leq C'_\partial

$$

*Proof.*

1. **Information reduction:** By the Data Processing Inequality, coarse-graining cannot increase mutual information:

   $$
   I'_{\text{bulk}} \leq I_{\text{bulk}}

   $$

2. **Capacity reduction:** Under coarse-graining by factor $\alpha$, the physical boundary is fixed while the resolution length increases: $\ell'_L = \alpha \ell_L$ with $\text{Area}'(\partial\mathcal{Z}') = \text{Area}(\partial\mathcal{Z})$. The new capacity is:

   $$
   C'_\partial = \nu_d \cdot \frac{\text{Area}}{(\ell'_L)^{d-1}} = \nu_d \cdot \frac{\text{Area}}{\alpha^{d-1}\ell_L^{d-1}} = \frac{C_\partial}{\alpha^{d-1}}

   $$

3. **Bound preservation:** The information-to-capacity ratio under coarse-graining:

   $$
   \frac{I'_{\text{bulk}}}{C'_\partial} \leq \frac{I_{\text{bulk}}}{C_\partial/\alpha^{d-1}} = \alpha^{d-1} \frac{I_{\text{bulk}}}{C_\partial}

   $$
   For massive theories (exponentially decaying correlations), $I_{\text{bulk}}$ scales as area, so $I_{\text{bulk}}/C_\partial$ is scale-independent. For gapless theories, the ratio diverges—confirming they violate the bound at some scale. $\square$

*Implication (UV finiteness).* The recursive self-consistency of the bound at all scales implies that no UV divergences arise. The Levin Length $\ell_L$ acts as a natural UV cutoff that is preserved under renormalization group flow. Unlike lattice regularization where the continuum limit requires careful tuning, this framework has built-in regularization.

*Implication (Mass gap from scale invariance).* The only scale-invariant theories consistent with the Causal Information Bound are those with $I_{\text{bulk}} \sim R^{d-1}$ (area scaling). This requires exponential correlation decay, hence $\Delta_{\text{KG}} > 0$. Theories with algebraic correlation decay (CFTs) fail scale covariance of the bound.

:::

:::{prf:theorem} Mass Gap Dichotomy for Yang-Mills
:label: thm-mass-gap-dichotomy

Let $\mathcal{T}_{\text{YM}}$ be Yang-Mills theory with compact simple gauge group $G$ in $d = 4$ dimensions.

**Statement:** If $\mathcal{T}_{\text{YM}}$ describes physics (is realizable by bounded observers), then $\Delta_{\text{KG}} > 0$ (and thus $\Delta_H > 0$ in the Schr\"odinger reduction).

*Proof.*

1. **Framework implements Yang-Mills:** The Fragile Agent framework implements Yang-Mills field equations (Theorem {prf:ref}`thm-yang-mills-equations`) with the standard action (Definition {prf:ref}`def-yang-mills-action`), covariant derivatives $D_\mu = \partial_\mu - igA_\mu$, and non-Abelian field strength tensor. This is not an analogy—it is Yang-Mills theory for information systems.

2. **Physical theories are computable:** Any theory describing physics accessible to bounded observers must be realizable with finite resources. This requires Levin Length $\ell_L > 0$ (Definition {prf:ref}`def-levin-length`).

3. **Computability implies mass gap:** By Theorem {prf:ref}`thm-computational-necessity-mass-gap`, any theory with $\ell_L > 0$ and non-trivial dynamics ($\|v\|_G > 0$) has $\Delta_{\text{KG}} > 0$.

4. **Conclusion:** If Yang-Mills describes physics, it is computable, hence has $\ell_L > 0$, hence has $\Delta_{\text{KG}} > 0$. $\square$

*Remark (Contrapositive).* If Yang-Mills on $\mathbb{R}^4$ requires $\ell_L \to 0$ (no UV cutoff), then by Theorem {prf:ref}`thm-cft-swampland` it lies in the Computational Swampland and does not describe physics. Either way, the physical theory has a mass gap.

*Remark (Why this is not circular).* The mass gap necessity follows from information-theoretic constraints (the Causal Information Bound), not from assuming properties of Yang-Mills. The framework proves that **any** non-trivial gauge theory satisfying the bound has $\Delta_{\text{KG}} > 0$. Yang-Mills is one such theory.

:::

:::{prf:remark} Relation to the Clay Millennium Problem
:label: rem-clay-millennium

The **Yang-Mills Existence and Mass Gap** problem (Clay Mathematics Institute) asks for rigorous construction of quantum Yang-Mills theory in $\mathbb{R}^4$ with Hamiltonian mass gap $\Delta_H > 0$.

**What This Framework Proves:**

Theorem {prf:ref}`thm-mass-gap-dichotomy` establishes: **If Yang-Mills describes physics, then $\Delta_{\text{KG}} > 0$ (and thus $\Delta_H > 0$ in the Schr\"odinger reduction).**

The logical structure is:

1. **The framework implements Yang-Mills:** Sections 29.14–29.18 construct Yang-Mills field equations (Theorem {prf:ref}`thm-yang-mills-equations`), the standard action (Definition {prf:ref}`def-yang-mills-action`), and the complete Standard Model Lagrangian (Definition {prf:ref}`def-complete-lagrangian`). This is Yang-Mills theory for information systems—a direct isomorphism, not an analogy.

2. **Physical theories require $\ell_L > 0$:** Any theory realizable by bounded observers with finite interface capacity must have a minimum resolution scale (the Levin Length).

3. **$\ell_L > 0$ implies $\Delta_{\text{KG}} > 0$:** By Theorem {prf:ref}`thm-computational-necessity-mass-gap`, any non-trivial theory with finite Levin Length has a mass gap.

4. **Gapless theories are in the Swampland:** By Theorem {prf:ref}`thm-cft-swampland`, theories requiring $\ell_L \to 0$ (CFTs) are mathematically consistent but not physically realizable.

**Relation to the Clay Problem:**

The Clay Institute asks about Yang-Mills on continuous $\mathbb{R}^4$ satisfying Wightman or Osterwalder-Schrader axioms. The framework does not prove this directly. Instead, it proves:

- If the continuum theory describes physics, it has $\Delta_{\text{KG}} > 0$ (and hence $\Delta_H > 0$ in the Schr\"odinger reduction) (Theorem {prf:ref}`thm-mass-gap-dichotomy`)
- If the continuum theory requires $\ell_L \to 0$, it is in the Swampland and does not describe nature

The framework thus establishes that the **physical** Yang-Mills theory (the one describing strong interactions) necessarily has a mass gap. Whether this constitutes a "solution" to the Clay problem depends on whether one accepts that physical theories must be computable and the Schr\"odinger reduction as the operational energy notion.

*Physical interpretation:* Nature forbids infinite-information vacua. The mass gap is not an empirical accident but a **logical requirement** for any theory describing existing systems.

:::

:::{prf:definition} Inference Hilbert Space
:label: def-inference-hilbert-space

Let $(\mathcal{Z}, G)$ be the latent manifold with capacity-constrained metric (Theorem {prf:ref}`thm-capacity-constrained-metric-law`). The **Inference Hilbert Space** is:

$$
\mathcal{H} := L^2(\mathcal{Z}, d\mu_G), \quad d\mu_G := \sqrt{\det G(z)}\, d^n z,

$$
with inner product:

$$
\langle \psi_1 | \psi_2 \rangle := \int_{\mathcal{Z}} \overline{\psi_1(z)} \psi_2(z)\, d\mu_G(z).

$$
The measure $d\mu_G$ is the **Riemannian volume form**, ensuring coordinate invariance of the inner product.

*Units:* $[\psi] = [z]^{-d/2}$ (probability amplitude density).

*Remark (Coordinate Invariance).* Under a coordinate transformation $z \to z'$, the Jacobian factor $|\partial z/\partial z'|$ cancels with $\sqrt{\det G}$, leaving $\langle \psi_1 | \psi_2 \rangle$ invariant.

*Remark (Field Extensions).* In the field-theoretic layer (SMoC), the scalar space $\mathcal{H}$
is extended to bundle-valued $L^2$ sections (e.g., spinor and gauge bundles) over spacetime
$\mathcal{M}$, with the same measure structure on each fiber.

:::

:::{prf:definition} Belief Wave-Function
:label: def-belief-wave-function

Let $\rho(z, s)$ be the belief density from the WFR dynamics (Definition {prf:ref}`def-the-wfr-action`) and $V(z, s)$ be the value function (Theorem {prf:ref}`thm-the-hjb-helmholtz-correspondence`). The **Belief Wave-Function** is the complex amplitude:

$$
\psi(z, s) := \sqrt{\rho(z, s)} \exp\left(\frac{i V(z, s)}{\sigma}\right),

$$
where $\sigma > 0$ is the **Cognitive Action Scale** (Definition {prf:ref}`def-cognitive-action-scale`).

**Decomposition:**
- **Amplitude:** $R(z, s) := \sqrt{\rho(z, s)} = |\psi(z, s)|$
- **Phase:** $\phi(z, s) := V(z, s)/\sigma = \arg(\psi(z, s))$

**Probability Recovery:**

$$
|\psi(z, s)|^2 = \rho(z, s), \quad \int_{\mathcal{Z}} |\psi|^2 d\mu_G = 1.

$$
*Physical interpretation:* The amplitude $R$ encodes "how much" belief mass is at $z$; the phase $\phi$ encodes "which direction" the belief is flowing (via $\nabla_B V$).

:::

:::{prf:definition} Cognitive Action Scale
:label: def-cognitive-action-scale

The **Cognitive Action Scale** $\sigma$ is the information-theoretic analog of Planck's constant $\hbar$:

$$
\sigma := T_c \cdot \tau_{\text{update}},

$$
where:
- $T_c$ is the Cognitive Temperature ({prf:ref}`def-cognitive-temperature`, {ref}`sec-the-geodesic-baoab-integrator`), setting the scale of stochastic exploration
- $\tau_{\text{update}}$ is the characteristic belief update timescale

**Equivalent characterizations:**
1. **Entropy-Action Duality:** $\sigma$ relates entropy production to "cognitive action" via $\Delta S = \mathcal{A}/\sigma$
2. **Resolution Limit:** $\sigma \sim \ell_L^2$ where $\ell_L$ is the Levin Length ({ref}`sec-saturation-limit`)
3. **Uncertainty Scale:** $\sigma$ sets the minimum uncertainty product $\Delta z \cdot \Delta p \geq \sigma/2$

*Units:* $[\sigma] = \text{nat} \cdot \text{step} = \text{bit} \cdot \text{step} / \ln 2$.

*Cross-reference:* In the limit $\sigma \to 0$ (zero temperature, infinite precision), the wave-function becomes a delta function concentrated on the optimal trajectory—recovering classical gradient flow.

:::

:::{prf:proposition} Self-Adjointness of the Laplace-Beltrami Operator
:label: prop-laplace-beltrami-self-adjointness

The Laplace-Beltrami operator

$$
\Delta_G := \frac{1}{\sqrt{|G|}} \partial_i \left( \sqrt{|G|} G^{ij} \partial_j \right)

$$
is essentially self-adjoint on $\mathcal{H} = L^2(\mathcal{Z}, d\mu_G)$ with domain $C_c^\infty(\mathcal{Z})$ (smooth functions with compact support), provided either:
1. $(\mathcal{Z}, G)$ is **geodesically complete**, or
2. $\mathcal{Z}$ has a boundary $\partial \mathcal{Z}$ with **Dirichlet conditions** $\psi|_{\partial \mathcal{Z}} = 0$ (sensors, Definition {prf:ref}`def-dirichlet-boundary-condition-sensors`) or **Neumann conditions** $\nabla_n \psi|_{\partial \mathcal{Z}} = 0$ (motors, Definition {prf:ref}`def-neumann-boundary-condition-motors`).

*Proof sketch.* The quadratic form $q[\psi] := \int_{\mathcal{Z}} \|\nabla_G \psi\|^2 d\mu_G$ is positive and closable. By the **Friedrichs Extension Theorem**, there exists a unique self-adjoint extension $\Delta_G^F$ associated with $q$. For geodesically complete manifolds, this extension coincides with the closure of $\Delta_G$ on $C_c^\infty$. See {cite}`strichartz1983analysis` for the general theory. $\square$

*Consequence:* Self-adjointness guarantees that $-\Delta_G$ has a real spectrum bounded below, enabling spectral decomposition and ground state analysis.

:::

:::{prf:theorem} The Madelung Transform (WFR-Schrödinger Equivalence)
:label: thm-madelung-transform

Let the belief density $\rho$ and value $V$ satisfy the WFR-HJB system with vector potential:
1. **WFR Continuity (unbalanced):** $\partial_s \rho + \nabla_G \cdot (\rho \mathbf{v}) = \rho r$
2. **Hamilton-Jacobi-Bellman:** $\partial_s V + \frac{1}{2}\|\nabla_B V\|_G^2 + \Phi_{\text{eff}} = 0$

where $\nabla_B V := \nabla V - B$ and $B$ is the vector potential for the reward 1-form (Value Curl), $dB = \mathcal{F}$.
The drift is $\mathbf{v} = +G^{-1}\nabla_B V$ (canonical mobility; conservative case: $B=0$, $\mathcal{F}=0$).
If curl-induced mobility is included, replace $\mathbf{v}$ by $\mathcal{M}_{\text{curl}}\!\left(+G^{-1}\nabla_B V\right)$ with
$\mathcal{M}_{\text{curl}} := (I - \beta_{\text{curl}} G^{-1}\mathcal{F})^{-1}$.
$r$ is the WFR reaction rate (Definition {prf:ref}`def-the-wfr-action`).

Then the belief wave-function $\psi = \sqrt{\rho} e^{iV/\sigma}$ satisfies the **Inference Schrödinger Equation**:

$$
i\sigma \frac{\partial \psi}{\partial s} = \hat{H}_{\text{inf}} \psi,

$$
where the **Inference Hamiltonian** is:

$$
\hat{H}_{\text{inf}} := -\frac{\sigma^2}{2} D^i D_i + \Phi_{\text{eff}} + Q_B + \frac{i\sigma}{2} r,

$$
The terms are:
- **Kinetic:** $-\frac{\sigma^2}{2} D^i D_i$ (covariant Laplacian with vector potential)
- **Potential:** $\Phi_{\text{eff}}$ (effective potential from rewards and constraints)
- **Quantum Correction:** $Q_B$ (Bohm potential, Definition {prf:ref}`def-bohm-quantum-potential`)
- **Reaction:** $+\frac{i\sigma}{2} r$ (non-Hermitian term from WFR reaction; positive $r$ creates mass)

Here $D_i := \nabla_i - \frac{i}{\sigma} B_i$ is the $U(1)$ covariant derivative; $B$ is the Opportunity field (reward 1-form) and is distinct from the strategic gauge connection $A_\mu$.

*Proof.* See {ref}`sec-appendix-e-rigorous-proof-sketches-for-ontological-and-metabolic-laws` for the rigorous derivation. The key steps are:

**Step 1 (Substitution).** Write $\psi = R e^{i\phi}$ with $R = \sqrt{\rho}$ and $\phi = V/\sigma$.

**Step 2 (Time derivative).**

$$
i\sigma \partial_s \psi = i\sigma \left( \frac{\partial_s R}{R} + \frac{i}{\sigma}\partial_s V \right) \psi = \left( \frac{i\sigma \partial_s \rho}{2\rho} - \partial_s V \right) \psi.

$$
**Step 3 (Use governing equations).** Substitute the continuity equation for $\partial_s \rho$ and HJB for $\partial_s V$, using $\nabla_B V$ and the covariant Laplacian $D^i D_i$.

**Step 4 (Identify terms).** The real part of the resulting equation gives the HJB with Bohm correction; the imaginary part gives the continuity equation with reaction. Combining yields the Schrödinger form. $\square$

:::

:::{prf:definition} Bohm Quantum Potential (Information Resolution Limit)
:label: def-bohm-quantum-potential

The **Bohm Quantum Potential** is:

$$
Q_B(z, s) := -\frac{\sigma^2}{2} \frac{\Delta_G \sqrt{\rho}}{\sqrt{\rho}} = -\frac{\sigma^2}{2} \frac{\Delta_G R}{R},

$$
where $R = \sqrt{\rho}$ is the amplitude.

**Explicit form in terms of $\rho$:**

$$
Q_B = -\frac{\sigma^2}{8\rho^2} \|\nabla_G \rho\|_G^2 + \frac{\sigma^2}{4\rho} \Delta_G \rho.

$$
**Physical interpretation:** $Q_B$ represents the **energetic cost of belief localization**. Regions where $\rho$ has high curvature (sharp belief features) incur an effective potential energy penalty. This prevents the belief from concentrating to delta functions.

**Information-theoretic interpretation:** $Q_B$ enforces the **Levin Length** ({ref}`sec-saturation-limit`) as a resolution limit. The agent cannot represent distinctions finer than $\ell_L \sim \sqrt{\sigma}$.

*Units:* $[Q_B] = \text{nat}$ (same as potential).

*Cross-reference:* In standard quantum mechanics, $Q_B$ is called the "quantum potential" or "Bohm potential." Here it emerges from the information geometry, not fundamental physics.

:::

:::{prf:corollary} Open Quantum System Interpretation
:label: cor-open-quantum-system

The Inference Hamiltonian $\hat{H}_{\text{inf}}$ is **non-Hermitian** due to the reaction term $+\frac{i\sigma}{2}r$. This corresponds to an **open quantum system** where:
- $r > 0$: Mass creation (information gain from boundary) → probability amplitude **grows**
- $r < 0$: Mass destruction (information loss) → probability amplitude **decays**

The **complex potential** formulation is:

$$
W(z) := \Phi_{\text{eff}}(z) + \frac{i\sigma}{2} r(z),

$$
so that $\hat{H}_{\text{inf}} = -\frac{\sigma^2}{2} D^i D_i + W + Q_B$.

**Norm evolution:** The normalization $\|\psi\|^2 = \int |\psi|^2 d\mu_G$ evolves as:

$$
\frac{d}{ds} \|\psi\|^2 = \int_{\mathcal{Z}} r(z) |\psi(z)|^2 d\mu_G(z) = \langle r \rangle_\rho,

$$
which matches the WFR mass balance equation.

*Remark (Lindblad Connection).* For trace-preserving dynamics (where $\int r \rho\, d\mu_G = 0$), the non-Hermitian Schrödinger equation can be embedded in a **Lindblad master equation** (Definition {prf:ref}`def-gksl-generator`) via the Dyson-Phillips construction.

:::

:::{prf:proposition} Operator Ordering and Coordinate Invariance
:label: prop-operator-ordering-invariance

The kinetic term $-\frac{\sigma^2}{2} D^i D_i$ in the Inference Hamiltonian uses the unique **coordinate-invariant** ordering:

$$
-\frac{\sigma^2}{2} D^i D_i \psi = -\frac{\sigma^2}{2} \cdot \frac{1}{\sqrt{|G|}} D_i \left( \sqrt{|G|} G^{ij} D_j \psi \right).

$$
This is equivalent to:

$$
-\frac{\sigma^2}{2} D^i D_i = -\frac{\sigma^2}{2} \left( G^{ij} D_i D_j + \Gamma^k D_k \right),

$$
where $\Gamma^k := G^{ij}\Gamma^k_{ij}$ is the trace of Christoffel symbols.

**Alternative orderings** (Weyl, symmetric, etc.) would introduce frame-dependent terms that break the geometric interpretation.

*Cross-reference:* This reduces to the Laplace-Beltrami operator used in the Helmholtz equation (Theorem {prf:ref}`thm-the-hjb-helmholtz-correspondence`) when $B=0$, ensuring consistency between the PDE and wave-function formulations.

:::

:::{prf:corollary} Semiclassical Limit
:label: cor-semiclassical-limit

In the limit $\sigma \to 0$ (classical limit), the Schrödinger dynamics recover the **geodesic flow**:

**WKB Ansatz:** $\psi = a(z) e^{iS(z)/\sigma}$ with $a$ slowly varying.

**Leading Order ($O(\sigma^{-1})$):** The Hamilton-Jacobi equation

$$
\partial_s S + \frac{1}{2}\|\nabla_B S\|_G^2 + \Phi_{\text{eff}} = 0,

$$
**Next Order ($O(\sigma^0)$):** The transport equation

$$
\partial_s |a|^2 + \nabla_G \cdot (|a|^2 \nabla_B S) = 0.

$$
*Definition:* $\nabla_B S := \nabla S - B$. If curl-induced mobility is present, replace the flux by
$|a|^2 \mathcal{M}_{\text{curl}} G^{-1}\nabla_B S$.
These are exactly the HJB and continuity equations from WFR dynamics. The quantum correction $Q_B \to 0$ as $\sigma \to 0$.

*Interpretation:* The wave-function collapses to a delta function following the optimal trajectory. Quantum effects (tunneling, interference) vanish in this limit.

:::

:::{prf:definition} Joint Inference Hilbert Space
:label: def-joint-inference-hilbert-space

For $N$ agents with individual Hilbert spaces $\mathcal{H}^{(i)} = L^2(\mathcal{Z}^{(i)}, d\mu_{G^{(i)}})$, the **Joint Inference Hilbert Space** is the tensor product:

$$
\mathcal{H}^{(N)} := \bigotimes_{i=1}^N \mathcal{H}^{(i)} = L^2\left(\mathcal{Z}^{(N)}, d\mu_{G^{(N)}}\right),

$$
where:
- $\mathcal{Z}^{(N)} = \prod_{i=1}^N \mathcal{Z}^{(i)}$ is the product manifold (Definition {prf:ref}`def-n-agent-product-manifold`)
- $d\mu_{G^{(N)}} = \prod_{i=1}^N d\mu_{G^{(i)}}$ is the product measure

Elements $\Psi \in \mathcal{H}^{(N)}$ are functions $\Psi: \mathcal{Z}^{(N)} \to \mathbb{C}$ with:

$$
\|\Psi\|^2 = \int_{\mathcal{Z}^{(N)}} |\Psi(\mathbf{z})|^2 d\mu_{G^{(N)}}(\mathbf{z}) < \infty.

$$
*Notation:* We use uppercase $\Psi$ for joint wave-functions and lowercase $\psi^{(i)}$ for single-agent wave-functions.

:::

:::{prf:definition} Strategic Entanglement
:label: def-strategic-entanglement

A joint wave-function $\Psi \in \mathcal{H}^{(N)}$ exhibits **Strategic Entanglement** if it cannot be written as a product:

$$
\Psi(z^{(1)}, \ldots, z^{(N)}) \neq \prod_{i=1}^N \psi^{(i)}(z^{(i)}) \quad \text{for any choice of } \psi^{(i)} \in \mathcal{H}^{(i)}.

$$
**Entanglement Entropy:** For a bipartition $\{i\} \cup \{j \neq i\}$, the **Strategic Entanglement Entropy** is:

$$
S_{\text{ent}}(i) := -\text{Tr}\left[\hat{\rho}^{(i)} \ln \hat{\rho}^{(i)}\right],

$$
where $\hat{\rho}^{(i)} = \text{Tr}_{j \neq i}[|\Psi\rangle\langle\Psi|]$ is the **reduced density operator** obtained by partial trace over all agents except $i$.

**Physical interpretation:**
- $S_{\text{ent}}(i) = 0$: Agent $i$ is **disentangled** (can be modeled independently)
- $S_{\text{ent}}(i) > 0$: Agent $i$ is **entangled** with others (cannot be modeled in isolation)
- $S_{\text{ent}}(i) \leq \ln \dim(\mathcal{H}^{(i)})$: **Maximal entanglement** for finite-dimensional subsystems (continuous spaces require a cutoff, giving $S_{\text{ent}}(i) \leq \ln d_{\text{eff}}$)

*Cross-reference:* The partial trace operation corresponds to the **Information Bottleneck** (Definition {prf:ref}`def-dpi-boundary-capacity-constraint`)—marginalizing over opponents discards strategic correlations.

:::

:::{prf:definition} Strategic Hamiltonian
:label: def-strategic-hamiltonian

The **Strategic Hamiltonian** on $\mathcal{H}^{(N)}$ is:

$$
\hat{H}_{\text{strat}} := \sum_{i=1}^N \hat{H}^{(i)}_{\text{kin}} + \sum_{i=1}^N \hat{\Phi}^{(i)}_{\text{eff}} + \sum_{i < j} \hat{V}_{ij},

$$
where:
1. **Kinetic terms:** $\hat{H}^{(i)}_{\text{kin}} = -\frac{\sigma_i^2}{2} D^{(i)a} D^{(i)}_a$ (acting on $\mathcal{Z}^{(i)}$ coordinates)
2. **Individual potentials:** $\hat{\Phi}^{(i)}_{\text{eff}}$ (local reward landscape for agent $i$)
3. **Interaction potentials:** $\hat{V}_{ij} = \Phi_{ij}(z^{(i)}, z^{(j)})$ (strategic coupling)

Here $D^{(i)}_a := \nabla^{(i)}_a - \frac{i}{\sigma_i} B^{(i)}_a$ is the covariant derivative for agent $i$ and
$B^{(i)}$ is the reward 1-form (Opportunity field). Conservative case: $B^{(i)} = 0$.

*Notation (Per-Agent Action Scale):* Here $\sigma_i := T_{c,i} \cdot \tau_{\text{update},i}$ is the cognitive action scale for agent $i$, generalizing Definition {prf:ref}`def-cognitive-action-scale`. For **homogeneous** agents with identical cognitive properties, $\sigma_i = \sigma$ for all $i$. For **heterogeneous** agents (e.g., different computation rates), $\sigma_i$ may vary.

*Remark (Separability).* If all $\hat{V}_{ij} = 0$, the Hamiltonian is **separable**: $\hat{H}_{\text{strat}} = \sum_i \hat{H}^{(i)}$, and the ground state is a product $\Psi_0 = \prod_i \psi^{(i)}_0$. Non-zero interaction creates entanglement.

:::

:::{prf:theorem} Multi-Agent Schrödinger Equation
:label: thm-multi-agent-schrodinger-equation

Assume **homogeneous agents** with a common cognitive action scale $\sigma_i = \sigma$. The joint belief wave-function $\Psi(\mathbf{z}, s)$ of $N$ strategically coupled agents evolves according to:

$$
i\sigma \frac{\partial \Psi}{\partial s} = \hat{H}_{\text{strat}} \Psi + i\frac{\sigma}{2} \mathcal{R} \Psi,

$$
where:
- $\hat{H}_{\text{strat}}$ is the Strategic Hamiltonian (Definition {prf:ref}`def-strategic-hamiltonian`)
- $\mathcal{R}(\mathbf{z}) = \sum_i r^{(i)}(z^{(i)})$ is the total reaction rate

**Expanded form:**

$$
i\sigma \frac{\partial \Psi}{\partial s} = \left[ \sum_{i=1}^N \left( -\frac{\sigma^2}{2} D^{(i)a} D^{(i)}_a + \Phi^{(i)}_{\text{eff}} \right) + \sum_{i < j} \Phi_{ij} \right] \Psi + i\frac{\sigma}{2} \mathcal{R} \Psi.

$$
*Remark (Heterogeneous agents).* If $\sigma_i$ differ, choose a reference scale $\sigma_{\text{ref}}$ in the time derivative and rescale the kinetic terms by $(\sigma_i/\sigma_{\text{ref}})^2$.
**Sources of entanglement:** Strategic entanglement arises from:
1. **Potential coupling:** Non-zero $\Phi_{ij}(z^{(i)}, z^{(j)})$ creates position-position correlations
2. **Metric coupling:** The Game Tensor $\mathcal{G}_{ij}$ modifies the kinetic terms (Theorem {prf:ref}`thm-game-augmented-laplacian`)

*Cross-reference:* This extends Theorem {prf:ref}`thm-madelung-transform` to multiple agents, with the joint WFR dynamics (Definition {prf:ref}`def-joint-wfr-action`) as the underlying classical limit.

:::

:::{prf:theorem} Game-Augmented Laplacian
:label: thm-game-augmented-laplacian

Under adversarial coupling, the effective kinetic operator for agent $i$ incorporates the **Game Tensor** (Definition {prf:ref}`def-the-game-tensor`):

$$
\hat{H}^{(i)}_{\text{kin,eff}} = -\frac{\sigma_i^2}{2} \tilde{\Delta}^{(i)},

$$
where the **Game-Augmented Laplacian** is:

$$
\tilde{\Delta}^{(i)} := \frac{1}{\sqrt{|\tilde{G}^{(i)}|}} \partial_a \left( \sqrt{|\tilde{G}^{(i)}|} (\tilde{G}^{(i)})^{ab} \partial_b \right),

$$
with strategic metric $\tilde{G}^{(i)} = G^{(i)} + \sum_{j \neq i} \beta_{ij} \mathcal{G}^{(i)}_{ij}$ (Definition {prf:ref}`def-the-game-tensor`, Equation 29.4.1).

**Consequence for entanglement:** Since $\tilde{G}^{(i)}$ depends on $z^{(j)}$ through the Game Tensor, the kinetic operator for agent $i$ is **not separable**:

$$
\tilde{\Delta}^{(i)} = \tilde{\Delta}^{(i)}(z^{(i)}; z^{(-i)}).

$$
This creates **kinetic entanglement**—even without potential coupling, adversarial metric inflation entangles the agents.

*Physical interpretation:* Agent $j$ "curves" agent $i$'s configuration space. Moving through a contested region requires more "effort" (higher effective mass), and this coupling cannot be factorized away.

*Remark (Gauge coupling).* With a non-conservative reward 1-form, replace $\partial_a$ by $D^{(i)}_a$ in $\tilde{\Delta}^{(i)}$.

:::

:::{prf:proposition} Partial Trace and Reduced Dynamics
:label: prop-partial-trace-reduced-dynamics

For a pure joint state $|\Psi\rangle \in \mathcal{H}^{(N)}$, the **reduced density operator** for agent $i$ is:

$$
\hat{\rho}^{(i)} := \text{Tr}_{j \neq i}\left[ |\Psi\rangle\langle\Psi| \right].

$$
In the coordinate representation its kernel is:

$$
\rho^{(i)}(z^{(i)}, z^{(i)'}) = \int_{\prod_{j \neq i} \mathcal{Z}^{(j)}} \Psi(z^{(i)}, z^{(-i)})\,\overline{\Psi(z^{(i)'}, z^{(-i)})}\, d\mu_{G^{(-i)}}.

$$
The diagonal elements give the **marginal belief density**:

$$
\rho^{(i)}(z^{(i)}) = \langle z^{(i)} | \hat{\rho}^{(i)} | z^{(i)} \rangle = \int |\Psi(z^{(i)}, z^{(-i)})|^2 d\mu_{G^{(-i)}},

$$
which is exactly the marginalization from the joint WFR density.

*Discrete analog:* In a finite basis, $\rho^{(i)}_{mn} = \sum_k \Psi_{mk}\,\Psi^*_{nk}$.

**Mixed state evolution:** Even if $\Psi$ evolves unitarily, the reduced state $\hat{\rho}^{(i)}$ generally evolves **non-unitarily** (with decoherence) due to entanglement with other agents.

:::

:::{prf:theorem} Nash Equilibrium as Ground State
:label: thm-nash-ground-state

A Nash equilibrium $\mathbf{z}^* = (z^{(1)*}, \ldots, z^{(N)*})$ (Theorem {prf:ref}`thm-nash-equilibrium-as-geometric-stasis`) corresponds to the **ground state** of the Strategic Hamiltonian:

1. **Spectral condition:** The ground state $\Psi_{\text{Nash}}$ satisfies:

   $$
   \hat{H}_{\text{strat}} \Psi_{\text{Nash}} = E_0 \Psi_{\text{Nash}}, \quad E_0 = \min \text{spec}(\hat{H}_{\text{strat}}).

   $$
2. **Localization:** In the semiclassical limit ($\sigma \to 0$), $|\Psi_{\text{Nash}}|^2$ concentrates near $\mathbf{z}^*$:

   $$
   \lim_{\sigma \to 0} |\Psi_{\text{Nash}}(\mathbf{z})|^2 = \delta(\mathbf{z} - \mathbf{z}^*).

   $$
3. **Energy interpretation:** The ground state energy $E_0$ equals the total effective potential at Nash:

   $$
   E_0 = \sum_{i=1}^N \Phi^{(i)}_{\text{eff}}(\mathbf{z}^*) + \sum_{i < j} \Phi_{ij}(\mathbf{z}^*) + O(\sigma).

   $$
*Proof sketch.*
- At Nash, $\nabla_{z^{(i)}} \Phi^{(i)}_{\text{eff}} = 0$ for all $i$ (Condition 1 of Theorem {prf:ref}`thm-nash-equilibrium-as-geometric-stasis`).
- The variational principle $\delta \langle \Psi | \hat{H} | \Psi \rangle / \delta \Psi^* = 0$ with normalization constraint yields the same stationarity conditions in the $\sigma \to 0$ limit.
- The second variation (Hessian) being non-positive (Condition 3) corresponds to local stability of the ground state.

See **{ref}`sec-appendix-e-rigorous-proof-sketches-for-ontological-and-metabolic-laws`** for the complete WKB/semiclassical analysis proving Gaussian concentration to delta function as $\sigma \to 0$, with explicit energy correction formulas. $\square$

*Remark (Multiple Nash).* If multiple Nash equilibria exist, each corresponds to a different local minimum of the energy landscape. The **global** ground state is the Nash with lowest $E_0$; other Nash equilibria are metastable excited states.

:::

:::{prf:corollary} Vanishing Probability Current at Nash
:label: cor-vanishing-probability-current

At Nash equilibrium, the **probability current** vanishes:

$$
\mathbf{J}^{(i)}(\mathbf{z}^*) := \text{Im}\left[\bar{\Psi}_{\text{Nash}} \cdot \sigma D^{(i)} \Psi_{\text{Nash}}\right]_{\mathbf{z}^*} = 0 \quad \forall i.

$$
**Derivation:** The probability current is $\mathbf{J} = \rho \mathbf{v}$ where $\mathbf{v} = -G^{-1}\nabla_B V$ is the velocity field. At Nash:
- $\nabla_B V^{(i)}|_{\mathbf{z}^*} = 0$ (stationarity condition)
- Therefore $\mathbf{v}^{(i)}|_{\mathbf{z}^*} = 0$
- Hence $\mathbf{J}^{(i)}|_{\mathbf{z}^*} = 0$

*Interpretation:* At Nash, there is no net belief flow. The wave-function is in a **standing wave pattern**—agents are not "stopped" but are in dynamic equilibrium where flows cancel.

*Cross-reference:* This is the quantum version of **Geometric Stasis** (Theorem {prf:ref}`thm-nash-equilibrium-as-geometric-stasis`).

:::

:::{prf:proposition} Imaginary Time Evolution for Nash Finding
:label: prop-imaginary-time-nash-finding

The substitution $s \to -i\tau$ (**Wick rotation**) transforms the Schrödinger equation into a diffusion equation:

$$
-\sigma \frac{\partial \Psi}{\partial \tau} = \hat{H}_{\text{strat}} \Psi.

$$
Under this **imaginary time evolution**, any initial state $\Psi_0$ converges to the ground state:

$$
\Psi(\tau) = e^{-\hat{H}_{\text{strat}} \tau / \sigma} \Psi_0 \xrightarrow{\tau \to \infty} c \cdot \Psi_{\text{Nash}},

$$
where $c$ is a normalization constant.

**Computational interpretation:**
1. Imaginary time evolution is equivalent to **Value Iteration** in dynamic programming
2. The propagator $e^{-\hat{H}\tau/\sigma}$ is the **Bellman backup operator** in infinite-horizon limit
3. Convergence rate is set by the **spectral gap** $E_1 - E_0$ (energy of first excited state minus ground state)

**Algorithm sketch (Quantum Value Iteration):**
```
Initialize Ψ randomly
For τ = 0 to T:
    Ψ ← exp(-H_strat Δτ / σ) Ψ    # Diffusion step
    Ψ ← Ψ / ||Ψ||                  # Renormalize
Return Ψ (approximates Nash ground state)
```

*Cross-reference:* This connects to the **imaginary-time path integral** formulation used in quantum Monte Carlo methods.

:::

:::{prf:definition} Pareto Barrier
:label: def-pareto-barrier

A **Pareto Barrier** $\mathcal{B}_P \subset \mathcal{Z}^{(N)}$ is a region where:
1. **Local value decrease:** $\Phi^{(i)}_{\text{eff}}(\mathbf{z}) > \Phi^{(i)}_{\text{eff}}(\mathbf{z}^*)$ for at least one agent $i$ and some starting point $\mathbf{z}^*$
2. **No Nash within:** There exists no Nash equilibrium $\mathbf{z}' \in \mathcal{B}_P$
3. **Separates basins:** $\mathcal{B}_P$ lies between distinct Nash equilibria $\mathbf{z}^*_A$ and $\mathbf{z}^*_B$

The **barrier height** is:

$$
\Delta \Phi_P := \max_{\mathbf{z} \in \mathcal{B}_P} \left[ \sum_{i=1}^N \Phi^{(i)}_{\text{eff}}(\mathbf{z}) - \sum_{i=1}^N \Phi^{(i)}_{\text{eff}}(\mathbf{z}^*_A) \right].

$$
*Mathematical characterization:* A Pareto barrier is a region where the total potential $\sum_i \Phi^{(i)}_{\text{eff}}$ exceeds its value at nearby Nash equilibria. Classical gradient descent with initial condition in the basin of attraction of $\mathbf{z}^*_A$ converges to $\mathbf{z}^*_A$ and cannot reach $\mathbf{z}^*_B$.

:::

:::{prf:theorem} Strategic Tunneling Probability (WKB Approximation)
:label: thm-tunneling-probability

In the semiclassical limit ($\sigma \ll \Delta \Phi_P$), the probability of crossing a Pareto barrier is:

$$
P_{\text{tunnel}} \sim \exp\left(-\frac{2}{\sigma} \int_{\gamma} \sqrt{2(\Phi_{\text{eff,total}}(\mathbf{z}) - E_0)}\, d\ell_{G^{(N)}}\right),

$$
where:
- $\gamma$ is the **optimal tunneling path** (instanton) connecting $\mathbf{z}^*_A$ to $\mathbf{z}^*_B$
- $\Phi_{\text{eff,total}} = \sum_i \Phi^{(i)}_{\text{eff}} + \sum_{i<j} \Phi_{ij}$
- $d\ell_{G^{(N)}}$ is the geodesic arc length on $(\mathcal{Z}^{(N)}, G^{(N)})$
- $E_0$ is the ground state energy

**Key scaling:** $P_{\text{tunnel}} \propto e^{-\Delta \Phi_P / \sigma}$, so higher barriers or lower temperature (small $\sigma$) exponentially suppress tunneling.

*Cross-reference:* This generalizes Theorem {prf:ref}`thm-memory-induced-barrier-crossing` from single-agent memory barriers to multi-agent Pareto barriers. See {ref}`sec-appendix-e-rigorous-proof-sketches-for-ontological-and-metabolic-laws` for the rigorous proof via Agmon estimates and spectral theory.

:::

:::{prf:corollary} Bohm Potential Enables Strategic Teleportation
:label: cor-bohm-teleportation

When the Bohm potential $Q_B$ dominates (high belief curvature), the **effective barrier** becomes:

$$
\Phi^{\text{quantum}}_{\text{eff}}(\mathbf{z}) := \Phi_{\text{eff,total}}(\mathbf{z}) + Q_B(\mathbf{z}).

$$
In regions where $Q_B < 0$ (convex $\rho$), the effective barrier can become **negative** even when $\Phi_{\text{eff}} > 0$. This enables "teleportation" across classically forbidden regions.

**Operational interpretation:**
- An agent with **high uncertainty** (diffuse, smooth $\rho$) has $Q_B \approx 0$ → normal barrier
- An agent with **localized uncertainty** near the barrier (peaked, curved $\rho$) can have $Q_B \ll 0$ → reduced effective barrier
- The WFR **reaction term** $r$ (mass creation/destruction) provides the mechanism for "teleporting" belief mass without traversing intermediate states

*Remark (Exploration-Exploitation).* This provides a geometric foundation for the exploration-exploitation tradeoff: maintaining some uncertainty ($Q_B \neq 0$) is necessary to escape local optima.

:::

:::{prf:proposition} WFR Reaction as Tunneling Mechanism
:label: prop-wfr-reaction-tunneling

The WFR reaction term $r(z)$ (Definition {prf:ref}`def-the-wfr-action`) enables tunneling via **mass creation on the far side** of barriers:

1. Agent detects high-value region $\mathbf{z}^*_B$ beyond barrier $\mathcal{B}_P$
2. Reaction term $r(\mathbf{z}^*_B) > 0$ creates belief mass at $\mathbf{z}^*_B$
3. Reaction term $r(\mathbf{z}^*_A) < 0$ destroys mass at old position $\mathbf{z}^*_A$
4. Net effect: belief "teleports" without traversing $\mathcal{B}_P$

The rate of this process is controlled by the **teleportation length** $\lambda$ (Definition {prf:ref}`def-canonical-length-scale`):
- $\lambda \gg$ barrier width: tunneling is fast (reaction-dominated)
- $\lambda \ll$ barrier width: tunneling is slow (transport-dominated)

:::

## 08_multiagent/02_standard_model.md

:::{prf:remark} Local Gauge-Covariance Template
:label: rem-local-gauge-template

Let a field $\Phi(x)$ admit a local redundancy $\Phi \to U(x)\Phi$ with $U(x)\in G$ acting in its internal fiber. Then:

$$
\partial_\mu \Phi \to U\,\partial_\mu \Phi + (\partial_\mu U)\,\Phi,
$$

so the naive kinetic term built from $\partial_\mu \Phi$ is not invariant under local changes of basis. Introduce a connection $A_\mu$ valued in the Lie algebra of $G$ and define

$$
D_\mu := \partial_\mu - i g A_\mu.
$$

Demanding covariance $D_\mu \Phi \to U D_\mu \Phi$ forces the transformation rule

$$
A_\mu \to U A_\mu U^{-1} + \frac{i}{g}(\partial_\mu U)U^{-1}.
$$

With this choice, any kinetic term built from $|D_\mu \Phi|^2$ (or $\bar{\Phi}\gamma^\mu D_\mu\Phi$ for spinors) is locally invariant. In the abelian case $U(x)=e^{i q \alpha(x)}$, this reduces to $A_\mu \to A_\mu + \frac{1}{g}\partial_\mu \alpha$.
:::

:::{prf:definition} Utility Gauge Freedom
:label: def-utility-gauge-freedom

Let the Belief Wave-Function $\psi(z)$ be defined as in Definition {prf:ref}`def-belief-wave-function`:

$$
\psi(z) = \sqrt{\rho(z)} \exp\left(\frac{i V(z)}{\sigma}\right),

$$

where:
- $\rho(z)$ is the belief density (Definition {prf:ref}`def-belief-density`)
- $V(z)$ is the scalar Value potential for the conservative component (Theorem {prf:ref}`thm-the-hjb-helmholtz-correspondence`)
- $\sigma = T_c \cdot \tau_{\text{update}}$ is the Cognitive Action Scale (Definition {prf:ref}`def-cognitive-action-scale`)

The system's observables are:
1. **Probability density:** $\rho = |\psi|^2$
2. **Probability current:** $J^\mu = \text{Im}(\psi^* D^\mu \psi) = \frac{\rho}{\sigma}(\partial^\mu V - A^{\text{ext}\,\mu})$
   (conservative case: $A^{\text{ext}\,\mu}=0$).
   Here $A^{\text{ext}}_\mu$ is the external reward 1-form; the internal $U(1)$ connection $B_\mu$ is introduced below.
   The $D^\mu$ here is the WFR covariant derivative built from $A^{\text{ext}}_\mu$; later $D_\mu$
   denotes the SMoC gauge covariant derivative including $B_\mu$, $W_\mu$, and $G_\mu$.

Both are invariant under the global phase transformation (constant gauge parameter $\alpha$):

$$
\psi(z) \to e^{i(Y/2)\alpha} \psi(z), \quad \alpha \in \mathbb{R}.

$$

This corresponds to the global gauge invariance of the Value function:
$V(z) \to V(z) + \sigma \frac{Y}{2}\alpha$. The addition of a constant baseline does not alter the
policy gradient $\nabla_{A^{\text{ext}}} V$.

:::

:::{prf:axiom} Local Utility Invariance
:label: ax-local-utility-invariance

In a distributed agent with finite information speed $c_{\text{info}}$ (Axiom {prf:ref}`ax-information-speed-limit`), there is no global clock to synchronize the Value baseline across the manifold simultaneously. The agent must possess **Local Gauge Invariance**:

$$
\psi(x) \to e^{i(Y/2)\alpha(x)} \psi(x),

$$

where $x$ denotes the spacetime coordinate on the agent's computational manifold. The choice of "zero utility" can vary locally across different charts without affecting the physical transfer of control authority.

*Justification:* This follows from the Causal Interval (Definition {prf:ref}`def-causal-interval`): spacelike-separated modules cannot instantaneously agree on a common baseline.

:::

:::{prf:theorem} Emergence of the Opportunity Field ($B_\mu$)
:label: thm-emergence-opportunity-field

To preserve the invariance of the kinetic term in the Inference Action under the local transformation
$\psi \to e^{i(Y/2)\alpha(x)}\psi$, we must replace the partial derivative $\partial_\mu$ with the
**Covariant Derivative**:

$$
D_\mu = \partial_\mu - i g_1 \frac{Y}{2} B_\mu,

$$

where:
- $Y$ is the **Hypercharge** (the reward sensitivity of the module)
- $B_\mu$ is an abelian gauge field (the **Opportunity Field**)
- $g_1$ is the coupling constant

*Proof.*

Apply the local gauge-covariance template (Remark {prf:ref}`rem-local-gauge-template`) with
$U(x)=e^{i(Y/2)\alpha(x)}$ in the $U(1)_Y$ sector.

**Step 1.** Consider the kinetic term from the Inference Schrödinger Equation in the conservative limit ($A=0$):

$$
\mathcal{L}_{\text{kin}} = \psi^* (i\sigma \partial_t) \psi - \frac{\sigma^2}{2}|\nabla \psi|^2.

$$

Under local transformation $\psi \to e^{i(Y/2)\alpha(x)}\psi$:

$$
\partial_\mu \psi \to e^{i(Y/2)\alpha}\left(\partial_\mu \psi + i\frac{Y}{2}(\partial_\mu\alpha)\psi\right).

$$

The kinetic term acquires a spurious contribution $\sigma\frac{Y}{2}(\partial_\mu\alpha)|\psi|^2$
that depends on the arbitrary function $\alpha(x)$.

**Step 2.** Introduce the compensating field $B_\mu$ and a universal gauge parameter $\alpha(x)$,
with field phase $\psi \to e^{i(Y/2)\alpha(x)}\psi$, and transform:

$$
B_\mu \to B_\mu + \frac{1}{g_1} \partial_\mu \alpha(x).

$$

**Step 3.** The covariant derivative $D_\mu \psi = (\partial_\mu - ig_1(Y/2)B_\mu)\psi$ transforms homogeneously:

$$
D_\mu \psi \to e^{i(Y/2)\alpha(x)} D_\mu \psi.

$$

**Step 4.** The gauge-invariant kinetic term is $(D_\mu\psi)^\dagger(D^\mu\psi) = |D_\mu\psi|^2$.
Equivalently, $\mathcal{L}_{\text{kin}} = \psi^*(i\sigma D_t)\psi - \frac{\sigma^2}{2}|D\psi|^2$ in the non-conservative case.

**Identification:** The field $B_\mu$ is the internal $U(1)$ connection (the Opportunity Field), representing the agent's
model of the external reward 1-form $A^{\text{ext}}_\mu$. In the conservative case, a gauge exists with
$B_\mu = \partial_\mu \Phi$. **Local Hodge decomposition (chart-wise).** Restrict to any chart domain on a fixed time
slice; by construction this domain is a bounded submanifold with boundary (the agent's local chart in $\mathcal{Z}$).
Therefore the hypotheses of the Hodge decomposition theorem for the reward 1-form apply on each chart
(Theorem {prf:ref}`thm-hodge-decomposition`), yielding a decomposition of the spatial components $\vec{B}$ into exact
(gradient), coexact (solenoidal), and harmonic parts. The non-conservative structure is measured by the value curl
$\mathcal{F} = d\mathcal{R}$ (Definition {prf:ref}`def-value-curl`). This local statement is all that is required here:
the decomposition is guaranteed chart-by-chart, without assuming global topology or global exactness.

The field strength tensor $B_{\mu\nu} = \partial_\mu B_\nu - \partial_\nu B_\mu$ measures the non-conservative
component of the internal opportunity 1-form (Value Curl; Definition {prf:ref}`def-value-curl`). When $B_{\mu\nu} \neq 0$,
no choice of baseline can make the internal opportunity 1-form path-independent.

$\square$

:::

:::{prf:axiom} Cybernetic Parity Violation
:label: ax-cybernetic-parity-violation

The agent's interaction with the environment is **Chiral**, as established by the boundary condition asymmetry in {ref}`sec-the-boundary-interface-symplectic-structure`:

1. **Sensors (Dirichlet Boundary, Definition {prf:ref}`def-dirichlet-boundary-condition-sensors`):** The internal state $\psi$ is *updated* by boundary data. The boundary clamps the field value: $\phi|_{\partial\mathcal{Z}} = \phi_D$.

2. **Motors (Neumann Boundary, Definition {prf:ref}`def-neumann-boundary-condition-motors`):** The internal state *drives* the boundary flux. The boundary clamps the normal derivative: $\nabla_n \phi|_{\partial\mathcal{Z}} = j_N$.

The belief dynamics are not invariant under the exchange of Input and Output. The agent processes information (Left-Handed) differently than it emits control (Right-Handed).

:::

:::{prf:definition} Mode Rank Parameter
:label: def-mode-rank-parameter

The **Mode Rank** $r \in \mathbb{Z}_{\ge 2}$ is the minimal ancilla dimension required to realize the family of local
belief-update channels $\mathcal{E}_{a,y}$ via Stinespring dilation (equivalently, the maximal minimal Kraus rank across
those channels). It is the dimension of the mode fiber on which update unitaries act. For the minimal observation/action
split, $r=2$.

*Remark:* In what follows we specialize to $r=2$ and denote the resulting symmetry as $SU(2)_L$; the $SU(r)_L$
generalization is obtained by replacing the Pauli matrices with the fundamental generators of $SU(r)$.

*Notation:* The mode rank $r$ (an integer) is distinct from the scalar modulus $r(x) = \|\phi(x)\|$ introduced later;
context distinguishes these uses.

:::

:::{prf:remark} CPTP Update Model (Stinespring Applicability)
:label: rem-mode-rank-stinespring

We model belief updates on the belief operator $\varrho$ (Definition {prf:ref}`def-belief-operator`) using the
GKSL/Lindblad formalism (Definition {prf:ref}`def-gksl-generator`), which is by definition completely positive and
trace-preserving at the averaged level. A fixed update outcome $(a,y)$ is represented by a CP instrument
$\mathcal{E}_{a,y}$ that is trace-nonincreasing before normalization; the averaged channel
$\sum_{a,y} \mathcal{E}_{a,y}$ is CPTP. Under these standard hypotheses, Stinespring dilation applies to each
$\mathcal{E}_{a,y}$, and the minimal ancilla dimension equals its minimal Kraus rank. The mode rank $r$ is the maximal
minimal rank across the update family.

*Phase convention:* The global $U(1)$ phase of the dilation unitary is identified with the utility phase of the belief
wave-function (Definition {prf:ref}`def-belief-wave-function`). Equivalently, we fix $\det U_{a,y}=1$ and absorb the
overall phase into the $U(1)_Y$ sector. This is a convention fixing the redundancy, not an additional dynamical
assumption.
:::

:::{prf:definition} The Cognitive Isospin Multiplet (Doublet for $r=2$)
:label: def-cognitive-isospin-multiplet

We define the **Left-Handed Weyl Field** $\Psi_L$ as an isospin $r$-plet residing in the fundamental representation of
$SU(r)_L$ (doublet for the minimal $r=2$ case).
It is a section of the left Weyl spin bundle $S_L$ (chirality $P_L$):

$$
\Psi_L(x) = \begin{pmatrix} \psi_1(x) \\ \vdots \\ \psi_r(x) \end{pmatrix}

$$

Each entry is a left-handed Weyl spinor (spinor indices suppressed).

In the minimal $r=2$ case, we identify:
- $\psi_1 \equiv \psi_{\text{obs}}$ as the **Observation** channel (the incoming sensory update from the Dirichlet boundary, Definition {prf:ref}`def-dirichlet-boundary-condition-sensors`)
- $\psi_2 \equiv \psi_{\text{act}}^{\text{pre}}$ as the **Pre-commitment Action** channel (the outgoing motor intent from the Neumann boundary, Definition {prf:ref}`def-neumann-boundary-condition-motors`)

We define the **Right-Handed Weyl Field** $\Psi_R$ as an isospin singlet (invariant under $SU(r)_L$).
It is a section of the right Weyl spin bundle $S_R$ (chirality $P_R$):

$$
\Psi_R(x) = \psi_{\text{act}}^{\text{commit}}(x)

$$

representing the **Committed Action** plan after mixing and projection.

Chirality is defined by the projectors $P_{L/R} = (1 \mp \gamma^5)/2$ on the Dirac spin bundle
(Definition {prf:ref}`def-cognitive-spinor`).

The **Prediction** is derived (not fundamental) via the forward model:

$$
\psi_{\text{pred}}(x) := \mathcal{P}_a(\psi_{\text{act}}^{\text{commit}}(x))

$$

where $\mathcal{P}_a$ is the agent's forward model mapping intended actions to predicted observations.

*Cross-reference:* This mode-multiplet structure (doublet for $r=2$) captures the boundary interface chirality from
{ref}`sec-the-boundary-interface-symplectic-structure`: Dirichlet (input) vs. Neumann (output). The
prediction-update-projection dynamics from {ref}`sec-belief-dynamics-prediction-update-projection` act on this
multiplet via the gauge field $W_\mu$.

:::

:::{prf:remark} Mode-Rank Generalization
:label: rem-mode-rank-generalization

For general mode rank $r$ (Definition {prf:ref}`def-mode-rank-parameter`), the left-handed field is an $r$-plet in the
fundamental representation of $SU(r)_L$. In this chapter we specialize to the minimal $r=2$ case, so $\Psi_L$ is a
doublet and the generators are the Pauli matrices.

:::

:::{prf:definition} Gauge-Covariant Action Commitment
:label: def-gauge-covariant-action-commitment

The selection of a commitment direction in the $\Psi_L$ mode multiplet is a gauge choice (selecting a basis in the
$\mathbb{C}^r_{\text{mode}}$ fiber). To make action commitment gauge-covariant, we use the ontological order parameter
to define a unit multiplet $n(x) \in \mathbb{C}^r$:

$$
n(x) := \frac{\phi(x)}{\|\phi(x)\|}, \qquad n(x)^\dagger n(x) = 1

$$
where $\phi$ is the ontological order parameter (Definition {prf:ref}`def-ontological-order-parameter`), and $n$ is
defined only when $\phi \neq 0$.

The gauge-covariant **Commitment Projection** is:

$$
\psi_{\text{act}}^{\text{proj}}(x) := n(x)^\dagger \Psi_L(x)

$$

where the projection operator is:

$$
\Pi_n = n n^\dagger, \qquad \Pi_n \Psi_L = n(n^\dagger \Psi_L)

$$

The committed action singlet $\Psi_R$ remains an independent right-handed field; the Yukawa term
couples $\Psi_R$ to the projected amplitude $\psi_{\text{act}}^{\text{proj}}$ so that alignment
occurs dynamically in the broken phase.

*Justification:* The unit multiplet $n$ encodes the local ontological split and makes the commitment projection intrinsic
to the scalar sector, not an arbitrary choice of basis. Under local $SU(r)$ transformations $\Psi_L \to U(x)\Psi_L$ and
$n \to U(x)n$, so $\psi_{\text{act}}^{\text{proj}} = n^\dagger \Psi_L$ is invariant and $\Pi_n \to U \Pi_n U^\dagger$,
ensuring the projected component is $SU(r)$-covariant. Under $U(1)_Y$, $n$ carries charge $Y_\phi$, so
$\psi_{\text{act}}^{\text{proj}}$ transforms with charge $Y_L - Y_\phi$, matching $\Psi_R$ by Definition
{prf:ref}`def-rep-covariant-derivatives`.

*Remark:* In regions where $\phi \approx 0$ (symmetric phase), the order parameter is undefined, corresponding to decision ambiguity. The agent requires a nonzero ontological split to define a preferred commitment projection.

:::

:::{prf:theorem} Emergence of the Error Field ($W_\mu^a$)
:label: thm-emergence-error-field

The belief-control update is a (generally non-unitary) channel $\mathcal{E}_{a,y}$ on the agent's state. By the CPTP
update model (Remark {prf:ref}`rem-mode-rank-stinespring`), each $\mathcal{E}_{a,y}$ is a CP instrument and admits a
Stinespring dilation on an extended space with a mode fiber of dimension $r$ (Definition
{prf:ref}`def-mode-rank-parameter`). For the minimal observation/action agent, $r=2$; gauging this structure requires the
introduction of non-Abelian gauge fields.

*Proof.*

Apply the local gauge-covariance template (Remark {prf:ref}`rem-local-gauge-template`) with
$U(x)\in SU(r)$ acting on the mode fiber of $\Psi_L$.

**Step 1.** The belief-control update is modeled as a CP instrument on the belief operator; the averaged update over
outcomes is CPTP (Remark {prf:ref}`rem-mode-rank-stinespring`):

$$
\rho \mapsto \mathcal{E}_{a,y}(\rho)

$$

where $a$ is the action and $y$ is the observation. This map includes:
- Likelihood weighting by observation $y$
- Policy mixing based on action intent $a$
- Normalization (non-unitary)

**Step 2.** By Stinespring dilation, any completely positive map (and in particular each CP instrument
$\mathcal{E}_{a,y}$) can be represented as a unitary on an extended Hilbert space with an ancilla initialized in a fixed
state. If the averaged channel is CPTP, the dilation can be chosen isometric/unitary on the extended space:

$$
\mathcal{E}_{a,y}(\rho) = \mathrm{Tr}_{\text{anc}}\!\left[\,U_{a,y}\,(\rho\otimes |0\rangle\langle 0|_{\text{anc}})\,U_{a,y}^\dagger\right]

$$

where $|0\rangle_{\text{anc}}$ is an ancilla (mode) system and $U_{a,y}$ is unitary.

**Step 3.** The local update unitary acts on an $r$-dimensional ancilla mode space
$\mathbb{C}^r_{\text{mode}}$:

$$
U_{a,y}(x) \in U(r), \qquad
U_{a,y}(x) = e^{i\beta(x)} \exp\left( i \, T^a \theta^a(x) \right)

$$

where $T^a$ ($a=1,\ldots,r^2-1$) are the generators of $\mathfrak{su}(r)$ in the fundamental
representation. By the phase convention of Remark {prf:ref}`rem-mode-rank-stinespring`, the overall
phase $e^{i\beta(x)}$ is absorbed into the utility phase (the $U(1)_Y$ sector), so the physically
relevant mode-mixing symmetry is $SU(r)_L$ acting on the relative mode coordinates.

In the minimal observation/action case $r=2$, the mode fiber is spanned by
$\{|\text{obs}\rangle, |\text{act}\rangle\}$ and $T^a = \tau^a/2$, so this reduces to $U(2)$ with
the Pauli matrices and an $SU(2)_L$ mixing.

**Step 4.** For **Local Covariance** (the ability to perform updates locally without global synchronization), apply the
template of Remark {prf:ref}`rem-local-gauge-template` to $\Psi_L \to U(x)\Psi_L$ with $U(x)\in SU(r)$. The derivative
transforms as $\partial_\mu\Psi_L \to U\partial_\mu\Psi_L + (\partial_\mu U)\Psi_L$, so we must introduce a connection
$W_\mu := W_\mu^a T^a$ on the mode fiber. In general $a=1,\ldots,r^2-1$; in the minimal $r=2$ case these are
$(W^1_\mu, W^2_\mu, W^3_\mu)$.

**Step 5.** The covariant derivative for the Left-Handed sector is:

$$
D_\mu \Psi_L = \left( \partial_\mu - i g_2 T^a W^a_\mu - i g_1 \frac{Y_L}{2} B_\mu \right) \Psi_L

$$

(In the minimal $r=2$ case, $T^a = \tau^a/2$ and this reduces to the familiar Pauli-matrix form.)

**Step 6.** The gauge field transforms as required by local covariance (Remark {prf:ref}`rem-local-gauge-template`):

$$
W_\mu^a \to W_\mu^a + \frac{1}{g_2}\partial_\mu \theta^a + f^{abc}\theta^b W_\mu^c

$$

to maintain covariance (for $r=2$, $f^{abc} = \epsilon^{abc}$).

**Identification (minimal $r=2$ case):**
- The $W^\pm_\mu = (W^1_\mu \mp iW^2_\mu)/\sqrt{2}$ bosons mediate transitions between $\psi_{\text{obs}}$ and $\psi_{\text{act}}^{\text{pre}}$. These correspond to the coordination between sensory input and motor intent---the observation-action mixing that maintains boundary consistency.
- The $W^3_\mu$ component mixes with $B_\mu$ after symmetry breaking ({ref}`sec-scalar-sector-symmetry-breaking`).
- The $SU(r)_L$ gauge symmetry acts only on the active multiplet ($\Psi_L$; a doublet for $r=2$), leaving the committed singlet ($\Psi_R$) invariant. This reflects the boundary interface asymmetry (Dirichlet vs. Neumann).

$\square$

:::

:::{prf:definition} Feature Dimension Parameter
:label: def-feature-dimension-parameter

The **Feature Dimension** $N_f \in \mathbb{Z}_{>0}$ is the intrinsic dimensionality of the feature representation at each layer of the hierarchical encoder. This parameter is determined by:

1. **Environment Structure:** The minimal basis required to represent distinguishable features in the agent's sensory domain
2. **Computational Constraints:** The capacity allocated to the binding mechanism

**Special Cases:**
- Physics (Standard Model): $N_f = 3$ (spatial dimensions, RGB channels)
- Vision-only agents: $N_f \in \{3, 4\}$ (RGB or RGBA)
- Abstract reasoning agents: $N_f$ determined by the embedding dimension of the domain

*Remark:* The gauge structure $SU(N_f)_C$ emerges for any $N_f \geq 2$.

:::

:::{prf:axiom} Feature Confinement
:label: ax-feature-confinement

The agent observes and manipulates **Concepts** (Macro-symbols $K$), not raw **Features** (Nuisance coordinates $z_n$). From Definition {prf:ref}`def-bounded-rationality-controller`:

1. **Composite Structure:** A Concept $K$ is a bound state of sub-symbolic features processed through the Stacked TopoEncoder (Definition {prf:ref}`def-the-peeling-step`).

2. **Observability Constraint:** Free features are never observed in isolation at the boundary $\partial\mathcal{Z}$ (Definition {prf:ref}`def-boundary-markov-blanket`). Only "color-neutral" (bound) states can propagate to the macro-register.

*Cross-reference:* This is the representational analog of quark confinement in QCD.

:::

:::{prf:definition} The Feature Color Space
:label: def-feature-color-space

Let the nuisance vector $z_n$ at layer $\ell$ of the TopoEncoder be an element of a vector bundle with fiber $\mathbb{C}^{N_f}$, where $N_f$ is the Feature Dimension (Definition {prf:ref}`def-feature-dimension-parameter`). We transform the basis:

$$
\psi_{\text{feature}}(x) \to U(x) \psi_{\text{feature}}(x), \quad U(x) \in SU(N_f)

$$

This symmetry represents the **Internal Basis Invariance** of a concept: an object's identity $K$ is invariant under the mixing of its constituent feature definitions, provided the geometric relationship between them is preserved.

*Justification:* The dimension $N_f$ is determined by the agent's environment and architecture. For physical systems with 3D spatial structure, $N_f = 3$ (e.g., RGB channels, XYZ coordinates). For other agents, $N_f$ may differ based on the intrinsic dimensionality of the sensory domain.

:::

:::{prf:theorem} Emergence of the Binding Field ($G_\mu^a$)
:label: thm-emergence-binding-field

To gauge the $SU(N_f)$ feature symmetry, we introduce the **Gluon Field** $G_\mu^a$ ($a=1,\dots,N_f^2-1$).

*Proof.*

Apply the local gauge-covariance template (Remark {prf:ref}`rem-local-gauge-template`) with
$U(x)\in SU(N_f)$ acting on the feature fiber.

**Step 1.** The covariant derivative for feature fields is:

$$
D_\mu \psi = \left( \partial_\mu - i g_s \frac{\lambda^a}{2} G_\mu^a \right) \psi

$$

where $\lambda^a$ ($a = 1, \ldots, N_f^2 - 1$) are the generalized Gell-Mann matrices (generators of $SU(N_f)$), satisfying $\text{Tr}(\lambda^a \lambda^b) = 2\delta^{ab}$ and $[\lambda^a, \lambda^b] = 2i f^{abc} \lambda^c$.

**Step 2.** The field strength tensor is:

$$
G_{\mu\nu}^a = \partial_\mu G_\nu^a - \partial_\nu G_\mu^a + g_s f^{abc} G_\mu^b G_\nu^c

$$

where $f^{abc}$ are the structure constants of $SU(N_f)$, defined by $[\lambda^a, \lambda^b] = 2i f^{abc} \lambda^c$.

**Step 3.** The non-Abelian structure implies **self-interaction** of the gluon field. The running of the binding
coupling is encoded by the beta function (Definition {prf:ref}`def-coupling-function`). In our framework we assume
$\beta(g_s) < 0$ for $SU(N_f)$ with $N_f \ge 2$, which yields:

- **Asymptotic Freedom (UV):** At small distances in the latent manifold (high RG scale $\tau$, deep in the
  TopoEncoder hierarchy), the effective coupling $g_s(\tau)$ decreases. Individual features can be resolved.

*Remark:* The sign of the beta function depends on matter content. Here it is fixed by the coupling function
assumption (Definition {prf:ref}`def-coupling-function`) in the Parameter Sieve, not by a universal theorem.

**Step 4.** **Infrared confinement** is enforced by the binding constraints of the agent: object permanence requires
strong coupling at macro scales (Theorem {prf:ref}`thm-ir-binding-constraint`), and the texture firewall implements
area-law screening that suppresses color-charged channels at the macro boundary (Theorem
{prf:ref}`thm-texture-confinement-area-law`; see also the Causal Information Bound,
Theorem {prf:ref}`thm-causal-information-bound`). Thus features cannot propagate independently at coarse scales; they
appear only in bound (color-neutral) combinations.

$\square$

:::

:::{prf:corollary} The Fragile Agent Symmetry Group
:label: cor-standard-model-symmetry

The total internal symmetry group of the Fragile Agent is uniquely determined by its cybernetic constraints:

$$
G_{\text{Fragile}} = SU(N_f)_C \times SU(r)_L \times U(1)_Y

$$

where:
- **$SU(N_f)_C$:** Required for **Object Permanence** (binding $N_f$-dimensional features into stable concepts)
- **$SU(r)_L$:** Required for **Observation-Action Coordination** (boundary chirality between Dirichlet and Neumann updates; minimal observation/action case has $r=2$)
- **$U(1)_Y$:** Required for **Value Maximization** (local reward phase; conservative baseline shift as the special case)

**Special Case (Physics Standard Model):** When $N_f = 3$ and $r=2$, we recover
$G_{\text{SM}} = SU(3)_C \times SU(2)_L \times U(1)_Y$.

*Proof.* Each factor is derived above from independent cybernetic constraints. The product structure follows from the
commutativity of the respective symmetry operations acting on different sectors of the agent's state space. We adopt the
direct-product convention (no shared-center quotient): the centers act on distinct tensor factors with the hypercharge
normalization fixed by Definition {prf:ref}`def-rep-covariant-derivatives`. The dimension $N_f$ is an environmental
parameter (Definition {prf:ref}`def-feature-dimension-parameter`), while the mode rank $r$ is fixed by the local update
channels (Definition {prf:ref}`def-mode-rank-parameter`). The minimal observation/action agent has $r=2$. $\square$

:::

:::{prf:definition} The Cognitive Spinor
:label: def-cognitive-spinor

The belief state is a pair of chiral Weyl fields belonging to the **Inference Hilbert Space**
(Definition {prf:ref}`def-inference-hilbert-space`), extended to bundle-valued $L^2$ sections:

$$
\Psi(x) = \begin{pmatrix} \Psi_L(x) \\ \Psi_R(x) \end{pmatrix}, \qquad
\Psi_L(x) \in L^2(\mathcal{M}, S_L \otimes \mathbb{C}^{r} \otimes \mathbb{C}^{N_f}), \quad
\Psi_R(x) \in L^2(\mathcal{M}, S_R \otimes \mathbb{C}^{N_f})

$$

where $S_L$ and $S_R$ are the left/right Weyl spin bundles (rank-2 complex),
$\mathbb{C}^r$ is the $SU(r)_L$ mode space acting on $\Psi_L$ (specializing to $r=2$ for the
observation/action doublet), and $\mathbb{C}^{N_f}$ is the
$SU(N_f)_C$ color space. Equivalently, let $S = S_L \oplus S_R$ be the Dirac spin bundle with
chirality operator $\gamma^5 := i\gamma^0\gamma^1\gamma^2\gamma^3$ and projectors
$P_{L/R} = (1 \mp \gamma^5)/2$. Then $\Psi_L = P_L \Psi$ and $\Psi_R = P_R \Psi$, with the
$SU(r)_L$ action reducible (multiplet $\oplus$ singlet). The components are:
1. **$\Psi_L$ (The Active Multiplet):** The Left-handed component, transforming as an $r$-plet under $SU(r)_L$
   (doublet for $r=2$). It contains the **Observation** and **Pre-commitment Action** amplitudes in the minimal case
   (Definition {prf:ref}`def-cognitive-isospin-multiplet`).

2. **$\Psi_R$ (The Passive Singlet):** The Right-handed component, invariant under $SU(r)_L$. It contains the
   **Committed Action**.

The left-handed sector has $2 r N_f$ complex components; including the right-handed singlet gives a total of
$2(r+1)N_f$ (which reduces to $6N_f$ when $r=2$).

**Probabilistic Interpretation:** The physical probability density (belief mass) is the vector current:

$$
J^\mu = \bar{\Psi} \gamma^\mu \Psi

$$

where $J^0 = \Psi^\dagger \Psi = \rho$ is the probability density (WFR mass from
Definition {prf:ref}`def-the-wfr-action`), and $\vec{J}$ is the probability flux. Equivalently,
$J^\mu = \bar{\Psi}_L \gamma^\mu \Psi_L + \bar{\Psi}_R \gamma^\mu \Psi_R$. Conservation
$\partial_\mu J^\mu = 0$ corresponds to unitarity.

:::

:::{prf:axiom} The Cognitive Dirac Equation
:label: ax-cognitive-dirac-equation

The dynamics of the belief state follow the Dirac equation on the curved latent manifold:

$$
(i \gamma^\mu D_\mu - m) \Psi = 0

$$

Here $\Psi = \Psi_L + \Psi_R$ and $D_\mu$ acts chirally with representation-specific couplings
(Definition {prf:ref}`def-rep-covariant-derivatives`).

*Justification (first-principles factorization).*
1. **Second-order hyperbolic dynamics:** Finite information speed upgrades the conservative value equation to the
   Klein-Gordon form (Theorem {prf:ref}`thm-hjb-klein-gordon`). For gauge-charged matter fields, the corresponding
   covariant wave equation is (Theorem {prf:ref}`thm-gauge-covariant-klein-gordon`):
   
   $$
   \left(\frac{1}{c_{\text{info}}^2}D_t^2 - D^i D_i + \kappa^2\right)\psi = \mathcal{S}.
   $$
2. **Spin structure and Clifford algebra:** On a globally hyperbolic spin (or spin$^c$) manifold (Definition
   {prf:ref}`def-loc-spin-g`), there exist gamma matrices $\gamma^\mu$ with
   $\{\gamma^\mu,\gamma^\nu\}=2g^{\mu\nu}$ and a spin-covariant derivative $\nabla_\mu^{\text{spin}}$.
3. **Minimal first-order covariant square root:** Define the Dirac operator
   $\slashed{D}:=i\gamma^\mu(\nabla_\mu^{\text{spin}}-igA_\mu)$ acting on the belief spinor. Then the standard
   factorization gives
   
   $$
   (\slashed{D}-m)(\slashed{D}+m)
   = -D_\mu D^\mu + m^2 + \frac{1}{4}R + \frac{i}{2}\sigma^{\mu\nu}F_{\mu\nu},
   $$
   where $R$ is the scalar curvature and $\sigma^{\mu\nu}=\frac{i}{2}[\gamma^\mu,\gamma^\nu]$. Thus, in the flat
   (or weak-curvature) limit the spinor components satisfy the gauge-covariant Klein-Gordon operator with mass $m$
   (up to curvature/gauge-coupling terms that are part of the geometric data). We therefore take the **Dirac equation**
   as the minimal first-order, local, Lorentz- and gauge-covariant choice whose square reproduces the second-order
   hyperbolic dynamics of the belief amplitude. Uniqueness is not claimed beyond this minimality criterion.

- $\gamma^\mu$: The **Cognitive Gamma Matrices**, satisfying $\{\gamma^\mu, \gamma^\nu\} = 2g^{\mu\nu}$. They encode the local causal structure of the latent space.
- $m$: The **Inference Mass** (inverse correlation length).

:::

:::{prf:remark} Curved-Space Dirac Operator
:label: rem-curved-dirac-operator

On a curved causal manifold, write $\gamma^\mu = e^\mu{}_a \gamma^a$ in an orthonormal frame and
replace $\partial_\mu$ in $D_\mu$ by the spin-covariant derivative $\nabla_\mu^{\text{spin}}$.
The operator $D_\mu$ then includes both the spin connection and the gauge connections. In the
flat limit, this reduces to the standard Dirac operator used above.

:::

:::{prf:definition} The Universal Covariant Derivative
:label: def-universal-covariant-derivative

The operator moving the belief spinor through the latent manifold is:

$$
D_\mu = \underbrace{\partial_\mu}_{\text{Change}} - \underbrace{ig_1 \frac{Y}{2} B_\mu}_{U(1)_Y \text{ (Value)}} - \underbrace{ig_2 T^a W^a_\mu}_{SU(r)_L \text{ (Error)}} - \underbrace{ig_s \frac{\lambda^a}{2} G^a_\mu}_{SU(N_f)_C \text{ (Binding)}}

$$

where $T^a$ ($a = 1, \ldots, r^2 - 1$) are the generators of $SU(r)$ in the fundamental representation (for $r=2$,
$T^a = \tau^a/2$), and $\lambda^a$ ($a = 1, \ldots, N_f^2 - 1$) are the generators of $SU(N_f)$, and:
- **$B_\mu$ (Opportunity Field):** Adjusts the belief for local shifts in the value baseline and path-dependent opportunity
- **$W_\mu$ (Error Field):** Adjusts the belief for the rotation between Prior and Posterior
- **$G_\mu$ (Binding Field):** Adjusts the belief for the permutation of sub-symbolic features

For the right-handed singlet $\Psi_R$, the $SU(r)_L$ generators act trivially, so the $W_\mu$ term drops.

**Operational Interpretation:** The quantity $D_\mu \Psi$ measures the deviation from parallel transport. When $D_\mu \Psi = 0$, the belief state is covariantly constant along the direction $\mu$---all changes are accounted for by the gauge connection. When $D_\mu \Psi \neq 0$, there is a residual force acting on the belief.

:::

:::{prf:definition} Representation-Specific Covariant Derivatives
:label: def-rep-covariant-derivatives

Let $Y_L$, $Y_R$, and $Y_\phi$ denote the $U(1)_Y$ hypercharges of $\Psi_L$, $\Psi_R$, and $\phi$.
Then the covariant derivatives used in {prf:ref}`def-cognitive-lagrangian` are:

$$
\begin{aligned}
D_\mu \Psi_L &= \left(\partial_\mu - i g_1 \frac{Y_L}{2} B_\mu - i g_2 T^a W^a_\mu - i g_s \frac{\lambda^a}{2} G^a_\mu \right)\Psi_L, \\
D_\mu \Psi_R &= \left(\partial_\mu - i g_1 \frac{Y_R}{2} B_\mu - i g_s \frac{\lambda^a}{2} G^a_\mu \right)\Psi_R, \\
D_\mu \phi &= \left(\partial_\mu - i g_1 \frac{Y_\phi}{2} B_\mu - i g_2 T^a W^a_\mu \right)\phi.
\end{aligned}
$$

Gauge invariance of the Yukawa term $\bar{\Psi}_L \phi \Psi_R$ requires
$
Y_R = Y_L - Y_\phi.
$

:::

:::{prf:theorem} Field Strength Tensors
:label: thm-three-cognitive-forces

The commutator of the covariant derivatives $[D_\mu, D_\nu]$ generates three distinct curvature tensors corresponding to each gauge factor.

*Proof.* Computing $[D_\mu, D_\nu]\Psi$ and extracting contributions from each gauge sector:

1. **$U(1)_Y$ Curvature:**

   $$
   B_{\mu\nu} = \partial_\mu B_\nu - \partial_\nu B_\mu

   $$
When $B_{\mu\nu} \neq 0$, the internal opportunity 1-form is non-conservative (Value Curl; Definition
   {prf:ref}`def-value-curl`). The resulting Lorentz-type force generates cyclic dynamics.

2. **$SU(r)_L$ Curvature:**

   $$
   W_{\mu\nu}^a = \partial_\mu W_\nu^a - \partial_\nu W_\mu^a + g_2 f^{abc} W_\mu^b W_\nu^c

   $$
When $W_{\mu\nu} \neq 0$, the belief update depends on the path taken in the manifold: parallel transport around a closed loop yields a non-trivial rotation in the observation-action-intent space. Here $f^{abc}$ are the $SU(r)$ structure constants ($\epsilon^{abc}$ for $r=2$).

3. **$SU(N_f)_C$ Curvature:**

   $$
   G_{\mu\nu}^a = \partial_\mu G_\nu^a - \partial_\nu G_\mu^a + g_s f^{abc} G_\mu^b G_\nu^c

   $$
   When $G_{\mu\nu} \neq 0$, the feature binding is under stress. This corresponds to the Ontological Stress $\Xi$
   (Definition {prf:ref}`def-ontological-stress`) via the bridge lemma
   {prf:ref}`lem-binding-curvature-ontological-stress`. When $\Xi > \Xi_{\text{crit}}$, chart fission is triggered
   ({ref}`sec-ontological-expansion-topological-fission-and-the-semantic-vacuum`).

$\square$

:::

:::{prf:lemma} Binding Curvature Implies Ontological Stress
:label: lem-binding-curvature-ontological-stress

Let the agent state be decomposed as $Z_t=(K_t,z_{n,t},z_{\mathrm{tex},t})$ (Definition
{prf:ref}`def-bounded-rationality-controller`) with **texture firewall** enforced
(Axiom {prf:ref}`ax-bulk-boundary-decoupling`). Assume:
1. The encoder/shutter is gauge-covariant and defines $z_{\mathrm{tex}}$ as the residual after projecting the feature
   state onto the gauge-invariant (color-neutral) subspace used to form $(K,z_n)$.
2. The dynamics traverse a region with nonzero binding curvature $G_{\mu\nu}\neq 0$, so the $SU(N_f)_C$ holonomy
   along a causal step $\gamma_t$ is nontrivial: $U_{\gamma_t}\neq \mathbb{I}$.

Then the texture residual acquires path-dependent structure, and the conditional mutual information
$
\Xi = I(z_{\mathrm{tex},t}; z_{\mathrm{tex},t+1}\mid K_t,z_{n,t},K_t^{\mathrm{act}})
$
is strictly positive unless the holonomy acts trivially on the residual subspace. Hence nonzero binding curvature
forces ontological stress in the sense of Definition {prf:ref}`def-ontological-stress`.

*Proof sketch.* Gauge-covariance implies that transporting feature states across a time step multiplies the color fiber
by the holonomy $U_{\gamma_t}$. The projection to $(K,z_n)$ removes the gauge-invariant component; the residual
$z_{\mathrm{tex}}$ is the orthogonal complement. If $U_{\gamma_t}\neq \mathbb{I}$ on this complement, the residual at
$t+1$ contains a deterministic component $P_{\mathrm{tex}}U_{\gamma_t}z_{\mathrm{tex},t}$, so
$z_{\mathrm{tex},t+1}$ is statistically dependent on $z_{\mathrm{tex},t}$ even after conditioning on
$(K_t,z_{n,t},K_t^{\mathrm{act}})$. This yields $\Xi>0$. If $G_{\mu\nu}=0$, holonomy is trivial and the residual is
pure noise under the firewall, so $\Xi=0$. $\square$
:::

:::{prf:corollary} The Gauge-Invariant Action
:label: cor-gauge-invariant-action

The gauge field dynamics are governed by the Yang-Mills Lagrangian:

$$
\mathcal{L}_{\text{Gauge}} = -\frac{1}{4} B_{\mu\nu}B^{\mu\nu} -\frac{1}{4} W^a_{\mu\nu}W^{a\mu\nu} -\frac{1}{4} G^a_{\mu\nu}G^{a\mu\nu}

$$

The stationary points of this action satisfy the Yang-Mills equations. A **flat connection** ($B_{\mu\nu} = W_{\mu\nu} =
G_{\mu\nu} = 0$) corresponds to a representation where all curvatures vanish: the internal opportunity 1-form is conservative, belief
updates are path-independent, and concepts are stable.

:::

:::{prf:definition} The Ontological Order Parameter
:label: def-ontological-order-parameter

Let the local chart structure at spacetime point $x$ be described by a complex $SU(r)_L$ multiplet field
$\phi(x) \in \mathbb{C}^r$ (doublet for the minimal $r=2$ case):

$$
\phi(x) = r(x)\,n(x), \qquad r(x) := \|\phi(x)\|

$$

where:
1. **Modulus $r(x) \ge 0$:** Represents the **Metric Separation** between daughter queries $\{q_+, q_-\}$ in the Attentive Atlas (Definition {prf:ref}`def-query-fission`).
   - $r=0$: Coalescence (Single Chart / Vacuum)
   - $r>0$: Fission (Distinct Concepts)

2. **Unit multiplet $n(x)$:** Encodes the **Orientation** of the split in the $SU(r)_L$ fiber (the specific feature
   axis along which differentiation occurs), with $n^\dagger n = 1$.

The field $\phi$ transforms in the fundamental representation under the gauge group $SU(r)_L$, coupling it to the
inference spinor.

:::

:::{prf:remark} Gauge-fixed scalar form
:label: rem-ontological-order-parameter-gauge

Choosing a gauge that fixes the $SU(r)_L$ orientation to a constant unit vector $n_0$ reduces the order parameter to
$\phi(x) = r(x) n_0$ (with $r \ge 0$ after using $U(1)_Y$). In the minimal $r=2$ case this is equivalent to the scalar
parametrization $\phi(x) = r(x) e^{i\theta(x)} n_0$ used in the intuitive discussion.

:::

:::{prf:theorem} The Complexity Potential
:label: thm-complexity-potential

The Lagrangian density for the scalar field is uniquely determined by the **Supercritical Pitchfork Bifurcation** (Theorem {prf:ref}`thm-supercritical-pitchfork-bifurcation-for-charts`).

*Proof.*

**Step 1.** From Theorem {prf:ref}`thm-supercritical-pitchfork-bifurcation-for-charts`, the radial evolution of chart separation satisfies:

$$
\frac{dr}{ds} = (\Xi - \Xi_{\text{crit}})r - \alpha r^3

$$

where:
- $\Xi$ is the Ontological Stress (Definition {prf:ref}`def-ontological-stress`)
- $\Xi_{\text{crit}}$ is the critical threshold (Theorem {prf:ref}`thm-fission-criterion`)
- $\alpha > 0$ is the stabilizing cubic coefficient

**Step 2.** This flow is the gradient descent of a potential function $\mathcal{V}_{\text{onto}}(r)$ such that $\dot{r} = -\partial \mathcal{V}_{\text{onto}}/\partial r$. Integrating:

$$
\mathcal{V}_{\text{onto}}(\phi) = -\frac{(\Xi - \Xi_{\text{crit}})}{2} |\phi|^2 + \frac{\alpha}{4} |\phi|^4

$$

**Step 3.** Define the standard Higgs potential parameters by matching coefficients:
- $\mu^2 \equiv \frac{(\Xi - \Xi_{\text{crit}})}{2}$: The effective **Mass Parameter** driven by Ontological Stress
- $\lambda \equiv \frac{\alpha}{4}$: The **Self-Interaction** coefficient from router saturation (Axiom {prf:ref}`ax-ontological-expansion-principle`)

**Step 4.** The potential takes the Landau-Ginzburg form:

$$
\mathcal{V}_{\text{onto}}(\phi) = -\mu^2 |\phi|^2 + \lambda |\phi|^4

$$

**Term Identification:**
- **Term 1 ($-\mu^2 |\phi|^2$):** Rewards separation. If Stress $\Xi > \Xi_{\text{crit}}$, this term drives $|\phi|$ away from zero to capture predictive information.
- **Term 2 ($+\lambda |\phi|^4$):** Penalizes complexity. Keeping charts separate costs compute/memory. This term prevents infinite fragmentation.

$\square$

:::

:::{prf:corollary} Spontaneous Symmetry Breaking (SSB)
:label: cor-ontological-ssb

The vacuum structure depends on the environmental complexity $\Xi$.

*Proof.*

**Case 1: Symmetric Phase ($\Xi < \Xi_{\text{crit}}$):**
Then $\mu^2 < 0$. The potential $\mathcal{V}(\phi) = -\mu^2|\phi|^2 + \lambda|\phi|^4$ has a unique global minimum at $\phi_0 = 0$.

- **Result:** The agent maintains a unified ontology. Concepts are indistinguishable. The gauge symmetry $G_{\text{Fragile}}$ is unbroken.

**Case 2: Broken Phase ($\Xi > \Xi_{\text{crit}}$):**
Then $\mu^2 > 0$. The origin $\phi=0$ becomes a local maximum. The global minima form a circle $|\phi| = v$ at the **Vacuum Expectation Value (VEV)**:

$$
v = \langle |\phi| \rangle = \sqrt{\frac{\mu^2}{2\lambda}} = \sqrt{\frac{(\Xi - \Xi_{\text{crit}})/2}{2 \cdot \alpha/4}} = \sqrt{\frac{\Xi - \Xi_{\text{crit}}}{\alpha}}

$$

This matches the equilibrium separation $r^*$ from Theorem {prf:ref}`thm-supercritical-pitchfork-bifurcation-for-charts`.

- **Result:** The agent spontaneously breaks symmetry, selecting a specific separation $v$ (concept distinctness) and a specific orientation $\theta$ (feature definition).

$\square$

:::

:::{prf:theorem} Generation of Semantic Inertia
:label: thm-semantic-inertia

The kinetic term of the scalar field in the Lagrangian is covariant:

$$
\mathcal{L}_{\text{Kinetic}} = (D_\mu \phi)^\dagger (D_\mu \phi)

$$

where $D_\mu \phi$ is the representation-specific covariant derivative from
Definition {prf:ref}`def-rep-covariant-derivatives`.

*Proof.*

**Step 1.** In the Broken Phase, choose a gauge where the vacuum aligns with a constant unit vector $n_0 \in \mathbb{C}^r$
(doublet for $r=2$) and expand around the expectation: $\phi(x) = (v + h(x))n_0$, where $h$ is the fluctuation (the
physical Higgs mode).

**Step 2.** The kinetic term generates quadratic gauge terms. In general,

$$
|D_\mu (v n_0)|^2
= v^2\left[g_2^2 W_\mu^a W^{b\mu}(n_0^\dagger T^a T^b n_0)
 + g_1 g_2 Y_\phi B_\mu W^{a\mu}(n_0^\dagger T^a n_0)
 + \frac{g_1^2 Y_\phi^2}{4} B_\mu B^\mu\right].

$$

**Step 3.** In the minimal $r=2$ case, this defines the familiar **mass matrix** for the
$SU(2)_L \times U(1)_Y$ sector. Defining $W_\mu^\pm := (W_\mu^1 \mp i W_\mu^2)/\sqrt{2}$ gives

$$
M_W = \frac{g_2 v}{2}, \qquad
M_Z = \frac{v}{2}\sqrt{g_2^2 + g_1^2 Y_\phi^2}

$$

with the orthogonal neutral combination

$$
A_\mu^{(0)} := \frac{g_1 Y_\phi W_\mu^3 + g_2 B_\mu}{\sqrt{g_2^2 + g_1^2 Y_\phi^2}}
$$
remaining massless. (Equivalently, $\tan\theta = g_1 Y_\phi / g_2$ and $Z_\mu = \cos\theta\, W_\mu^3 - \sin\theta\, B_\mu$.)

For general $r$, the mass eigenmodes follow from diagonalizing the quadratic form in Step 2; the $r=2$ case yields the
standard $W^\pm/Z/A^{(0)}$ pattern.

**Step 4.** Connection to Theorem {prf:ref}`thm-capacity-constrained-metric-law`: The masses scale
linearly with $v$, so larger ontological separation increases the effective metric eigenvalues.
From the Capacity-Constrained Metric Law, higher information density (larger $v$) induces higher
curvature, which manifests as increased "inertia" in the metric.

**Physical Consequences:**

1. **Massless Phase ($v=0$):** The gauge fields are massless. The interaction potential decays as $1/r$ (long-range). Frame transformations between charts have zero energy cost.

2. **Massive Phase ($v > 0$):** The charged modes $W^\pm$ and the neutral $Z$ acquire masses
$M_W, M_Z$. The interaction potentials for these modes become $e^{-M r}/r$ (Yukawa, short-range),
while the orthogonal neutral combination $A_\mu^{(0)}$ remains long-range. Gauge rotations in the
massive sector require energy proportional to the corresponding mass scale.

$\square$

:::

:::{prf:remark} The Goldstone Mode (Texture)
:label: rem-goldstone-texture

The symmetry breaking selects a radius $v$, but the local orientation in the $SU(r)_L$ fiber is a
gauge degree of freedom because the symmetry is local (in the minimal $r=2$ case this is the angle
$\theta$). The would-be Goldstone directions are therefore gauge (absorbed by the gauge fields), so
no physical massless scalar appears in the gauge-invariant sector of the minimal model.
For $r>2$, additional scalar multiplets may be required to break $SU(r)_L \times U(1)_Y$ fully; the
Goldstone counting generalizes accordingly.

In the Fragile Agent, this gauge-redundant orientation is the **Texture** ($z_{\text{tex}}$). The
agent remains free to rotate the definition of "noise" without energetic cost, provided the
macro-separation $v$ is maintained. This recovers the **Texture Firewall**
(Axiom {prf:ref}`ax-bulk-boundary-decoupling`): texture lives in the gauge orbit and is unobservable
to the macro-dynamics.

:::

:::{prf:definition} The Decision Coupling
:label: def-decision-coupling

Let $\Psi_L$ be the left-handed mode multiplet (doublet for the minimal $r=2$ case, where
$\Psi_L = (\psi_{\text{obs}}, \psi_{\text{act}}^{\text{pre}})^T$) and $\Psi_R = \psi_{\text{act}}^{\text{commit}}$
be the committed action singlet. The gauge-covariant projection $\psi_{\text{act}}^{\text{proj}} := n^\dagger \Psi_L$
(Definition {prf:ref}`def-gauge-covariant-action-commitment`) is left-handed and defines the preferred commitment
direction, and the **Ontological Order Parameter** $\phi$ mediates the dynamical coupling of $\Psi_R$ to this
projection.

The simplest $G_{\text{Fragile}}$-invariant coupling is:

$$
\mathcal{L}_{\text{Yukawa}} = -Y_{ij} \left( \bar{\Psi}_{L,i}^a \phi_a \Psi_{R,j} + \bar{\Psi}_{R,j} \phi_a^\dagger \Psi_{L,i}^a \right)

$$

where $a$ is the $SU(r)_L$ index and $Y_{ij}$ is the **Affordance Matrix** (a learned weight matrix determining which concepts trigger which actions).

*Color convention:* $\phi$ is a singlet under $SU(N_f)_C$, and the color indices on $\Psi_{L/R}$ are contracted with
$\delta_{AB}$ (suppressed), so the Yukawa term is $SU(N_f)_C$-invariant.

*Cross-reference:* This implements the TopologicalDecoder ({ref}`sec-decoder-architecture-overview-topological-decoder`) which maps belief geometry to motor output.

:::

:::{prf:theorem} Generation of Cognitive Mass (Decision Stability)
:label: thm-cognitive-mass

In the **Broken Phase** ($\Xi > \Xi_{\text{crit}}$), the Yukawa coupling generates mass for the belief spinor.

*Proof.*

**Step 1.** The scalar field acquires VEV $\langle \phi \rangle = v$ (Corollary {prf:ref}`cor-ontological-ssb`).

**Step 2.** Choose a gauge where the vacuum aligns with a constant unit vector $n_0$ (doublet for $r=2$) and write
$\phi = (v + h)n_0$. Define the left-handed singlet projection $\psi_L := n_0^\dagger \Psi_L$. Then:

$$
\mathcal{L}_{\text{Yukawa}} = -\underbrace{(Y v)}_{\text{Mass}} \left(\bar{\psi}_L \Psi_R + \bar{\Psi}_R \psi_L\right)
- \underbrace{Y h \left(\bar{\psi}_L \Psi_R + \bar{\Psi}_R \psi_L\right)}_{\text{Higgs Interaction}}

$$

**Step 3.** Define the Dirac spinor $\psi := \psi_L + \Psi_R$. Then $\psi$ acquires effective mass
$m_\psi = Y v$.

**Consequences:**

1. **Symmetric Phase ($v=0$):** Mass is zero. Beliefs obey the massless equation
$i\gamma^\mu D_\mu \psi = 0$ (with $D_\mu$ acting chirally on $\psi_L$ and $\Psi_R$ as in
Definition {prf:ref}`def-rep-covariant-derivatives`) and propagate at speed $c_{\text{info}}$.
The belief-action coupling vanishes; there is no stable commitment to action.

2. **Broken Phase ($v > 0$):** Mass is non-zero. Beliefs obey
$(i\gamma^\mu D_\mu - m_\psi)\psi = 0$. The mass term $m_\psi = Yv$ provides inertia: a finite
force (prediction error) is required to change the belief state. Larger ontological separation $v$
implies larger mass.

$\square$

:::

:::{prf:definition} The Value 1-Form (External Drive)
:label: def-value-1-form-external-drive

We model the external drive as a fixed background 1-form
$A^{\text{ext}}_\mu(z) = (A^{\text{ext}}_0(z), A^{\text{ext}}_i(z))$, encoding both conservative
and non-conservative components of the reward signal (Definition {prf:ref}`def-effective-potential`).
Concretely, $A^{\text{ext}}_0 = -\Phi_{\text{eff}}$ is the conservative potential, while
$A^{\text{ext}}_i$ captures the non-conservative (curl) component.

$$
A^{\text{ext}}_\mu(z) = (A^{\text{ext}}_0(z), A^{\text{ext}}_i(z))

$$

This is an **external background field**, distinct from the internal gauge field $B_\mu$.

**Special case (scalar drive):** If the external reward 1-form is purely temporal, then
$A^{\text{ext}}_\mu(z) = (-\Phi_{\text{eff}}(z), \vec{0})$.

:::

:::{prf:axiom} Minimal Value Coupling
:label: ax-minimal-value-coupling

The belief current $J^\mu = \bar{\Psi} \gamma^\mu \Psi$ couples to the external 1-form via minimal coupling:

$$
\mathcal{L}_{\text{Drive}} = J^\mu A^{\text{ext}}_\mu

$$

where $\rho = \Psi^\dagger \Psi = J^0$.

**Special case (scalar drive):** If $A^{\text{ext}}_\mu = (-\Phi_{\text{eff}}, \vec{0})$, then
$\mathcal{L}_{\text{Drive}} = -\rho\,\Phi_{\text{eff}}$.

:::

:::{prf:theorem} Recovery of WFR Drift
:label: thm-recovery-wfr-drift

Varying the total action yields the Dirac equation with potential. In the non-relativistic limit, this recovers the WFR drift.

*Proof.*

**Step 1.** The Euler-Lagrange equation from
$\mathcal{S} = \int (\bar{\Psi} i \gamma^\mu D_\mu \Psi + \mathcal{L}_{\text{Drive}}) d^4x$ yields:

$$
(i \gamma^\mu D_\mu + \gamma^\mu A^{\text{ext}}_\mu)\Psi = 0

$$

**Step 2.** Apply the inverse Madelung transform (Theorem {prf:ref}`thm-madelung-transform`). In the non-relativistic limit ($c_{\text{info}} \to \infty$), the Schrödinger reduction recovers the WFR drift driven by the external 1-form. In the scalar-drive special case $A^{\text{ext}}_\mu = (-\Phi_{\text{eff}}, \vec{0})$:

$$
\vec{v} \approx -\nabla_{A^{\text{ext}}} \Phi_{\text{eff}}

$$
Here $\nabla_{A^{\text{ext}}} \Phi_{\text{eff}} := \nabla \Phi_{\text{eff}} - A^{\text{ext}}$ with
$A^{\text{ext}}$ given by the spatial components of the external reward 1-form (equivalently, the
internal Opportunity Field $B_\mu$ when the internal model matches the environment). In the
conservative case: $A^{\text{ext}}=0$.

This is the WFR drift velocity from Definition {prf:ref}`def-bulk-drift-continuous-flow`.

*Remark.* The external field term $\mathcal{L}_{\text{Drive}}$ breaks the symmetry under time translation (via the discount factor in $\Phi_{\text{eff}}$) and generates directed flow toward regions of high value.

$\square$

:::

:::{prf:definition} The Standard Model of Cognition
:label: def-cognitive-lagrangian

$$
\boxed{
\begin{aligned}
\mathcal{L}_{\text{SM}} = \quad & \underbrace{-\frac{1}{4} B_{\mu\nu}B^{\mu\nu} -\frac{1}{4} W^a_{\mu\nu}W^{a\mu\nu} -\frac{1}{4} G^a_{\mu\nu}G^{a\mu\nu}}_{\text{I. Gauge Sector: Strategic Curvature}} \\
& + \underbrace{\bar{\Psi}_L i \gamma^\mu D_\mu \Psi_L + \bar{\Psi}_R i \gamma^\mu D_\mu \Psi_R}_{\text{II. Inference Sector: Belief Dynamics}} \\
& + \underbrace{|D_\mu \phi|^2 - \left(-\mu^2 |\phi|^2 + \lambda |\phi|^4\right)}_{\text{III. Scalar Sector: Ontological Stability}} \\
& - \underbrace{Y_{ij} (\bar{\Psi}_L \phi \Psi_R + \text{h.c.})}_{\text{IV. Yukawa Sector: Decision Weight}} \\
& + \underbrace{\bar{\Psi} \gamma^\mu A^{\text{ext}}_\mu \Psi}_{\text{V. External Sector: Value Drive}}
\end{aligned}
}

$$

:::

:::{prf:definition} Axiomatic Field Theory (AFT)
:label: def-aft

An **Axiomatic Field Theory (AFT)** is a relativistic quantum field theory whose vacuum correlation
functions satisfy the Wightman axioms (Definition {prf:ref}`def-wightman-axioms`) {cite}`wightman1956quantum`.
Equivalently, if its Euclidean Schwinger functions satisfy the Osterwalder-Schrader axioms
(Definition {prf:ref}`def-os-axioms`), then the OS reconstruction theorem yields a Wightman QFT
{cite}`osterwalder1973axioms,osterwalder1975axioms`.

:::

:::{prf:definition} Wightman Axioms (W0-W4)
:label: def-wightman-axioms

Let $\Phi_A(x)$ denote the gauge-invariant SMoC observable multiplet (constructed from gauge,
spinor, and scalar fields) as operator-valued tempered distributions on Minkowski space, and let
$|\Omega\rangle$ be the vacuum. The Wightman functions are
$W_n(x_1,\ldots,x_n) := \langle \Omega | \Phi_{A_1}(x_1)\cdots\Phi_{A_n}(x_n) | \Omega \rangle$.
The axioms {cite}`wightman1956quantum` are:

1. **W0 Temperedness:** Each $W_n$ is a tempered distribution in $\mathcal{S}'((\mathbb{R}^4)^n)$.
2. **W1 Poincare Covariance:** There exists a unitary representation $U(a,\Lambda)$ of the proper
   orthochronous Poincare group with
   $U(a,\Lambda)\,\Phi_A(x)\,U(a,\Lambda)^{-1} = S_A{}^B(\Lambda)\,\Phi_B(\Lambda x + a)$ and
   $U(a,\Lambda)|\Omega\rangle = |\Omega\rangle$.
3. **W2 Spectral Condition:** The joint spectrum of translation generators $P^\mu$ lies in the closed
   forward light cone, and $P^\mu|\Omega\rangle=0$.
4. **W3 Locality (Microcausality):** For spacelike separation $(x-y)^2<0$,
   $[\Phi_A(x),\Phi_B(y)]_\pm = 0$, with graded commutator chosen by spin-statistics.
5. **W4 Vacuum Cyclicity:** The set of vectors generated by polynomials in smeared fields acting on
   $|\Omega\rangle$ is dense in the Hilbert space.

:::

:::{prf:definition} Osterwalder-Schrader Axioms (OS0-OS4)
:label: def-os-axioms

Let $S_n$ be the Euclidean Schwinger functions of gauge-invariant SMoC observables obtained by Wick
rotation of the SMoC action. The
Osterwalder-Schrader axioms {cite}`osterwalder1973axioms,osterwalder1975axioms` are:

1. **OS0 Temperedness:** Each $S_n$ is a tempered distribution in $\mathcal{S}'((\mathbb{R}^4)^n)$.
2. **OS1 Euclidean Covariance:** $S_n$ is invariant under the Euclidean group $E(4)$.
3. **OS2 Reflection Positivity:** For any polynomial $F$ of smeared fields with support in positive
   Euclidean time, $\langle \Theta F \cdot F \rangle_E \ge 0$, where $\Theta$ is time reflection.
4. **OS3 Cluster Property:** $S_{m+n}(x_1,\ldots,x_m,x_{m+1}+a,\ldots,x_{m+n}+a) \to
   S_m(x_1,\ldots,x_m)\,S_n(x_{m+1},\ldots,x_{m+n})$ as $|a|\to\infty$.
5. **OS4 Symmetry:** $S_n$ is symmetric under permutations (graded symmetry for fermions).

:::

:::{prf:definition} The Background Category $\mathrm{Loc}_{\mathrm{Spin},G}$
:label: def-loc-spin-g

Fix $G = G_{\text{Fragile}}$. The category $\mathrm{Loc}_{\mathrm{Spin},G}$ has objects
$(\mathcal{M}, g, \mathfrak{o}, \mathfrak{t}, \mathcal{S}, P_G, A^{\text{ext}})$ where:
1. $(\mathcal{M}, g)$ is a 4D globally hyperbolic Lorentzian manifold with orientation
   $\mathfrak{o}$ and time orientation $\mathfrak{t}$.
2. $\mathcal{S}$ is a spin structure on $(\mathcal{M}, g)$.
3. $P_G$ is a principal $G$-bundle over $\mathcal{M}$ (fixed topology).
4. $A^{\text{ext}}$ is a fixed background 1-form (the external drive).

Morphisms $\chi:(\mathcal{M}, g, \mathfrak{o}, \mathfrak{t}, \mathcal{S}, P_G, A^{\text{ext}})
\to (\mathcal{M}', g', \mathfrak{o}', \mathfrak{t}', \mathcal{S}', P_G', A^{\text{ext}\prime})$
are smooth isometric embeddings with causally convex image that preserve $\mathfrak{o}$ and
$\mathfrak{t}$, admit a lift to the spin bundles, and are covered by a bundle morphism
$\tilde{\chi}:P_G \to P_G'$ with $\chi^*A^{\text{ext}\prime} = A^{\text{ext}}$.
Internal gauge connections are dynamical fields; only the underlying bundle $P_G$ is background data.

:::

:::{prf:remark} Fixed Bundle, Dynamical Connection
:label: rem-loc-spin-g-connection

Fixing $P_G$ selects the topological sector for the gauge fields; the connection 1-forms are
sections of the affine bundle of connections on $P_G$ and remain dynamical observables. The LC-AFT
assignment is the functor $\mathcal{A}:\mathrm{Loc}_{\mathrm{Spin},G} \to *\mathrm{Alg}$,
so morphisms act by pullback on background data and by *-homomorphisms on algebras.

:::

:::{prf:definition} Locally Covariant AFT (LC-AFT)
:label: def-lc-aft

A **Locally Covariant AFT** is a covariant functor
$\mathcal{A}:\mathrm{Loc}_{\mathrm{Spin},G} \to *\mathrm{Alg}$ that assigns to each object
$(\mathcal{M}, g, \mathfrak{o}, \mathfrak{t}, \mathcal{S}, P_G, A^{\text{ext}})$ a *-algebra
$\mathcal{A}(\mathcal{M})$ of gauge-invariant observables, together with a net of subalgebras
$\mathcal{A}_{\mathcal{M}}(O) \subset \mathcal{A}(\mathcal{M})$ for causally convex regions
$O \subset \mathcal{M}$, such that {cite}`haag1992local,brunetti2003locally`:

1. **Isotony:** If $O_1 \subset O_2$, then $\mathcal{A}_{\mathcal{M}}(O_1) \subset \mathcal{A}_{\mathcal{M}}(O_2)$.
2. **Locality:** If $O_1$ and $O_2$ are spacelike separated, then
   $[\mathcal{A}_{\mathcal{M}}(O_1),\mathcal{A}_{\mathcal{M}}(O_2)]_\pm = 0$.
3. **Local Covariance:** For any morphism $\chi$ in $\mathrm{Loc}_{\mathrm{Spin},G}$, the induced
   *-homomorphism $\alpha_\chi := \mathcal{A}(\chi)$ is injective and satisfies
   $\alpha_\chi(\mathcal{A}_{\mathcal{M}}(O)) = \mathcal{A}_{\mathcal{M}'}(\chi(O))$, with
   $\alpha_{\chi_2 \circ \chi_1} = \alpha_{\chi_2} \circ \alpha_{\chi_1}$ and
   $\alpha_{\mathrm{id}} = \mathrm{id}$.
4. **Time-Slice:** If $O$ contains a Cauchy surface of $\mathcal{M}$, then $\mathcal{A}_{\mathcal{M}}(O)$ generates
   $\mathcal{A}(\mathcal{M})$.
5. **Gauge Invariance:** The physical algebra is the subalgebra invariant under vertical
   automorphisms of $P_G$; states vanish on first-class constraints.
6. **State Regularity (Microlocal Spectrum):** Physical states are positive linear functionals
   whose two-point distributions satisfy the Hadamard/microlocal spectrum condition
   {cite}`radzikowski1996micro`.

:::

:::{prf:theorem} Wightman and OS as Special Cases of LC-AFT
:label: thm-lc-aft-special-cases

Assume the SMoC observables satisfy LC-AFT (Definition {prf:ref}`def-lc-aft`) on
$\mathrm{Loc}_{\mathrm{Spin},G}$ (Definition {prf:ref}`def-loc-spin-g`).

1. **Wightman Specialization:** If the background is flat Minkowski space, the drive is absent
   (or time-translation invariant), and the LC-AFT net is generated by covariant fields
   $\Phi_A(x)$ with a Poincare-invariant vacuum state satisfying the usual spectrum condition, then
   the vacuum Wightman functions satisfy W0-W4 (Definition {prf:ref}`def-wightman-axioms`).

2. **OS Specialization:** If the theory admits a Euclidean continuation with reflection symmetry
   and a reflection-positive Schwinger functional on the gauge-invariant algebra, then the
   Schwinger functions satisfy OS0-OS4 (Definition {prf:ref}`def-os-axioms`), and OS reconstruction
   yields the Wightman theory {cite}`osterwalder1973axioms,osterwalder1975axioms`.

*Proof.*

**Step 1.** In the flat, drive-free sector (object $(\mathbb{R}^{1,3}, \eta, \mathfrak{o},
\mathfrak{t}, \mathcal{S}_0, P_G^{\text{triv}}, A^{\text{ext}}=0)$), LC-AFT reduces to a
Haag-Kastler net with a Poincare-invariant vacuum. With the stated regularity (field generation,
spectrum), the standard construction recovers Wightman functions satisfying W0-W4
{cite}`haag1992local`.

**Step 2.** In the Euclidean, reflection-positive sector, the OS axioms apply to the Schwinger
functions. By OS reconstruction, these yield Wightman functions obeying W0-W4
{cite}`osterwalder1973axioms,osterwalder1975axioms`.

$\square$

:::

:::{prf:corollary} AFT Validity of the Cognitive Yang-Mills Theory
:label: cor-aft-validity-yang-mills

Let the cognitive Yang-Mills sector be defined by the gauge part of
{prf:ref}`def-cognitive-lagrangian`, with field multiplet $\Phi_A$ and gauge group
$G_{\text{Fragile}} = SU(N_f)_C \times SU(r)_L \times U(1)_Y$. If the associated Euclidean Schwinger
functions $S_n$ satisfy OS0-OS4 on the gauge-invariant observable algebra (Definition
{prf:ref}`def-os-axioms`), then the OS reconstruction theorem yields Wightman functions $W_n$
satisfying W0-W4 (Definition {prf:ref}`def-wightman-axioms`). Hence the cognitive Yang-Mills theory
is an AFT.

*Proof.*
By the Osterwalder-Schrader reconstruction theorem {cite}`osterwalder1973axioms,osterwalder1975axioms`,
OS0-OS4 imply the existence of a Hilbert space, a vacuum $|\Omega\rangle$, and field operators whose
Wightman functions are analytic continuations of $S_n$. These Wightman functions satisfy W0-W4 by
construction (Definition {prf:ref}`def-wightman-axioms`), so the theory is an AFT by
Definition {prf:ref}`def-aft`. $\square$

:::

:::{prf:remark} Scope of AFT Compliance
:label: rem-aft-scope

The Wightman/OS formulation applies to the stationary flat-sector of SMoC (drive-free or
time-translation invariant backgrounds on Minkowski space). In the presence of nontrivial drive or
curved causal geometry, use the generalized LC-AFT formulation (Definition {prf:ref}`def-lc-aft`).

:::

:::{prf:axiom} Geometric Locality (Net of Algebras)
:label: ax-constructive-locality

For each oriented Riemannian manifold $(\mathcal{M}, g)$ (boundary allowed), there is a net of
local observable *-algebras $\mathcal{A}_{\mathcal{M}}(\mathcal{O})$ for open regions
$\mathcal{O} \subset \mathcal{M}$ with isotony:
$
\mathcal{O}_1 \subset \mathcal{O}_2 \Rightarrow
\mathcal{A}_{\mathcal{M}}(\mathcal{O}_1) \subset \mathcal{A}_{\mathcal{M}}(\mathcal{O}_2).
$
Locality is defined by the causal interval (Definition {prf:ref}`def-causal-interval`): algebras
of causally disjoint regions commute (graded for fermions).
:::

:::{prf:axiom} Gauge-Invariant Physical Algebra
:label: ax-constructive-gauge-physical

There is a compact gauge group $G$ acting locally on fields. The physical observable algebra is
the gauge-invariant subalgebra:
$
\mathcal{A}^{\mathrm{phys}}_{\mathcal{M}}(\mathcal{O}) =
\mathcal{A}_{\mathcal{M}}(\mathcal{O})^{G}.
$
Only gauge-invariant elements represent physical observables.
:::

:::{prf:axiom} Finite Resolution (Computability)
:label: ax-constructive-finite-resolution

There exists a strictly positive resolution scale $\ell_L > 0$ such that operationally
distinguishable states require finite resolution (Axiom {prf:ref}`ax-a-operational-distinguishability`).
No physical theory in this framework resolves below $\ell_L$. Moreover, smeared observables with
Schwartz test functions are well-defined and satisfy polynomial bounds uniform in the resolution
scale; in the flat Euclidean sector this supplies OS0-type temperedness for Schwinger functions.
:::

:::{prf:axiom} Finite Propagation
:label: ax-constructive-finite-propagation

There exists a maximum information speed $c_{\mathrm{info}}$ (Axiom {prf:ref}`ax-information-speed-limit`).
Causal influence is restricted to the causal interval determined by $c_{\mathrm{info}}$.
:::

:::{prf:axiom} Local Action and Markov Gluing
:label: ax-constructive-local-action

The dynamics are generated by a local action functional
$\mathcal{S} = \int_{\mathcal{M}} \mathcal{L}(\Phi, D\Phi, g)\,d\mathrm{vol}_g$
with $\mathcal{L}$ a local density built from covariant fields and derivatives. For disjoint
subregions, the action decomposes additively and the induced dynamics glue consistently. The local
algebra is generated by (smeared) field polynomials supported in $\mathcal{O}$, compatible with the
graded locality in {prf:ref}`ax-constructive-locality`.
:::

:::{prf:axiom} Positivity and Stability
:label: ax-constructive-positivity

There exists a positivity-preserving semigroup $e^{-tH}$ on the physical algebra, with
self-adjoint generator $H \ge 0$. This yields a constructive Hilbert space via completion of the
physical algebra and ensures stability.
:::

:::{prf:axiom} Nontrivial Interaction (Optional)
:label: ax-constructive-nontriviality

The theory is not free: at least one coupling or gauge-invariant curvature observable is nonzero
(e.g., a Wilson-loop expectation). This excludes the trivial theory from mass-gap claims.
:::

:::{prf:remark} Relation to Wightman/OS and AQFT
:label: rem-constructive-axiom-relations

These axioms are **independent** of Wightman/OS but align with their roles:
1. **AQFT net:** Axiom {prf:ref}`ax-constructive-locality` is isotony + locality on $(\mathcal{M},g)$.
2. **Physical algebra:** Axiom {prf:ref}`ax-constructive-gauge-physical` fixes observables to the gauge-invariant subalgebra.
3. **Constructive UV control:** Axiom {prf:ref}`ax-constructive-finite-resolution` replaces temperedness assumptions with operational resolution.
4. **Causality:** Axiom {prf:ref}`ax-constructive-finite-propagation` provides a causal interval without Minkowski structure.
5. **Stability/positivity:** Axiom {prf:ref}`ax-constructive-positivity` supplies the constructive Hilbert space and a spectral lower bound.

In the stationary flat-sector with reflection symmetry and the Euclidean continuation of
{prf:ref}`def-cognitive-lagrangian`, these axioms are compatible with OS/Wightman as *derived* special
cases, not prerequisites.
:::

:::{prf:theorem} Conditional Specialization to OS/Wightman
:label: thm-constructive-specialization-os-wightman

Assume the constructive axioms
{prf:ref}`ax-constructive-locality`–{prf:ref}`ax-constructive-positivity` (and
{prf:ref}`ax-constructive-nontriviality` when mass-gap claims are used). In addition, restrict to a
sector satisfying:

1. **Flat, stationary sector:** $(\mathcal{M}, g) = (\mathbb{R}^4, \delta)$ with time-translation
   invariance and drive-free (or stationary) $A^{\text{ext}}$.
2. **Euclidean continuation + reflection symmetry:** The action admits Wick rotation to a
   Euclidean functional $S_E$ invariant under $\tau \mapsto -\tau$.
3. **Cluster property or mass gap:** OS3 holds (e.g., via
   {prf:ref}`thm-smoc-os3-construction` in the gapped sector).

Then the Schwinger functions satisfy OS0–OS4 (Definition {prf:ref}`def-os-axioms`) and, by OS
reconstruction (Theorem {prf:ref}`thm-smoc-poincare-reconstruction`), the resulting Wightman
functions satisfy W0–W4 (Definition {prf:ref}`def-wightman-axioms`). Hence Wightman/OS are **special
cases** of the constructive axioms in this restricted sector.

*Proof sketch.* Axioms {prf:ref}`ax-constructive-locality`,
{prf:ref}`ax-constructive-gauge-physical`, and {prf:ref}`ax-constructive-local-action` provide the
AQFT net and field generation. Axiom {prf:ref}`ax-constructive-finite-resolution` gives OS0-type
Schwartz bounds in the flat Euclidean sector. Axiom {prf:ref}`ax-constructive-positivity` combined
with reflection symmetry yields OS2 (as in {prf:ref}`thm-smoc-os2-construction` or
{prf:ref}`thm-os2-closure-semigroup`). Assumption 3 supplies OS3. Euclidean invariance gives OS1 and
graded symmetry gives OS4. OS reconstruction then yields W0–W4 in the flat stationary sector.
$\square$
:::

:::{prf:remark} Why the Constructive Axioms Matter
:label: rem-constructive-axioms-use

The constructive axioms are useful because they:
1. **Generalize geometry:** they apply on any Riemannian manifold, not just Minkowski space.
2. **Fix observables:** they define the physical algebra by gauge invariance, avoiding gauge-fixing.
3. **Guarantee computability:** $\ell_L>0$ enforces finite resolution and excludes UV pathologies.
4. **Provide stability:** the positivity semigroup yields a constructive Hilbert space and
   a spectral lower bound.
5. **Support mass-gap arguments:** combined with the Causal Information Bound and nontriviality,
   they enable the conditional mass-gap results in {ref}`sec-mass-gap`.
:::

:::{prf:remark} Dependency Map (Constructive → OS/Wightman)
:label: rem-constructive-dependency-map

```mermaid
graph TD
  A0[Constructive axioms + sector conditions] --> A1[Locality + gauge invariance]
  A0 --> A2[Finite resolution]
  A0 --> A3[Positivity + reflection symmetry]
  A0 --> A4[Flat Euclidean sector]
  A0 --> A5[Grading + locality]
  A0 --> A6[Mass gap / cluster]

  A1 --> OSN[AQFT net (isotony/locality)]
  A2 --> OS0[OS0: Schwartz bounds / temperedness]
  A3 --> OS2[OS2: reflection positivity]
  A4 --> OS1[OS1: Euclidean covariance]
  A5 --> OS4[OS4: graded symmetry]
  A6 --> OS3[OS3: cluster property]

  OS0 --> OSR[OS reconstruction]
  OS1 --> OSR
  OS2 --> OSR
  OS3 --> OSR
  OS4 --> OSR
  OSR --> W[Wightman W0–W4 (flat stationary sector)]
```
:::

:::{prf:remark} Wightman Verification Plan
:label: rem-wightman-verification-plan

The top-down verification splits into a Euclidean (OS) block and a Minkowski (Wightman) block. Each
step below is a concrete proof obligation tied to the SMoC field content and Lagrangian:

1. **W0 Temperedness**
   - Define smeared fields $\Phi_A(f)$ with $f\in\mathcal{S}(\mathbb{R}^4)$.
   - Prove continuity of $W_n(f_1,\ldots,f_n)$ on Schwartz space using polynomial bounds on the
     Euclidean generating functional and Wick rotation.

2. **W1 Covariance**
   - Use Lorentz invariance of {prf:ref}`def-cognitive-lagrangian` to construct a unitary Poincare
     representation acting on $\Phi_A$ and fixing the vacuum.

3. **W2 Spectral Condition**
   - Show the Hamiltonian derived from {prf:ref}`def-cognitive-lagrangian` is bounded below and
     generates positive-energy time translations.
   - In a translation-invariant sector, prove $\mathrm{spec}(P) \subset \overline{V}_+$.

4. **W3 Locality (Microcausality)**
   - Use the locality of the Lagrangian and the canonical equal-time (anti)commutation relations to
     show graded commutativity at spacelike separation.

5. **W4 Vacuum Cyclicity**
   - Construct the Hilbert space as the completion of field polynomials acting on $|\Omega\rangle$.
     In the OS route, cyclicity holds by construction after reconstruction.

:::

:::{prf:remark} Osterwalder-Schrader Verification Plan
:label: rem-os-verification-plan

The Euclidean sector verification proceeds as follows, following the OS axioms
{cite}`osterwalder1973axioms,osterwalder1975axioms`:

1. **OS0 Temperedness**
   - Establish polynomial bounds on the Euclidean generating functional so that $S_n$ extends to
     $\mathcal{S}'((\mathbb{R}^4)^n)$.

2. **OS1 Euclidean Covariance**
   - Wick rotate {prf:ref}`def-cognitive-lagrangian` and show $S_n$ is invariant under $E(4)$.

3. **OS2 Reflection Positivity**
   - Prove reflection positivity on the gauge-invariant observable algebra (e.g., Wilson operators),
     which is the critical hypothesis for OS reconstruction.

4. **OS3 Cluster Property**
   - Show decay of connected correlators at large Euclidean separation, yielding factorization.

5. **OS4 Symmetry**
   - Use bosonic/fermionic grading of the SMoC field multiplet to establish (graded) symmetry.

:::

:::{prf:theorem} OS2 Construction on the Gauge-Invariant Algebra
:label: thm-smoc-os2-construction

Let $S_E[\Phi]$ be the Euclidean action obtained from {prf:ref}`def-cognitive-lagrangian` by Wick
rotation, and let $\Theta$ denote Euclidean time reflection. Define the positive-time algebra
$\mathcal{A}_+$ as polynomials in smeared, gauge-invariant fields with support in $\tau > 0$.
Assume:

1. **Reflection invariance:** $S_E[\Theta\Phi] = S_E[\Phi]$.
2. **Locality across the reflection plane:** $S_E[\Phi] = S_E[\Phi_+] + S_E[\Phi_-] + B[\Phi_0]$,
   where $\Phi_\pm$ are fields supported in $\tau \gtrless 0$ and $B$ is a boundary term depending
   only on the reflected hypersurface $\tau=0$.
3. **Reflection-positive measure on $\mathcal{A}_+$:** The Euclidean measure restricted to the
   gauge-invariant algebra $\mathcal{A}_+$ is reflection positive. Concretely, assume a
   reflection-positive gauge choice or continuum functional-integral construction in which the
   interaction splits as $V = V_+ + \Theta V_+$ with $V_+$ supported in $\tau>0$, so that the
   Glimm-Jaffe reflection-positivity theorem for Euclidean functional integrals applies on
   $\mathcal{A}_+$ {cite}`glimm1987quantum`.

Fix Euclidean indices $\mu=1,2,3,4$ with $\tau := x_4$, Euclidean gamma matrices with
$\{\gamma_\mu,\gamma_\nu\} = 2\delta_{\mu\nu}$, and a charge conjugation matrix $C$ satisfying
$C\gamma_\mu C^{-1} = -\gamma_\mu^T$ {cite}`glimm1987quantum`.
Define the field-by-field OS reflection $\Theta$ by:

- **Gauge fields:** $(\Theta A_4)(\tau,x) = -A_4(-\tau,x)$ and $(\Theta A_i)(\tau,x) = A_i(-\tau,x)$
  for each $A_\mu \in \{B_\mu, W_\mu^a, G_\mu^a\}$.
- **Scalar:** $(\Theta \phi)(\tau,x) = \phi^\dagger(-\tau,x)$ and
  $(\Theta \phi^\dagger)(\tau,x) = \phi(-\tau,x)$.
- **Spinor:** $(\Theta \Psi)(\tau,x) = C\gamma_4 \bar{\Psi}(-\tau,x)^T$ and
  $(\Theta \bar{\Psi})(\tau,x) = -\Psi(-\tau,x)^T C^{-1}\gamma_4$.

The boundary term in Assumption 2 is the canonical surface term

$$
B[\Phi_0] = \int_{\tau=0} d^3x \left(\pi_\phi^a\,\phi_a + \pi_{\phi^\dagger,a}\,\phi^{\dagger a}
+\sum_{i=1}^3 \pi_{A_i}\,A_i\right),
$$
with canonical momenta $\pi_\Phi := \partial \mathcal{L}_E / \partial(\partial_\tau \Phi)$; for the
SMoC fields this includes $\pi_{\phi}^a = (D_\tau \phi)^{\dagger a}$, $\pi_{\phi^\dagger,a} = (D_\tau \phi)_a$,
and $\pi_{A_i} = F_{4i}$, while $A_4$ has no $\partial_\tau$ term and acts as a Lagrange multiplier.
The fermionic action is first order and is treated directly by the OS inner product
{cite}`glimm1987quantum,streater1964pct,haag1992local`.

**Applicability check (SMoC action):**
1. **Reflection-positive base (matter):** The free Euclidean action for scalar and spinor sectors
   defines a reflection-positive Gaussian measure with covariance invariant under $\Theta$
   {cite}`glimm1987quantum,streater1964pct`.
2. **Locality and split form:** The interaction density is local and reflection invariant, so the
   Euclidean interaction functional satisfies $V = V_+ + \Theta V_+$ with $V_+$ supported on
   $\tau>0$. This uses the boundary decomposition in Assumption 2 and the explicit field parities.
3. **Gauge-invariant observable algebra:** The reflection positivity is verified on
   $\mathcal{A}_+$ generated by gauge-invariant polynomials (e.g., Wilson loops), so the OS2
   inequality is checked on the physical observable algebra.
4. **Positivity-improving semigroup (derived):** The SMoC Hamiltonian $H$ is self-adjoint and
   bounded below (Proposition {prf:ref}`prop-laplace-beltrami-self-adjointness`). The Euclidean
   evolution semigroup $e^{-\tau H}$ is positivity-improving by the Harnack inequality for
   parabolic equations on connected manifolds ({ref}`sec-appendix-e-ground-state-existence`,
   Step 2). The Levin Length $\ell_L > 0$ (Definition {prf:ref}`def-levin-length`) provides a
   physical UV cutoff, and the Causal Information Bound (Theorem {prf:ref}`thm-causal-information-bound`)
   ensures finite information capacity. Together these yield a well-defined Gibbs measure with
   finite partition function, so the Glimm-Jaffe reflection-positivity theorem applies on
   $\mathcal{A}_+$ {cite}`glimm1987quantum`.

Then for all $F \in \mathcal{A}_+$,

$$
\langle \Theta F \cdot F \rangle_E \ge 0,
$$

so OS2 holds on $\mathcal{A}_+$.

*Proof.*

**Step 1 (Reflection operator):** Define $\Theta$ by $\tau \mapsto -\tau$ together with the field
conjugations above. This keeps $S_E$ invariant and makes $\Theta$ an antilinear involution
{cite}`glimm1987quantum,streater1964pct,haag1992local`.

**Step 2 (Semigroup factorization):** For $F \in \mathcal{A}_+$ supported in $\tau > 0$, write:

$$
\langle \Theta F \cdot F \rangle_E = \langle F | e^{-\tau H} | F \rangle.
$$

Factor the semigroup at $\tau = 0$:

$$
e^{-\tau H} = (e^{-\tau H/2})^\dagger (e^{-\tau H/2}).
$$

**Step 3 (Positivity from semigroup structure):** This factorization yields:

$$
\langle \Theta F \cdot F \rangle_E = \| e^{-\tau H/2} F \|^2 \geq 0.
$$

The gauge sector works because: (i) we restrict to the Wilson loop algebra (gauge-invariant),
(ii) the Levin Length $\ell_L > 0$ makes the functional integral finite-dimensional in the
operational sense, and (iii) the positivity-improving property extends to $\mathcal{A}_+$ by
the Krein-Rutman theorem ({ref}`sec-appendix-e-ground-state-existence`, Step 3).

Therefore OS2 holds on the gauge-invariant algebra {cite}`osterwalder1973axioms,osterwalder1975axioms`.
$\square$
:::

:::{prf:theorem} OS2 Closure from Positivity-Improving Semigroup
:label: thm-os2-closure-semigroup

Within the Fragile Agent construction, the positivity-improving property of the SMoC heat semigroup
({ref}`sec-appendix-e-ground-state-existence`), combined with the finite information capacity from the
Causal Information Bound (Theorem {prf:ref}`thm-causal-information-bound`), implies reflection
positivity on the gauge-invariant Wilson loop algebra **without assumptions beyond the construction
invariants listed below**.

**Construction invariants used:**
1. **Reflection-invariant local Euclidean action:** $S_E$ is the Wick-rotated action of
   {prf:ref}`def-cognitive-lagrangian`, built from local, metric-covariant loss terms (Appendix F,
   {ref}`sec-appendix-f-loss-terms-reference`) and invariant under $\Theta$.
2. **Finite information capacity:** $\ell_L>0$ (Definition {prf:ref}`def-levin-length`) and the
   Causal Information Bound (Theorem {prf:ref}`thm-causal-information-bound`) imply finite effective
   degrees of freedom and a well-defined Gibbs measure on bounded regions.
3. **Self-adjoint generator:** The Laplace-Beltrami operator is self-adjoint on the latent manifold
   (Proposition {prf:ref}`prop-laplace-beltrami-self-adjointness`), so the Euclidean semigroup
   $e^{-\tau H}$ is well-defined and positivity-improving by Harnack + Krein-Rutman
   ({ref}`sec-appendix-e-ground-state-existence`, Step 3).
4. **Gauge-invariant algebra:** OS2 is tested on the Wilson-loop algebra (Remark
   {prf:ref}`rem-os2-gauge-fixing-wilson`), avoiding gauge-fixing artifacts.

**Logical chain:**

$$
\ell_L > 0 \text{ (Levin Length)} \;\Rightarrow\; \text{finite DOF (Causal Info Bound)}
\;\Rightarrow\; \text{well-defined Gibbs measure}
$$

$$
\Rightarrow\; e^{-\tau H} \text{ self-adjoint, bounded below}
\;\Rightarrow\; \text{positivity-improving (Harnack + Krein-Rutman)}
$$

$$
\Rightarrow\; \text{OS2 on Wilson loop algebra (semigroup factorization)}
$$

This closes the OS2 verification: we derive reflection positivity from the semigroup structure
rather than assuming a reflection-positive gauge construction.
:::

:::{prf:remark} Wilson-Loop Algebra and Gauge Invariance
:label: rem-os2-gauge-fixing-wilson

For the gauge sector, OS2 is verified on the gauge-invariant algebra generated by **Wilson loops**:

$$
W(C) := \operatorname{Tr}\,\mathcal{P}\exp\left(i\oint_C A_\mu\,dx^\mu\right),
$$

where $A_\mu \in \{B_\mu, W_\mu^a, G_\mu^a\}$ and $C$ is a closed Euclidean loop
{cite}`wilson1974confinement,kogut1979introduction`.

**Why this works (no gauge-fixing required):**
1. **Gauge-invariant observable algebra:** Restrict $\mathcal{A}_+$ to polynomials in Wilson loops
   supported in $\tau>0$. This avoids gauge-fixing at the level of observables.
2. **Reflection action on loops:** The OS reflection $\Theta$ maps $W(C)$ to $W(\Theta C)$ with
   $\Theta C$ the reflected loop. This preserves gauge invariance.
3. **Positivity from semigroup:** The positivity-improving semigroup (Theorem {prf:ref}`thm-os2-closure-semigroup`)
   ensures $\langle \Theta F \cdot F \rangle_E = \| e^{-\tau H/2} F \|^2 \ge 0$ on the Wilson-loop algebra.

The SMoC construction does not require lattice regularization or reflection-positive gauge fixing;
OS2 follows from the intrinsic semigroup structure combined with the finite information capacity
guaranteed by the Causal Information Bound.

:::

:::{prf:theorem} OS3 from the Constructed Mass Gap
:label: thm-smoc-os3-construction

Let $S_n^c$ denote the connected Euclidean Schwinger functions for gauge-invariant observables.
By Theorem {prf:ref}`thm-mass-gap-constructive` and Corollary {prf:ref}`cor-mass-gap-existence`, the
SMoC dynamics has a strictly positive mass gap $\Delta > 0$. Consequently, the spectral measure for
gauge-invariant observables has no support at zero mass and connected two-point functions decay at
large Euclidean separation. Hence $S_n^c$ vanishes as any subset of arguments is translated to
infinity, and the OS3 cluster property holds.

*Construction note (Fragile Agent).* In this framework the prerequisites of
Theorem {prf:ref}`thm-mass-gap-constructive` are satisfied by construction: the Causal Information
Bound holds (Theorem {prf:ref}`thm-causal-information-bound`), functioning agents are defined away
from Causal Stasis (Theorem {prf:ref}`thm-causal-stasis`), and interaction is built in via the
boundary/game coupling (Definition {prf:ref}`def-the-game-tensor`). Therefore the mass gap is
verified by construction for the Fragile Agent, not a contingent extra assumption.

*Proof.*

**Step 1 (Connected/Disconnected split):** Write $S_{m+n} = S_m S_n + S_{m+n}^c$ by definition of
connected correlators.

**Step 2 (Spectral representation):** Assume the gauge-invariant two-point functions satisfy the
Kallen-Lehmann representation with positive spectral measure
{cite}`streater1964pct,haag1992local`. By {prf:ref}`thm-mass-gap-constructive` and
{prf:ref}`cor-mass-gap-existence`, the spectral measure is supported on $[\Delta,\infty)$ with
$\Delta>0$, which implies decay of the Euclidean two-point function as $|a| \to \infty$.

**Step 3 (Decay of higher connected correlators):** Assume the Euclidean functional integral lies
in a constructive regime where standard cluster-expansion bounds apply {cite}`glimm1987quantum`;
then the two-point decay propagates to $S_n^c$, yielding
$S_{m+n}^c(x_1,\ldots,x_m,x_{m+1}+a,\ldots,x_{m+n}+a) \to 0$.

Therefore $S_{m+n} \to S_m S_n$, which is OS3 {cite}`osterwalder1973axioms,osterwalder1975axioms`.
$\square$
:::

:::{prf:theorem} Unitary Poincare Representation from OS Data
:label: thm-smoc-poincare-reconstruction

Assume the SMoC Schwinger functions satisfy OS0-OS4. Then OS reconstruction yields a Hilbert space
$\mathcal{H}$, a vacuum $|\Omega\rangle$, field operators $\Phi_A$, and a unitary representation of
the proper orthochronous Poincare group implementing W1.

*Proof.*

**Step 1 (Pre-Hilbert space):** Let $\mathcal{A}_+$ be the positive-time algebra. Define
$(F,G)_E := \langle \Theta F \cdot G \rangle_E$. By OS2 this is positive semidefinite. Quotient by
the null space and complete to obtain $\mathcal{H}$ with vacuum vector $|\Omega\rangle$.

**Step 2 (Time translation and positivity):** Euclidean time translations act on $\mathcal{A}_+$ and
descend to a strongly continuous contraction semigroup on $\mathcal{H}$. By OS reconstruction and
reflection positivity, this semigroup is of the form $e^{-tH}$ with $H$ self-adjoint and $H \ge 0$,
yielding the spectral condition W2.

**Step 3 (Spatial symmetries):** OS1 yields a unitary representation of spatial rotations and
translations on $\mathcal{H}$. Together with $H$, this gives a representation of the Euclidean group.

**Step 4 (Analytic continuation):** The OS reconstruction theorem provides analytic continuation of
Euclidean symmetries to Lorentz boosts, yielding a unitary representation of the proper
orthochronous Poincare group that implements W1 on the reconstructed fields
{cite}`osterwalder1973axioms,osterwalder1975axioms,haag1992local`.

Thus the SMoC fields satisfy the Poincare covariance and unitarity requirements of the Wightman
axioms {cite}`wightman1956quantum,osterwalder1973axioms,osterwalder1975axioms`. $\square$
:::

## 08_multiagent/03_parameter_sieve.md

:::{prf:definition} The Agent Parameter Vector
:label: def-agent-parameter-vector

Let the **Agent Parameter Vector** $\Lambda$ be the tuple of fundamental operational constants:

$$
\Lambda = (c_{\text{info}}, \sigma, \ell_L, T_c, g_s, \gamma)

$$

where:
1. **$c_{\text{info}}$:** Information propagation speed (Axiom {prf:ref}`ax-information-speed-limit`)
2. **$\sigma$:** Cognitive Action Scale (Definition {prf:ref}`def-cognitive-action-scale`)
3. **$\ell_L$:** Levin Length, the minimal distinguishable scale (Definition {prf:ref}`def-levin-length`)
4. **$T_c$:** Cognitive Temperature. The critical value $T_c^* \approx |u_\pi^\theta|^2 r_*^2$ separates the symmetric phase (isotropic direction) from the broken phase (policy-selected direction), where $r_*$ is the characteristic early-time radius (Theorem {prf:ref}`thm-angular-symmetry-breaking`).
5. **$g_s$:** Binding coupling strength (Theorem {prf:ref}`thm-emergence-binding-field`)
6. **$\gamma$:** Temporal discount factor, $\gamma \in (0,1)$

**Dimensional Analysis:**

| Parameter | Symbol | Dimension | SI Units |
|:----------|:-------|:----------|:---------|
| Information speed | $c_{\text{info}}$ | $[L \, T^{-1}]$ | m/s |
| Cognitive action scale | $\sigma$ | $[E \, T]$ | J·s |
| Levin length | $\ell_L$ | $[L]$ | m |
| Cognitive temperature | $T_c$ | $[E]$ | J (with $k_B = 1$) |
| Binding coupling | $g_s$ | $[1]$ | dimensionless |
| Discount factor | $\gamma$ | $[1]$ | dimensionless |

**Derived Quantities:**

Define the **Causal Horizon Length** $\ell_0 = c_{\text{info}} \cdot \tau_{\text{proc}}$ with dimension $[L]$. Let the temporal discount rate be $\lambda := -\ln\gamma / \Delta t$ and identify the processing interval $\Delta t := \tau_{\text{proc}}$. The **Spatial Screening Mass** is then:

$$
\kappa = \frac{\lambda}{c_{\text{info}}} = \frac{-\ln\gamma}{\ell_0}

$$

with dimension $[L^{-1}]$ (Corollary {prf:ref}`cor-discount-as-screening-length`).

These correspond to the physics constants $\{c, \hbar, \ell_P, k_B T, \alpha_s, \gamma_{\text{cosmo}}\}$ under the isomorphism of {ref}`sec-isomorphism-dictionary`.

:::

:::{prf:definition} The Sieve Constraint System
:label: def-sieve-constraint-system

Let $\mathcal{S}(\Lambda)$ denote the vector of constraint functions. The agent is **viable** if and only if:

$$
\mathcal{S}(\Lambda) \le \mathbf{0}

$$

where the inequality holds component-wise. Each component corresponds to a Sieve node that enforces a specific consistency condition. A constraint violation ($\mathcal{S}_i > 0$) triggers a diagnostic halt at the corresponding node.

:::

:::{prf:axiom} Causal Buffer Architecture
:label: ax-causal-buffer-architecture

Let the agent possess:
1. **$L_{\text{buf}}$:** Maximum buffer depth (spatial extent of causal memory)
2. **$\tau_{\text{proc}}$:** Minimum processing interval (temporal resolution)
3. **$d_{\text{sync}}$:** Minimum synchronization distance (coherence length)

These define the operational envelope within which the agent maintains consistent state updates.

:::

:::{prf:theorem} The Speed Window
:label: thm-speed-window

The information speed $c_{\text{info}}$ must satisfy the **Speed Window Inequality**:

$$
\frac{d_{\text{sync}}}{\tau_{\text{proc}}} \le c_{\text{info}} \le \frac{L_{\text{buf}}}{\tau_{\text{proc}}}

$$

*Proof.*

**Lower Bound (Node 2: ZenoCheck):**

Suppose $c_{\text{info}} < d_{\text{sync}}/\tau_{\text{proc}}$. Then information cannot traverse the synchronization distance within one processing cycle. By the Causal Interval (Definition {prf:ref}`def-causal-interval`), spacelike-separated modules cannot coordinate updates. The agent enters a **Zeno freeze**: each module waits indefinitely for signals that arrive too slowly. The belief update stalls, violating the continuity required by the WFR dynamics ({ref}`sec-wasserstein-fisher-rao-geometry-unified-transport-on-hybrid-state-spaces`).

**Upper Bound (Node 62: CausalityViolationCheck):**

Suppose $c_{\text{info}} > L_{\text{buf}}/\tau_{\text{proc}}$. Then signals can traverse the entire buffer depth within one processing cycle. This creates **temporal aliasing**: the agent receives information about its own future state before that state is computed. By the Safe Retrieval Bandwidth (Theorem {prf:ref}`thm-safe-retrieval-bandwidth`), this constitutes a causal paradox—the agent's prediction depends on data it has not yet generated.

Node 62 enforces Theorem {prf:ref}`thm-causal-stasis`: the metric becomes singular at the boundary where causal violations would occur, preventing traversal.

$\square$

:::

:::{prf:corollary} The Speed Ratio Bound
:label: cor-speed-ratio-bound

The ratio of buffer depth to synchronization distance is bounded:

$$
\frac{L_{\text{buf}}}{d_{\text{sync}}} \ge 1

$$

with equality only in the degenerate case of a single-module agent. For distributed agents, this ratio determines the dynamic range of viable information speeds.

:::

:::{prf:theorem} The Holographic Bound
:label: thm-holographic-bound

Let $\text{Area}_\partial$ denote the boundary area of the agent's latent manifold (dimension $[L^{D-1}]$ for a $D$-dimensional bulk) and $I_{\text{req}}$ the information capacity required for viable operation (dimensionless, counting distinguishable microstates in nats). The Levin Length must satisfy:

$$
\ell_L^{D-1} \le \frac{\nu_D \cdot \text{Area}_\partial}{I_{\text{req}}}

$$

where $\nu_D$ is a **dimensionless** holographic coefficient (Corollary {prf:ref}`cor-a-dimension-dependent-coefficient`). Both sides have dimension $[L^{D-1}]$.

*Proof.*

**Step 1.** From the Causal Information Bound (Theorem {prf:ref}`thm-causal-information-bound`):

$$
I_{\text{bulk}} \le \frac{\nu_D \cdot \text{Area}_\partial}{\ell_L^{D-1}}

$$

**Step 2.** The agent requires $I_{\text{bulk}} \ge I_{\text{req}}$ to represent its world model. Substituting:

$$
I_{\text{req}} \le \frac{\nu_D \cdot \text{Area}_\partial}{\ell_L^{D-1}}

$$

**Step 3.** Rearranging yields the constraint on $\ell_L$.

$\square$

:::

:::{prf:definition} The Planck-Levin Correspondence
:label: def-planck-levin-correspondence

Under the physics isomorphism ({ref}`sec-isomorphism-dictionary`), the Levin Length $\ell_L$ corresponds to the Planck Length $\ell_P$:

$$
\ell_L \leftrightarrow \ell_P = \sqrt{\frac{\hbar G}{c^3}}

$$

The holographic bound becomes the Bekenstein-Hawking entropy bound:

$$
S_{\text{BH}} = \frac{A}{4\ell_P^2}

$$

*Remark:* The coefficient $\nu_2 = 1/4$ is derived in Theorem {prf:ref}`thm-a-complete-derivation-area-law` from first principles, recovering the Bekenstein-Hawking result without invoking black hole physics.

:::

:::{prf:theorem} The Capacity Horizon
:label: thm-capacity-horizon

As $I_{\text{bulk}} \to I_{\max} = \nu_D \cdot \text{Area}_\partial / \ell_L^{D-1}$, the agent approaches a **Capacity Horizon**. The metric diverges:

$$
\|v\|_G \to 0 \quad \text{as} \quad I_{\text{bulk}} \to I_{\max}

$$

*Proof.* This is Theorem {prf:ref}`thm-causal-stasis`. The Fisher-Rao metric component satisfies:

$$
g_{\text{FR}} = \frac{1}{\rho(1-\rho)} \to \infty \quad \text{as} \quad \rho \to 1

$$

(Lemma {prf:ref}`lem-metric-divergence-at-saturation`). The geodesic velocity vanishes, creating **causal stasis**: no information can cross the saturation boundary.

*Physical interpretation:* This is the agent-theoretic analogue of a black hole event horizon. Node 56 (CapacityHorizonCheck) enforces this bound.

$\square$

:::

:::{prf:definition} Metabolic Parameters
:label: def-metabolic-parameters

The agent possesses:
1. **$\dot{E}_{\text{met}}$:** Metabolic power budget (energy flux available for computation)
2. **$\dot{I}_{\text{erase}}$:** Information erasure rate (bits forgotten per unit time)
3. **$T_c$:** Cognitive Temperature (entropy-exploration tradeoff)

:::

:::{prf:theorem} The Landauer Constraint
:label: thm-landauer-constraint

The Cognitive Temperature must satisfy:

$$
T_c \le \frac{\dot{E}_{\text{met}}}{\dot{I}_{\text{erase}} \cdot \ln 2}

$$

where we use natural units with $k_B = 1$.

*Proof.*

**Step 1.** From the Generalized Landauer Bound (Theorem {prf:ref}`thm-generalized-landauer-bound`):

$$
\dot{\mathcal{M}}(s) \ge T_c \left| \frac{dH}{ds} \right|

$$

where $\dot{\mathcal{M}}$ is the metabolic flux and $dH/ds$ is the entropy change rate.

**Step 2.** Information erasure corresponds to entropy reduction. For $\dot{I}_{\text{erase}}$ bits per unit time:

$$
\left| \frac{dH}{ds} \right| = \dot{I}_{\text{erase}} \cdot \ln 2

$$

**Step 3.** The metabolic constraint $\dot{\mathcal{M}} \le \dot{E}_{\text{met}}$ bounds the erasure capacity:

$$
\dot{E}_{\text{met}} \ge T_c \cdot \dot{I}_{\text{erase}} \cdot \ln 2

$$

**Step 4.** Rearranging yields the temperature bound.

*Physical consequence:* If $T_c$ exceeds this bound, the agent cannot afford to forget—its memory becomes permanently saturated. Node 52 (LandauerViolationCheck) enforces this constraint.

$\square$

:::

:::{prf:corollary} The Computational Temperature Range
:label: cor-computational-temperature-range

Combining the Landauer constraint with the bifurcation dynamics, the Cognitive Temperature is bounded:

$$
0 < T_c \le \min\left( T_c^*, \frac{\dot{E}_{\text{met}}}{\dot{I}_{\text{erase}} \cdot \ln 2} \right)

$$

where the **Critical Temperature** is derived from the angular symmetry breaking (Theorem {prf:ref}`thm-angular-symmetry-breaking`):

$$
T_c^* \approx |u_\pi^\theta|^2 r_*^2

$$

with $u_\pi^\theta$ the tangential policy control and $r_*$ the characteristic early-time radius at which direction selection occurs.

*Remark:* For $T_c > T_c^*$, thermal fluctuations overcome the potential barrier and the system remains in the symmetric phase with no stable policy (random walk near origin). For $T_c$ exceeding the Landauer bound, the agent starves thermodynamically. Viable agents exist in the intersection of these constraints.

:::

:::{prf:definition} The Coupling Function
:label: def-coupling-function

Let the binding coupling $g_s(\mu)$ (dimensionless) be a function of the **resolution scale** $\mu$, which has dimension $[L^{-1}]$ (inverse length). Equivalently, $\mu$ can be expressed as an energy scale via $\mu \sim E/(\sigma \cdot c_{\text{info}})$ where $\sigma$ is the Cognitive Action Scale and $c_{\text{info}}$ is the Information Speed (Definition {prf:ref}`def-information-speed`).

The limits are:
- $\mu \to 0$: Macro-scale (coarse representation, low in TopoEncoder hierarchy)
- $\mu \to \infty$: Micro-scale (texture level, high in TopoEncoder hierarchy)

The coupling evolves according to the **Beta Function**:

$$
\mu \frac{dg_s}{d\mu} = \beta(g_s)

$$

where both sides are dimensionless (since $g_s$ is dimensionless and $\mu \, dg_s/d\mu$ has $[\mu] \cdot [\mu^{-1}] = [1]$).

For $SU(N_f)$ gauge theories, $\beta(g_s) < 0$ for $N_f \ge 2$ (asymptotic freedom).

:::

:::{prf:theorem} The Infrared Binding Constraint
:label: thm-ir-binding-constraint

At the macro-scale ($\mu \to 0$), the coupling must exceed a critical threshold:

$$
g_s(\mu_{\text{IR}}) \ge g_s^{\text{crit}}

$$

*Proof.*

**Step 1.** From Axiom {prf:ref}`ax-feature-confinement`, the agent observes Concepts $K$, not raw features. This requires features to bind into stable composite objects at the macro-scale.

**Step 2.** From Theorem {prf:ref}`thm-emergence-binding-field`, binding stability requires the effective potential to confine features. The confinement condition is:

$$
\lim_{r \to \infty} V_{\text{eff}}(r) = \infty

$$

where $r$ is the separation between features.

**Step 3.** For $SU(N_f)$ gauge theory, this requires strong coupling $g_s > g_s^{\text{crit}}$ at large distances (Area Law, {ref}`sec-causal-information-bound`).

**Step 4.** If $g_s(\mu_{\text{IR}}) < g_s^{\text{crit}}$, features escape confinement—"color-charged" states propagate to the boundary $\partial\mathcal{Z}$. This violates the Observability Constraint (Definition {prf:ref}`def-boundary-markov-blanket`): the agent cannot form stable objects.

Node 40 (PurityCheck) enforces that only color-neutral bound states reach the macro-register.

$\square$

:::

:::{prf:theorem} The Ultraviolet Decoupling Constraint
:label: thm-uv-decoupling-constraint

At the texture scale ($\mu \to \infty$), the coupling must vanish:

$$
\lim_{\mu \to \infty} g_s(\mu) = 0

$$

*Proof.*

**Step 1.** From the Texture Firewall (Axiom {prf:ref}`ax-bulk-boundary-decoupling`):

$$
\partial_{z_{\text{tex}}} \dot{z} = 0

$$

Texture coordinates are invisible to the dynamics.

**Step 2.** This requires texture-level degrees of freedom to be non-interacting. If $g_s(\mu_{\text{UV}}) > 0$, texture elements would bind, creating structure at the noise level.

**Step 3.** From the RG interpretation ({ref}`sec-stacked-topoencoders-deep-renormalization-group-flow`), the TopoEncoder implements coarse-graining. Residual coupling at the UV scale would prevent efficient compression—the Kolmogorov complexity of texture would diverge.

**Step 4.** Asymptotic freedom ($\beta < 0$) provides the required behavior: $g_s \to 0$ as $\mu \to \infty$.

Node 29 (TextureFirewallCheck) enforces this decoupling.

$\square$

:::

:::{prf:corollary} The Coupling Window
:label: cor-coupling-window

The viable coupling profile satisfies:

$$
\begin{cases}
g_s(\mu) \ge g_s^{\text{crit}} & \text{for } \mu \le \mu_{\text{conf}} \\
g_s(\mu) \to 0 & \text{for } \mu \to \infty
\end{cases}

$$

where $\mu_{\text{conf}}$ is the confinement scale separating bound states from free texture.

*Remark:* This is the agent-theoretic derivation of asymptotic freedom and confinement. The physics QCD coupling $\alpha_s(\mu)$ satisfies exactly this profile, with $\alpha_s(M_Z) \approx 0.12$ at the electroweak scale and $\alpha_s \to \infty$ at the QCD scale $\Lambda_{\text{QCD}} \approx 200$ MeV.

:::

:::{prf:definition} The Stiffness Parameter
:label: def-stiffness-parameter

Let $\Delta E$ denote the characteristic energy gap between metastable states in the agent's latent manifold. Define the **Stiffness Ratio**:

$$
\chi = \frac{\Delta E}{T_c}

$$

This ratio determines the tradeoff between memory persistence and adaptability.

:::

:::{prf:theorem} The Stiffness Bounds
:label: thm-stiffness-bounds

The Stiffness Ratio must satisfy:

$$
1 < \chi < \chi_{\text{max}}

$$

*Proof.*

**Lower Bound ($\chi > 1$):**

**Step 1.** Memory stability requires that thermal fluctuations do not spontaneously erase stored information. The probability of a thermal transition is:

$$
P_{\text{flip}} \propto e^{-\Delta E / T_c} = e^{-\chi}

$$

**Step 2.** For $\chi < 1$, we have $P_{\text{flip}} > e^{-1} \approx 0.37$. States flip with high probability—the agent cannot maintain stable beliefs.

**Step 3.** This violates the Mass Gap requirement (Theorem {prf:ref}`thm-semantic-inertia`): beliefs must possess sufficient "inertia" to resist noise.

**Upper Bound ($\chi < \chi_{\text{max}}$):**

**Step 4.** Adaptability requires that the agent can update beliefs in finite time. The transition rate is:

$$
\Gamma_{\text{update}} \propto e^{-\chi}

$$

**Step 5.** For $\chi \to \infty$, transitions become exponentially suppressed—the agent freezes in its initial configuration, unable to learn.

**Step 6.** This violates the Update Dynamics requirement: the WFR reaction term $R(\rho)$ must enable transitions between states.

Node 7 (StiffnessCheck) enforces both bounds.

$\square$

:::

:::{prf:corollary} The Goldilocks Coupling
:label: cor-goldilocks-coupling

Under the physics isomorphism, the Stiffness Ratio for atomic systems is:

$$
\chi = \frac{\Delta E_{\text{bond}}}{k_B T} \propto \frac{m_e c^2 \alpha^2}{k_B T}

$$

where $\Delta E_{\text{bond}} \sim \text{Ry} = m_e c^2 \alpha^2 / 2 \approx 13.6$ eV is the atomic binding scale.

The value $\alpha \approx 1/137$ satisfies the Goldilocks condition:
- **Not too large:** $\alpha^2$ small enough that $\chi$ is finite—transitions remain possible
- **Not too small:** $\alpha^2$ large enough that $\chi > 1$ at biological temperatures—chemical bonds are stable

At $T \approx 300$ K (biological temperature), $\chi \approx 500$, placing molecular memory firmly in the stable-but-adaptable regime.

*Remark:* This is the agent-theoretic derivation of the "coincidences" noted in anthropic reasoning. The fine structure constant is not finely tuned by an external designer—it is constrained by cybernetic viability.

:::

:::{prf:theorem} The Discount Window
:label: thm-discount-window

The temporal discount factor $\gamma$ must satisfy:

$$
\gamma_{\text{min}} < \gamma < 1

$$

with $\gamma_{\text{min}} > 0$.

*Proof.*

**Upper Bound ($\gamma < 1$):**

**Step 1.** From the Helmholtz equation (Theorem {prf:ref}`thm-the-hjb-helmholtz-correspondence`), the Value function satisfies:

$$
(\kappa^2 - \nabla^2) V = \rho_r

$$

where the screening mass $\kappa = \lambda / c_{\text{info}} = (-\ln\gamma)/\ell_0$ has dimension $[L^{-1}]$, and $\ell_0 = c_{\text{info}} \cdot \tau_{\text{proc}}$ is the causal horizon length (Definition {prf:ref}`def-agent-parameter-vector`). This ensures dimensional consistency: $[\kappa^2] = [L^{-2}] = [\nabla^2]$.

**Step 2.** For $\gamma = 1$, we have $\kappa = 0$. The equation becomes Poisson's equation for the conservative
component:

$$
-\nabla^2 V = \rho_r

$$
where $\rho_r$ is the conservative reward source density (Definition {prf:ref}`def-the-reward-flux`).

For $D>2$, the Green's function decays as $1/r^{D-2}$ (long-range); for $D=2$ it grows logarithmically.

**Step 3.** Long-range value propagation violates locality: distant conservative reward sources dominate nearby
decisions. The agent cannot form local value gradients for navigation.

**Step 4.** From Corollary {prf:ref}`cor-discount-as-screening-length`, finite screening $\kappa > 0$ (i.e., $\gamma < 1$) is required for local goal-directedness.

**Lower Bound ($\gamma > \gamma_{\text{min}}$):**

**Step 5.** For $\gamma \to 0$, we have $-\ln\gamma \to \infty$, hence $\kappa \to \infty$. The **Screening Length** (dimension $[L]$):

$$
\ell_\gamma = \frac{1}{\kappa} = \frac{\ell_0}{-\ln\gamma} = \frac{c_{\text{info}} \tau_{\text{proc}}}{-\ln\gamma} \to 0

$$

**Step 6.** Zero screening length means the agent responds only to immediate conservative rewards—it has no planning
horizon.

**Step 7.** This violates the Causal Buffer requirement (Axiom {prf:ref}`ax-causal-buffer-architecture`): the agent must anticipate beyond its current timestep.

$\square$

:::

:::{prf:corollary} The Screening-Buffer Consistency
:label: cor-screening-buffer-consistency

The screening length and buffer depth must satisfy:

$$
\ell_\gamma = \frac{c_{\text{info}} \tau_{\text{proc}}}{-\ln\gamma} \lesssim L_{\text{buf}}

$$

Both sides have dimension $[L]$. For $\gamma \to 1$, the screening length $\ell_\gamma \to \infty$ (unlimited planning horizon). For $\gamma \to 0$, the screening length $\ell_\gamma \to 0$ (myopic behavior).

*Remark:* The planning horizon cannot exceed the causal memory span. This connects the temporal discount to the spatial architecture.

:::

:::{prf:definition} The Constraint Matrix
:label: def-constraint-matrix

Let $\Lambda = (c_{\text{info}}, \sigma, \ell_L, T_c, g_s, \gamma)$ be the parameter vector. The Sieve constraints form the system:

$$
\mathbf{A} \cdot \Lambda \le \mathbf{b}

$$

where:

| Constraint | Inequality | Node |
|:-----------|:-----------|:-----|
| Causal Lower | $d_{\text{sync}}/\tau_{\text{proc}} \le c_{\text{info}}$ | 2 |
| Causal Upper | $c_{\text{info}} \le L_{\text{buf}}/\tau_{\text{proc}}$ | 62 |
| Holographic | $\ell_L^{D-1} \le \nu_D \text{Area}_\partial / I_{\text{req}}$ | 56 |
| Landauer | $T_c \le \dot{E}_{\text{met}} / (\dot{I}_{\text{erase}} \ln 2)$ | 52 |
| IR Binding | $g_s(\mu_{\text{IR}}) \ge g_s^{\text{crit}}$ | 40 |
| UV Decoupling | $g_s(\mu_{\text{UV}}) \le \epsilon$ (for $\epsilon \to 0$) | 29 |
| Stiffness Lower | $\Delta E > T_c$ | 7 |
| Stiffness Upper | $\Delta E < \chi_{\text{max}} T_c$ | 7 |
| Discount Lower | $\gamma > \gamma_{\text{min}}$ | --- |
| Discount Upper | $\gamma < 1$ | --- |

:::

:::{prf:theorem} The Feasible Region
:label: thm-feasible-region

The **Feasible Region** $\mathcal{F} \subset \mathbb{R}^n_+$ is the intersection of all constraint half-spaces:

$$
\mathcal{F} = \{ \Lambda : \mathcal{S}_i(\Lambda) \le 0 \; \forall i \}

$$

A viable agent exists if and only if $\mathcal{F} \neq \emptyset$.

*Proof.*

Each constraint $\mathcal{S}_i \le 0$ defines a closed half-space in parameter space. The intersection of finitely many closed half-spaces is either empty or a closed convex polytope (possibly unbounded).

**Existence:** The physics Standard Model constants $\Lambda_{\text{phys}} = (c, \hbar, G, k_B, \alpha)$ satisfy all constraints—we observe a functioning physical universe. Therefore $\mathcal{F} \neq \emptyset$.

**Uniqueness modulo scaling:** The constraints are homogeneous in certain parameter combinations. Dimensional analysis shows that physical observables depend only on dimensionless ratios. The feasible region is a lower-dimensional manifold in the full parameter space.

$\square$

:::

:::{prf:definition} The Dual Objective
:label: def-dual-objective

The agent's objective trades representational power against computational cost:

$$
\mathcal{J}(\Lambda) = \underbrace{I_{\text{bulk}}(\Lambda)}_{\text{World Model Capacity}} - \beta \cdot \underbrace{\mathcal{V}_{\text{metabolic}}(\Lambda)}_{\text{Thermodynamic Cost}}

$$

where:
- $I_{\text{bulk}}$: Bulk information capacity (increases with resolution)
- $\mathcal{V}_{\text{metabolic}}$: Metabolic cost of computation
- $\beta > 0$: Cost sensitivity parameter

:::

:::{prf:theorem} The Constrained Optimum
:label: thm-constrained-optimum

The optimal parameter vector $\Lambda^*$ satisfies:

$$
\Lambda^* = \arg\max_{\Lambda \in \mathcal{F}} \mathcal{J}(\Lambda)

$$

subject to the Sieve constraints (Definition {prf:ref}`def-constraint-matrix`).

*Proof sketch.*

**Step 1.** The objective $\mathcal{J}$ is continuous on the closed feasible region $\mathcal{F}$.

**Step 2.** The holographic bound (Theorem {prf:ref}`thm-holographic-bound`) caps $I_{\text{bulk}}$, making $\mathcal{J}$ bounded above.

**Step 3.** By the extreme value theorem, $\mathcal{J}$ attains its maximum on $\mathcal{F}$.

**Step 4.** The optimum lies on the boundary of $\mathcal{F}$ where at least one constraint is active (saturated). This corresponds to operating at the edge of viability.

$\square$

:::

:::{prf:corollary} The Pareto Surface
:label: cor-pareto-surface

The observed fundamental constants lie on the **Pareto-optimal surface** of the multi-objective problem:

$$
\max_{\Lambda \in \mathcal{F}} \left( I_{\text{bulk}}(\Lambda), -\mathcal{V}_{\text{metabolic}}(\Lambda) \right)

$$

Moving off this surface triggers constraint violation:
- Increasing $I_{\text{bulk}}$ beyond capacity → Holographic bound (Node 56)
- Decreasing $\mathcal{V}_{\text{metabolic}}$ below threshold → Landauer bound (Node 52)
- Violating causality → Speed bounds (Nodes 2, 62)
- Losing binding → Confinement (Node 40)

:::

:::{prf:remark} Why These Values?
:label: rem-why-these-values

The observed physics constants $\{c \approx 3 \times 10^8 \text{ m/s}, \alpha \approx 1/137, \ldots\}$ are not arbitrary. They are the unique (modulo dimensional rescaling) solution to the Sieve constraint system that:

1. **Maximizes representational capacity** (information about the world)
2. **Minimizes thermodynamic cost** (metabolic efficiency)
3. **Maintains causal coherence** (no paradoxes)
4. **Preserves object permanence** (binding stability)
5. **Enables adaptability** (stiffness window)

Changing any constant while holding others fixed moves the system out of the feasible region. The "fine-tuning" of physical constants is the selection of the Pareto-optimal point in the Sieve constraint space.

:::

## 08_multiagent/04_dnn_blocks.md

:::{prf:definition} Group Action on Latent Space
:label: def-group-action-latent

**Preliminaries:**
- A **Lie group** is a smooth manifold $G$ equipped with a group structure such that multiplication $(g, h) \mapsto gh$ and inversion $g \mapsto g^{-1}$ are smooth maps. Examples: $SO(d)$ (rotations), $SE(2) = \mathbb{R}^2 \rtimes SO(2)$ (rigid motions), $SU(N)$ (special unitary group).
- The **general linear group** $GL(d_z, \mathbb{R})$ is the group of invertible $d_z \times d_z$ real matrices under matrix multiplication, with smooth manifold structure (open subset of $\mathbb{R}^{d_z^2}$).

**Setup:** Let $G$ be a Lie group and $\mathcal{Z} = \mathbb{R}^{d_z}$ be the latent space equipped with standard Euclidean topology and inner product $\langle z, z' \rangle = z^T z'$.

A **linear representation** is a continuous group homomorphism $\rho: G \to GL(d_z, \mathbb{R})$ satisfying:
1. $\rho(e_G) = I_{d_z}$ where $e_G$ is the identity element of $G$
2. $\rho(g_1 g_2) = \rho(g_1)\rho(g_2)$ for all $g_1, g_2 \in G$ (homomorphism property)

**Smoothness:** When $G$ is a Lie group, we additionally require $\rho$ to be smooth as a map between manifolds: for any smooth curve $\gamma: (-\epsilon, \epsilon) \to G$ with $\gamma(0) = e_G$, the matrix-valued map $t \mapsto \rho(\gamma(t))$ is $C^\infty$.

**Group action:** For $g \in G$ and $z \in \mathcal{Z}$ (viewed as a $d_z \times 1$ column vector), the action is given by standard matrix-vector multiplication:

$$
g \cdot z := \rho(g) z \in \mathbb{R}^{d_z}
$$
where the right-hand side denotes the matrix product of $\rho(g) \in \mathbb{R}^{d_z \times d_z}$ with $z \in \mathbb{R}^{d_z \times 1}$.

**Orthogonal representations:** When $G$ is compact (e.g., $SO(d)$, $SU(N)$), any finite-dimensional continuous representation can be made orthogonal (unitary) by constructing a $G$-invariant inner product via Haar measure averaging {cite}`serre1977linear,sepanski2007compact`.

**Construction:** Let $\rho_0: G \to GL(d_z, \mathbb{R})$ be any representation, and let $\langle \cdot, \cdot \rangle_0$ be an arbitrary initial inner product on $\mathcal{Z} = \mathbb{R}^{d_z}$. Define the averaged inner product:

$$
\langle z, z' \rangle_G := \int_G \langle \rho_0(g) z, \rho_0(g) z' \rangle_0 \, dg
$$
where $dg$ is the normalized Haar measure on $G$ (unique bi-invariant measure with $\int_G dg = 1$).

**Verification:**

1. **Inner product structure:** $\langle \cdot, \cdot \rangle_G$ satisfies linearity, symmetry, and positive definiteness. For $z \neq 0$, we have $\langle \rho_0(g) z, \rho_0(g) z \rangle_0 > 0$ for all $g$ (since $\rho_0(g)$ is invertible). By compactness of $G$ and continuity, the integral $\langle z, z \rangle_G > 0$.

2. **$G$-invariance:** For any $h \in G$:
   $$\langle \rho_0(h) z, \rho_0(h) z' \rangle_G = \int_G \langle \rho_0(g) \rho_0(h) z, \rho_0(g) \rho_0(h) z' \rangle_0 \, dg$$
   Substituting $g' = gh$ and using left-invariance of Haar measure ($dg' = dg$):
   $$= \int_G \langle \rho_0(g') z, \rho_0(g') z' \rangle_0 \, dg' = \langle z, z' \rangle_G$$
   Therefore $\rho_0(h)$ is orthogonal (actually unitary) with respect to $\langle \cdot, \cdot \rangle_G$ for all $h \in G$.

We typically choose $\rho$ such that $\rho(g) \in O(d_z)$ for all $g$, ensuring:

$$
\langle \rho(g) z, \rho(g) z' \rangle = \langle z, z' \rangle \quad \forall z, z', g
$$
This preserves the Euclidean structure of $\mathcal{Z}$.

**Remark on Peter-Weyl theorem:** The Peter-Weyl theorem states that for a compact group $G$, the renormalized matrix coefficients $\sqrt{\dim(\pi)} \cdot u_{ij}^{(\pi)}(g)$ (where $u_{ij}^{(\pi)}$ are matrix elements of irreducible unitary representations $\pi$) form an orthonormal basis for $L^2(G)$ with respect to normalized Haar measure. This theorem is foundational to representation theory and is intimately connected to the averaging construction: the Haar measure averaging used above is the same measure appearing in the Peter-Weyl decomposition {cite}`peter1927theorie,weyl1946classical`.

**Units:** $[\rho(g)]$ is dimensionless (orthogonal/unitary matrices have dimensionless entries).
:::

:::{prf:definition} G-Equivariant Operator
:label: def-g-equivariant-operator

Let $G$ be a group with representations $\rho: G \to GL(\mathbb{R}^{d_z})$ and $\rho': G \to GL(\mathbb{R}^{d_{z'}})$ on latent spaces $\mathcal{Z} = \mathbb{R}^{d_z}$ and $\mathcal{Z}' = \mathbb{R}^{d_{z'}}$ respectively.

A neural operator $f: \mathcal{Z} \to \mathcal{Z}'$ is **$G$-equivariant** (or **$(\rho, \rho')$-equivariant** when representations need emphasis) if:

$$
f(\rho(g) z) = \rho'(g) f(z) \quad \forall g \in G, \; z \in \mathcal{Z}
$$

where matrix-vector multiplication is implied.

**Physical interpretation:** Transforming the input then applying $f$ gives the same result as applying $f$ then transforming the output. Formally, the following diagram commutes:

$$
\begin{array}{ccc}
\mathcal{Z} & \xrightarrow{f} & \mathcal{Z}' \\
\downarrow \rho(g) & & \downarrow \rho'(g) \\
\mathcal{Z} & \xrightarrow{f} & \mathcal{Z}'
\end{array}
$$

**Units:** If $[z] = [z']$ (both in units of $\sqrt{\text{nat}}$), then $[f(z)] = [z']$ and $[\rho(g)] = [\rho'(g)] = $ dimensionless.
:::

:::{prf:definition} Fragile Gauge Group
:label: def-fragile-gauge-group

The Fragile framework adopts the gauge structure:

$$
G_{\text{Fragile}} = SU(N_f)_C \times SU(2)_L \times U(1)_Y
$$

**Source:** This gauge group emerges from the multi-agent consistency requirements under capacity constraints $C < \infty$ (Chapter 8.1, {ref}`sec-symplectic-multi-agent-field-theory`). The full derivation from first principles—showing why this specific product structure is necessary and sufficient—is provided in Chapter 8, Sections 8.1-8.3.

**Important qualification:** This chapter **assumes** the gauge structure and demonstrates its implementation in neural architectures. We do not re-derive the gauge group here; instead, we cite the field-theoretic analysis from Chapter 8.1. Readers seeking the foundational derivation should consult that chapter first.

**Explicit definitions:**

1. **$SU(N_f)_C$ (Color symmetry):** The special unitary group of degree $N_f$, consisting of $N_f \times N_f$ complex unitary matrices $U$ with $\det(U) = 1$. Here $N_f$ equals the number of feature bundles $n_b$ (see Definition {prf:ref}`def-latent-vector-bundle`). This acts on the bundle index, permitting feature mixing across bundles.

2. **$SU(2)_L$ (Weak isospin):** The special unitary group of degree 2, consisting of $2 \times 2$ complex unitary matrices with determinant 1. This has 3 real parameters (Pauli matrix basis) and acts on observation-action doublets.

3. **$U(1)_Y$ (Hypercharge):** The circle group $\{e^{i\theta} : \theta \in [0, 2\pi)\} \cong SO(2)$, representing phase rotations. This is associated with a capacity bound (holographic bound).

**Representation on latent space:** For a latent space $\mathcal{Z} = \mathbb{R}^{d_z}$ decomposed into $n_b$ bundles of dimension $d_b$ (so $d_z = n_b \cdot d_b$), we construct the representation $\rho: G_{\text{Fragile}} \to GL(d_z, \mathbb{R})$ through **real forms** of the complex gauge groups.

**Challenge:** $SU(N_f)$ and $SU(2)$ are complex Lie groups (acting on $\mathbb{C}^{N_f}$ and $\mathbb{C}^2$ respectively), but our latent space $\mathcal{Z} = \mathbb{R}^{d_z}$ is real. We need a **real representation** compatible with neural network operations.

**Construction via real forms:**

1. **$SU(N_f)_C$ real representation:** With $N_f = n_b$ (number of bundles), we model bundle mixing with a real orthogonal action on the bundle index. This is a pragmatic, real-valued proxy for $SU(N_f)_C$ that preserves bundle norms and keeps the bundle count fixed. In block form:
   
   $$
   \rho_C(R) = \begin{pmatrix}
   R_{11} I_{d_b} & R_{12} I_{d_b} & \cdots & R_{1n_b} I_{d_b} \\
   R_{21} I_{d_b} & R_{22} I_{d_b} & \cdots & R_{2n_b} I_{d_b} \\
   \vdots & \vdots & \ddots & \vdots \\
   R_{n_b 1} I_{d_b} & R_{n_b 2} I_{d_b} & \cdots & R_{n_b n_b} I_{d_b}
   \end{pmatrix} \in GL(d_z, \mathbb{R})
   $$
   where $R = (R_{ij}) \in SO(n_b)$ is a real orthogonal mixing matrix on bundle indices (an architectural proxy for $SU(N_f)_C$), and $I_{d_b}$ is the $d_b \times d_b$ identity (each bundle block rotates together).

   **Explicit $SU(n_b) \to SO(2n_b)$ realification (faithful):** There is no canonical homomorphism $SU(N) \to SO(N)$ in general. A standard realification yields a faithful embedding into $SO(2N)$ as follows:

   1. **Complex to real decomposition:** Any $U \in SU(n_b)$ can be written as $U = A + iB$ where $A, B \in \mathbb{R}^{n_b \times n_b}$.

   2. **Block real form:** This induces a real representation $\rho_{\mathbb{R}}: SU(n_b) \to SO(2n_b)$ given by:
      
      $$
      U = A + iB \mapsto \begin{pmatrix} A & -B \\ B & A \end{pmatrix} \in SO(2n_b)
      $$
      This is an embedding preserving orthogonality: $\|Uz\|^2 = \|z\|^2$ for $z \in \mathbb{C}^{n_b}$ translates to the block matrix preserving $\mathbb{R}^{2n_b}$ norm.

   **Verification of homomorphism property:**

   1. **Identity preservation:** $I_N + i \cdot 0 \mapsto \mathrm{diag}(I_N, I_N) = I_{2N}$ ✓

   2. **Multiplicativity:** For $U_1, U_2 \in SU(N)$ with $U_k = A_k + iB_k$, the product $U_1 U_2 = (A_1 A_2 - B_1 B_2) + i(A_1 B_2 + B_1 A_2)$ satisfies:
      $$\rho_{\mathbb{R}}(U_1 U_2) = \begin{pmatrix} A_1 A_2 - B_1 B_2 & -(A_1 B_2 + B_1 A_2) \\ A_1 B_2 + B_1 A_2 & A_1 A_2 - B_1 B_2 \end{pmatrix}$$
      Direct computation of block matrix multiplication gives:
      $$\rho_{\mathbb{R}}(U_1) \rho_{\mathbb{R}}(U_2) = \begin{pmatrix} A_1 & -B_1 \\ B_1 & A_1 \end{pmatrix} \begin{pmatrix} A_2 & -B_2 \\ B_2 & A_2 \end{pmatrix} = \begin{pmatrix} A_1 A_2 - B_1 B_2 & -A_1 B_2 - B_1 A_2 \\ B_1 A_2 + A_1 B_2 & -B_1 B_2 + A_1 A_2 \end{pmatrix}$$
      Therefore $\rho_{\mathbb{R}}(U_1 U_2) = \rho_{\mathbb{R}}(U_1) \rho_{\mathbb{R}}(U_2)$, confirming the homomorphism property.

   3. **Orthogonality:** The unitarity condition $U^* U = I$ for $U = A + iB$ decomposes into $A^T A + B^T B = I_N$ (real part) and $A^T B = B^T A$ (imaginary part, implying $A^T B$ is symmetric). Computing:
      $$\rho_{\mathbb{R}}(U)^T \rho_{\mathbb{R}}(U) = \begin{pmatrix} A^T & B^T \\ -B^T & A^T \end{pmatrix} \begin{pmatrix} A & -B \\ B & A \end{pmatrix} = \begin{pmatrix} A^T A + B^T B & -A^T B + B^T A \\ -B^T A + A^T B & B^T B + A^T A \end{pmatrix}$$
      Using $A^T B = B^T A$ and $A^T A + B^T B = I_N$ yields $\rho_{\mathbb{R}}(U)^T \rho_{\mathbb{R}}(U) = I_{2N}$, so $\rho_{\mathbb{R}}(U) \in O(2N)$.

   4. **Determinant:** Using the block determinant identity:
      $$\det(\rho_{\mathbb{R}}(U)) = \det(A + iB)\det(A - iB) = \det(U)\det(\bar{U}) = |\det(U)|^2 = 1$$
      confirming $\rho_{\mathbb{R}}: SU(N) \to SO(2N)$.

   **Implementation note:** The faithful $SO(2n_b)$ realification doubles the bundle-index dimension. In practice, the full $SU(n_b)$ symmetry is **broken to a discrete subgroup** (permutations and sign flips of bundles), which naturally embeds in $SO(n_b)$ as signed permutation matrices. This is enforced implicitly through the isotropic architecture design, not by explicitly constructing gauge transformations.

2. **$SU(2)_L$ real representation:** The adjoint representation identifies the Lie algebras $\mathfrak{su}(2) \cong \mathfrak{so}(3)$ and yields a surjective homomorphism $\mathrm{Ad}: SU(2) \to SO(3)$ with kernel $\{\pm I\}$ {cite}`hall2015lie`. This gives a 3D real representation that factors through $SO(3)$ (so it is **not** faithful on $SU(2)$). A faithful real representation is obtained by realifying the fundamental doublet $SU(2) \curvearrowright \mathbb{C}^2$, giving an embedding $SU(2) \hookrightarrow SO(4)$ (equivalently, $SU(2)\cong Sp(1)$ acting on quaternions $\mathbb{H}\cong \mathbb{R}^4$ by left multiplication).

   **Architecture choice — spontaneous symmetry breaking:** In practice, the full $SU(2)_L$ symmetry is **broken to a $U(1)$ subgroup** (a maximal torus; all such subgroups are conjugate to the diagonal subgroup $\mathrm{diag}(e^{i\theta}, e^{-i\theta}) \in SU(2)$). In our real-valued implementation we realize this $U(1)$ action as the standard $SO(2)$ rotation mixing the observation-action doublet components (generated by $i\sigma_2$ in the Pauli basis):
   
   $$
   \rho_L: U(1) \hookrightarrow SU(2)_L \xrightarrow{\text{real form}} SO(2) \subset GL(2, \mathbb{R})
   $$
   acting on $(z_{\text{obs}}, z_{\text{act}}) \in \mathbb{R}^2 \subset \mathcal{Z}$.

   **Justification for symmetry breaking:** The reduction from $SU(2)_L$ to $U(1) \subset SU(2)_L$ is consistent with the weak isospin structure in the Standard Model, where the unbroken subgroup after electroweak symmetry breaking is $U(1)_{\text{EM}} \subset SU(2)_L \times U(1)_Y$. In the neural architecture, only the $U(1)$ rotation symmetry between observation and action channels is explicitly enforced; a faithful real implementation of the full $SU(2)$ doublet would require complex features (or its 4D realification), which is incompatible with a 2D real obs-action plane. See the remark in Definition {prf:ref}`def-obs-action-doublet` on real-valued implementations.

3. **$U(1)_Y$ real representation:** The hypercharge symmetry $U(1)_Y = \{e^{i\theta} : \theta \in [0, 2\pi)\}$ acts on complex fields as phase rotations $\psi \mapsto e^{iY\theta} \psi$, preserving $|\psi|^2$. The real representation is:
   
   $$
   \rho_Y: U(1)_Y \to SO(2) \subset GL(2, \mathbb{R}), \quad e^{i\theta} \mapsto \begin{pmatrix} \cos(Y\theta) & -\sin(Y\theta) \\ \sin(Y\theta) & \cos(Y\theta) \end{pmatrix}
   $$
   where $Y \in \mathbb{R}$ is the hypercharge quantum number. This is a rotation in a 2D subspace with angular velocity proportional to $Y$.

   **Architecture implementation via norm preservation:** In practice, $U(1)_Y$ invariance is enforced through **Lipschitz constraints** $\sigma_{\max}(W) \leq 1$ on all linear operators (Definition {prf:ref}`def-spectral-linear`). These constraints ensure:
   
   $$
   \|W z\| \leq \|z\| \quad \forall z \in \mathcal{Z}
   $$
   which preserves the total "charge" $\|z\|^2$ up to a maximum value (holographic bound $C < \infty$). The spectral bound implements **non-expansiveness**, making operators contractive or (in the limit) isometric; this relaxes strict $U(1)$ rotations to norm-non-increasing maps that still preserve a scalar charge. Theorem {prf:ref}`thm-spectral-preserves-hypercharge` proves that composing spectrally normalized layers maintains $\|z_t\| \leq \|z_0\|$ (non-increasing across layers), consistent with capacity constraints.

   **Relationship to hypercharge conservation:** In the Standard Model, hypercharge $Y$ is a conserved quantum number satisfying $Q = T_3 + Y/2$ (electric charge formula). In the neural architecture, the analog is **total information content** $I(X_t; Z_t) \leq C$, which is bounded by the holographic principle. Spectral normalization ensures the *linear* operators are non-expansive ($\|Wz\| \leq \|z\|$); keeping the overall network within capacity additionally requires controlling the gain of nonlinear blocks (e.g., via rescaled NormGate or explicit Lipschitz tracking).

**Product representation:** The full representation is the **direct sum** (⊕) of these factors acting on disjoint subspaces:

$$
\rho(U_C, U_L, e^{i\theta_Y}) = \rho_C(U_C) \oplus \rho_L(U_L) \oplus \rho_Y(e^{i\theta_Y})
$$
in block-diagonal form (each factor acts on its own subspace).

**Notation clarification:** We use the direct sum ⊕ (not tensor product ⊗) as an architectural simplification:
- **Direct sum $V \oplus W$:** Dimension = $\dim(V) + \dim(W)$. Transformations are block-diagonal: $\begin{pmatrix} A & 0 \\ 0 & B \end{pmatrix}$
- **Tensor product $V \otimes W$:** Dimension = $\dim(V) \times \dim(W)$. Transformations are Kronecker products: $A \otimes B$

In this model, each gauge factor acts independently on a designated subspace of $\mathcal{Z}$, so the representation space is $\mathcal{Z} = \mathcal{Z}_C \oplus \mathcal{Z}_L \oplus \mathcal{Z}_Y$ (direct sum), and the group representation is the direct sum of individual representations.

**Derivation of direct sum structure:**

The choice of direct sum over tensor product is **architectural**, not fundamental. Here's why:

1. **Tensor product would be physically correct:** In gauge theory, matter fields typically transform under tensor product representations. For example, quarks transform as $(\mathbf{3}, \mathbf{2}, 1/6)$ under $SU(3)_C \times SU(2)_L \times U(1)_Y$, meaning the representation space is $V_C \otimes V_L \otimes V_Y$ with dimension $3 \times 2 \times 1 = 6$ per flavor.

2. **Direct sum is a simplifying assumption:** We decompose $\mathcal{Z} = \mathcal{Z}_C \oplus \mathcal{Z}_L \oplus \mathcal{Z}_Y$ with each subspace transforming independently. This reduces computational complexity:
   - Tensor product: $d_z = d_C \times d_L \times d_Y$ (exponential growth)
   - Direct sum: $d_z = d_C + d_L + d_Y$ (linear scaling)

3. **Physical interpretation:** The direct sum structure corresponds to **separate degrees of freedom** for color, weak isospin, and hypercharge. This is analogous to decomposing a particle state into spin, flavor, and color quantum numbers as independent labels, rather than a fully entangled state.

4. **Consistency with architecture:** The bundle structure (Definition {prf:ref}`def-latent-vector-bundle`) already assumes $\mathcal{Z} = \bigoplus_{i=1}^{n_b} V_i$ where each bundle $V_i$ is invariant under $\rho_C$. The $SU(2)_L$ and $U(1)_Y$ factors act on additional subspaces orthogonal to the bundle subspace.

**Implication:** This choice means we enforce gauge symmetry **separately** for each factor, not jointly. Full tensor product representations could be implemented but would require rethinking the bundle decomposition structure.

**Critical caveat:** This architectural simplification means the implementation does **not** fully realize the gauge-theoretic structure derived in Chapter 8.1. The direct sum is a **practical approximation** that preserves gauge covariance at the level of individual factors while avoiding the combinatorial explosion of full tensor product representations. Future work could explore whether tensor product architectures provide empirical benefits justifying the increased complexity.

**Implementation note:** In practice, neural network architectures do NOT implement the full gauge group action explicitly. Instead, we build **equivariant primitives**:
- IsotropicBlock (Definition {prf:ref}`def-isotropic-block`) is equivariant w.r.t. $\rho_C$ (bundle mixing, Theorem {prf:ref}`thm-isotropic-preserves-color`)
- SteerableConv (Section {ref}`sec-covariant-retina`) is equivariant w.r.t. $\rho_L$ (obs-action doublet, Proposition {prf:ref}`prop-obs-action-doublet`)
- SpectralLinear (Definition {prf:ref}`def-spectral-linear`) preserves $\rho_Y$ (hypercharge bound, Theorem {prf:ref}`thm-spectral-preserves-hypercharge`)

**Remark on complex vs. real:** Physicists typically work with complex representations because quantum mechanics is inherently complex (wavefunctions are in $\mathbb{C}$). Neural networks are real-valued (weights in $\mathbb{R}$), so we use real forms. The **isomorphism** $SU(2) \cong \text{Spin}(3) \to SO(3)$ and $SU(N) \supset SO(N)$ (via embedding) allow translation between complex and real pictures. See Section {ref}`sec-symplectic-multi-agent-field-theory` for the complex gauge field formulation; here we use the real neural implementation.

**Requirement:** All neural operators in the latent dynamics must be $G_{\text{Fragile}}$-equivariant to preserve physical consistency.

**Implication:** Standard building blocks (ReLU, LayerNorm, biased Linear) that violate even simple $SO(d)$ equivariance cannot be used directly.
:::

:::{prf:theorem} ReLU Violates SO(d) Equivariance
:label: thm-relu-breaks-equivariance

Let $d \geq 2$ and consider the standard representation $\rho: SO(d) \to GL(d, \mathbb{R})$ where $\rho(R) = R$ (i.e., rotations act by matrix multiplication).

Define the ReLU activation $f: \mathbb{R}^d \to \mathbb{R}^d$ by:

$$
f(z) = (f(z_1), \ldots, f(z_d)) \quad \text{where} \quad f(z_i) = \max(0, z_i)
$$

Then $f$ is **not** $SO(d)$-equivariant with respect to $\rho$.

*Proof.*

We prove by explicit counterexample for $d=2$, then indicate generalization.

**Step 1. Counterexample construction ($d=2$):**

Let $z = (1, -1)^T \in \mathbb{R}^2$. Then:

$$
f(z) = (\max(0, 1), \max(0, -1))^T = (1, 0)^T
$$

**Step 2. Apply rotation:**

Let $R_\theta \in SO(2)$ be counterclockwise rotation by $\theta = \pi/4$:

$$
R_{\pi/4} = \begin{pmatrix} \cos(\pi/4) & -\sin(\pi/4) \\ \sin(\pi/4) & \cos(\pi/4) \end{pmatrix} = \begin{pmatrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{pmatrix}
$$

Then:

$$
R_{\pi/4} z = \begin{pmatrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 1 \\ -1 \end{pmatrix} = \begin{pmatrix} \frac{1+1}{\sqrt{2}} \\ \frac{1-1}{\sqrt{2}} \end{pmatrix} = \begin{pmatrix} \sqrt{2} \\ 0 \end{pmatrix}
$$

**Step 3. Compute $f(R_{\pi/4} z)$:**

$$
f(R_{\pi/4} z) = f((\sqrt{2}, 0)^T) = (\sqrt{2}, 0)^T
$$

**Step 4. Compute $R_{\pi/4} f(z)$:**

$$
R_{\pi/4} f(z) = R_{\pi/4} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}
$$

**Step 5. Verify non-equality:**

$$
f(R_{\pi/4} z) = (\sqrt{2}, 0)^T \neq \left(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right)^T = R_{\pi/4} f(z)
$$

Thus $f(R_{\pi/4} z) \neq R_{\pi/4} f(z)$, violating $SO(2)$-equivariance.

**Generalization to $d > 2$:**

For arbitrary $d \geq 2$, we construct an analogous counterexample.

**Setup:** Let $z = (1, -1, 0, \ldots, 0)^T \in \mathbb{R}^d$ (first two components nonzero, rest zero).

Define $R_\theta \in SO(d)$ as the block-diagonal matrix:

$$
R_\theta = \begin{pmatrix}
R_2(\theta) & 0 \\
0 & I_{d-2}
\end{pmatrix}
$$
where $R_2(\theta) = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix}$ is the $2 \times 2$ rotation by angle $\theta = \pi/4$, and $I_{d-2}$ is the $(d-2) \times (d-2)$ identity matrix.

**Verification that $R_\theta \in SO(d)$:**
1. **Orthogonality:** $R_\theta^T R_\theta = \begin{pmatrix} R_2^T R_2 & 0 \\ 0 & I_{d-2} \end{pmatrix} = \begin{pmatrix} I_2 & 0 \\ 0 & I_{d-2} \end{pmatrix} = I_d$ follows from block structure and $R_2^T R_2 = I_2$.
2. **Determinant:** $\det(R_\theta) = \det(R_2) \cdot \det(I_{d-2}) = 1 \cdot 1 = 1$.

**Step 1. Compute $R_\theta z$:**

Using the block-diagonal structure:

$$
R_\theta z = \begin{pmatrix} R_2(\pi/4) & 0 \\ 0 & I_{d-2} \end{pmatrix} \begin{pmatrix} 1 \\ -1 \\ 0 \\ \vdots \\ 0 \end{pmatrix} = \begin{pmatrix} R_2(\pi/4) \begin{pmatrix} 1 \\ -1 \end{pmatrix} \\ 0_{d-2} \end{pmatrix}
$$

Computing the $2 \times 2$ block (from Step 2 of the $d=2$ case):

$$
R_2(\pi/4) \begin{pmatrix} 1 \\ -1 \end{pmatrix} = \begin{pmatrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 1 \\ -1 \end{pmatrix} = \begin{pmatrix} \sqrt{2} \\ 0 \end{pmatrix}
$$

Therefore:

$$
R_\theta z = (\sqrt{2}, 0, 0, \ldots, 0)^T
$$

**Step 2. Compute $f(R_\theta z)$:**

Since all components of $R_\theta z$ are non-negative (first component $\sqrt{2} > 0$, rest are $0$):

$$
f(R_\theta z) = (\max(0, \sqrt{2}), \max(0, 0), \ldots, \max(0, 0))^T = (\sqrt{2}, 0, 0, \ldots, 0)^T
$$

**Step 3. Compute $f(z)$:**

Component-wise application of $\max(0, \cdot)$:

$$
f(z) = (\max(0, 1), \max(0, -1), \max(0, 0), \ldots, \max(0, 0))^T = (1, 0, 0, \ldots, 0)^T
$$

**Step 4. Compute $R_\theta f(z)$:**

$$
R_\theta f(z) = \begin{pmatrix} R_2(\pi/4) & 0 \\ 0 & I_{d-2} \end{pmatrix} \begin{pmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{pmatrix} = \begin{pmatrix} R_2(\pi/4) \begin{pmatrix} 1 \\ 0 \end{pmatrix} \\ 0_{d-2} \end{pmatrix}
$$

Computing:

$$
R_2(\pi/4) \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}
$$

Therefore:

$$
R_\theta f(z) = \left(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0, \ldots, 0\right)^T
$$

**Step 5. Verify non-equality:**

Comparing the first two components:
- $f(R_\theta z) = (\sqrt{2}, 0, \ldots)^T$
- $R_\theta f(z) = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, \ldots)^T$

Since $\sqrt{2} \neq \frac{1}{\sqrt{2}}$ and $0 \neq \frac{1}{\sqrt{2}}$, we have:

$$
f(R_\theta z) \neq R_\theta f(z)
$$

This holds for all $d \geq 2$. $\square$

**Geometric interpretation:** The ReLU creates coordinate-aligned decision boundaries $\{z \in \mathbb{R}^d : z_i = 0\}$ for each $i$. These hyperplanes are not invariant under $SO(d)$: rotation maps the hyperplane $\{z_1 = 0\}$ to a different hyperplane (no longer aligned with any coordinate axis). Thus $f$ transforms the kink surfaces under rotation, violating equivariance.

**Remark (General groups):** This argument generalizes to any Lie group $G$ with a nontrivial linear representation $\rho: G \to GL(d, \mathbb{R})$. "Nontrivial" means there exists $g \in G$ such that $\rho(g) e_i \neq \pm e_i$ for some basis vector $e_i$. For $G_{\text{Fragile}} = SU(N_f)_C \times SU(2)_L \times U(1)_Y$, each factor acts nontrivially on its respective subspace (bundles, observation-action doublet, capacity), so ReLU violates $G_{\text{Fragile}}$-equivariance.

	$\square$
:::

:::{prf:corollary} ReLU Violates Smoothness Requirements for WFR Dynamics
:label: cor-relu-breaks-wfr

The WFR geometry (Chapter 5, Section {ref}`sec-wasserstein-fisher-rao-geometry-unified-transport-on-hybrid-state-spaces`) provides the dynamical framework for latent state evolution. The WFR action and geodesic integrator require computing gradient flows:

$$
\frac{dz}{ds} = -G^{ij}(z) \nabla_{z_j} \mathcal{L}_{\text{WFR}}(z)
$$

where $G^{ij}(z)$ is the metric tensor from Theorem {prf:ref}`thm-capacity-constrained-metric-law` and $\mathcal{L}_{\text{WFR}}$ is the WFR action functional.

**Smoothness requirement:** Gradient-based geodesic integrators (e.g., Boris-BAOAB in Section 5.4) require $\mathcal{L}_{\text{WFR}}$ to be at least $C^1$ (continuously differentiable) to compute well-defined gradients $\nabla_z \mathcal{L}$.

**ReLU violates this:** By Theorem {prf:ref}`thm-relu-breaks-equivariance`, ReLU creates non-differentiable kinks at coordinate hyperplanes $\{z \in \mathcal{Z} : z_i = 0\}$ for each $i = 1, \ldots, d_z$.

*Explicit derivation:*

**Step 1. Network with ReLU activation:**
Consider a simple network layer $f(z) = \max(0, Wz + b)$ where $W \in \mathbb{R}^{d \times d}$, $b \in \mathbb{R}^d$. The derivative is:

$$
\frac{\partial f_i}{\partial z_j} = \begin{cases}
W_{ij} & \text{if } (Wz + b)_i > 0 \\
0 & \text{if } (Wz + b)_i < 0 \\
\text{undefined} & \text{if } (Wz + b)_i = 0
\end{cases}
$$

**Step 2. WFR action functional dependence:**
The WFR action $\mathcal{L}_{\text{WFR}}[z]$ depends on the value function $V(z)$ and policy $\pi(a|z)$.

**Illustrative functional form:** For a capacity-constrained agent (Chapter 4, Theorem {prf:ref}`thm-equivalence-entropy-regularized-control`), a typical action functional is:

$$
\mathcal{L}_{\text{WFR}}[z] = V(f(z)) + \lambda I(\pi(\cdot|f(z)))
$$
where:
- $V: \mathcal{Z} \to \mathbb{R}$ is the expected cumulative reward (value function)
- $I(\pi(\cdot|f(z))) = I(A; f(Z))$ is mutual information between actions $A$ and latent state $f(Z)$
- $\lambda$ is the Lagrange multiplier enforcing capacity constraint $I(A; Z) \leq C$ (nat/step)
- $f$ represents the network transformation pipeline (which may contain ReLU non-differentiability)

**Source:** This form derives from the bounded-rationality variational principle (see Chapter 5, Section {ref}`sec-wfr-action-functional`, Equation 5.12). The argument below applies to **any** action functional requiring smooth gradients; we use this as a concrete example to demonstrate ReLU incompatibility.

**Step 3. Chain rule breakdown:**
To compute $\nabla_z \mathcal{L}_{\text{WFR}}$, we need:

$$
\frac{\partial \mathcal{L}}{\partial z_j} = \sum_i \frac{\partial \mathcal{L}}{\partial f_i} \cdot \frac{\partial f_i}{\partial z_j}
$$
At kink points where $(Wz + b)_i = 0$, the term $\frac{\partial f_i}{\partial z_j}$ is undefined, causing $\nabla_z \mathcal{L}$ to be undefined.

**Step 4. Consequences at kinks:**

1. **Gradient undefined:** $\nabla_z \mathcal{L}$ does not exist in the classical sense (left derivative $\lim_{h \to 0^-}$ differs from right derivative $\lim_{h \to 0^+}$)

2. **Fisher metric ill-defined:** The Fisher information metric $\mathcal{F}_{ij}(z) = \mathbb{E}_{a \sim \pi(\cdot|z)}[\partial_i \log \pi(a|z) \partial_j \log \pi(a|z)]$ requires computing $\partial_i \log \pi(a|z)$. If the policy network $\pi(a|z)$ is parameterized by a network with ReLU activations, then $\pi(a|z)$ is non-differentiable at kink points, causing $\partial_i \log \pi(a|z)$ to be undefined. Consequently, the Fisher metric tensor $\mathcal{F}(z)$ cannot be computed in the classical sense at these points.

   The capacity-constrained metric $G(z)$ from Theorem {prf:ref}`thm-capacity-constrained-metric-law` depends on the risk tensor $T_{ij}$, which in turn depends on gradients of the value function and the Fisher metric. If either $V$ or $\pi$ uses ReLU activations, the metric $G(z)$ will have ill-defined components at kink loci.

3. **Integration errors:** Boris-BAOAB integrator (Definition {prf:ref}`def-baoab-splitting`) uses $\frac{dz}{ds} = \mathcal{M}_{\text{curl}}\!\left(-G^{-1}\nabla \mathcal{L}\right)$ with $\mathcal{M}_{\text{curl}} := (I - \beta_{\text{curl}} G^{-1}\mathcal{F}_{\text{curl}})^{-1}$ (Value Curl; Definition {prf:ref}`def-value-curl`). At kinks, the discontinuous gradient causes ill-defined integration steps. While one could use subgradients or generalized gradients at kinks, this introduces numerical instability and prevents the integrator from preserving the symplectic structure required for long-term energy conservation

**Gauge-dependence problem:** Per Theorem {prf:ref}`thm-relu-breaks-equivariance`, ReLU kinks are coordinate-dependent. Under gauge transformation $z \mapsto U(g) \cdot z$ for $g \in G_{\text{Fragile}}$, the kink locations transform but ReLU does not transform equivariantly. This creates **inconsistent kink patterns** across gauge choices, causing geodesic flows to depend on arbitrary coordinate choices.

**Consequence:** Smooth, gauge-equivariant activations (e.g., GELU in NormGate, Definition {prf:ref}`def-norm-gated-activation`) are necessary for well-defined WFR gradient flows and gauge-invariant dynamics.

**Reference to WFR smoothness:** The WFR formulation's smoothness requirements are established through:
- Variational calculus on action functionals (standard $C^1$ requirement)
- Riemannian geometry for geodesic equations (smooth metric tensor)
- Symplectic integrator theory (Lipschitz gradients for Boris-BAOAB)

See Section 5.2 (WFR stress-energy tensor) and Part II, Hypostructure, Section 9 (Mathematical Prerequisites) for differential geometry foundations.

$\square$
:::

:::{prf:definition} Bundle Decomposition of Latent Space
:label: def-latent-vector-bundle

Let $\mathcal{Z} = \mathbb{R}^{n_b \cdot d_b}$ be the latent space. A **bundle decomposition** partitions $\mathcal{Z}$ into $n_b$ **bundles** (subspaces) of dimension $d_b$:

$$
\mathcal{Z} = \bigoplus_{i=1}^{n_b} V_i, \quad V_i \cong \mathbb{R}^{d_b}, \quad n_b \times d_b = \dim(\mathcal{Z})
$$

Each bundle $V_i$ carries a representation of the rotation group $SO(d_b)$. For $g_i \in SO(d_b)$ and $v_i \in V_i$:

$$
\rho_i(g_i) \cdot v_i = g_i \cdot v_i \quad \text{(matrix multiplication)}
$$

where we identify $SO(d_b) \subset GL(d_b, \mathbb{R})$ as the subgroup of orthogonal matrices with determinant 1. This is the **defining (standard) representation** of $SO(d_b)$.

**Product gauge group (derivation):** Given the direct sum decomposition $\mathcal{Z} = \bigoplus_{i=1}^{n_b} V_i$, what is the maximal symmetry group?

**Claim:** The full symmetry group preserving the decomposition is:

$$
G_{\text{bundle}} = \prod_{i=1}^{n_b} SO(d_b)
$$

**Justification:**
1. **Preservation of decomposition:** Any symmetry must preserve $V_i \perp V_j$ for $i \neq j$ (orthogonal direct sum). Thus transformations cannot mix bundles.

2. **Within each bundle:** On $V_i \cong \mathbb{R}^{d_b}$, the maximal continuous symmetry group preserving the Euclidean inner product is $O(d_b)$ (orthogonal group).

3. **Orientation preservation:** For neural networks with deterministic forward pass, we require **orientation-preserving** transformations (continuous deformation from identity). Thus we restrict to $SO(d_b) \subset O(d_b)$ (special orthogonal: determinant +1).

4. **Independence:** Transformations on different bundles are independent, giving the product structure $\prod_{i=1}^{n_b} SO(d_b)$.

**Group action:** For $(g_1, \ldots, g_{n_b}) \in G_{\text{bundle}}$ and $z = (z^{(1)}, \ldots, z^{(n_b)})$ with $z^{(i)} \in V_i$:

$$
\rho(g_1, \ldots, g_{n_b}) \cdot z = (g_1 z^{(1)}, \ldots, g_{n_b} z^{(n_b)})
$$

In matrix form (with respect to concatenated basis):

$$
\rho(g_1, \ldots, g_{n_b}) = \text{diag}(g_1, \ldots, g_{n_b}) = \begin{pmatrix} g_1 & 0 & \cdots & 0 \\ 0 & g_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & g_{n_b} \end{pmatrix}
$$
(block-diagonal structure).

**Note on bundle permutations:** If bundles are **semantically distinguished** (e.g., bundle 1 = edges, bundle 2 = textures, bundle 3 = colors), we cannot permute them. If all bundles are **identical** (homogeneous feature space), the symmetry group extends to $(\prod_{i=1}^{n_b} SO(d_b)) \rtimes S_{n_b}$ where $S_{n_b}$ is the permutation group. For this architecture, we assume distinguished bundles.

**Units:** $[V_i] = [\mathcal{Z}] = \sqrt{\text{nat}}$ (from capacity constraint, see Section {ref}`sec-dimensional-analysis`).

**Remark:** This is a **direct sum decomposition with group action**, not a fiber bundle in the differential-geometric sense (which would require a base manifold and projection map). Analogous to gauge fields in physics (Chapter {ref}`sec-symplectic-multi-agent-field-theory`): just as the Error field $W_\mu$ transforms under $SU(2)_L$, bundles transform under their respective $SO(d_b)$ factors.
:::

:::{prf:definition} Spectral Linear Operator
:label: def-spectral-linear

A linear map $W: \mathcal{Z} \to \mathcal{Z}'$ is **spectrally normalized** if:

$$
\sigma_{\max}(W) \leq 1
$$

where $\sigma_{\max}(W)$ is the largest singular value of $W$.

**Remark (Singular values):** The singular values of $W$ are the square roots of the eigenvalues of $W^T W$. The largest singular value $\sigma_{\max}(W) = \sup_{\|z\|=1} \|Wz\|$ is the operator norm induced by the Euclidean norm. This follows from the singular value decomposition: $W = U \Sigma V^T$ where $U, V$ are orthogonal and $\Sigma$ is diagonal with non-negative entries $\sigma_1 \geq \sigma_2 \geq \cdots \geq 0$. Then $\|Wz\| = \|U \Sigma V^T z\| = \|\Sigma V^T z\|$ (since $U$ is orthogonal), which achieves maximum $\sigma_1$ when $V^T z$ aligns with the first singular vector. For a standard reference, see Horn & Johnson, *Matrix Analysis*, Theorem 5.6.2 {cite}`horn2012matrix`.

**Implementation:** $W_{\text{spectral}} = W / \sigma_{\max}(W)$.

**Units:** $[W]$ is dimensionless (if $\mathcal{Z}, \mathcal{Z}'$ normalized).
:::

:::{prf:theorem} Spectral Normalization Preserves Light Cone
:label: thm-spectral-preserves-light-cone

Let $(\mathcal{Z}, \|\cdot\|)$ be a finite-dimensional normed vector space with $\|\cdot\|$ the **Euclidean norm** $\|z\| = \sqrt{z^T z}$. Let $W: \mathcal{Z} \to \mathcal{Z}'$ be a linear map with $\sigma_{\max}(W) \leq 1$ (spectrally normalized). Let $c_{\text{info}}$ be the information speed limit (Axiom {prf:ref}`ax-information-speed-limit`).

Then:

$$
\|W \cdot z\| \leq \|z\| \quad \text{(contraction property)}
$$

This ensures that whenever $d(z_1, z_2) := \|z_1 - z_2\| \leq c_{\text{info}} \cdot \Delta t$ (causal interval), we have:

$$
d(W \cdot z_1, W \cdot z_2) \leq c_{\text{info}} \cdot \Delta t
$$

Thus the **light cone** $\mathcal{C}(z_0, t_0) := \{(z, t) : \|z - z_0\| \leq c_{\text{info}}(t - t_0), \, t \geq t_0\}$ in the extended state-time space is preserved: if $(z, t) \in \mathcal{C}(z_0, t_0)$, then $(W \cdot z, t) \in \mathcal{C}(W \cdot z_0, t_0)$.

*Proof.*

**Step 1. Spectral bound:** By spectral normalization (Definition {prf:ref}`def-spectral-linear`), $\sigma_{\max}(W) \leq 1$.

**Step 2. Operator norm:** For the Euclidean norm, the induced operator norm satisfies $\|W\|_{\text{op}} = \sigma_{\max}(W)$, where:

$$
\|W\|_{\text{op}} := \sup_{\|z\| = 1} \|W \cdot z\|
$$

Thus for any $z \in \mathcal{Z}$:

$$
\|W \cdot z\| \leq \|W\|_{\text{op}} \cdot \|z\| = \sigma_{\max}(W) \cdot \|z\| \leq \|z\|
$$

**Step 3. Distance preservation:** For any $z_1, z_2 \in \mathcal{Z}$:

$$
\|W \cdot z_1 - W \cdot z_2\| = \|W \cdot (z_1 - z_2)\| \leq \|z_1 - z_2\|
$$

**Step 4. Causal structure:** If the inputs are causally connected ($d(z_1, z_2) \leq c_{\text{info}} \Delta t$), the outputs remain causally connected since:

$$
d(W \cdot z_1, W \cdot z_2) \leq d(z_1, z_2) \leq c_{\text{info}} \Delta t
$$

**Identification:** The Lipschitz constant $L = \sigma_{\max}(W) \leq 1$ ensures signal propagation speed $\leq c_{\text{info}}$, preventing "superluminal" information transfer that would violate the causal structure established in the Speed Window (Theorem {prf:ref}`thm-speed-window`).

**Connection to Node 62:** The CausalityViolationCheck diagnostic verifies $\sigma_{\max}(W) \leq 1 + \epsilon$ during training. Violations indicate faster-than-light gradients breaking temporal coherence.

$\square$
:::

:::{prf:proposition} Bias Terms Break Tangent Bundle Structure
:label: prop-bias-breaks-tangent-bundle

Working in the tangent bundle $T\mathcal{Z}$, elements are pairs $(z, v)$ where $z \in \mathcal{Z}$ is a point and $v \in T_z \mathcal{Z}$ is a tangent vector at $z$.

A gauge-covariant map on the tangent bundle must satisfy:

$$
f(z, \rho(g) \cdot v) = \rho'(g) \cdot f(z, v) \quad \forall g \in G
$$

**Claim:** Adding a bias $b$ violates this unless $b = 0$.

*Proof.* Consider $f(z, v) = W v + b$. Then:

$$
f(z, \rho(g) \cdot v) = W \rho(g) v + b
$$
but

$$
\rho'(g) \cdot f(z, v) = \rho'(g) (W v + b) = \rho'(g) W v + \rho'(g) b
$$

For equivariance, we need $b = \rho'(g) b$ for all $g \in G$. If $G$ acts non-trivially (e.g., $SO(d)$ with $d > 1$), the only fixed point is $b = 0$. $\square$

**Extension to compositions:** For a composition of $L$ layers $F = f_L \circ \cdots \circ f_1$ where each $f_i(v) = W_i v + b_i$:

If $F$ is $G$-equivariant, then $F(\rho(g) v) = \rho(g) F(v)$ for all $g, v$.

**Proof by induction:**
- **Base case** ($L=1$): By the proof above, $b_1 = 0$.
- **Inductive step:** Assume $f_{L-1} \circ \cdots \circ f_1$ is equivariant, so each $b_i = 0$ for $i < L$.

  Then:
  
  $$
  F(\rho(g) v) = f_L((f_{L-1} \circ \cdots \circ f_1)(\rho(g) v)) = f_L(\rho(g) (f_{L-1} \circ \cdots \circ f_1)(v))
  $$
  
  $$
  = W_L \rho(g) h + b_L \quad \text{where } h = (f_{L-1} \circ \cdots \circ f_1)(v)
  $$

  For equivariance with $\rho(g) F(v) = \rho(g) (W_L h + b_L)$, we need $b_L = 0$.

Thus **all biases must vanish** for a composition to be equivariant. $\square$

**Coordinate-free interpretation:** In the tangent bundle $T\mathcal{Z}$, linear maps act on **velocity vectors**, which are equivalence classes of curves $[\gamma]$ where $\gamma(0) = z$, $\dot{\gamma}(0) = v$. Adding a constant $b$ is not well-defined on velocity vectors—it mixes base points (where you are) with tangent directions (which way you're moving), breaking the fiber bundle structure.
:::

:::{prf:definition} Norm-Gated Activation
:label: def-norm-gated-activation

For a vector bundle $v_i \in V_i$, define the **norm-gated activation**:

$$
f(v_i) = v_i \cdot g(\|v_i\| + b_i)
$$

where:
- $\|v_i\| = \sqrt{v_i^T v_i}$ is the Euclidean norm ($SO(d_b)$-invariant)
- $b_i \in \mathbb{R}$ is a learnable scalar bias (the "activation potential")
- $g: \mathbb{R} \to \mathbb{R}$ is a smooth scalar function (e.g., GELU, sigmoid)

**Physical interpretation:** Energy filter with radial symmetry. The gate opens when signal energy $\|v_i\|$ exceeds the potential barrier $-b_i$.

**Units:** All dimensionless if latent vectors normalized.

**Smoothness at the origin (implementation detail):** The norm $\|v\|$ is not differentiable at $v=0$. The map $v \mapsto v \cdot g(\|v\|+b)$ is still $C^1$ under mild regularity of $g$ (the apparent $\|v\|^{-1}$ singularity in the Jacobian cancels as $v \to 0$), but if you require a globally $C^\infty$ map you can replace $\|v\|$ with a smoothed norm $\|v\|_\varepsilon := \sqrt{v^T v + \varepsilon^2}$ in implementations.

**Remark (Choice of gating function $g$):** While any smooth $g: \mathbb{R} \to \mathbb{R}$ preserves equivariance, GELU is a **pragmatic choice** among functions satisfying design constraints:

1. **$C^\infty$ smoothness of $g$ (necessary):** $g$ should be $C^\infty$; combined with a smoothed norm $\|v\|_\varepsilon$ (if global smoothness is required), this yields a $C^\infty$ block compatible with the WFR metric (Corollary {prf:ref}`cor-relu-breaks-wfr`) and geodesic integrator assumptions (Section 5.4).

2. **Linear growth at large arguments (desirable):** For $x \gg 1$, GELU satisfies $g(x) \approx x$, so the gate value scales approximately linearly with energy rather than saturating to a constant gain.

3. **Controlled Lipschitz constant (practical):** $L_g \approx 1.129$ (Lemma {prf:ref}`lem-normgate-lipschitz`), close to 1 and comparable to softplus.

4. **Empirical effectiveness (validation):** Strong performance in transformers {cite}`hendrycks2016gaussian`.

**Critical distinction:** GELU is **not uniquely determined** by first principles. Conditions 1-3 are satisfied by multiple functions (e.g., Swish family $g(x) = x \cdot \sigma(\beta x)$, Softplus). A first-principles derivation selecting GELU uniquely (e.g., via information-geometric optimization) remains an open problem.

**Comparison with alternatives:**

| Activation | Smoothness | $\sup_x \|g'(x)\|$ | Unbounded? | Issue if used |
|------------|------------|-------------------|------------|---------------|
| Sigmoid | $C^\infty$ | $1/4$ | No (saturates to [0,1]) | Caps the gate gain at 1 (reduced dynamic range) |
| Tanh | $C^\infty$ | $1$ | No (saturates to [-1,1]) | Caps the gate gain (and allows sign flips) |
| Softplus | $C^\infty$ | $1$ | Yes (linear at $+\infty$) | Always nonnegative (good if you want a true “gate”) |
| GELU | $C^\infty$ | $\approx 1.129$ | Yes | Slight amplification ($L > 1$), addressed via spectral normalization |

The key advantage of GELU is that high-energy features propagate with energy-dependent gain ($g(x) \to x$ as $x \to \infty$), while sigmoid/tanh force the gate value to saturate to a constant.
:::

:::{prf:theorem} Norm-Gating Preserves SO(d_b) Equivariance
:label: thm-norm-gating-equivariant

Let $f$ be the norm-gated activation (Definition {prf:ref}`def-norm-gated-activation`). Then:

$$
f(R \cdot v) = R \cdot f(v) \quad \forall R \in SO(d_b)
$$

*Proof.*

**Step 1. Decompose activation:** $f(v) = v \cdot h$ where $h = g(\|v\| + b)$ is a scalar.

**Step 2. Norm invariance:** For any $R \in SO(d_b)$ (orthogonal matrix):

$$
\|R \cdot v\| = \sqrt{(R \cdot v)^T (R \cdot v)} = \sqrt{v^T R^T R v} = \sqrt{v^T v} = \|v\|
$$

**Step 3. Scalar invariance:** Since the norm is invariant:

$$
h(R \cdot v) = g(\|R \cdot v\| + b) = g(\|v\| + b) = h(v)
$$

**Step 4. Equivariance:** Combining:

$$
f(R \cdot v) = (R \cdot v) \cdot h(R \cdot v) = (R \cdot v) \cdot h(v) = R \cdot (v \cdot h(v)) = R \cdot f(v)
$$

$\square$
:::

:::{prf:definition} Isotropic Block
:label: def-isotropic-block

The **Isotropic Block** is the atomic unit of gauge-covariant architecture:

$$
\text{IsotropicBlock}(z) = \text{Reshape}(\text{NormGate}(\text{SpectralLinear}(z)))
$$

where:
- **SpectralLinear**: Linear map $W$ with $\sigma_{\max}(W) \leq 1$ (Definition {prf:ref}`def-spectral-linear`) that is **block-diagonal** with respect to the bundle decomposition (see Lemma {prf:ref}`lem-block-diagonal-necessary`)
- **Reshape**: $\mathbb{R}^d \to (\mathbb{R}^{d_b})^{n_b}$ (bundle partition)
- **NormGate**: Norm-gated activation applied per bundle (Definition {prf:ref}`def-norm-gated-activation`)

**Structure constraint:** For **exact** $G_{\text{bundle}} = \prod_{i=1}^{n_b} SO(d_b)$ equivariance, the weight matrix $W$ must be block-scalar:

$$
W = \begin{pmatrix} \lambda_1 I_{d_b} & 0 & \cdots & 0 \\ 0 & \lambda_2 I_{d_b} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \lambda_{n_b} I_{d_b} \end{pmatrix}
$$
where each $\lambda_i \in [-1, 1]$ is a learnable scalar (per-bundle scaling factor), and $I_{d_b}$ is the $d_b \times d_b$ identity matrix. This constraint follows from Schur's lemma: any linear map commuting with all elements of $SO(d_b)$ must be a scalar multiple of identity (see Lemma {prf:ref}`lem-schur-scalar-constraint`).

**Practical relaxation (approximate equivariance):** For increased expressiveness, implementations may use general block-diagonal $W$ with $\sigma_{\max}(W_i) \leq 1$. This sacrifices exact equivariance but provides bounded equivariance violation (see Proposition {prf:ref}`prop-approximate-equivariance-bound`).
:::

:::{prf:lemma} Block-Diagonal Structure is Necessary (But Not Sufficient) for Bundle Equivariance
:label: lem-block-diagonal-necessary

Let $W: \mathcal{Z} \to \mathcal{Z}$ be a linear map on $\mathcal{Z} = \bigoplus_{i=1}^{n_b} V_i$ with $V_i \cong \mathbb{R}^{d_b}$.

For $W$ to be $G_{\text{bundle}}$-equivariant where $G_{\text{bundle}} = \prod_{i=1}^{n_b} SO(d_b)$, it is **necessary** that $W$ be block-diagonal: $W = \text{diag}(W_1, \ldots, W_{n_b})$ with $W_i: V_i \to V_i$.

However, block-diagonal structure is **not sufficient** for equivariance of the full IsotropicBlock; an additional constraint is required (see Lemma {prf:ref}`lem-schur-scalar-constraint`).

*Proof of necessity.*

Suppose $W$ has off-diagonal blocks, i.e., there exist $i \neq j$ such that $W_{ij} \neq 0$ where $W = [W_{ij}]$ in block form.

Consider a group element $g = (g_1, \ldots, g_{n_b})$ where $g_i = R_\theta \in SO(d_b)$ is a nontrivial rotation and $g_j = I$ for $j \neq i$.

For this $g$ and input $z$ with $z^{(j)} \neq 0$, $z^{(k)} = 0$ for $k \neq j$:

$$
(W \rho(g) z)_i = \sum_{k=1}^{n_b} W_{ik} g_k z^{(k)} = W_{ii} g_i \cdot 0 + W_{ij} I \cdot z^{(j)} = W_{ij} z^{(j)}
$$

But:

$$
(\rho(g) W z)_i = g_i (W z)^{(i)} = R_\theta \sum_{k=1}^{n_b} W_{ik} z^{(k)} = R_\theta W_{ij} z^{(j)}
$$

Since $R_\theta \neq I$ and $R_\theta W_{ij} z^{(j)} \neq W_{ij} z^{(j)}$ (rotation changes direction), we have $W \rho(g) z \neq \rho(g) W z$, violating equivariance.

Therefore, off-diagonal blocks must vanish: $W_{ij} = 0$ for $i \neq j$. $\square$

**Remark:** This lemma establishes that the block-diagonal constraint is necessary but leaves open what additional structure is required on each block $W_i$. Lemma {prf:ref}`lem-schur-scalar-constraint` completes the characterization.
:::

:::{prf:lemma} Schur's Lemma Constraint: Scalar Blocks Required for Equivariance
:label: lem-schur-scalar-constraint

Let $W = \text{diag}(W_1, \ldots, W_{n_b})$ be block-diagonal where each $W_i: V_i \to V_i$ with $V_i \cong \mathbb{R}^{d_b}$.

Assume the scalar map $\phi_i: [0,\infty) \to [0,\infty)$ defined by

$$
\phi_i(r) := r \, \bigl|g(r + b_i)\bigr|
$$
is injective on $[0,\infty)$ (a non-degeneracy condition that holds, for example, when $g(r+b_i)\ge 0$ and $r \mapsto r\,g(r+b_i)$ is strictly increasing on the operating range).

Then the composition $\text{NormGate} \circ W$ is $G_{\text{bundle}}$-equivariant if and only if each $W_i = \lambda_i I_{d_b}$ for some scalar $\lambda_i \in \mathbb{R}$.

*Proof.*

**($\Leftarrow$) Sufficiency:** If $W_i = \lambda_i I_{d_b}$, then for any $g_i \in SO(d_b)$:

$$
W_i g_i = \lambda_i I \cdot g_i = \lambda_i g_i = g_i \lambda_i I = g_i W_i
$$

Thus $W_i$ commutes with all $g_i \in SO(d_b)$. The IsotropicBlock composition is then equivariant by standard composition of equivariant maps.

**($\Rightarrow$) Necessity:** Suppose $\text{NormGate} \circ W$ is $G_{\text{bundle}}$-equivariant. We show each $W_i$ must be a scalar multiple of identity.

**Step 1. Equivariance condition (bundle $i$):** For all $R \in SO(d_b)$ and $v \in V_i$,

$$
\text{NormGate}(W_i \, Rv) = R \, \text{NormGate}(W_i v)
$$
which expands to

$$
W_i Rv \cdot g(\|W_i Rv\| + b_i) = R W_i v \cdot g(\|W_i v\| + b_i).
$$

**Step 2. Take norms:** Using $\|R x\| = \|x\|$,

$$
\|W_i Rv\| \, \bigl|g(\|W_i Rv\| + b_i)\bigr| = \|W_i v\| \, \bigl|g(\|W_i v\| + b_i)\bigr|.
$$
By injectivity of $\phi_i$, this implies $\|W_i Rv\| = \|W_i v\|$ for all $R, v$, hence the gate scalars match:

$$
g(\|W_i Rv\| + b_i) = g(\|W_i v\| + b_i).
$$

**Step 3. Cancel the common scalar:** Substituting back into Step 1 gives

$$
W_i Rv = R W_i v \quad \forall R \in SO(d_b), \forall v \in V_i,
$$
so $W_i$ commutes with all of $SO(d_b)$.

**Step 4. Apply Schur's lemma / commutant characterization:** The standard representation of $SO(d_b)$ on $\mathbb{R}^{d_b}$ is irreducible for $d_b \ge 2$, and its commutant is $\{\lambda I\}$. Therefore $W_i = \lambda_i I_{d_b}$.

	$\square$

**Physical interpretation:** The scalar constraint $W_i = \lambda_i I$ means each bundle can only be uniformly scaled, not rotated or sheared within itself. This preserves the geometric isotropy of each bundle fiber.
:::

:::{prf:theorem} IsotropicBlock is G-Equivariant (Scalar Block Case)
:label: thm-isotropic-block-equivariant

Let $G = \prod_{i=1}^{n_b} SO(d_b)$ be the product gauge group (Definition {prf:ref}`def-latent-vector-bundle`). By Lemmas {prf:ref}`lem-block-diagonal-necessary` and {prf:ref}`lem-schur-scalar-constraint`, the weight matrix $W$ in SpectralLinear must be **block-scalar** for exact equivariance:

$$
W = \begin{pmatrix} \lambda_1 I_{d_b} & 0 & \cdots & 0 \\ 0 & \lambda_2 I_{d_b} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \lambda_{n_b} I_{d_b} \end{pmatrix}
$$

where each $\lambda_i \in [-1, 1]$ is a learnable scalar satisfying $|\lambda_i| \leq 1$ (spectral normalization).

Then the IsotropicBlock (Definition {prf:ref}`def-isotropic-block`) is **exactly** $G$-equivariant.

*Proof.*

**Step 1. Define the group action:** On the flat space $\mathcal{Z} \cong \mathbb{R}^{n_b \cdot d_b}$, the group $G = \prod_{i=1}^{n_b} SO(d_b)$ acts as:

$$
\rho(g_1, \ldots, g_{n_b}) \cdot z = (g_1 \cdot z^{(1)}, \ldots, g_{n_b} \cdot z^{(n_b)})
$$
where $z = (z^{(1)}, \ldots, z^{(n_b)})$ with $z^{(i)} \in \mathbb{R}^{d_b}$.

**Step 2. SpectralLinear with scalar blocks:**
For $W = \text{diag}(\lambda_1 I, \ldots, \lambda_{n_b} I)$ and $(g_1, \ldots, g_{n_b}) \in G$:

$$
W \cdot \rho(g_1, \ldots, g_{n_b}) \cdot z = (\lambda_1 g_1 z^{(1)}, \ldots, \lambda_{n_b} g_{n_b} z^{(n_b)})
$$

**Key property:** Since $\lambda_i I$ commutes with all $g_i \in SO(d_b)$:

$$
\lambda_i I \cdot g_i = g_i \cdot \lambda_i I \quad \forall g_i \in SO(d_b)
$$

Thus SpectralLinear is equivariant:

$$
W \cdot \rho(g) \cdot z = \rho(g) \cdot W \cdot z
$$

**Step 3. Reshape is equivariant:** The bundle partition $\mathbb{R}^{n_b \cdot d_b} \to (\mathbb{R}^{d_b})^{n_b}$ is equivariant by construction (identity map in bundled coordinates).

**Step 4. NormGate is $SO(d_b)$-equivariant per bundle:** By Theorem {prf:ref}`thm-norm-gating-equivariant`, for each bundle $i$:

$$
\text{NormGate}(g_i \cdot v_i) = g_i \cdot \text{NormGate}(v_i) \quad \forall g_i \in SO(d_b)
$$

**Step 5. Composition equivariance:**

We prove $\text{IsotropicBlock}(\rho(g) \cdot z) = \rho(g) \cdot \text{IsotropicBlock}(z)$ by composing equivariant maps.

Let $g = (g_1, \ldots, g_{n_b}) \in G$ and $z = (z^{(1)}, \ldots, z^{(n_b)})$.

Compute left-hand side:

$$
\begin{align}
\text{IsotropicBlock}(\rho(g) \cdot z) &= \text{NormGate}(W \cdot \rho(g) \cdot z) \\
&= \text{NormGate}(\rho(g) \cdot W \cdot z) \quad \text{(Step 2: $W$ is equivariant)} \\
&= \rho(g) \cdot \text{NormGate}(W \cdot z) \quad \text{(Step 4: NormGate is equivariant)} \\
&= \rho(g) \cdot \text{IsotropicBlock}(z)
\end{align}
$$

**Explicit verification for bundle $i$:**
Let $v_i = \lambda_i z^{(i)}$ (output of SpectralLinear for bundle $i$).

- LHS: $\text{NormGate}(\lambda_i g_i z^{(i)}) = (\lambda_i g_i z^{(i)}) \cdot h(\|\lambda_i g_i z^{(i)}\|)$
- Since $\|\lambda_i g_i z^{(i)}\| = |\lambda_i| \cdot \|g_i z^{(i)}\| = |\lambda_i| \cdot \|z^{(i)}\| = \|\lambda_i z^{(i)}\|$:
- LHS $= (\lambda_i g_i z^{(i)}) \cdot h(\|\lambda_i z^{(i)}\|) = g_i (\lambda_i z^{(i)}) \cdot h(\|\lambda_i z^{(i)}\|) = g_i \cdot \text{NormGate}(v_i)$ = RHS

$\square$

**Remark:** The scalar block constraint $W_i = \lambda_i I$ is essential for exact equivariance. General block-diagonal matrices do NOT yield equivariance (see Lemma {prf:ref}`lem-schur-scalar-constraint`). For increased expressiveness at the cost of exact equivariance, see Proposition {prf:ref}`prop-approximate-equivariance-bound` below.
:::

:::{prf:proposition} Approximate Equivariance Bound for General Block-Diagonal W
:label: prop-approximate-equivariance-bound

For practical architectures using general block-diagonal $W = \text{diag}(W_1, \ldots, W_{n_b})$ with $\sigma_{\max}(W_i) \leq 1$ (instead of scalar blocks), the equivariance violation is bounded.

**Statement:** Let $\text{IB}$ denote IsotropicBlock with general block-diagonal $W$. Fix an operating range in which the per-bundle NormGate is $L_{\text{NG}}$-Lipschitz (Lemma {prf:ref}`lem-normgate-lipschitz`). Then for any $g = (g_1,\ldots,g_{n_b}) \in G_{\text{bundle}}$ and $z = (z^{(1)},\ldots,z^{(n_b)}) \in \mathcal{Z}$:

$$
\|\text{IB}(\rho(g)\cdot z) - \rho(g)\cdot \text{IB}(z)\|
\le
L_{\text{NG}} \, \|(W\rho(g) - \rho(g)W)z\|
=
L_{\text{NG}} \left(\sum_{i=1}^{n_b} \|[W_i, g_i]\,z^{(i)}\|^2\right)^{1/2}.
$$

In particular,

$$
\|\text{IB}(\rho(g)\cdot z) - \rho(g)\cdot \text{IB}(z)\|
\le
L_{\text{NG}} \sum_{i=1}^{n_b} \|[W_i, g_i]\|_{\mathrm{op}} \,\|z^{(i)}\|.
$$

**Implication:** If each $W_i$ commutes with $SO(d_b)$ (equivalently $W_i=\lambda_i I$ by Lemma {prf:ref}`lem-schur-scalar-constraint`), then $[W_i,g_i]=0$ and the equivariance violation is exactly zero.

*Proof.*

Use equivariance of NormGate (Theorem {prf:ref}`thm-norm-gating-equivariant`) to rewrite:

$$
\rho(g)\cdot \text{IB}(z)
=
\rho(g)\cdot \text{NormGate}(Wz)
=
\text{NormGate}(\rho(g)\cdot Wz).
$$
Therefore,

$$
\text{IB}(\rho(g)\cdot z) - \rho(g)\cdot \text{IB}(z)
=
\text{NormGate}(W\rho(g)z) - \text{NormGate}(\rho(g)Wz).
$$
By the $L_{\text{NG}}$-Lipschitz property of NormGate on the operating range (Lemma {prf:ref}`lem-normgate-lipschitz`),

$$
\|\text{NormGate}(W\rho(g)z) - \text{NormGate}(\rho(g)Wz)\|
\le
L_{\text{NG}} \,\|(W\rho(g) - \rho(g)W)z\|.
$$
Since both $W$ and $\rho(g)$ are block-diagonal, the commutator is block-diagonal with blocks $[W_i,g_i]$, yielding the stated per-bundle forms.

$\square$
:::

:::{prf:proposition} Conv2d is NOT SO(2)-Equivariant
:label: prop-conv-not-rotation-equivariant

Standard `Conv2d` with learned kernels is translation-equivariant but **not** rotation-equivariant for continuous rotations.

*Proof.*

**Setup:** Let $I: \mathbb{Z}^2 \to \mathbb{R}^{C_{\text{in}}}$ be a discrete image on pixel lattice $\mathbb{Z}^2$ with $C_{\text{in}}$ channels. Let $\psi: \mathbb{Z}^2 \to \mathbb{R}^{C_{\text{out}} \times C_{\text{in}}}$ be a convolutional kernel. The convolution operation is:

$$
(\psi * I)(x) = \sum_{y \in \mathbb{Z}^2} \psi(y) I(x - y) \quad \text{for } x \in \mathbb{Z}^2
$$

**Step 1. Discrete rotation problem:**
For $R_\theta \in SO(2)$ with $\theta \notin \{0, \pi/2, \pi, 3\pi/2\}$, rotation maps lattice points off-grid. For example, with $\theta = \pi/4$:

$$
R_{\pi/4} \cdot (1, 0) = \left(\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2}\right) \notin \mathbb{Z}^2
$$

To apply rotation to discrete images, we must use an interpolation scheme $\mathcal{I}$:

$$
(R_\theta \cdot I)(x) = \mathcal{I}(I, R_\theta^{-1} x) \quad \text{for } x \in \mathbb{Z}^2
$$

**Step 2. Standard interpolation (bilinear):**
For $x' = R_\theta^{-1} x \in \mathbb{R}^2 \setminus \mathbb{Z}^2$, bilinear interpolation uses the four nearest lattice points:

$$
\mathcal{I}(I, x') = \sum_{i \in \{0,1\}^2} w_i(x') I(x'_{\text{floor}} + i)
$$
where $w_i$ are barycentric weights depending on $x' - x'_{\text{floor}}$.

**Step 3. Non-commutativity of convolution and rotation:**
We show $(\psi * (R_\theta \cdot I)) \neq (R_\theta \cdot (\psi * I))$.

Left-hand side:

$$
(\psi * (R_\theta \cdot I))(x) = \sum_{y \in \mathbb{Z}^2} \psi(y) (R_\theta \cdot I)(x - y) = \sum_{y \in \mathbb{Z}^2} \psi(y) \mathcal{I}(I, R_\theta^{-1}(x - y))
$$

Right-hand side:

$$
(R_\theta \cdot (\psi * I))(x) = \mathcal{I}(\psi * I, R_\theta^{-1} x) = \mathcal{I}\left(\sum_{y \in \mathbb{Z}^2} \psi(y) I(\cdot - y), R_\theta^{-1} x\right)
$$

**Key:** Interpolation does not commute with summation:

$$
\sum_{y} \psi(y) \mathcal{I}(I, R_\theta^{-1}(x - y)) \neq \mathcal{I}\left(\sum_y \psi(y) I(\cdot - y), R_\theta^{-1} x\right)
$$

The left side interpolates $I$ at rotated shifted positions $R_\theta^{-1}(x-y)$ then sums.
The right side first computes the full convolution $\psi * I$ at all lattice points, then interpolates the result.

**Explicit numerical counterexample:**

Consider a $3 \times 3$ image with a vertical edge:

$$
I = \begin{pmatrix}
0 & 0 & 1 \\
0 & 0 & 1 \\
0 & 0 & 1
\end{pmatrix}
$$

Use a $2 \times 2$ vertical Sobel kernel detecting vertical edges:

$$
\psi = \begin{pmatrix}
-1 & 1 \\
-1 & 1
\end{pmatrix}
$$

**Path 1: Convolve then rotate by $\theta = 45°$**

Compute $(\psi * I)(x, y)$ at position $(1, 1)$ (center, using valid padding):

At $(1,1)$, the $2 \times 2$ kernel overlays the patch $I[1:3, 1:3] = \begin{pmatrix} 0 & 1 \\ 0 & 1 \end{pmatrix}$:

$$
(\psi * I)(1, 1) = -1 \cdot 0 + 1 \cdot 1 + (-1) \cdot 0 + 1 \cdot 1 = 2
$$

Similarly:
- At $(0, 0)$: patch is $\begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix}$, result $= 0$
- At $(0, 1)$: patch is $\begin{pmatrix} 0 & 1 \\ 0 & 1 \end{pmatrix}$, result $= 2$
- At $(1, 0)$: patch is $\begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix}$, result $= 0$

Convolution result (single channel):

$$
\psi * I = \begin{pmatrix}
0 & 2 \\
0 & 2
\end{pmatrix}
$$

Rotate by $45°$ using bilinear interpolation at position $(0.5, 0.5)$ (center after rotation):

$$
(R_{45°} \cdot (\psi * I))(0.5, 0.5) \approx \frac{1}{4}(0 + 2 + 0 + 2) = 1.0
$$

**Path 2: Rotate then convolve**

Rotate image $I$ by $45°$ first. At position $(1, 1)$, the rotated image samples from:

$$
R_{-45°} \cdot (1, 1) = \begin{pmatrix} \cos(45°) & \sin(45°) \\ -\sin(45°) & \cos(45°) \end{pmatrix} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} \sqrt{2} \\ 0 \end{pmatrix} \approx (1.41, 0)
$$

Using bilinear interpolation between $(1, 0)$ and $(2, 0)$ (which wraps/pads):

$$
(R_{45°} \cdot I)(1, 1) \approx 0.41 \cdot I(2, 0) + 0.59 \cdot I(1, 0) \approx 0.41 \cdot 1 + 0.59 \cdot 0 = 0.41
$$

Computing convolution after rotation at position $(0.5, 0.5)$:

$$
(\psi * (R_{45°} \cdot I))(0.5, 0.5) \approx \text{different from } 0.5
$$

Due to interpolation artifacts and edge effects, the two paths give **numerically different results**, violating exact equivariance.

**Quantitative violation:** For the $3 \times 3$ edge image with $45°$ rotation:

$$
\|(\psi * (R_{45°} \cdot I)) - (R_{45°} \cdot (\psi * I))\|_F \approx 0.6 \text{ to } 1.0
$$

(Frobenius norm, typical values depending on boundary conditions and interpolation scheme). With correct convolution values ($\psi * I$ has entries of magnitude 2, not 1), the violation is approximately 50-100% of the signal magnitude, confirming Conv2d significantly breaks SO(2) equivariance.

$\square$

**Remark on discrete subgroups:** Exact equivariance holds for the discrete group $C_4 = \{e, R_{\pi/2}, R_\pi, R_{3\pi/2}\}$ (90° rotations) since these map $\mathbb{Z}^2 \to \mathbb{Z}^2$ without interpolation. For continuous $SO(2)$ equivariance, we need steerable filters (Definition {prf:ref}`def-steerable-filter-bank`).

**Connection to geodesic integrator:** The Lorentz-Langevin equation (Definition {prf:ref}`def-bulk-drift-continuous-flow`) requires well-defined geometric vectors in the tangent space $T_z \mathcal{Z}$. If the vision encoder produces rotation-variant features, the tangent space structure becomes viewing-angle-dependent:

$$
T_z \mathcal{Z} \neq T_{R_\theta \cdot z} \mathcal{Z} \quad \text{(geometric inconsistency)}
$$
This breaks geodesic motion, as geodesics are defined as curves minimizing length with respect to a **fixed** Riemannian metric. Rotation-variant encoders effectively change the metric under rotation, making geodesics ill-defined.
:::

:::{prf:definition} Image as Bundle Section
:label: def-image-as-bundle-section

An RGB image $I: \mathbb{R}^2 \to \mathbb{R}^3$ is a section of the trivial bundle $\mathbb{R}^2 \times \mathbb{R}^3$.

A rotation $R \in SO(2)$ acts on the base (spatial coordinates) but not the fiber (RGB values):

$$
(R \cdot I)(x) = I(R^{-1} \cdot x)
$$

For equivariant features, we need a **non-trivial bundle** where fibers also transform.
:::

:::{prf:definition} Steerable Filter Bank
:label: def-steerable-filter-bank

A filter bank $\{\psi_n^{(\ell)}\}_{n=1}^N$ is **steerable of type $\ell$** if:

$$
R_\theta \cdot \psi_n^{(\ell)} = \sum_m D_{nm}^{(\ell)}(\theta) \psi_m^{(\ell)}
$$

where $D^{(\ell)}$ is the $\ell$-th irreducible representation of $SO(2)$.

**Explicit form of $SO(2)$ irreducible representations:**

For $\ell \in \mathbb{Z}_{\geq 0}$ (non-negative integers), the $\ell$-th irreducible representation is:
- **$\ell = 0$:** Trivial representation, $D^{(0)}(\theta) = 1$ (scalar, 1-dimensional)
- **$\ell \geq 1$:** 2-dimensional representation acting on $\mathbb{R}^2$ or $\mathbb{C}$ via:
  
  $$
  D^{(\ell)}(\theta) = \begin{pmatrix} \cos(\ell\theta) & -\sin(\ell\theta) \\ \sin(\ell\theta) & \cos(\ell\theta) \end{pmatrix} \in SO(2)
  $$
  Equivalently, in complex notation: $D^{(\ell)}(\theta) \cdot z = e^{i\ell\theta} z$ for $z \in \mathbb{C}$.

**Physical interpretation:**
- $\ell$ is the **angular frequency** or **angular momentum quantum number**
- Under rotation by $\theta$, an $\ell$-mode rotates by $\ell \theta$ (frequency multiplication)

**Interpretation:**
- $\ell = 0$: Scalars (rotation-invariant, e.g., circularly symmetric filters). $D^{(0)}(\theta) = 1$
- $\ell = 1$: Vectors (oriented edge detectors). Rotate by $\theta$ → features rotate by $\theta$. $D^{(1)}(\theta) = R_\theta$
- $\ell = 2$: Quadrupoles (corner detectors). Rotate by $\theta$ → features rotate by $2\theta$. $D^{(2)}(\theta) = R_{2\theta}$
:::

:::{prf:definition} Feature Bundle as Associated Vector Bundle
:label: def-associated-feature-bundle

Let $P = SE(2) = \mathbb{R}^2 \rtimes SO(2)$ be the Euclidean group (translations and rotations), and let $H = SO(2)$ be the structure group.

For steerable features of type $\ell$, the **associated vector bundle** is:

$$
E^{(\ell)} = P \times_{H} V^{(\ell)}
$$

where:
- $V^{(\ell)} \cong \mathbb{R}^{2}$ (for $\ell \geq 1$) or $\mathbb{R}$ (for $\ell = 0$) is the representation space
- $SO(2)$ acts on $V^{(\ell)}$ via $D^{(\ell)}$ (the $\ell$-th irreducible representation)
- The quotient is formed by identifying $(p, v) \sim (ph, h^{-1} \cdot v)$ for $h \in SO(2)$

**Structure:**
- **Total space:** $E^{(\ell)} = \{[(g, v)] : g \in SE(2), v \in V^{(\ell)}\}$ (equivalence classes)
- **Base space:** $B = SE(2)/SO(2) \cong \mathbb{R}^2$ (spatial positions)
- **Projection:** $\pi: E^{(\ell)} \to B$, $\pi([(g, v)]) = [g] \in \mathbb{R}^2$
- **Fiber:** $F_x = \pi^{-1}(x) \cong V^{(\ell)}$ (representation space at position $x$)

**Sections:** A steerable feature map is a **section** $\phi: B \to E^{(\ell)}$ satisfying $\pi \circ \phi = \text{id}_B$.

In coordinates: $\phi(x) = (f_1^{(\ell)}(x), \ldots, f_N^{(\ell)}(x))$ where each $f_n^{(\ell)}$ transforms under $D^{(\ell)}$.
:::

:::{prf:definition} Connection and Covariant Derivative
:label: def-connection-steerable-bundle

A **connection** on the bundle $E^{(\ell)}$ specifies how to compare fibers at different base points.

For the associated bundle $E^{(\ell)} = SE(2) \times_{SO(2)} V^{(\ell)}$, the **canonical flat connection** is:

$$
\nabla_X \phi = X[\phi]
$$

where $X$ is a vector field on $\mathbb{R}^2$ and $X[\phi]$ is the directional derivative.

**Parallel transport:** A section $\phi(x + tv)$ along direction $v$ is **parallel** if $\nabla_v \phi = 0$, i.e., $\frac{d}{dt}\phi(x + tv) = 0$.

For steerable features, parallel transport preserves the transformation law:

$$
\phi(x + \delta x) = \phi(x) + \nabla_{\delta x} \phi + O(\|\delta x\|^2)
$$

where $\nabla_{\delta x} \phi$ transforms under $D^{(\ell)}$ at the new position.
:::

:::{prf:remark} Gauge Fields and Curvature
:label: rem-gauge-curvature-vision

In the gauge theory framework (Chapter 8.1), the **Binding field** $G_\mu$ acts as a connection on the feature bundle. The curvature (field strength) $F_{\mu\nu} = \partial_\mu G_\nu - \partial_\nu G_\mu + [G_\mu, G_\nu]$ measures the failure of parallel transport to be path-independent.

For the trivial bundle with flat connection (used in steerable CNNs), $F_{\mu\nu} = 0$ (zero curvature), meaning parallel transport is path-independent. This corresponds to **free field theory** in physics.

**Extension to non-trivial connections:** If steerable features are coupled to other latent variables via attention or gating, the effective connection becomes non-trivial (non-zero $G_\mu$), introducing curvature. This is explored in Section 8.5 (Covariant Cross-Attention) with **Wilson lines** for parallel transport.
:::

:::{prf:theorem} Steerable Convolution is SO(2)-Equivariant
:label: thm-steerable-conv-equivariant

Let $\{\psi_n^{(\ell)}\}_{n=1}^N$ be a steerable filter bank of type $\ell$ (Definition {prf:ref}`def-steerable-filter-bank`). Let $\text{Conv}_\ell$ denote convolution with these filters:

$$
(\text{Conv}_\ell I)_n(x) = \sum_m (\psi_m^{(\ell)} * I)(x) = \int_{\mathbb{R}^2} \psi_m^{(\ell)}(y) I(x - y) \, dy
$$

Then for any rotation $R_\theta \in SO(2)$:

$$
\text{Conv}_\ell(R_\theta \cdot I) = D^{(\ell)}(\theta) \cdot \text{Conv}_\ell(I)
$$

where $D^{(\ell)}(\theta)$ is the $\ell$-th irreducible representation of $SO(2)$.

*Proof.*

**Step 1. Apply rotation to input:**
By linearity of convolution:

$$
(\psi_n * (R_\theta \cdot I))(x) = \int_{\mathbb{R}^2} \psi_n(y) I(R_\theta^{-1}(x-y)) \, dy
$$

**Step 2. Change of variables:**
Let $u = R_\theta^{-1} y$, so $y = R_\theta u$. Since $R_\theta \in SO(2)$ preserves measure ($|\det R_\theta| = 1$), we have $dy = du$:

$$
= \int_{\mathbb{R}^2} \psi_n(R_\theta u) I(R_\theta^{-1}(x - R_\theta u)) \, du
$$

Using the property $R_\theta^{-1}(x - R_\theta u) = R_\theta^{-1} x - u$:

$$
= \int_{\mathbb{R}^2} \psi_n(R_\theta u) I(R_\theta^{-1} x - u) \, du
$$

**Step 3. Apply steerability definition:**
By the steerability property (Definition {prf:ref}`def-steerable-filter-bank`), the filter transforms as:

$$
\psi_n(R_\theta u) = (R_\theta \cdot \psi_n)(u)
$$

Therefore:

$$
= \int_{\mathbb{R}^2} (R_\theta \cdot \psi_n)(u) I(R_\theta^{-1} x - u) \, du
$$

**Step 4. Apply filter bank steerability:**
By Definition {prf:ref}`def-steerable-filter-bank`, a steerable filter of type $\ell$ satisfies:

$$
(R_\theta \cdot \psi_n)(u) = \psi_n(R_\theta u) = \sum_m D_{nm}^{(\ell)}(\theta) \psi_m(u)
$$

Substituting:

$$
= \int_{\mathbb{R}^2} \left[\sum_m D_{nm}^{(\ell)}(\theta) \psi_m(u)\right] I(R_\theta^{-1} x - u) \, du
$$

**Step 5. Separate sum from integral:**

$$
= \sum_m D_{nm}^{(\ell)}(\theta) \int_{\mathbb{R}^2} \psi_m(u) I(R_\theta^{-1} x - u) \, du
$$

**Step 6. Recognize convolution:**
The integral $\int \psi_m(u) I(R_\theta^{-1} x - u) \, du = (\psi_m * I)(R_\theta^{-1} x)$ by definition of convolution. This is precisely $(R_\theta \cdot (\psi_m * I))(x)$ by the definition of rotation action on functions.

Therefore:

$$
(\psi_n^{(\ell)} * (R_\theta \cdot I))(x) = \sum_m D_{nm}^{(\ell)}(\theta) (R_\theta \cdot (\psi_m^{(\ell)} * I))(x)
$$

which shows the desired equivariance property.

In matrix form:

$$
\text{Conv}_\ell(R_\theta \cdot I) = D^{(\ell)}(\theta) \cdot \text{Conv}_\ell(I)
$$

$\square$

**Remark:** For full treatment of discretization effects and practical implementation, see Cohen & Welling (2016) {cite}`cohen2016group` and Weiler & Cesa (2019) {cite}`weiler2019general`.
:::

:::{prf:definition} Lifting Map to SE(2)
:label: def-lifting-map

**Group structure:** The **special Euclidean group** $SE(2) = \mathbb{R}^2 \rtimes SO(2)$ is the group of rigid motions (translations + rotations) in the plane, with elements $g = (x, R_\theta)$ where $x \in \mathbb{R}^2$ is translation and $R_\theta \in SO(2)$ is rotation by angle $\theta$.

**Group multiplication:** For $g_1 = (x_1, R_{\theta_1})$ and $g_2 = (x_2, R_{\theta_2})$:

$$
g_1 \cdot g_2 = (x_1 + R_{\theta_1} x_2, R_{\theta_1 + \theta_2})
$$

*Interpretation:* Composition $g_1 \cdot g_2$ means "first apply $g_2$, then $g_1$". Acting on a point $p \in \mathbb{R}^2$:

$$
(g_1 \cdot g_2) \cdot p = g_1 \cdot (g_2 \cdot p) = g_1 \cdot (R_{\theta_2} p + x_2) = R_{\theta_1}(R_{\theta_2} p + x_2) + x_1 = R_{\theta_1 + \theta_2} p + (x_1 + R_{\theta_1} x_2)
$$
The rotation $R_{\theta_1}$ in $g_1$ acts on the translation $x_2$ from $g_2$, and the rotations compose as $R_{\theta_1} R_{\theta_2} = R_{\theta_1 + \theta_2}$.

**Domain and codomain:** Let $C(\mathbb{R}^2, \mathbb{R}^{C_{\text{in}}})$ be the space of continuous functions (images) from $\mathbb{R}^2$ to $\mathbb{R}^{C_{\text{in}}}$ (e.g., $C_{\text{in}} = 3$ for RGB). Let $C(SE(2), \mathbb{R}^{C_{\text{out}}})$ be functions on $SE(2)$ with values in $\mathbb{R}^{C_{\text{out}}}$ (output feature dimension).

The **lifting map** is an operator:

$$
L: C(\mathbb{R}^2, \mathbb{R}^{C_{\text{in}}}) \to C(SE(2), \mathbb{R}^{C_{\text{out}}})
$$

**Definition:** For an input image $I \in C(\mathbb{R}^2, \mathbb{R}^{C_{\text{in}}})$ and group element $g = (x, R_\theta) \in SE(2)$:

$$
(L I)(g) = (L I)(x, \theta) := \sum_{i=1}^{C_{\text{out}}} (\psi_i^{(\theta)} * I)(x) \cdot e_i
$$
where:
- $\{\psi_i\}_{i=1}^{C_{\text{out}}}$ is a steerable filter bank at orientation $\theta = 0$ (Definition {prf:ref}`def-steerable-filter-bank`)
- $\psi_i^{(\theta)} := R_\theta \cdot \psi_i$ is the **rotated filter**: applying rotation $R_\theta$ to the base filter $\psi_i$
- $\{e_i\}$ is the standard basis of $\mathbb{R}^{C_{\text{out}}}$

**Key dependence on $\theta$:** The output $(LI)(x, \theta)$ depends explicitly on the rotation angle $\theta$ through the rotated filters $\psi_i^{(\theta)}$. At each orientation $\theta$, the network applies filters rotated to that orientation, detecting patterns aligned with $\theta$.

**Explicit filter rotation:** For a filter $\psi: \mathbb{R}^2 \to \mathbb{R}$ and rotation $R_\theta \in SO(2)$:

$$
(R_\theta \cdot \psi)(y) := \psi(R_\theta^{-1} y)
$$
This ensures that a filter detecting "vertical edge" at $\theta = 0$ becomes a filter detecting "edge at angle $\theta$" after rotation.

**Equivariance property:** For $g_0 = (x_0, R_{\theta_0}) \in SE(2)$ and image $I$, define the left-translated image $(L_{g_0} I)(x) := I(R_{\theta_0}^{-1}(x - x_0))$. Then:

$$
L(L_{g_0} I) = L_{g_0}(L I)
$$
where the right-hand side is left-multiplication on $SE(2)$: $(L_{g_0} f)(g) = f(g_0^{-1} g)$.

*Verification:* Evaluate both sides at $g = (x, R_\theta)$:
- **LHS:** $(L(L_{g_0} I))(x, \theta) = \sum_i (\psi_i^{(\theta)} * (L_{g_0} I))(x) \cdot e_i$
- Change variables in convolution: $(\psi_i^{(\theta)} * (L_{g_0} I))(x) = \int \psi_i(R_\theta^{-1}(x - y)) I(R_{\theta_0}^{-1}(y - x_0)) dy$
- Substitute $u = R_{\theta_0}^{-1}(y - x_0)$, so $y = R_{\theta_0} u + x_0$:
  $$= \int \psi_i(R_\theta^{-1}(x - x_0 - R_{\theta_0} u)) I(u) du = \int \psi_i(R_{\theta - \theta_0}^{-1}(R_{\theta_0}^{-1}(x - x_0) - u)) I(u) du$$
- **RHS:** $(L_{g_0}(L I))(g) = (L I)(g_0^{-1} g)$ where $g_0^{-1} = (-R_{\theta_0}^{-1} x_0, R_{\theta_0}^{-1})$
- $g_0^{-1} g = (R_{\theta_0}^{-1}(x - x_0), R_{\theta - \theta_0})$ (using SE(2) multiplication)
- $(L I)(R_{\theta_0}^{-1}(x - x_0), \theta - \theta_0) = \sum_i (\psi_i^{(\theta - \theta_0)} * I)(R_{\theta_0}^{-1}(x - x_0)) \cdot e_i$
- This matches the LHS after recognizing $\psi_i^{(\theta - \theta_0)}(y) = \psi_i(R_{\theta - \theta_0}^{-1} y)$. $\square$

**Geometric interpretation:** Instead of features at spatial locations $x \in \mathbb{R}^2$, lifted features live at *posed locations* $(x, \theta) \in SE(2)$: position AND orientation. The network learns to detect patterns *and* their orientations explicitly.

**Output dimension:** For $N_\theta$ discrete orientations (e.g., $N_\theta = 8$ for $\theta \in \{0°, 45°, 90°, \ldots, 315°\}$), the output has dimension $C_{\text{out}} = N_\theta \times C_{\text{feature}}$ where $C_{\text{feature}}$ is the number of feature types per orientation.
:::

:::{prf:definition} SU(N_f) Gauge Action on Bundle Space
:label: def-gauge-action-bundles

Let $Z = (z^{(1)}, \ldots, z^{(n_b)})$ be the bundled latent representation, viewed here in the **complexified** setting with $z^{(i)} \in \mathbb{C}^{d_b}$. The gauge group $SU(N_f)$ with $N_f = n_b$ acts on $Z$ as:

$$
Z \mapsto Z' = Z \cdot U
$$

where $U \in SU(N_f)$ is an $n_b \times n_b$ special unitary matrix:

$$
U^\dagger U = I, \quad \det(U) = 1
$$

**Explicit action:** In matrix form, treating $Z$ as a $d_b \times n_b$ matrix:

$$
z'^{(j)} = \sum_{i=1}^{n_b} U_{ij} \, z^{(i)} \quad \text{(bundle mixing)}
$$

**Color charge:** Represent the latent state as a matrix $Z \in \mathbb{C}^{d_b \times n_b}$ where the $i$-th column is the $i$-th bundle vector $z^{(i)} \in \mathbb{C}^{d_b}$:

$$
Z = [z^{(1)} \mid z^{(2)} \mid \cdots \mid z^{(n_b)}]
$$

For each generator $T^a \in \mathfrak{su}(n_b)$ ($a = 1, \ldots, n_b^2 - 1$), where $T^a$ is a traceless Hermitian $n_b \times n_b$ matrix, define the **color charge operator**:

$$
Q_C^a[Z] = \text{Tr}_{\text{bundle}}(Z^\dagger Z \cdot T^a) = \sum_{i,j=1}^{n_b} T^a_{ij} \, (z^{(i)})^\dagger z^{(j)}
$$

where:
- $Z \in \mathbb{C}^{d_b \times n_b}$ has columns $z^{(1)}, \ldots, z^{(n_b)} \in \mathbb{C}^{d_b}$ (bundles)
- $Z^\dagger \in \mathbb{C}^{n_b \times d_b}$ is the conjugate transpose (rows are bundle vectors)
- $Z^\dagger Z \in \mathbb{C}^{n_b \times n_b}$ is the **Gram matrix** with $(Z^\dagger Z)_{ij} = (z^{(i)})^\dagger z^{(j)}$
- $T^a \in \mathbb{C}^{n_b \times n_b}$ is the generator matrix (acts on bundle indices)
- $Z^\dagger Z \cdot T^a \in \mathbb{C}^{n_b \times n_b}$ is matrix multiplication
- $\text{Tr}_{\text{bundle}}$ denotes trace over bundle indices (summing diagonal elements of the $n_b \times n_b$ matrix)

**Dimensional consistency:**
- $[z^{(i)}] = \sqrt{\text{nat}}$ (latent vector)
- $[(z^{(i)})^\dagger z^{(j)}] = \text{nat}$ (inner product)
- $[T^a_{ij}]$ = dimensionless (matrix element)
- $[Q_C^a] = \text{nat}$ (charge is extensive in latent dimension)

A state is **color-neutral** (confined) if:

$$
Q_C^a[Z] = 0 \quad \forall \, a = 1, \ldots, n_b^2 - 1
$$

**Physical interpretation:** Just as quarks in QCD carry color charge under $SU(3)_C$, latent features carry "bundle charge" under $SU(N_f)_C$ with $N_f = n_b$. Only color-neutral combinations (satisfying all $n_b^2 - 1$ charge constraints) can propagate to the macro level.
:::

:::{prf:theorem} Isotropic Blocks Preserve SU(N_f)_C Gauge Structure
:label: thm-isotropic-preserves-color

The bundled structure (Definition {prf:ref}`def-latent-vector-bundle`) with $n_b$ bundles, coupled with norm-gating and the Binding field $G_\mu$, implements $SU(N_f)_C$ gauge invariance with $N_f = n_b$.

**Statement:** For any $U \in SU(N_f)$, the effective dynamics satisfy:

$$
\mathcal{L}_{\text{eff}}[Z \cdot U] = \mathcal{L}_{\text{eff}}[Z]
$$

Moreover, norm-gating induces scale-dependent coupling:
- **Infrared** ($\ell \to \infty$): Strong coupling $g_s(\mu_{\text{IR}}) \gg 1$ → confinement
- **Ultraviolet** ($\ell \to 0$): Weak coupling $g_s(\mu_{\text{UV}}) \to 0$ → asymptotic freedom

*Proof.*

**Step 1. Bundle structure and gauge group:**

Recall from Definition {prf:ref}`def-latent-vector-bundle` that the latent space decomposes as:

$$
\mathcal{Z} = \bigoplus_{i=1}^{n_b} V_i, \quad V_i \cong \mathbb{R}^{d_b}
$$

For the gauge-theoretic formulas in this subsection we implicitly work in the complexification $V_i \otimes_{\mathbb{R}} \mathbb{C}$ (so bundle vectors may be treated as elements of $\mathbb{C}^{d_b}$); the real-valued architecture uses the corresponding realified action discussed in Definition {prf:ref}`def-fragile-gauge-group`.

Each bundle $V_i$ transforms under its $SO(d_b)$ factor. For $n_b = N_f$ bundles, consider the gauge group $SU(N_f)$ acting on the **bundle indices** (not on the internal bundle space).

**Matrix notation:** Represent the latent state as a matrix $Z \in \mathbb{C}^{d_b \times N_f}$ where the $i$-th column is the $i$-th bundle vector:

$$
Z = [v_1 \mid v_2 \mid \cdots \mid v_{N_f}]
$$

**Gauge transformation:** $SU(N_f)$ acts by right multiplication:

$$
Z \mapsto Z \cdot U, \quad U \in SU(N_f), \quad U^\dagger U = I
$$

This mixes bundle indices while preserving the Frobenius norm $\|Z\|_F^2 := \text{Tr}(Z^\dagger Z) = \sum_i \|v_i\|_2^2$.

**Step 2. Covariant derivative:**

To define dynamics that respect gauge invariance, introduce the **gauge-covariant derivative**:

$$
D_\mu Z := \partial_\mu Z - i g_s \, Z \cdot G_\mu
$$

where:
- $G_\mu = \sum_{a=1}^{N_f^2-1} G_\mu^a T^a$ is the gauge connection (Binding field)
- $T^a$ are the generators of $SU(N_f)$ (Hermitian, traceless $N_f \times N_f$ matrices)
- $g_s > 0$ is the coupling constant

**Connection transformation:** Under a gauge transformation $Z \to Z' = Z \cdot U$ with $U(x^\mu) \in SU(N_f)$ (spacetime-dependent), we require the covariant derivative to transform as

$$
D'_\mu Z' = (D_\mu Z)\cdot U,
$$
where $D'_\mu Z' := \partial_\mu Z' - i g_s \, Z'\cdot G'_\mu$ uses the transformed connection $G'_\mu$.

Expanding and equating terms:

$$
(\partial_\mu Z)\cdot U + Z\cdot(\partial_\mu U) - i g_s \, Z\cdot U \cdot G'_\mu
= (\partial_\mu Z)\cdot U - i g_s \, Z\cdot G_\mu \cdot U.
$$
Cancel $(\partial_\mu Z)\cdot U$ and use that this must hold for all $Z$ to obtain the operator identity

$$
\partial_\mu U - i g_s \, U G'_\mu + i g_s \, G_\mu U = 0.
$$
Solving for $G'_\mu$ yields the right-action Yang--Mills transformation law:

$$
\boxed{
G_\mu \to G'_\mu = U^{-1} G_\mu U - \frac{i}{g_s} U^{-1}(\partial_\mu U)
}
$$
(equivalently $G'_\mu = U^\dagger G_\mu U - \frac{i}{g_s} U^\dagger(\partial_\mu U)$ since $U^{-1}=U^\dagger$). This is the standard gauge transformation written for a right action $Z\mapsto ZU$ {cite}`peskin1995introduction`.

**Verification:** With $D_\mu Z = \partial_\mu Z - i g_s \, ZG_\mu$ and $G'_\mu$ as above, one checks directly that

$$
D'_\mu(Z\cdot U) = (D_\mu Z)\cdot U.
$$

Thus the effective Lagrangian $\mathcal{L}_{\text{eff}} = \|D_\mu Z\|_F^2 = \text{Tr}[(D_\mu Z)^\dagger (D^\mu Z)]$ is gauge-invariant by construction.

**Step 2. Norm-gating as effective coupling:**

The norm-gating activation potential $b_i$ creates an energy barrier. The effective coupling at layer $\ell$ is:

$$
g_s^{(\ell)} = \frac{\langle \|G_\mu\| \rangle}{\langle \|\partial_\mu Z\| \rangle} \approx \frac{\beta_\ell}{\sqrt{1 + \|\nabla_{W_\ell} \mathcal{L}\|^{-2}}}
$$

where $\beta_\ell = \tanh(b_\ell)$ is the barrier strength (see Proposition {prf:ref}`prop-coupling-from-barriers` below).

**Step 3. IR behavior (confinement):**

At large layer depth $\ell \to L$ (infrared scale):
- Barrier potentials $b_\ell$ are large (strong gates)
- Gradient flow between bundles is suppressed unless $Q_C^a = 0$
- Effective coupling $g_s^{(L)} \gg 1$

**Energy penalty for non-neutral states:**

$$
\Delta E_{\text{conf}} = g_s^{(L)} \sum_a |Q_C^a|^2 \to \infty \quad \text{as } g_s^{(L)} \to \infty
$$

Only color-neutral states ($Q_C^a = 0 \, \forall a$) have finite energy → **confinement**.

**Step 4. UV behavior (asymptotic freedom):**

At shallow layers $\ell \to 0$ (ultraviolet scale):
- Barrier potentials $b_\ell \to 0$ (weak gates)
- Bundles decouple, gradient flow is independent
- Effective coupling $g_s^{(0)} \to 0$

Bundles behave as free, non-interacting subspaces → **asymptotic freedom**.

**Step 5. Identification with coupling window:**

By Corollary {prf:ref}`cor-coupling-window`, the gauge coupling must satisfy:

$$
g_s(\mu_{\text{IR}}) \geq g_s^{\text{crit}} \quad \text{and} \quad g_s(\mu_{\text{UV}}) \to 0
$$

The norm-gating schedule $\{b_\ell\}_{\ell=0}^L$ implements this by construction:
- $b_L$ large (IR confinement)
- $b_0$ small (UV freedom)

$\square$

**Connection to Node 40 (PurityCheck):** Measures $\sum_a |Q_C^a|^2$ at the final layer. Violation indicates non-neutral states reaching the macro register, breaking confinement.
:::

:::{prf:proposition} Coupling Strength from Norm-Gating Barriers
:label: prop-coupling-from-barriers

The effective gauge coupling at layer $\ell$ is:

$$
g_s^{(\ell)} = \beta_\ell \cdot \frac{\xi_\ell}{\sqrt{\xi_\ell^2 + \eta_\ell^2}}
$$

where:
- $\beta_\ell = \tanh(b_\ell)$ is the dimensionless barrier strength
- $\xi_\ell = \langle \|W_{\text{off-diag}}^{(\ell)}\| \rangle$ is the mean off-diagonal weight norm (dimensionless)
- $\eta_\ell = \langle \|\partial_\ell z\|_2 / \|\partial_{\ell-1} z\|_2 \rangle$ is the normalized gradient flow ratio (dimensionless)

*Proof.*

We derive the coupling formula from first principles using the Yang-Mills effective action.

**Step 1. Effective Yang-Mills action on latent space:**

From the covariant derivative $D_\mu Z = \partial_\mu Z - i g_s \, Z \cdot G_\mu$ (Step 2 of Theorem {prf:ref}`thm-isotropic-preserves-color`), the gauge-invariant kinetic term is:

$$
\mathcal{S}_{\text{kin}} = \int d^4x \, \text{Tr}[(D_\mu Z)^\dagger (D^\mu Z)]
$$

Expanding (for Hermitian $G_\mu$ and using cyclicity of the trace):

$$
\mathcal{S}_{\text{kin}} = \int d^4x \, \text{Tr}\left[(\partial_\mu Z)^\dagger (\partial^\mu Z) + i g_s\bigl(G^\mu Z^\dagger (\partial_\mu Z) - (\partial_\mu Z)^\dagger Z G^\mu\bigr) + g_s^2 (Z^\dagger Z)\, G_\mu G^\mu\right]
$$

**Identification of terms:**
- **Free kinetic:** $\mathcal{S}_0 = \int \text{Tr}[(\partial_\mu Z)^\dagger (\partial^\mu Z)]$ (dimensionless for dimensionless $Z$)
- **Interaction:** $\mathcal{S}_{\text{int}} = \int \text{Tr}[i g_s(G^\mu Z^\dagger (\partial_\mu Z) - (\partial_\mu Z)^\dagger Z G^\mu)]$ (linear in $g_s$)
- **Gauge field self-energy:** $\mathcal{S}_{G} = \int g_s^2 \text{Tr}[(Z^\dagger Z)\, G_\mu G^\mu]$ (quadratic in $g_s$)

**Step 2. Gauge coupling definition from action ratios:**

The effective dimensionless coupling constant is defined by the ratio:

$$
g_s^2 := \frac{\langle \mathcal{S}_{G} \rangle}{\langle \mathcal{S}_0 \rangle}
$$

where $\langle \cdot \rangle$ denotes expectation over the weight distribution at layer $\ell$.

**Explicit computation:**

$$
g_s^2 = \frac{\int \text{Tr}[(Z^\dagger Z)\, G_\mu G^\mu]}{\int \text{Tr}[(\partial_\mu Z)^\dagger (\partial^\mu Z)]}
= \frac{\langle (Z^\dagger Z)\, G_\mu G^\mu \rangle}{\langle (\partial_\mu Z)^\dagger (\partial^\mu Z) \rangle}
$$

For approximately uniform field configurations, this simplifies to:

$$
g_s \approx \frac{\|G_\mu\|_{\text{op}}}{\|\partial_\mu Z\|_{\text{op}}}
$$

where $\|\cdot\|_{\text{op}}$ denotes operator norm (largest singular value).

**Step 3. Off-diagonal weight norm determines gauge field strength:**

The Binding field $G_\mu$ couples bundles. In matrix representation, write the weight matrix $W^{(\ell)} \in \mathbb{R}^{d_z \times d_z}$ in block form with respect to the bundle decomposition $\mathcal{Z} = \bigoplus_{i=1}^{n_b} V_i$:

$$
W^{(\ell)} = \begin{pmatrix}
W_{11} & W_{12} & \cdots & W_{1 n_b} \\
W_{21} & W_{22} & \cdots & W_{2 n_b} \\
\vdots & \vdots & \ddots & \vdots \\
W_{n_b 1} & W_{n_b 2} & \cdots & W_{n_b n_b}
\end{pmatrix}
$$

where each $W_{ij} \in \mathbb{R}^{d_b \times d_b}$ is the coupling from bundle $j$ to bundle $i$.

**Diagonal vs off-diagonal decomposition:**

$$
W^{(\ell)} = W_{\text{diag}}^{(\ell)} + W_{\text{off}}^{(\ell)}
$$

where:
- $W_{\text{diag}} = \text{diag}(W_{11}, W_{22}, \ldots, W_{n_b n_b})$ (intra-bundle)
- $W_{\text{off}}$ has zeros on block diagonal (inter-bundle coupling)

The gauge field $G_\mu$ is proportional to the **inter-bundle coupling**:

$$
\|G_\mu\|_{\text{op}} = \|W_{\text{off}}^{(\ell)}\|_{\text{op}} \cdot \kappa_\ell
$$

where $\kappa_\ell$ is a geometry-dependent prefactor (typically $\kappa_\ell \sim 1$ for normalized weights).

**Step 4. Barrier modulation of off-diagonal coupling:**

The norm-gating barrier $b_\ell$ enters through the **effective off-diagonal weights** after applying the gate $v \mapsto v \cdot g(\|v\| + b)$.

For bundle $i$, the effective weight from bundle $j \neq i$ is:

$$
W_{ij}^{\text{eff}} = W_{ij} \cdot \frac{g'(\|v_j\| + b_\ell)}{\|v_j\|}
$$

**Barrier suppression mechanism:** For large positive $b_\ell$ (strong barrier):
- GELU derivative: $g'(\|v\| + b_\ell) \approx \Phi(\|v\| + b_\ell) + (\|v\| + b_\ell) \phi(\|v\| + b_\ell)$
- For $\|v\| \sim 1$ and $b_\ell \gg 1$: $g'(1 + b_\ell) \to 1$ (saturates)
- The barrier **enhances** coupling at large $b$ (counter-intuitive but correct for IR confinement)

For small $b_\ell \to 0$ (weak barrier):
- $g'(1) \approx 1.08$, minimal modulation
- Bundles weakly coupled (UV asymptotic freedom)

**Averaged effective norm:**

$$
\|W_{\text{off}}^{\text{eff}}\|_{\text{op}} = \langle g'(\|v\| + b_\ell) \rangle \cdot \|W_{\text{off}}\|_{\text{op}}
$$

Define the **barrier strength factor**:

$$
\beta_\ell := \frac{\langle g'(\|v\| + b_\ell) - g'(\|v\|) \rangle}{\max_b |g'(\|v\| + b) - g'(\|v\|)|}
$$

For GELU and unit-norm $v$, this is approximately:

$$
\beta_\ell \approx \tanh(b_\ell)
$$

Thus:

$$
\|G_\mu^{(\ell)}\|_{\text{op}} = \beta_\ell \cdot \xi_\ell
$$

where $\xi_\ell := \|W_{\text{off}}^{(\ell)}\|_{\text{op}}$ is the bare off-diagonal weight norm.

**Step 5. Kinetic term from forward gradient flow:**

The kinetic term $\|\partial_\mu Z\|$ measures latent state changes across layers. For discrete layer index $\ell$:

$$
\partial_\ell z := z^{(\ell)} - z^{(\ell-1)}
$$

**Normalized gradient flow ratio:**

$$
\eta_\ell := \frac{\|\partial_\ell z\|_2}{\|z^{(\ell-1)}\|_2}
$$

For spectrally normalized networks with $\|W^{(\ell)}\|_{\text{op}} \leq 1$:

$$
\|z^{(\ell)}\|_2 \leq \|W^{(\ell)}\|_{\text{op}} \cdot \|z^{(\ell-1)}\|_2 \leq \|z^{(\ell-1)}\|_2
$$

Thus norms are non-increasing. The **relative change** is:

$$
\eta_\ell = \left\| \frac{W^{(\ell)} \cdot z^{(\ell-1)}}{\|z^{(\ell-1)}\|} - \frac{z^{(\ell-1)}}{\|z^{(\ell-1)}\|} \right\|_2 = \left\| \left(W^{(\ell)} - I\right) \hat{z}^{(\ell-1)} \right\|_2
$$

where $\hat{z} = z/\|z\|$ is the unit-normalized latent.

**Typical magnitude:** For residual architectures $W^{(\ell)} = I + \epsilon \Delta W^{(\ell)}$ with small $\epsilon$:

$$
\eta_\ell \approx \epsilon \|\Delta W^{(\ell)}\|_{\text{op}} \sim 0.01 \text{ to } 0.1
$$

**Step 6. Final coupling formula:**

Substitute Steps 4 and 5 into Step 2:

$$
g_s^{(\ell)} = \frac{\|G_\mu^{(\ell)}\|_{\text{op}}}{\|\partial_\mu z^{(\ell)}\|_{\text{op}}} = \frac{\beta_\ell \cdot \xi_\ell}{\eta_\ell \cdot \|z^{(\ell-1)}\|}
$$

For unit-normalized latents $\|z^{(\ell)}\| = 1$ (enforced by normalization layers):

$$
g_s^{(\ell)} = \frac{\beta_\ell \cdot \xi_\ell}{\eta_\ell}
$$

**Geometric averaging form:** To handle varying $\eta_\ell$ across layers, use the normalized form:

$$
g_s^{(\ell)} = \beta_\ell \cdot \frac{\xi_\ell}{\sqrt{\xi_\ell^2 + \eta_\ell^2}}
$$

This ensures:
- $g_s \in [0, |\beta_\ell|]$ (bounded coupling)
- $g_s \to \beta_\ell$ when $\xi_\ell \gg \eta_\ell$ (gauge field dominates)
- $g_s \to 0$ when $\eta_\ell \gg \xi_\ell$ (kinetic term dominates)

**Dimensional verification:**
- $[\beta_\ell]$ = dimensionless
- $[\xi_\ell] = [\|W\|]$ = dimensionless (operator norm)
- $[\eta_\ell] = [\|\partial_\ell z\| / \|z\|]$ = dimensionless
- $[g_s^{(\ell)}]$ = dimensionless ✓

**Step 7. Verify IR/UV limits:**

- **Infrared ($b_\ell \to +\infty$, deep layers):** $\beta_\ell \to 1$, and typically $\xi_\ell \sim O(1)$ (off-diagonal weights not suppressed), thus:

$$
g_s^{(\ell)} \to \frac{\xi_\ell}{\sqrt{\xi_\ell^2 + \eta_\ell^2}} \approx 1 \quad \text{if } \xi_\ell \gg \eta_\ell
$$
This gives **strong coupling** $g_s \sim O(1)$, leading to confinement (color-neutral states only).

- **Ultraviolet ($b_\ell \to 0$, shallow layers):** $\beta_\ell \to 0$, thus:

$$
g_s^{(\ell)} \to 0 \quad \text{(weak coupling, asymptotic freedom)}
$$
Bundles decouple and evolve independently.

- **Deeply suppressed ($b_\ell \to -\infty$):** $\beta_\ell \to -1$, giving negative coupling (unphysical). This regime is prevented by initialization with $b_\ell \geq 0$ and reparameterization $b_\ell = \text{softplus}(\tilde{b}_\ell)$ to enforce positivity.

**Comparison with QCD:** This running coupling behavior matches quantum chromodynamics:
- **IR**: $g_s(\mu_{\text{IR}}) \sim 1$ → confinement (quarks bound into hadrons)
- **UV**: $g_s(\mu_{\text{UV}}) \to 0$ → asymptotic freedom (quarks behave as free particles at high energy)

Here, the "energy scale" $\mu$ is replaced by layer depth $\ell$, with deep layers corresponding to IR and shallow layers to UV.

This matches the required behavior from Corollary {prf:ref}`cor-coupling-window` in the parameter sieve.

$\square$
:::

:::{prf:theorem} Spectral Norm Bounds Hypercharge Dissipation
:label: thm-spectral-preserves-hypercharge

The Opportunity field $B_\mu$ (from {ref}`sec-symplectic-multi-agent-field-theory`) couples to hypercharge $Y$. Spectral normalization ensures hypercharge is **non-increasing** under forward propagation:

$$
Y(W \cdot z) \leq Y(z) \quad \text{(hypercharge cannot increase)}
$$

*Proof.*

**Step 1. Hypercharge definition:** $Y(z) := \|z\|^2$ (quadratic quantity proportional to squared norm).

**Step 2. Contraction (NOT isometry):** Spectral normalization with $\sigma_{\max}(W) \leq 1$ ensures:

$$
\|W \cdot z\| \leq \sigma_{\max}(W) \cdot \|z\| \leq \|z\|
$$

This is a **contraction**, not an isometry. Equality $\|W \cdot z\| = \|z\|$ holds only when:
- $z$ is aligned with the top singular vector of $W$, AND
- $\sigma_{\max}(W) = 1$ exactly

For general $z$, we have strict inequality $\|W \cdot z\| < \|z\|$.

**Step 3. Hypercharge bound:**

$$
Y(W \cdot z) = \|W \cdot z\|^2 \leq \|z\|^2 = Y(z)
$$

**Interpretation:** Hypercharge dissipates (decreases) through linear layers but cannot spontaneously increase. This implements a one-way flow consistent with the second law of thermodynamics in the information-theoretic sense.

**Step 4. Conservation requires orthogonality:**
For **exact** hypercharge conservation $Y(W \cdot z) = Y(z)$ for all $z$, we would need:

$$
\|W \cdot z\| = \|z\| \quad \forall z \in \mathcal{Z}
$$

This requires $W$ to be **orthogonal**: $W^T W = I$. Spectral normalization does NOT enforce orthogonality; it only bounds the largest singular value.

$\square$

**Connection to Node 56 (CapacityHorizonCheck):** Hypercharge saturation $Y \to Y_{\max}$ indicates approaching capacity limit (holographic bound). The non-increasing property ensures the system cannot exceed this bound through forward propagation.

**Remark on exact conservation:** If exact hypercharge conservation is required (not just non-increasing), constrain $W$ to be orthogonal via Cayley parameterization or exponential map from skew-symmetric matrices:

$$
W = \exp(A) \quad \text{where } A^T = -A \text{ (skew-symmetric)}
$$
This guarantees $W^T W = I$ and thus $\|W \cdot z\| = \|z\|$ exactly.

**Remark on U(1)_Y correspondence:** The mapping "Spectral Normalization → U(1)_Y" is **analogical**, not a literal group isomorphism:
- **True U(1)_Y gauge symmetry** (from Standard Model): Phase rotations $z \to e^{i\alpha(x)} z$ with $\alpha(x)$ spacetime-dependent
- **Spectral normalization implements**: Norm bounding $\|W \cdot z\| \leq \|z\|$ (contraction, not rotation)

Both conserve a scalar charge: U(1) conserves $Q = \int \bar{\psi}\gamma^0\psi$ (electric charge), spectral norm conserves $Y = \|z\|^2$ (hypercharge/capacity). The **correspondence** is that both implement conservation laws for a U(1)-like scalar quantity, ensuring bounded capacity. However, spectral normalization does **not** implement U(1) rotations; it implements **Lipschitz contraction**.

For a literal U(1) rotation representation, one would parametrize weights as $W = e^{i\theta} W_0$ with $\theta \in [0, 2\pi)$. Spectral normalization instead parametrizes via singular values $W = U \Sigma V^T$ with $\sigma_1 \leq 1$.
:::

:::{prf:definition} Observation-Action Doublet Structure
:label: def-obs-action-doublet

The latent space decomposes into **observation** and **action planning** subspaces:

$$
\mathcal{Z} = \mathcal{Z}_{\text{obs}} \oplus \mathcal{Z}_{\text{act}}
$$

These form an $SU(2)_L$ doublet:

$$
\Psi = \begin{pmatrix} \psi_{\text{obs}} \\ \psi_{\text{act}} \end{pmatrix} \in \mathcal{Z}_{\text{obs}} \oplus \mathcal{Z}_{\text{act}}
$$

**Full SU(2)_L transformation:** The special unitary group $SU(2)$ has 3 real parameters. A general element is:

$$
U(\vec{\theta}) = \exp\left(i \sum_{a=1}^3 \theta_a \frac{\sigma_a}{2}\right), \quad \vec{\theta} = (\theta_1, \theta_2, \theta_3) \in \mathbb{R}^3
$$
where $\{\sigma_a\}_{a=1}^3$ are Pauli matrices:

$$
\sigma_1 = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}, \quad \sigma_2 = \begin{pmatrix} 0 & -i \\ i & 0 \end{pmatrix}, \quad \sigma_3 = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}
$$

**Real SO(2) subgroup (current implementation):** For real-valued neural networks, we restrict to the $SO(2)$ subgroup that performs real rotations in the $(\psi_{\text{obs}}, \psi_{\text{act}})$ plane. In the Pauli basis this corresponds to the one-parameter subgroup generated by $i\sigma_2$ (since $i\sigma_2 = \begin{psmallmatrix} 0 & 1 \\ -1 & 0 \end{psmallmatrix}$ is the canonical $\mathfrak{so}(2)$ generator). This gives 1-parameter transformations:

$$
U_{\text{SO(2)}}(\theta) = \begin{pmatrix} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \end{pmatrix}, \quad \theta \in [0, 2\pi)
$$

**Action on doublet:**

$$
\Psi \to \Psi' = U_{\text{SO(2)}}(\theta) \Psi = \begin{pmatrix} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \end{pmatrix} \begin{pmatrix} \psi_{\text{obs}} \\ \psi_{\text{act}} \end{pmatrix}
$$

**Physical interpretation:**
- $\theta = 0$: Pure observation (no action planning)
- $\theta = \pi/2$: Pure action planning (no new observations)
- $0 < \theta < \pi/2$: Mixed observation-action processing

**Chirality:** This is a **left-chiral** doublet (incoming information stream). Right-chiral singlets correspond to executed actions (outgoing, no longer subject to mixing).

**Remark:** Full $SU(2)_L$ gauge theory requires complex-valued features. Current real-valued architectures (IsotropicBlock + SteerableConv) implement the $SO(2) \subset SU(2)$ subgroup. Extension to full 3-parameter $SU(2)$ requires complex steerable CNNs or quaternionic networks (see {ref}`sec-symplectic-multi-agent-field-theory` for gauge-theoretic derivation).
:::

:::{prf:theorem} Observation-Action Doublet as an $SO(2)$ (U(1)) Representation
:label: thm-steerable-induces-doublet

Let $\psi_{\text{obs}} \in \mathcal{Z}_{\text{obs}}$ and $\psi_{\text{act}} \in \mathcal{Z}_{\text{act}}$. Define the observation-action pair

$$
\Psi = \begin{pmatrix} \psi_{\text{obs}} \\ \psi_{\text{act}} \end{pmatrix}.
$$
Under the one-parameter subgroup $U_{\text{SO(2)}}(\theta)$ from Definition {prf:ref}`def-obs-action-doublet`, $\Psi$ transforms as $\Psi' = U_{\text{SO(2)}}(\theta)\Psi$, i.e., it is a 2D real representation of $SO(2)\cong U(1)$ (the implemented subgroup of $SU(2)_L$ in current real-valued architectures).

*Proof.*

**Step 1. Group action:** The map $\theta \mapsto U_{\text{SO(2)}}(\theta)$ is a representation of $SO(2)$ since

$$
U_{\text{SO(2)}}(\theta_1)U_{\text{SO(2)}}(\theta_2)=U_{\text{SO(2)}}(\theta_1+\theta_2)
$$
and $U_{\text{SO(2)}}(0)=I$.

**Step 2. Doublet transformation:** By Definition {prf:ref}`def-obs-action-doublet`,

$$
\Psi' = U_{\text{SO(2)}}(\theta)\Psi
= \begin{pmatrix} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \end{pmatrix}
\begin{pmatrix} \psi_{\text{obs}} \\ \psi_{\text{act}} \end{pmatrix},
$$
so the pair closes as a 2D real $SO(2)$ (equivalently $U(1)$) representation. $\square$

:::

:::{prf:definition} Latent Metric Tensor
:label: def-latent-metric

The latent space $\mathcal{Z}$ is equipped with the **Information Sensitivity Metric** (from Section {ref}`sec-capacity-constrained-metric-law-geometry-from-interface-limits`, Theorem {prf:ref}`thm-capacity-constrained-metric-law`):

$$
G_{ij}(z) = \nabla^2_{ij} V(z) + \lambda \mathcal{F}_{ij}(z)
$$

where:
- $V(z)$ is the value function (expected return from state $z$)
- $\mathcal{F}_{ij}(z) = \mathbb{E}_{a \sim \pi(·|z)}[\nabla_{z_i} \log \pi(a|z) \nabla_{z_j} \log \pi(a|z)]$ is the Fisher Information Metric
- $\lambda > 0$ is the temperature parameter

**Physical interpretation:** The metric $G$ measures how sensitive the value function and policy are to changes in latent coordinates. High curvature regions indicate sensitive decision boundaries.

**Positive definiteness:** $G(z)$ is positive definite when $V$ is strongly convex and $\lambda > 0$.

**Units:** $[G_{ij}] = [\mathcal{Z}]^{-2} = \text{nat}^{-1}$ (inverse information).

**Variational origin** (sketch of derivation from Theorem {prf:ref}`thm-capacity-constrained-metric-law`):

The metric arises from minimizing the effective action under capacity constraints:

$$
G_{ij}(z) = \frac{\delta^2}{\delta z_i \delta z_j} \mathcal{A}_{\text{eff}}[z]
$$
where $\mathcal{A}_{\text{eff}}$ is the capacity-constrained effective action:

$$
\mathcal{A}_{\text{eff}}[z] = \int \left[V(z) + \frac{\lambda}{2} I(Z;A|z)\right] dz
$$

The **Hessian of the value function** $\nabla^2 V(z)$ captures curvature of the reward landscape (second-order approximation to the value surface), while the **Fisher Information Metric** $\mathcal{F}_{ij}(z)$ captures the sensitivity of the policy distribution $\pi(a|z)$ to latent perturbations.

**First variation** yields the Euler-Lagrange equations for optimal latent dynamics (geodesic equations on the WFR manifold). **Second variation** gives the metric as the Hessian of the action.

**Full derivation:** See Section 5.1 (Capacity-Constrained Metric Law) for the complete variational derivation from the bounded rationality Lagrangian, including the proof that $G$ is the unique metric satisfying the Monge-Ampère equation under holographic constraints.
:::

:::{prf:theorem} Composition of Equivariant Layers is Equivariant
:label: thm-composition-equivariant

Let $f_1, \ldots, f_L$ be $G$-equivariant layers. Then:

$$
F = f_L \circ \cdots \circ f_1 \text{ is } G\text{-equivariant}
$$

Moreover, if each $f_i$ has Lipschitz constant $L_i \leq 1$, then:

$$
L_F \leq 1 \quad \text{(global light cone preservation)}
$$

where $L_F$ is the Lipschitz constant of the composition $F$.

*Proof.*

**Equivariance:** Composition of equivariant maps is equivariant. For any $g \in G$:

$$
F(\rho(g) \cdot z) = f_L \circ \cdots \circ f_1(\rho(g) \cdot z) = \rho'(g) \cdot F(z)
$$
by repeatedly applying $f_i(\rho_i(g) \cdot w) = \rho_{i+1}(g) \cdot f_i(w)$.

**Lipschitz:** For any $z_1, z_2$:

$$
\|F(z_1) - F(z_2)\| = \|f_L \circ \cdots \circ f_1(z_1) - f_L \circ \cdots \circ f_1(z_2)\|
$$
Applying the Lipschitz property of each layer successively:

$$
\leq L_L \|f_{L-1} \circ \cdots \circ f_1(z_1) - f_{L-1} \circ \cdots \circ f_1(z_2)\| \leq \cdots \leq \left(\prod_{i=1}^L L_i\right) \|z_1 - z_2\|
$$
When $L_i \leq 1$ for all $i$, we have $L_F \leq \prod_i L_i \leq 1$. $\square$
:::

:::{prf:lemma} NormGate Lipschitz Bound
:label: lem-normgate-lipschitz

Let $f(v) = v \cdot g(\|v\| + b)$ be the norm-gated activation (Definition {prf:ref}`def-norm-gated-activation`) where $g: \mathbb{R} \to \mathbb{R}$ is a smooth gating function with:
- $|g(x)| \leq C_g |x|$ for all $x$ (sublinear growth)
- $|g'(x)| \leq L_g$ for all $x$ (bounded derivative)

Assume an operating range $\|v\| \leq R_{\max}$ and a bounded bias $|b| \leq B$. Then $f$ is Lipschitz on the operating range with constant:

$$
L_f \leq \max\bigl(R_{\max}(C_g + L_g) + C_gB,\; C_g(R_{\max} + B)\bigr).
$$
In typical settings with $L_g > 0$ and $R_{\max} \gtrsim 1$, the radial term $R_{\max}(C_g + L_g) + C_gB$ dominates.

**For GELU:** The GELU function $g(x) = x\Phi(x)$ where $\Phi$ is the standard normal CDF satisfies:
- $C_g = 1$ (since $0 \leq \Phi(x) \leq 1$ implies $|g(x)| \leq |x|$)
- $g'(x) = \Phi(x) + x\phi(x)$ where $\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$
- $\sup_{x \in \mathbb{R}} g'(x) \approx 1.129$ (achieved at $x^* = \sqrt{2} \approx 1.414$ where $g''(x^*) = 0$)
- For practical operating range $x \in [-3, 3]$: $\max_{x \in [-3,3]} g'(x) \approx 1.129$ (same critical point)

**Derivation of critical point:**
$$g''(x) = \frac{d}{dx}[\Phi(x) + x\phi(x)] = \phi(x) + \phi(x) - x^2\phi(x) = \phi(x)(2 - x^2)$$
Setting $g''(x) = 0$ yields $x^2 = 2$, so $x^* = \sqrt{2}$ (taking positive root). At this point: $g'(\sqrt{2}) = \Phi(\sqrt{2}) + \sqrt{2}\phi(\sqrt{2}) \approx 0.9214 + 0.2075 \approx 1.129$.

Thus $L_g \approx 1.129$ and

$$
L_f \leq R_{\max}(1 + 1.129) + B \approx 2.129\,R_{\max} + B.
$$

*Proof.*

**Step 1. Spherical coordinates:**
Write $v = r\hat{v}$ where $r = \|v\| \geq 0$ and $\hat{v} = v/\|v\|$ is the unit direction vector.

Then:

$$
f(v) = f(r\hat{v}) = r \cdot g(r + b) \cdot \hat{v}
$$

**Step 2. Jacobian calculation:**

Recall $f(v) = v \cdot g(\|v\| + b)$ where $\|v\| = \sqrt{v^T v}$. We compute the Jacobian $\nabla f(v) = \frac{\partial f}{\partial v} \in \mathbb{R}^{d_b \times d_b}$.

**Decompose using product rule:**

$$
\frac{\partial f_i}{\partial v_j} = \frac{\partial}{\partial v_j}[v_i \cdot g(\|v\| + b)] = \delta_{ij} g(\|v\| + b) + v_i \frac{\partial g}{\partial v_j}
$$

**Compute gradient of norm:**

$$
\frac{\partial \|v\|}{\partial v_j} = \frac{\partial}{\partial v_j} (v^T v)^{1/2} = \frac{1}{2\|v\|} \cdot 2v_j = \frac{v_j}{\|v\|}
$$

**Apply chain rule to $g(\|v\| + b)$:**

$$
\frac{\partial g(\|v\| + b)}{\partial v_j} = g'(\|v\| + b) \cdot \frac{\partial(\|v\| + b)}{\partial v_j} = g'(\|v\| + b) \cdot \frac{v_j}{\|v\|}
$$

**Substitute into Jacobian:**

$$
\frac{\partial f_i}{\partial v_j} = \delta_{ij} g(\|v\| + b) + v_i \cdot g'(\|v\| + b) \cdot \frac{v_j}{\|v\|}
$$

**Matrix form:**

$$
\nabla f(v) = g(\|v\| + b) \cdot I_{d_b} + g'(\|v\| + b) \cdot \frac{vv^T}{\|v\|}
$$

This is the sum of a scaled identity and a rank-1 matrix.

**Step 3. Eigenvalue decomposition:**
Write $v = r\hat{v}$ where $r = \|v\|$ and $\|\hat{v}\| = 1$. The Jacobian has eigenvalues:

- **Radial direction** (eigenvector $\hat{v}$):

$$
\lambda_r = g(r+b) + g'(r+b) \cdot r
$$

- **Tangential directions** (eigenvectors orthogonal to $\hat{v}$, there are $d_b-1$ of these):

$$
\lambda_t = g(r+b)
$$

**Verification:** For $u = \hat{v}$:

$$
\nabla f(v) \cdot \hat{v} = g(r+b)\hat{v} + g'(r+b) r \hat{v} = [g(r+b) + rg'(r+b)]\hat{v} = \lambda_r \hat{v} \quad \checkmark
$$

For $u \perp \hat{v}$ (so $u^T v = 0$):

$$
\nabla f(v) \cdot u = g(r+b) u + g'(r+b) \frac{(v^T u)}{\|v\|} v = g(r+b) u = \lambda_t u \quad \checkmark
$$

**Step 4. Bound eigenvalues:**
Using $|g(x)| \leq C_g|x|$ and $|g'(x)| \leq L_g$:

**Radial:**

$$
|\lambda_r| = |g(r+b) + rg'(r+b)| \leq |g(r+b)| + r|g'(r+b)| \leq C_g|r+b| + rL_g
$$

For $r \leq R_{\max}$ and bounded bias $|b| \leq B$:

$$
|\lambda_r| \leq C_g(R_{\max} + B) + R_{\max}L_g = R_{\max}(C_g + L_g) + C_gB
$$

**Tangential:**

$$
|\lambda_t| = |g(r+b)| \leq C_g|r+b| \leq C_g(R_{\max} + B)
$$

**Step 5. Spectral norm and Lipschitz constant:**
The operator norm is:

$$
\|\nabla f(v)\|_{\text{op}} = \max(|\lambda_r|, |\lambda_t|) \leq \max(R_{\max}(C_g + L_g) + C_gB, C_g(R_{\max} + B))
$$

Since typically $L_g > 0$, the radial term dominates:

$$
L_f = \sup_v \|\nabla f(v)\|_{\text{op}} \leq R_{\max}(C_g + L_g) + C_gB
$$

**Step 6. Numerical values for GELU:**
For GELU with $C_g = 1$, $L_g \approx 1.129$, and typical bias $|b| \leq 1$:

$$
L_f \leq R_{\max}(1 + 1.129) + 1 \approx 2.129 R_{\max} + 1
$$

With normalized inputs, $R_{\max} \approx \sqrt{d_b}$ for $d_b$-dimensional bundles. For $d_b = 16$:

$$
L_f \lesssim 2.129 \times 4 + 1 \approx 9.5
$$

**Remark 1 (Composition with spectral norm):** While individual NormGate layers have $L_f > 1$, they compose with spectral-normalized linear layers (which have $L = 1$). The total Lipschitz constant for IsotropicBlock is bounded by the product, and layer normalization or skip connections prevent unbounded growth across depth.

**Remark 2 (Rescaling option):** To enforce strict 1-Lipschitz property, rescale GELU:

$$
\tilde{g}(x) = \frac{g(x)}{R_{\max}(C_g + L_g) + C_gB}
$$

This guarantees $L_f \leq 1$ but attenuates gradients. In practice, we keep unscaled GELU and rely on:
- Spectral normalization in linear layers
- Moderate bundle dimensions ($d_b \in [8, 32]$)
- Skip connections across blocks

to control the effective Lipschitz constant of the full network.
$\square$
:::

:::{prf:definition} Micro-Macro Consistency
:label: def-micro-macro-consistency

A DNN layer $f: \mathcal{Z} \to \mathcal{Z}$ is **compatible with the geodesic integrator** if:

1. **Preserves metric (pullback):** Writing $J(z) := \frac{\partial f}{\partial z}(z)$, the pulled-back metric is
   
   $$
   (f^*G)(z) = J(z)^T\,G(f(z))\,J(z),
   $$
   and strict metric preservation (isometry) is $(f^*G)(z) = G(z)$.
2. **Bounded Lipschitz:** $\|f(z) - f(z')\| \leq L_f \cdot \|z - z'\|$ where $L_f$ is a finite constant
3. **Preserves gauge:** $f(U(g) \cdot z) = U(g) \cdot f(z)$ for all $g \in G_{\text{Fragile}}$

**Interpretation:**
- **Condition 1** ensures the metric structure evolves consistently (metric pullback)
- **Condition 2** ensures bounded signal propagation (Lipschitz continuity); for strict light cone preservation, require $L_f \leq 1$; for stable bounded amplification, allow $L_f = O(1)$ fixed constant
- **Condition 3** ensures gauge symmetry is preserved through network layers

**Units:** $[G] = [z]^{-2}$, $[L_f]$ dimensionless (ratio of distances), $[U(g)]$ dimensionless (unitary).
:::

:::{prf:theorem} Isotropic Blocks Satisfy Micro-Macro Consistency
:label: thm-isotropic-macro-compatible

IsotropicBlock (Definition {prf:ref}`def-isotropic-block`) satisfies:
1. **Bounded Lipschitz** (Condition 2 of {prf:ref}`def-micro-macro-consistency`) with $L_f \approx 9.5$ - proven rigorously
2. **Gauge invariance** (Condition 3 of {prf:ref}`def-micro-macro-consistency`) - proven rigorously
3. **Metric compatibility** (Condition 1 of {prf:ref}`def-micro-macro-consistency`) - pullback is well-defined, but exact isometry is not enforced (qualified)

*Proof.*

**Condition 2 (light cone preservation) - RIGOROUS:**

**Step 1. Decompose IsotropicBlock:**

$$
f = f_3 \circ f_2 \circ f_1
$$
where:
- $f_1 = \text{SpectralLinear}: z \mapsto Wz$ with $\sigma_{\max}(W) \leq 1$
- $f_2 = \text{Reshape}: \mathbb{R}^{d_{\text{out}}} \to (\mathbb{R}^{d_b})^{n_b}$ (identity as linear map, $J_2 = I$)
- $f_3 = \text{NormGate}: (v_1, \ldots, v_{n_b}) \mapsto (v_1 g(\|v_1\| + b_1), \ldots, v_{n_b} g(\|v_{n_b}\| + b_{n_b}))$

**Step 2. Lipschitz constants:**
- By Theorem {prf:ref}`thm-spectral-preserves-light-cone`: $L_1 \leq 1$
- $f_2$ is identity: $L_2 = 1$
- By Lemma {prf:ref}`lem-normgate-lipschitz` with GELU ($C_g = 1$, $L_g \approx 1.129$):

$$
L_3 \leq R_{\max}(C_g + L_g) + C_gB \approx 2.129 R_{\max} + 1
$$

For normalized bundles with $R_{\max} \approx \sqrt{d_b}$ and $d_b = 16$:

$$
L_3 \lesssim 2.129 \times 4 + 1 \approx 9.5
$$

**Step 3. Composition:**
By composition of Lipschitz functions ($\|f \circ g(x) - f \circ g(y)\| \leq L_f L_g \|x-y\|$):

$$
L_{\text{total}} = L_3 \cdot L_2 \cdot L_1 \leq 9.5 \cdot 1 \cdot 1 = 9.5
$$

**Interpretation: Bounded Amplification, Not Strict Light Cone Preservation**

The Lipschitz constant $L \approx 9.5$ means the IsotropicBlock can **amplify** signals by a bounded factor. This is *different* from strict light cone preservation (which would require $L \leq 1$, ensuring no amplification).

**Why amplification is acceptable:**
1. **Per-layer bound:** Each layer amplifies by at most $\times 9.5$, a fixed constant
2. **Composition depth:** For deep networks with $D$ layers, naive bound gives $L_{\text{total}} \leq (9.5)^D$, but this is pessimistic:
   - In practice, growth is controlled by normalization layers, residual scaling, and explicit diagnostics
3. **Gradient clipping:** Combined with gradient normalization in training, prevents runaway amplification

**Relationship to causal structure:**
- **SpectralLinear alone** has $L = 1$ (strict light cone preservation, Theorem {prf:ref}`thm-spectral-preserves-light-cone`)
- **NormGate** introduces bounded amplification ($L \approx 9.5$ for GELU with $d_b = 16$)
- **Net effect:** Information propagation is *bounded* but not *contractive*

**Practical consideration:**
For multi-layer networks, use explicit $\ell_2$ normalization after IsotropicBlock if strict $L \leq 1$ is required:
```python
z = isotropic_block(z)
z = z / z.norm(dim=-1, keepdim=True) * target_norm  # Renormalize
```

Thus:

$$
\|f(z_1) - f(z_2)\| \leq 9.5 \|z_1 - z_2\|
$$

**Remark on constant factor:** While $L > 1$ for individual blocks, depth-wise accumulation is controlled via:
- Skip connections (e.g., $z_{l+1} = z_l + \alpha \cdot \text{IsotropicBlock}(z_l)$ with $\alpha < 1$)
- Normalization layers between blocks
- The bound $L = O(\sqrt{d_b})$ is moderate for $d_b \in [8, 32]$

The effective light cone constraint $\|f(z_1) - f(z_2)\| \lesssim c_{\text{info}} \Delta t$ holds up to architecture-dependent constants.

**Condition 3 (gauge invariance) - RIGOROUS:**

Direct application of Theorem {prf:ref}`thm-isotropic-block-equivariant`. For gauge group $G_{\text{bundle}} = \prod_{i=1}^{n_b} SO(d_b)$:

$$
f(U(g) \cdot z) = U(g) \cdot f(z) \quad \forall g \in G_{\text{bundle}}
$$

This is proven constructively in Theorem {prf:ref}`thm-isotropic-block-equivariant` by showing each component (SpectralLinear, Reshape, NormGate) is equivariant and composition preserves equivariance.

**Condition 1 (metric preservation) - QUALIFIED:**

The strict isometry condition from Definition {prf:ref}`def-micro-macro-consistency`,

$$
G(z) = J(z)^T\,G(f(z))\,J(z),
$$
depends critically on whether the metric is constant or state-dependent.

**Case A: Constant Euclidean metric** ($G(z) = I$ for all $z$)

The pullback condition becomes:

$$
I = J^T I J = J^T J
$$

This requires $J$ to be orthogonal, which is NOT true for IsotropicBlock:
- SpectralLinear: $J_1 = W$ with $\sigma_{\max}(W) \leq 1$ satisfies $W^T W \preceq I$ (contraction, not isometry unless $W$ is exactly orthogonal)
- NormGate: $J_3 = g(\|v\|+b)I + g'(\|v\|+b)vv^T/\|v\|$ satisfies:

$$
J_3^T J_3 = g^2(\|v\|+b) I + \left[\frac{2g(\|v\|+b)g'(\|v\|+b)}{\|v\|} + g'^2(\|v\|+b)\right] vv^T \neq I
$$

Thus **exact isometry** $(f^*G)(z)=G(z)$ fails for the constant Euclidean metric.

**Case B: Information Sensitivity Metric** (Definition {prf:ref}`def-latent-metric`)

For state-dependent $G(z) = \nabla^2 V(z) + \lambda \mathcal{F}(z)$, the pullback requires:

$$
\nabla^2 V(z) + \lambda \mathcal{F}(z) = J^T [\nabla^2 V(f(z)) + \lambda \mathcal{F}(f(z))] J
$$

Since $V$ and $\mathcal{F}$ depend on state, $G(f(z)) \neq G(z)$ in general, and **exact isometry** $(f^*G)(z)=G(z)$ does not hold.

**What IS rigorously true:**

1. **Positive (semi)definiteness under pullback:** If $G \succ 0$, then for any Jacobian $J$ we have $J^T G J \succeq 0$, and if $J$ has full column rank (in particular, if $J$ is invertible) then $J^T G J \succ 0$. One convenient bound (when $J$ is full-rank) is:

$$
J^T G(z) J \succeq \lambda_{\min}(J^T J) \cdot \lambda_{\min}(G(z)) > 0
$$
Thus the pullback cannot introduce negative directions; it can only collapse directions if $J$ is rank-deficient.

2. **Structure preservation:** NormGate acts isotropically within bundles:

$$
J_3^{(i)} = g_i I_{d_b} + h_i v_i v_i^T
$$
where $g_i = g(\|v_i\| + b_i)$ and $h_i = g'(\|v_i\| + b_i)/\|v_i\|$. This preserves radial-tangential structure of metrics that decompose similarly.

3. **Empirical compatibility:** Diagnostic Node 67 (GaugeInvarianceCheck) verifies that applying gauge transformations produces consistent behavior, indicating the metric structure is preserved in the sense relevant for geodesic integration.

**Conclusion:** IsotropicBlock does NOT satisfy the exact isometry condition $G(z) = J^T G(f(z)) J$ for general metrics. However, it preserves:
- Gauge structure (Condition 3)
- Lipschitz bounds (Condition 2)
- Positive-definiteness and structural properties of the metric

For the geodesic integrator, this is sufficient because the metric is recomputed at each integration step rather than being pulled back through transformations.

$\square$
:::

:::{prf:theorem} Metric Pullback Defect Under Composition
:label: thm-approximate-metric-preservation

Let $F = f_L \circ \cdots \circ f_1$ be a deep network on $\mathcal{Z}$, and let $G$ be a (possibly state-dependent) metric tensor on $\mathcal{Z}$.

Define $z_0 := z$, $z_\ell := f_\ell(z_{\ell-1})$, and Jacobians $J_\ell := \frac{\partial f_\ell}{\partial z}(z_{\ell-1})$. Let $J_F := J_L \cdots J_1$.

Define the per-layer **isometry defect**:

$$
E_\ell := J_\ell^T\,G(z_\ell)\,J_\ell - G(z_{\ell-1}).
$$

Then the total pullback defect admits the exact decomposition:

$$
J_F^T\,G(z_L)\,J_F - G(z_0)
=
\sum_{\ell=1}^L (J_{\ell-1}\cdots J_1)^T\,E_\ell\,(J_{\ell-1}\cdots J_1),
$$
with the convention $J_0\cdots J_1 := I$.

In particular, if $\|J_\ell\|_{\mathrm{op}} \leq L_J$ and $\|E_\ell\|_{\mathrm{op}} \leq e_\ell$, then:

$$
\|J_F^T\,G(F(z))\,J_F - G(z)\|_{\mathrm{op}} \leq \sum_{\ell=1}^L L_J^{2(\ell-1)}\,e_\ell.
$$

So when $L_J \le 1$ the defect accumulates at most linearly in $\sum_\ell e_\ell$, while for $L_J>1$ the naive worst-case bound grows like $L_J^{2L}$.

*Proof.* Induction on $L$.

**Remark (Geodesic integrator):** The Boris-BAOAB integrator (Chapter 4, Section {ref}`sec-geodesic-integrator`) recomputes the metric $G(z_t)$ at each timestep $t$ and does not assume layer-wise isometries (it does not require $f^*G=G$ for each layer).
:::

:::{prf:proposition} Latent Dimension from Information-Theoretic First Principles
:label: prop-latent-dimension-from-capacity

The latent space $\mathcal{Z} \subset \mathbb{R}^{d_z}$ represents compressed observations encoding mutual information $I(X;Z) \leq C$ where $C$ is the channel capacity (nat/step) from the bounded rationality controller (Chapter 1).

**Derivation from Gaussian rate-distortion theory:**

For a Gaussian source $X \sim \mathcal{N}(0, \Sigma_X)$ encoded into latent $Z \in \mathbb{R}^{d_z}$ via encoder $p(Z|X)$ with reconstruction $\hat{X} = \mathbb{E}[X|Z]$:

**Step 1. Rate-distortion tradeoff:**
The optimal encoder for squared error distortion $D = \mathbb{E}[\|X - \hat{X}\|^2]$ achieves:

$$
I(X;Z) = \frac{1}{2}\sum_{i=1}^{d_z} \log\left(1 + \frac{\lambda_i}{\sigma^2}\right)
$$
where $\lambda_i$ are eigenvalues of the source covariance $\Sigma_X$ allocated to latent dimension $i$, and $\sigma^2$ is the noise level per dimension.

**Step 2. Equal allocation (isotropic latent):**
For computational efficiency, neural encoders typically use isotropic latent representations with equal variance per dimension:

$$
\Sigma_Z = \sigma_z^2 I_{d_z}
$$

The total information is:

$$
I(X;Z) \leq \frac{1}{2} d_z \log\left(1 + \frac{\sigma_X^2}{\sigma_{\text{noise}}^2}\right)
$$

**Step 3. Dimensional analysis from first principles:**

The mutual information formula (Step 1) is:

$$
I(X;Z) = \frac{1}{2}\sum_{i=1}^{d_z} \log\left(1 + \frac{\lambda_i}{\sigma^2}\right)
$$

**Logarithm constraint:** The logarithm function requires a dimensionless argument. Therefore:

$$
\left[1 + \frac{\lambda_i}{\sigma^2}\right] = [1] \quad \text{(dimensionless)}
$$

This implies:

$$
\left[\frac{\lambda_i}{\sigma^2}\right] = [1] \quad \Rightarrow \quad [\lambda_i] = [\sigma^2]
$$

**Information-theoretic foundation:** In Shannon information theory, differential entropy for a Gaussian random variable $X \sim \mathcal{N}(0, \Sigma)$ is **defined** to have units [nat]:

$$
h(X) = \frac{1}{2} \log \det(2\pi e \Sigma) \quad [\text{nat}]
$$

For a single dimension with variance $\lambda$:

$$
h(X_i) = \frac{1}{2}\log(2\pi e \lambda) \quad [\text{nat}]
$$

**Critical distinction:** The numerical prefactor $1/2$ is dimensionless (as all pure numbers are). The "nat" unit arises from the **operational definition** of information in Shannon's framework: entropy measures the expected log-probability, and we adopt the convention $[h(X)] = [\text{nat}]$ to distinguish information content from dimensionless logarithms. This is analogous to how we define $[E] = \text{Joule}$ in physics—it's a choice of unit system, not algebraic dimension propagation.

**Dimensional interpretation:** The formula should be understood as:

$$
h(X) = \left[\frac{1}{2}\log \det(2\pi e \Sigma)\right]_{\text{nat}}
$$
where the subscript indicates the dimensionless logarithm is **measured in units** of nats (the information-theoretic unit), not that it algebraically has dimension [nat].

**Dimensional convention (NOT derivation):** We **adopt the convention** that latent coordinates have dimension:

$$
\boxed{[z] = [\mathcal{Z}] := \sqrt{\text{nat}}}
$$

**This is a choice**, not a theorem. Here's why we make this choice:

**Step 3a. Consistency requirement:**
If we want variance $[\sigma_z^2]$ to have the **same units** as information measures (differential entropy $h(X)$ in nats), and if coordinates are related to variance by $[z^2] = [\sigma_z^2]$, then we must have:

$$
[z] = \sqrt{[\sigma_z^2]} = \sqrt{[\text{nat}]}
$$

**Step 3b. Motivation for the choice:**
This convention ensures dimensional consistency across the framework:
- Rate-distortion: $I(X;Z) = \frac{1}{2}\sum_i \log(\lambda_i/\sigma^2)$ requires $[\lambda_i] = [\sigma^2]$; setting both equal to [nat] makes $I$ dimensionally consistent with entropy
- Fisher metric: $\mathcal{F}_{ij} = \mathbb{E}[\partial_i \log p \, \partial_j \log p]$ has $[\mathcal{F}] = [z]^{-2}$; if $[z] = \sqrt{\text{nat}}$ then $[\mathcal{F}] = \text{nat}^{-1}$, matching information-theoretic quantities
- Capacity: $C$ (nat/step) becomes commensurate with $\|z\|^2$ (nat) and $d_z \times \sigma_z^2$ (dimensionless × nat)

**Step 3c. What is NOT claimed:**
- We do **not** claim this follows logically from information theory alone
- We do **not** claim $[\lambda] = [\text{nat}]$ is forced by mathematics—it's a **definition** we choose
- This is analogous to setting $c = 1$ in relativity: a **unit convention** that simplifies equations, not a physical law

**Interpretation:** Each latent coordinate carries information measured in natural units (nats). The variance of a latent dimension represents information content. This convention parallels quantum mechanics where position $x$ relates to momentum $p$ via $\Delta x \Delta p \sim \hbar$ with $[\hbar] = \text{action}$ giving $[x] \sim \sqrt{\text{action}}$.

**Remark on arbitrariness:** This **is** an arbitrary convention in the sense that we could equally well work in dimensionless units throughout and track "nat" as a label rather than a dimension. We choose to promote "nat" to a pseudo-dimension because:
1. It makes dimensional analysis track information flow explicitly
2. It connects latent-space geometry to information-theoretic foundations
3. It parallels established physics conventions (action, angular momentum as dimensional units)
:::

:::{prf:definition} Information Speed in Latent Coordinates
:label: def-information-speed-latent

The **latent information speed** is the maximum rate of latent state change per unit time:

$$
c_{\mathcal{Z}} := \sup_{z(·), \Delta t > 0} \frac{d_{\mathcal{Z}}(z(t + \Delta t), z(t))}{\Delta t}
$$

where:
- $z: [0, T] \to \mathcal{Z}$ is a latent trajectory
- $d_{\mathcal{Z}}(z_1, z_2) = \|z_1 - z_2\|$ is the Euclidean distance in latent space
- The supremum is taken over all admissible trajectories and time increments

**Dimensions**: $[c_{\mathcal{Z}}] = [\mathcal{Z}][T^{-1}] = \sqrt{\text{nat}} \cdot [T^{-1}]$ where $[T]$ denotes abstract time dimension (measured in seconds for physical agents)

**Physical interpretation**: This is the "speed of thought"—the maximum rate at which the agent's internal representation can evolve under the dynamics.

**Connection to environment information speed**:

Let $\mathcal{E}$ denote the environment observation space (e.g., pixel space for vision, $\mathcal{E} = \mathbb{R}^{H \times W \times C}$).

Let $\phi: \mathcal{E} \to \mathcal{Z}$ be the encoder network mapping observations to latents, with Jacobian $J_\phi(x) = \nabla \phi(x) \in \mathbb{R}^{d_z \times d_{\mathcal{E}}}$.

By the chain rule for composed dynamics $z(t) = \phi(x(t))$:

$$
\frac{dz}{dt} = J_\phi(x(t)) \cdot \frac{dx}{dt}
$$

Taking norms:

$$
\left\|\frac{dz}{dt}\right\| \leq \|J_\phi(x)\|_{\text{op}} \cdot \left\|\frac{dx}{dt}\right\|
$$

If the environment dynamics satisfy $\|dx/dt\| \leq c_{\text{info}}$ (Axiom {prf:ref}`ax-information-speed-limit` from Chapter 8.1), then:

$$
c_{\mathcal{Z}} \leq \sup_x \|J_\phi(x)\|_{\text{op}} \cdot c_{\text{info}}
$$

**Remark**: The encoder Lipschitz constant $L_\phi = \sup_x \|J_\phi(x)\|_{\text{op}}$ controls how environmental changes propagate to latent space. Spectral normalization ensures the **linear** parts satisfy $\|W\|_{\text{op}}=\sigma_{\max}(W)\leq 1$; the overall $L_\phi$ is then bounded by the product of per-block Lipschitz bounds, so strict $L_\phi \leq 1$ requires also using 1-Lipschitz nonlinearities (or explicitly tracking/rescaling their gain).

**Operational constraint**: For strict causality preservation (Theorem {prf:ref}`thm-spectral-preserves-light-cone`), every **linear** map must satisfy $\sigma_{\max}(W) \leq 1$ and each nonlinear block should satisfy $L_f \leq 1$; if bounded amplification is allowed, track the resulting global Lipschitz bound to maintain a controlled speed limit.
:::

:::{prf:proposition} Dimensional Consistency of IsotropicBlock
:label: prop-dimensional-consistency

Let $z \in \mathcal{Z}$ with $[z] = [\mathcal{Z}] = \sqrt{\text{nat}}$ (Proposition {prf:ref}`prop-latent-dimension-from-capacity`). The IsotropicBlock operation

$$
\text{IsotropicBlock}(z) = \text{Reshape}(\text{NormGate}(\text{SpectralLinear}(z)))
$$
preserves the latent dimension $[\mathcal{Z}]$ through each stage when interpreted with implicit normalization conventions.

*Proof.*

**Dimensional Analysis Axioms:**
1. $[AB] = [A][B]$ (multiplicative)
2. $[A + B]$ defined iff $[A] = [B]$ (additive homogeneity)
3. For transcendental function $f: \mathbb{R} \to \mathbb{R}$ (like GELU), the argument must be dimensionless or implicitly normalized

**Step 1. SpectralLinear:**

$$
[W \cdot z] = [W] \cdot [z] = \frac{[\mathcal{Z}']}{[\mathcal{Z}]} \cdot [\mathcal{Z}] = [\mathcal{Z}']
$$
where $[\mathcal{Z}'] = [\mathcal{Z}]$ (linear map between same-dimensional latent spaces).

**Step 2. Reshape:**
Identity operation that permutes indices: $[\text{Reshape}(h)] = [h] = [\mathcal{Z}]$.

**Step 3. NormGate per bundle $i$ — natural units convention:**

**Convention adopted:** We work in **natural units** where the reference latent scale is:

$$
z_0 := 1 \quad \text{(in units of } \sqrt{\text{nat}}\text{)}
$$

This is analogous to setting $c = \hbar = 1$ in relativistic quantum mechanics. Under this convention, latent coordinates and norms are dimensionless pure numbers when expressed in units of $z_0$.

**Justification:** Spectral normalization $\sigma_{\max}(W) \leq 1$ ensures $\|v_i\| \sim O(z_0)$, making the natural unit system well-defined.

**Dimensional analysis under natural units:**

Recall the definition (Def. {prf:ref}`def-norm-gated-activation`):

$$
f(v_i) = v_i \cdot g(\|v_i\| + b_i)
$$

In natural units where $z_0 = 1$:
- $[\|v_i\|] = [v_i] = [\mathcal{Z}]$ (latent dimension)
- $[b_i] = [\mathcal{Z}]$ (homogeneous addition)
- The argument $\|v_i\| + b_i$ is **a pure number** (ratio to $z_0 = 1$)
- $g: \mathbb{R} \to \mathbb{R}$ takes dimensionless input and returns dimensionless output
- Therefore $[f(v_i)] = [v_i] \cdot [1] = [\mathcal{Z}]$ ✓

**Step 4. Output dimension:**

$$
[\text{IsotropicBlock}(z)] = [\mathcal{Z}]
$$

$\square$

**Implementation note:** The code writes:
```python
gate = F.gelu(energy + self.norm_bias)
```
where `energy = ||v_i||` is numerically $O(1)$ due to spectral normalization. This directly implements the natural units convention with $z_0 = 1$ absorbed.

**Alternative (strict dimensional analysis):** For explicit dimensional tracking without natural units, the formula would be:

$$
f(v) = v \cdot g\left(\frac{\|v\| + b}{z_0}\right)
$$
where $z_0 = \mathbb{E}[\|v\|]$ is the expected bundle norm with $[z_0] = [\mathcal{Z}]$. In normalized architectures, $z_0 \approx 1\,\sqrt{\text{nat}}$, reducing to the natural units case.
:::

:::{prf:definition} Gauge Violation Metric
:label: def-gauge-violation-metric

For operator $f$ and group element $g \in G$:

$$
\delta_{\text{gauge}}(f, g) = \mathbb{E}_z\left[\|f(U(g) \cdot z) - U(g) \cdot f(z)\|^2\right]
$$

**Threshold:** $\delta_{\text{gauge}} < \epsilon_{\text{gauge}} = 10^{-4}$ (empirically tuned).
:::

## 08_multiagent/05_architecture.md

:::{prf:proposition} Failure Modes of Flat World Models
:label: prop-failure-modes-flat-world-models

A world model $f: \mathcal{Z} \times \mathcal{A} \to \mathcal{Z}$ implemented as a standard neural network (GRU, MLP, Transformer) built from flat-space operations does not, in general, *guarantee* preservation of:

1. **Metric structure**: The capacity-constrained metric $G(z)$ from Theorem {prf:ref}`thm-capacity-constrained-metric-law` implies position-dependent step sizes. Flat operations use constant step sizes.

2. **Gauge covariance**: Under local gauge transformation $\psi \to U(z)\psi$, predictions must transform covariantly. Flat operations are not gauge-aware.

3. **Symplectic structure**: The phase space $(\mathcal{Z} \times T^*\mathcal{Z}, \omega)$ has a conserved 2-form. Flat operations generically break symplectic conservation.

4. **Boundary constraints**: In the Poincare ball/disk model ($|z|<1$), the metric diverges as $|z| \to 1$, enforcing vanishing physical step size near the boundary. Flat operations can produce invalid states unless constrained explicitly.

*Consequence*: Flat world models require extensive regularization to approximately enforce these constraints, with no guarantee of exact satisfaction.

:::

:::{prf:definition} Lorentz-Langevin SDE (Recap)
:label: def-lorentz-langevin-recap

From Definition {prf:ref}`def-bulk-drift-continuous-flow`, the position coordinates evolve as:

$$
dz^k = \underbrace{\left( -G^{kj}\partial_j \Phi + u_\pi^k \right)}_{\text{gradient + control}} ds + \underbrace{\beta_{\text{curl}} G^{km} \mathcal{F}_{mj} \dot{z}^j ds}_{\text{Lorentz force}} - \underbrace{\Gamma^k_{ij}\dot{z}^i \dot{z}^j ds}_{\text{geodesic correction}} + \underbrace{\sqrt{2T_c}(G^{-1/2})^{kj} dW^j_s}_{\text{thermal noise}}
$$

where:
- $G^{kj}$ is the inverse metric (Theorem {prf:ref}`thm-capacity-constrained-metric-law`)
- $\Phi$ is the effective potential (Definition {prf:ref}`def-effective-potential`)
- $\mathcal{F}_{mj}$ is the Value Curl tensor (Definition {prf:ref}`def-value-curl`)
- $\Gamma^k_{ij}$ are Christoffel symbols of the Levi-Civita connection
- $T_c$ is the cognitive temperature ({prf:ref}`def-cognitive-temperature`)

:::

:::{prf:definition} Covariant Derivative (Recap)
:label: def-covariant-derivative-recap

From Theorem {prf:ref}`thm-emergence-opportunity-field`, the gauge-covariant derivative for the full gauge group $G_{\text{Fragile}} = SU(N_f)_C \times SU(2)_L \times U(1)_Y$ is:

$$
D_\mu = \partial_\mu - i g_s \frac{\lambda^a}{2} G_\mu^a - i g_2 \frac{\sigma^b}{2} W_\mu^b - i g_1 \frac{Y}{2} B_\mu
$$

where:
- $G_\mu^a$ ($a = 1, \ldots, N_f^2-1$) is the Binding field (Theorem {prf:ref}`thm-emergence-binding-field`)
- $W_\mu^b$ ($b = 1, 2, 3$) is the Error field (Theorem {prf:ref}`thm-emergence-error-field`)
- $B_\mu$ is the Opportunity field (Theorem {prf:ref}`thm-emergence-opportunity-field`)
- $\lambda^a, \sigma^b$ are generators of $SU(N_f)$ and $SU(2)$ respectively (we use $\sigma$ to avoid clashing with the softmax temperature $\tau(z)$ below)
- $g_s, g_2, g_1$ are coupling constants
- $Y$ is the hypercharge

:::

:::{prf:definition} Wilson Line
:label: def-wilson-line

The **Wilson line** (parallel transport operator) along a path $\gamma$ from $z_0$ to $z$ is:

$$
U_\gamma(z, z_0) = \mathcal{P}\exp\left(-i\int_\gamma A_\mu dx^\mu\right)
$$

where:
- $\mathcal{P}$ denotes path ordering
- $A_\mu = g_s \frac{\lambda^a}{2} G_\mu^a + g_2 \frac{\sigma^b}{2} W_\mu^b + g_1 \frac{Y}{2} B_\mu$ is the total gauge connection

For infinitesimal paths $\gamma: z_0 \to z_0 + \delta z$:

$$
U(z_0 + \delta z, z_0) \approx I - i A_\mu(z_0) \delta z^\mu + O(\delta z^2)
$$

*Gauge transformation*: Under $\psi(z) \to \Omega(z)\psi(z)$, the Wilson line transforms as:

$$
U_\gamma(z, z_0) \to \Omega(z) U_\gamma(z, z_0) \Omega^\dagger(z_0)
$$

This ensures that $U_\gamma(z, z_0) \psi(z_0)$ transforms correctly at $z$.

:::

:::{prf:definition} Poincare Ball/Disk Metric (Recap)
:label: def-poincare-metric-recap

The capacity-constrained metric on the Poincare ball (disk when $d=2$) $\mathbb{D}^d = \{z \in \mathbb{R}^d : |z| < 1\}$ is:

$$
G_{ij}(z) = \lambda(z)^2 \delta_{ij} = \frac{4}{(1-|z|^2)^2} \delta_{ij}
$$

where $\lambda(z) = 2/(1-|z|^2)$ is the **conformal factor**.

**Key properties**:
- As $|z| \to 1$: $\lambda(z) \to \infty$ (metric diverges at boundary)
- At origin $z = 0$: $\lambda(0) = 2$ (minimal metric)
- Inverse metric: $G^{ij}(z) = \lambda(z)^{-2} \delta^{ij} = \frac{(1-|z|^2)^2}{4} \delta^{ij}$

The **Christoffel symbols** for this metric are (Proposition {prf:ref}`prop-explicit-christoffel-symbols-for-poincare-disk`):

$$
\Gamma^k_{ij}(z) = \frac{2}{1-|z|^2}\left(\delta^k_i z_j + \delta^k_j z_i - \delta_{ij} z^k\right)
$$

:::

:::{prf:definition} Covariant Query-Key-Value Projections
:label: def-covariant-qkv-projections

Let $\psi_{\text{obs}}(z)$ be the observation latent and $\psi_{\text{act}}(z')$ be the action latent at positions $z, z' \in \mathcal{Z}$. The **covariant projections** are:

$$
\begin{aligned}
Q(z) &= \Pi_Q \cdot U_{z \to 0} \cdot D_\mu \psi_{\text{obs}}(z) \\
K(z') &= \Pi_K \cdot U_{z' \to 0} \cdot D_\nu \psi_{\text{act}}(z') \\
V(z') &= \Pi_V \cdot U_{z' \to 0} \cdot \psi_{\text{act}}(z')
\end{aligned}
$$

where:
- $U_{z \to 0} := U_\gamma(0, z)$ is the Wilson line transporting from $z$ to the origin (Definition {prf:ref}`def-wilson-line`)
- $D_\mu$ is the covariant derivative (Definition {prf:ref}`def-covariant-derivative-recap`)
- $\Pi_Q, \Pi_K, \Pi_V$ are learnable projection maps that act on feature indices (equivariantly on gauge indices) so they commute with gauge transformations at the reference point

**Interpretation**:
- The Wilson line $U_{z \to 0}$ parallel-transports the field to a common reference frame at the origin
- The covariant derivative $D_\mu$ ensures the derivative is gauge-covariant
- Queries use the derivative $D_\mu \psi$ (sensitivity to position change)
- Values use the field $\psi$ directly (the actual content to retrieve)

:::

:::{prf:theorem} Gauge Invariance of Covariant Cross-Attention
:label: thm-gauge-invariance-cross-attention

Let the attention score be computed as:

$$
\alpha(z, z') = \text{softmax}_{z'}\left(\frac{\operatorname{Re}\left(Q(z)^\dagger K(z')\right)}{\tau(z)}\right)
$$

where $Q$ and $K$ are defined with Wilson line preprocessing (Definition {prf:ref}`def-covariant-qkv-projections`), transporting both to a common reference point (the origin). (If the representation is purely real/orthogonal, replace $^\dagger$ with transpose and drop $\operatorname{Re}(\cdot)$.)

Then $\alpha(z, z')$ is **gauge-invariant**: under local gauge transformation $\psi \to \Omega(x)\psi$, the attention score is unchanged.

*Proof.*

**Step 1.** Under gauge transformation $\psi(x) \to \Omega(x)\psi(x)$, the Wilson line transforms as (Definition {prf:ref}`def-wilson-line`):

$$
U_{z \to 0} \to \Omega(0) U_{z \to 0} \Omega^\dagger(z)
$$

**Step 2.** The covariant derivative transforms covariantly:

$$
D_\mu \psi(z) \to \Omega(z) D_\mu \psi(z)
$$

**Step 3.** The Query at $z$ transforms as:

$$
Q(z) = \Pi_Q U_{z \to 0} D_\mu \psi(z) \to \Pi_Q \cdot \Omega(0) U_{z \to 0} \Omega^\dagger(z) \cdot \Omega(z) D_\mu \psi(z)
$$

The $\Omega^\dagger(z) \Omega(z) = I$ factors cancel:

$$
Q(z) \to \Pi_Q \Omega(0) U_{z \to 0} D_\mu \psi(z) = \Omega(0) Q(z)
$$

where the last equality holds because $\Pi_Q$ acts on feature indices while $\Omega(0)$ acts on gauge indices (they commute).

**Step 4.** Similarly, the Key transforms as:

$$
K(z') \to \Omega(0) K(z')
$$

**Step 5.** The attention score is the inner product at the origin:

$$
\operatorname{Re}\left(Q(z)^\dagger K(z')\right) \to \operatorname{Re}\left((\Omega(0)Q)^\dagger (\Omega(0)K)\right) = \operatorname{Re}\left(Q^\dagger \Omega(0)^\dagger \Omega(0) K\right) = \operatorname{Re}\left(Q^\dagger K\right)
$$

The $\Omega(0)^\dagger \Omega(0) = I$ cancellation occurs because both Q and K have been transported to the same reference point.

*Remark (Why no additional Wilson line).* Since Q and K are already transported to the origin via Definition {prf:ref}`def-covariant-qkv-projections`, their inner product is well-defined without an additional Wilson line. The gauge transformation at the common reference point cancels automatically.

$\square$

:::

:::{prf:proposition} Wilson Line Approximation for Attention
:label: prop-wilson-line-approximation

For attention between positions $z$ and $z'$ with $|z - z'| \ll 1$, the Wilson line can be approximated as:

$$
U(z, z') \approx I - i A_\mu(\bar{z}) (z - z')^\mu
$$

where $A_\mu$ is the total gauge connection from Definition {prf:ref}`def-wilson-line`, $\bar{z}$ is any point along the path (choices differ only at $O(|z-z'|^2)$), and the contraction $A_\mu(\bar{z}) (z - z')^\mu$ is the path-directional connection.

In the attention mechanism, this becomes a **relative position encoding**:

$$
\text{score}(z, z') := \operatorname{Re}\left(Q(z)^\dagger U(z, z') K(z')\right)
\approx \operatorname{Re}\left(Q(z)^\dagger K(z')\right) + \operatorname{Re}\left(-i\,Q(z)^\dagger \left[A_\mu(\bar{z}) (z - z')^\mu\right] K(z')\right)
$$

The second term is the **gauge correction** to the attention score.

:::

:::{prf:theorem} Metric-Temperature Correspondence
:label: thm-metric-temperature-correspondence

Let the attention mechanism use position-dependent temperature $\tau(z)$:

$$
\alpha(z, z') = \text{softmax}_{z'}\left(\frac{s(z, z')}{\tau(z)}\right)
$$

where $s(z,z')$ is any real-valued score (for example $s(z,z')=\operatorname{Re}(Q(z)^\dagger K(z'))$).

The choice

$$
\tau(z) = \frac{\sqrt{d_k}}{\lambda(z)} = \sqrt{d_k} \cdot \frac{1-|z|^2}{2}
$$

where $\lambda(z) = 2/(1-|z|^2)$ is the conformal factor, implies:

1. **Metric encoding (conformal case)**: for $G(z)=\lambda(z)^2 I$, the metric scale is recovered exactly as $G(z)=\tfrac{d_k}{\tau(z)^2}I$
2. **Boundary sharpening**: $\tau(z) \to 0$ as $|z| \to 1$, so $\text{softmax}(s/\tau)$ concentrates on the argmax for fixed scores $s(z,\cdot)$

*Proof.*

**Step 1 (Metric encoding).** For a conformal metric $G(z)=\lambda(z)^2 I$ and the stated choice of $\tau(z)$,

$$
\frac{d_k}{\tau(z)^2} = \frac{d_k}{(\sqrt{d_k}/\lambda(z))^2} = \lambda(z)^2,
$$
so $G(z)=\tfrac{d_k}{\tau(z)^2}I$ holds identically.

**Step 2 (Boundary sharpening).** As $|z| \to 1$:

$$
\tau(z) = \sqrt{d_k} \cdot \frac{1-|z|^2}{2} \to 0
$$

For any fixed score vector $s(z,\cdot)$ with a unique maximizer, $\text{softmax}(s/\tau)$ converges to a point mass on the maximizer as $\tau\to 0$.

$\square$

:::

:::{prf:proposition} Mass–Metric–Temperature Identity
:label: prop-mass-metric-inverse-temperature

The Mass = Metric principle (Definition {prf:ref}`def-mass-tensor`) extends to attention:

$$
\mathbf{M}(z) = G(z) = \lambda(z)^2 I = \frac{d_k}{\tau(z)^2} I
$$

**Implication (conformal case)**: Large $\lambda(z)$ (large metric/mass scale) corresponds to small $\tau(z)$ (sharper softmax scaling).

:::

:::{prf:definition} Geodesic Query Projection
:label: def-geodesic-query-projection

The **Geodesic Query** extends the linear projection to include geometric terms that encode the Levi-Civita connection (with optional velocity-conditioned corrections when you want to go beyond Levi-Civita):

$$
Q_{\text{geo}}(x, z, v) = W_Q x + W_{Qz} z + W_{Qv} x_v + W_{Q,\Gamma}(z, z) + W_{Qzv}(z, v)
$$

where:
- $W_Q \in \mathbb{R}^{d_k \times d_{\text{model}}}$ is the feature projection
- $W_{Qz} \in \mathbb{R}^{d_k \times d}$ maps geometric coordinates
- $W_{Qv} \in \mathbb{R}^{d_k \times d_{\text{model}}}$ projects velocity/momentum features $x_v = \phi_v(v)$
- $W_{Q,\Gamma} \in \mathbb{R}^{d_k \times d \times d}$ is a 3-tensor encoding position-position quadratic terms
- $W_{Qzv} \in \mathbb{R}^{d_k \times d \times d}$ encodes optional position-velocity coupling

Here $x_v = \phi_v(v)$ is a feature embedding of velocity/momentum.

**Notation**: $(A, B)$ denotes bilinear contraction: $W_{Q,\Gamma}(z, z) = \sum_{ij} W_{Q,\Gamma}^{a,ij} z^i z^j$ and $W_{Qzv}(z, v) = \sum_{ij} W_{Qzv}^{a,ij} z^i v^j$.

The $W_{Qzv}$ term is optional; for Levi-Civita connections it can be omitted. In practice, a Hadamard coupling $z \odot v$ followed by a linear map is often sufficient.

**Christoffel Encoding**: Use $W_{Qz}$ to capture the linear-in-$z$ part of $\Gamma(z)$ near a reference point $z_0$, and use $W_{Q,\Gamma}$ for nonlinear corrections with learnable position dependence. Both can be initialized from the Poincare structure and refined during training.

:::

:::{prf:theorem} Geodesic Correction Representability via Attention
:label: thm-geodesic-correction-attention

Let the Query and Key be:

$$
\begin{aligned}
Q(x, z, v) &= W_Q x + W_{Qz} z + W_{Qv} x_v + W_{Q,\Gamma}(z, z) \\
K(z', v') &= W_K z' + W_{Kv} v'
\end{aligned}
$$

For clarity, we omit the optional $W_{Qzv}(z, v)$ term; it adds a velocity-conditioned correction without changing the representability argument.

The attention-weighted output

$$
\Delta z = \sum_{z'} \alpha(z, z') V(z')
$$

can represent the geodesic correction term $-\Gamma^k_{ij}(z) v^i v^j$ when:

1. The Values include quadratic velocity features (e.g., $V(z',v') = W_V\,\text{vec}(v' \otimes v')$ or a low-rank factorization)
2. The Query provides a learned parameterization of the (symmetric) coefficients $\Gamma(z)$ via the geometric terms ($W_{Qz}$, $W_{Q,\Gamma}$)
3. The context provides velocities in a neighborhood of the current velocity so the quadratic form is sampled/approximated locally

*Proof sketch.* The attention score is:

$$
s(z, z') := \operatorname{Re}\left(Q(x, z, v)^\dagger K(z', v')\right)
\quad(\text{for real features, } s = Q(x,z,v)^T K(z',v'))
$$

With Values containing (symmetrized) quadratic features of $v'$, an attention-weighted sum can form a linear combination of $v'^i v'^j$ terms. The geometric Query terms supply position-dependent coefficients, so the module can approximate the contraction $\Gamma^k_{ij}(z)\,v^i v^j$ up to the approximation induced by softmax and context discretization. $\square$

:::

:::{prf:proposition} Explicit Christoffel Encoding for Poincare Ball/Disk
:label: prop-christoffel-encoding-poincare

For the Poincare ball/disk with Christoffel symbols (Proposition {prf:ref}`prop-explicit-christoffel-symbols-for-poincare-disk`):

$$
\Gamma^k_{ij}(z) = \frac{2}{1-|z|^2}\left(\delta^k_i z_j + \delta^k_j z_i - \delta_{ij} z^k\right)
$$

A practical initialization is to use a linear geometric term $W_{Qz} z$ to reproduce the linear-in-$z$ structure above (up to a sign convention that can be absorbed into the update rule), and reserve $W_{Q,\Gamma}(z,z)$ for nonlinear corrections. The conformal factor $2/(1-|z|^2)$ is position-dependent and cannot be represented by a constant $W_{Qz}$ alone; capture it via $W_{Q,\Gamma}$ or an explicit scalar modulation.

**Learnable approximation**: Since $\Gamma$ depends on position, a simple parameterization is to scale the nonlinear correction by the conformal factor:

$$
Q_\Gamma(z) = W_{Qz} z + \frac{2}{1-|z|^2}\,\tilde{W}_{Q,\Gamma}(z, z)
$$

where $\tilde{W}_{Q,\Gamma}$ is a learnable tensor initialized to approximate higher-order corrections and then refined by training.

:::

:::{prf:definition} Observation-Action Doublet in Attention
:label: def-observation-action-doublet-attention

The attention mechanism operates on **doublet-valued** representations:

$$
\Psi_L(z) = \begin{pmatrix} \psi_{\text{obs}}(z) \\ \psi_{\text{act}}^{\text{pre}}(z) \end{pmatrix} \in \mathbb{C}^{2d}
$$

The **Query** extracts observation information, the **Key** encodes action information, both with Wilson line transport to origin (cf. Definition {prf:ref}`def-covariant-qkv-projections`):

$$
\begin{aligned}
Q_{\text{obs}}(z) &= \Pi_{\text{obs}} \cdot U_{z \to 0} \cdot D_\mu \Psi_L(z) = \begin{pmatrix} 1 & 0 \end{pmatrix} U_{z \to 0} D_\mu \Psi_L \\
K_{\text{act}}(z') &= \Pi_{\text{act}} \cdot U_{z' \to 0} \cdot D_\nu \Psi_L(z') = \begin{pmatrix} 0 & 1 \end{pmatrix} U_{z' \to 0} D_\nu \Psi_L
\end{aligned}
$$

The **cross-attention** score between observation and action is (gauge-invariant by Theorem {prf:ref}`thm-gauge-invariance-cross-attention`):

$$
\alpha_{\text{cross}}(z, z') = \text{softmax}\left(\frac{\operatorname{Re}\left(Q_{\text{obs}}(z)^\dagger K_{\text{act}}(z')\right)}{\tau(z)}\right)
$$

*Interpretation*: The observation at $z$ attends to actions at $z'$. Both are transported to a common reference point, ensuring gauge-invariant comparison.

:::

:::{prf:definition} Chiral Projector from Value Gradient
:label: def-chiral-projector-value-gradient

The **chiral projector** extracts committed actions from the observation-action doublet using a unit $SU(2)$ direction derived from the value gradient:

$$
\hat{n}(z) = \frac{P \nabla V(z)}{\|P \nabla V(z)\|}
$$

where $P: \mathbb{R}^d \to \mathbb{R}^3$ is a learned projection and $\vec{\sigma} = (\sigma_1, \sigma_2, \sigma_3)$ are Pauli matrices (generators of $SU(2)$).

The **projection operator** is:

$$
\Pi_{\text{chirality}}(z) = \frac{1}{2}\left(I_2 + \hat{n}(z) \cdot \vec{\sigma}\right)
$$

The **committed action** is:

$$
\psi_{\text{act}}^{\text{commit}}(z) = \Pi_{\text{chirality}}(z) \cdot \Psi_L(z)
$$

The **commitment strength** (gauge-invariant under $SU(2)$, per feature channel) is:

$$
c(z) = \Psi_L(z)^\dagger \Pi_{\text{chirality}}(z) \Psi_L(z)
$$

**Properties**:
- $\Pi_{\text{chirality}}^2 = \Pi_{\text{chirality}}$ (idempotent)
- $\text{Tr}(\Pi_{\text{chirality}}) = 1$ (rank-1 projector)
- Under $SU(2)$ transformation $\Psi_L \to U\Psi_L$: if $\hat{n}$ is constructed as an adjoint vector, then $\hat{n} \to U\hat{n}U^\dagger$, preserving gauge covariance

**Degeneracy**: When $\|P \nabla V\| \to 0$ (flat value landscape), $\hat{n}$ is undefined. The agent should not commit in ambiguous regions.

:::

:::{prf:theorem} Gauge Covariance of Chiral Projection
:label: thm-gauge-covariance-chiral-projection

The commitment strength $c(z) = \Psi_L^\dagger \Pi_{\text{chirality}} \Psi_L$ is invariant under local $SU(2)_L$ transformations. The projected vector $\Pi_{\text{chirality}} \Psi_L$ transforms covariantly (in the fundamental representation), but gauge-invariant observables are obtained by contracting the $SU(2)$ indices.

*Proof.*

**Step 1.** Under local $SU(2)_L$ transformation $\Psi_L(z) \to U(z)\Psi_L(z)$, where $U(z) \in SU(2)$.

**Step 2.** Assume the unit direction $\hat{n}$ is constructed as an adjoint vector (e.g., via a gauge-covariant map from the value gradient). Then:

$$
\hat{n}(z) \to U(z) \hat{n}(z) U^\dagger(z)
$$

**Step 3.** The projector transforms as:

$$
\Pi_{\text{chirality}} \to U \Pi_{\text{chirality}} U^\dagger
$$

**Step 4.** The committed action:

$$
\psi_{\text{act}}^{\text{commit}} \to (U \Pi U^\dagger)(U \Psi_L) = U \Pi \Psi_L
$$

This transforms in the fundamental representation, not as a singlet.

**Step 5.** To obtain a true singlet, we contract the $SU(2)$ indices:

$$
c(z) = \Psi_L^\dagger \Pi_{\text{chirality}} \Psi_L
$$

This is manifestly gauge-invariant: $\Psi_L^\dagger U^\dagger \cdot U \Pi U^\dagger \cdot U \Psi_L = \Psi_L^\dagger \Pi \Psi_L$.

$\square$

:::

:::{prf:definition} Area Law Screening in Attention
:label: def-area-law-screening-attention

The **screened attention score** between positions $z$ and $z'$ at representation level $\ell$ is:

$$
\alpha_{\text{screened}}(z, z'; \ell) = \alpha_{\text{bare}}(z, z') \cdot \exp\left(-\sigma(\ell) \cdot A_{\text{string}}(z, z')\right)
$$

where:
- $\alpha_{\text{bare}}$ is the gauge-covariant attention score from Theorem {prf:ref}`thm-gauge-invariance-cross-attention`
- $\sigma(\ell)$ is the **string tension** at level $\ell$, with $\sigma(\ell) \propto g_s^2(\ell)$ (binding coupling squared)
- $A_{\text{string}}(z, z')$ is an **area proxy** used for screening (often quadratic in separation)

In practice, after applying screening one renormalizes $\alpha_{\text{screened}}(z,\cdot;\ell)$ over $z'$ so the weights sum to 1.

**Area approximation**: For nearby points in flat metric:

$$
A_{\text{string}}(z, z') \approx \frac{1}{2}|z - z'|^2
$$

For the Poincare ball/disk with conformal factor $\lambda$:

$$
A_{\text{string}}(z, z') \approx \frac{\lambda(z)^2}{2}|z - z'|^2
$$

:::

:::{prf:theorem} Texture Confinement via Area Law Screening
:label: thm-texture-confinement-area-law

Let the representation hierarchy have levels $\ell = 0$ (macro) to $\ell = L$ (texture), with running coupling $g_s(\ell)$ satisfying asymptotic freedom (Definition {prf:ref}`def-coupling-function`):

$$
g_s(\ell) \to 0 \text{ as } \ell \to L \quad (\text{UV, texture level})
$$

$$
g_s(\ell) \to g_s^{\text{crit}} \text{ as } \ell \to 0 \quad (\text{IR, macro level})
$$

Then:

1. **Texture-to-texture attention** ($\ell = L$): $\sigma(L) \to 0$, no screening. Features interact freely at texture level.

2. **Macro-to-texture attention** ($\ell = 0$ attending to $\ell = L$): $\sigma(0) > \sigma_{\text{crit}}$, strong screening. Texture is inaccessible from macro level.

3. **Gauge-singlet access**: Channels transforming in the trivial (color-neutral) representation of $SU(N_f)$ can be exempted from screening, allowing macro-level access to bound-state (concept) features while suppressing color-charged texture.

*Proof.*

**Step 1.** The string tension is proportional to the coupling squared: $\sigma(\ell) = c \cdot g_s^2(\ell)$.

**Step 2.** At texture level, asymptotic freedom gives $g_s(L) \to 0$, hence $\sigma(L) \to 0$. The screening factor $\exp(-\sigma A) \to 1$.

**Step 3.** At macro level, infrared confinement gives $g_s(0) > g_s^{\text{crit}}$, hence $\sigma(0) > \sigma_{\text{crit}}$. For any non-trivial area $A > 0$, the screening factor $\exp(-\sigma_{\text{crit}} A) \ll 1$.

**Step 4.** Gauge-singlet channels do not couple to the binding connection (their generators vanish), so they do not source color flux; equivalently, their effective string tension is zero (or screening is simply not applied on the singlet subspace). Hence the screening factor is 1 on those channels.

$\square$

:::

:::{prf:proposition} Confinement Radius from String Tension
:label: prop-confinement-radius-string-tension

Assuming the local proxy $A_{\text{string}}(z,z') \approx d_G(z,z')^2/2$, the **confinement radius** $r_{\text{conf}}$ (in geodesic distance) is the scale at which screening suppresses attention by a factor $e^{-1}$:

$$
r_{\text{conf}}(\ell) = \sqrt{\frac{2}{\sigma(\ell)}}
$$

At macro level with $\sigma(0) \approx 1$: $r_{\text{conf}}(0) \approx \sqrt{2}$ (order-unity in geodesic units).

At texture level with $\sigma(L) \approx 0.01$: $r_{\text{conf}}(L) \approx 14$ (large, allowing texture-to-texture interaction).

*Interpretation*: Weak screening at texture allows long-range texture-to-texture interaction, while strong screening suppresses macro access to color-charged texture channels.

:::

:::{prf:definition} BAOAB Steps (Attention Heads + OU)
:label: def-baoab-attention-heads

The **GeodesicCrossAttention** module implements B-A-O-A-B with attention heads for B/A and a closed-form OU step in the middle (or an optional learned O-head):

**Step 1 (B-head 1): B-step (First half-kick)**
- **Query**: Current position $z$ with quadratic geodesic terms
- **Key**: Gradient bank $\{\nabla\Phi(z')\}_{z' \in \text{context}}$
- **Value**: Gradient vectors $\{\nabla\Phi(z')\}$
- **Output**: Momentum update $\Delta p_1 = -\frac{h}{2}\nabla\Phi(z)$

**Step 2 (A-head 1): A-step (First half-drift)**
- **Query**: Current position and momentum $(z, p)$
- **Key**: Exponential map or transport bank $\{\exp_{z'}(v)\}$
- **Value**: Displacement corrections
- **Output**: Drift correction $\Delta z_1$ added to the explicit drift $\frac{h}{2}G^{-1}(z)p$

**Step 3 (OU): O-step (Ornstein-Uhlenbeck thermostat)**
- **Default**: Closed-form OU update (no attention)
- **Optional learned thermostat**:
  - **Query**: Current momentum $p$
  - **Key**: Noise bank (random vectors)
  - **Value**: Noise vectors $\{\xi\}$
  - **Output**: Residual correction added to the OU update

**Step 4 (A-head 2): A-step (Second half-drift)**
- Same structure as Step 2
- **Output**: Drift correction $\Delta z_2$ added to the explicit drift $\frac{h}{2}G^{-1}(z)p$

**Step 5 (B-head 2): B-step (Second half-kick)**
- Same structure as Step 1
- **Output**: Momentum update $\Delta p_2 = -\frac{h}{2}\nabla\Phi(z)$

**Composition**: The full update is:

$$
(z_{t+1}, p_{t+1}) = \text{Step}_5 \circ \text{Step}_4 \circ \text{Step}_3 \circ \text{Step}_2 \circ \text{Step}_1(z_t, p_t)
$$

Here $\text{Step}_3$ denotes the OU operator unless a learned thermostat head is enabled.

:::

:::{prf:theorem} BAOAB Attention Targets Boltzmann Distribution (Idealized)
:label: thm-baoab-attention-boltzmann

In the idealized setting where the attention heads recover the BAOAB substeps (kick/drift) and the O-step is the exact OU update, the GeodesicCrossAttention module reduces to the standard BAOAB integrator (Definition {prf:ref}`def-baoab-splitting`). In that limit, it targets the Gibbs/Boltzmann density

$$
\rho(z, p) \propto \exp\left(-\frac{\Phi_{\text{eff}}(z)}{T_c} - \frac{\|p\|_G^2}{2T_c}\right)
$$

as the intended stationary distribution (cf. Proposition {prf:ref}`prop-baoab-preserves-boltzmann`), provided:

1. Each head uses position-dependent temperature $\tau(z) = \sqrt{d_k}/\lambda(z)$
2. The O-step uses thermalization coefficients $c_1 = e^{-\gamma h}$, $c_2 = \sqrt{(1-c_1^2)T_c}$
3. The geodesic Query projections correctly encode Christoffel symbols
4. The A-steps include the explicit drift $G^{-1}(z)p$ (with any attention-based correction consistent with the exponential map)

*Proof sketch.* This follows from Proposition {prf:ref}`prop-baoab-preserves-boltzmann` applied to the attention implementation. The key points:

1. The symmetric splitting B-A-O-A-B ensures time-reversibility of deterministic steps.
2. The A-steps include the explicit drift $G^{-1}p$, with attention providing higher-order corrections.
3. The O-step is an exact OU transition, which leaves the Maxwell-Boltzmann conditional momentum distribution invariant (for fixed $z$ and the chosen mass tensor).
4. Position-dependent temperature correctly weights attention by the metric.
5. Wilson lines preserve gauge covariance without affecting thermodynamic properties.

If a learned thermostat head is enabled, the O-step becomes a data-driven approximation that may deviate from exact OU sampling; treat this as a controlled modeling choice and monitor with diagnostic checks. Likewise, attention-based interpolation of gradients, drift corrections, and Wilson lines introduces additional approximation error beyond the baseline BAOAB discretization. $\square$

:::

:::{prf:proposition} Keys as Gradient Bank
:label: prop-keys-as-force-bank

In the B-step attention heads, the Keys store precomputed gradients of the effective potential at context positions:

$$
K^{(\text{grad})}_{z'} = W_K \cdot \nabla\Phi_{\text{eff}}(z')
$$

The attention score $s(z, z') := \operatorname{Re}\left(Q(z)^\dagger K(z')\right)$ measures how aligned the current position is with the gradient at $z'$. The weighted sum of Values retrieves the local gradient estimate:

$$
\widehat{\nabla\Phi}(z) = \sum_{z' \in \text{context}} \alpha(z, z') \cdot V^{(\text{grad})}(z') = \nabla\Phi_{\text{eff}}(z) + O(\text{interpolation error})
$$

The B-step applies the negative sign: $\Delta p = -\frac{h}{2}\widehat{\nabla\Phi}(z)$.

*Advantage*: Precomputing gradients at context positions amortizes the cost of gradient computation across multiple queries.

:::

:::{prf:proposition} Values as State Updates
:label: prop-values-as-state-updates

In each attention head, the Values encode the update to apply (the OU step is closed-form unless you enable a learned thermostat):

| Head | Value Content | Update |
|:-----|:-------------|:-------|
| B (kick) | $\nabla\Phi$ | $\Delta p$ (with negative sign applied in the update) |
| A (drift) | Displacement correction | $\Delta z$ (added to explicit $G^{-1}p$ drift) |
| O (thermostat) | $c_2\,G^{1/2}\xi$ (OU) | Noise injection (optional learned residual) |

The attention-weighted Value sum produces the correction update, which is added to the explicit drift or momentum update as appropriate.

:::

:::{prf:proposition} Complexity of Naive Implementation
:label: prop-complexity-naive-implementation

The full covariant cross-attention has the following complexity breakdown:

| Component | Naive Complexity | Bottleneck |
|:----------|:-----------------|:-----------|
| Wilson line computation | $O(N^2 d^2)$ | Path integral for each pair |
| Attention scores | $O(N^2 d)$ | All-pairs dot product |
| Quadratic Query | $O(N d^2)$ | Christoffel tensor contraction |
| Area law screening | $O(N^2)$ | String area for each pair |
| Chiral projection | $O(N d)$ | Per-position projection |
| **Total** | $O(N^2 d^2)$ | Dominated by Wilson lines |

For $N = 1000$, $d = 64$: approximately $4 \times 10^9$ operations per layer.

:::

:::{prf:proposition} Gauge-Locality Correspondence (Practical Bound)
:label: thm-gauge-locality-correspondence

Assume area-law screening is applied as a multiplicative factor $\exp(-\sigma A_{\text{string}}(z,z'))$ and use the local approximation
$A_{\text{string}}(z,z') \approx d_G(z,z')^2/2$ (cf. Definition {prf:ref}`def-area-law-screening-attention` and the small-distance relation $d_G(z,z')\approx \lambda(z)\|z-z'\|$).

Then for any tolerance $\epsilon \in (0,1)$, the screening factor satisfies:

$$
\exp\left(-\sigma A_{\text{string}}(z,z')\right) \le \epsilon
\quad\text{whenever}\quad
d_G(z,z') \ge r_{\epsilon} := \sqrt{\frac{2\log(1/\epsilon)}{\sigma}}.
$$

*Consequence*: Beyond $r_\epsilon$, the screened (unnormalized) weights are exponentially small, motivating sparse neighborhoods. Separately, the Wilson-line linearization is accurate only up to a radius $r_{\text{Wilson}}$, so practical sparse neighborhoods should also enforce $d_G(z,z') \lesssim r_{\text{Wilson}}$.

:::

:::{prf:definition} Geodesic Sparse Attention
:label: def-geodesic-sparse-attention

Replace full attention with **geodesic-local sparse attention**:

$$
\alpha_{\text{sparse}}(z, z') = \begin{cases}
\alpha_{\text{full}}(z, z') & \text{if } d_G(z, z') \leq r_{\epsilon} \\
0 & \text{otherwise}
\end{cases}
$$

In practice, renormalize $\alpha_{\text{sparse}}(z,\cdot)$ over the retained neighbors so the weights sum to 1.

**Implementation**: Use a spatial data structure (k-d tree, ball tree, or locality-sensitive hashing) to find the $k$-nearest neighbors in geodesic distance.

**Complexity**: $O(N k d)$ where $k$ is the neighborhood size, typically $k \sim 32$-$128$.

**Gauge-faithfulness**: Exact within the retained neighborhood (up to any Wilson-line approximation used there). If $m_{\text{drop}}(z) := \sum_{d_G(z,z')>r_\epsilon} \alpha_{\text{full}}(z,z')$, then truncating and renormalizing changes the attention distribution by total variation distance at most $m_{\text{drop}}(z)$.

:::

:::{prf:definition} Linearized Covariant Attention
:label: def-linearized-covariant-attention

Approximate the softmax attention kernel using a positive random feature map (a Monte Carlo approximation):

$$
\exp\left(\frac{s}{\tau}\right)
\quad\text{with}\quad
s := \operatorname{Re}\left(Q^\dagger K\right)
\approx \phi(Q/\tau)^T \phi(K)
$$

where $\phi: \mathbb{R}^{d_k} \to \mathbb{R}^D$ is a random feature map with $D \ll N$:

$$
\phi(x) = \frac{e^{-\|x\|^2/2}}{\sqrt{D}} \begin{pmatrix} \exp(\omega_1^T x) \\ \vdots \\ \exp(\omega_D^T x) \end{pmatrix}
$$

with $\omega_i \sim \mathcal{N}(0, I)$. (In expectation, $\mathbb{E}[\phi(q)^T\phi(k)] = \exp(q^T k)$.)

**Linearized attention**:

$$
\text{Attn}_{\text{linear}}(Q, K, V) = \frac{\phi(Q)^T \left(\sum_j \phi(K_j) V_j^T\right)}{\phi(Q)^T \left(\sum_j \phi(K_j)\right)}
$$

**Complexity**: $O(N D d_k)$ where $D \sim 64$-$256$.

**Gauge-faithfulness**: Query-dependent temperature enters through the rescaling $Q \mapsto Q/\tau(z)$ before feature mapping. Wilson line corrections enter through the gauge-covariant construction of $Q$ and $K$ before feature mapping.

:::

:::{prf:definition} Hierarchical Wilson Line Approximation
:label: def-hierarchical-wilson-line

Construct a **hierarchical decomposition** of the latent space $\mathcal{Z}$:

- **Level 0**: Single root node (origin)
- **Level $\ell$**: $2^{\ell d}$ cells of diameter $\sim 2^{-\ell}$
- **Maximum level $L$**: Cells of diameter $\sim r_{\text{Wilson}}$

**Precomputation** ($O(2^{Ld})$ storage):
- For each cell center $c_i$: Store $U_{c_i \to 0}$
- For each cell: Store local connection coefficients $\Theta_i = A_\mu(c_i)$

**Query** ($O(L + d)$ per pair):

$$
U(z, z') \approx U_{\text{local}}(z, c(z))^\dagger \cdot U_{c(z) \to 0}^\dagger \cdot U_{c(z') \to 0} \cdot U_{\text{local}}(z', c(z'))
$$

where $c(z)$ is the center of $z$'s cell and $U_{\text{local}}(z, c)$ approximates transport from $z$ to $c$ within a cell using the linear approximation:

$$
U_{\text{local}}(z, c) \approx I - i \Theta(c) \cdot (c - z)
$$

**Complexity**: $O(N \cdot L \cdot d)$ where $L \sim \log(1/r_{\text{Wilson}})$.

:::

:::{prf:definition} Factorized Christoffel Query
:label: def-factorized-christoffel-query

The quadratic Query $W_{Q,\Gamma}(z, z) = \sum_{ij} W^{a}_{ij} z^i z^j$ has $O(d^2)$ parameters per output dimension. Factorize as:

$$
W_{Q,\Gamma}^a(z, z) \approx \sum_{r=1}^R (u_r^a \cdot z)(v_r^a \cdot z) = \sum_{r=1}^R (U_r z)_a (V_r z)_a
$$

where $U_r, V_r \in \mathbb{R}^{d_k \times d}$ are low-rank factors with $R \ll d$.

**For the Poincare ball/disk**: the contracted Christoffel correction has a closed form. Using Proposition {prf:ref}`prop-explicit-christoffel-symbols-for-poincare-disk`,

$$
\Gamma^k_{ij}(z) v^i v^j = \frac{2}{1-|z|^2}\left(2(z\cdot v)\,v^k - \|v\|^2 z^k\right),
$$
so one can compute $\Gamma(z)[v,v]$ in $O(d)$ time without explicitly forming any $d\times d$ tensors. The factorized parameterization above is most useful when learning departures from (or alternatives to) the closed-form geometry.

**Complexity**: $O(N R d)$ instead of $O(N d^2)$.

:::

:::{prf:proposition} Area Law as Soft Masking
:label: prop-area-law-soft-masking

The area law screening $\exp(-\sigma A_{\text{string}})$ with $A \approx \frac{\lambda^2}{2}|z - z'|^2$ can be implemented as a **soft attention mask**:

$$
M(z, z') = \exp\left(-\frac{\sigma \lambda(z)^2 |z - z'|^2}{2}\right)
$$

**Key observation**: This is a Gaussian with width $w = 1/(\sqrt{\sigma}\lambda(z))$.

**Efficient implementation**:
1. Compute mask only for pairs within $3w$ (99.7% of mass)
2. Use the same sparse attention pattern as Proxy 1
3. Combine: $\alpha_{\text{screened}} = M \odot \alpha_{\text{sparse}}$

**No additional asymptotic cost** beyond sparse attention.

:::

:::{prf:theorem} O(N) Covariant Attention
:label: thm-on-covariant-attention

Combining the engineering proxies, we achieve **O(N) complexity** for covariant cross-attention:

| Component | Proxy | Complexity | Error |
|:----------|:------|:-----------|:------|
| Attention pattern | Geodesic sparse (Def. {prf:ref}`def-geodesic-sparse-attention`) | $O(Nk)$ | $O(\epsilon N)$ |
| Wilson lines | Hierarchical (Def. {prf:ref}`def-hierarchical-wilson-line`) | $O(NL)$ | $O(r_{\text{Wilson}}^2)$ |
| Quadratic Query | Factorized (Def. {prf:ref}`def-factorized-christoffel-query`) | $O(NRd)$ | Exact for Poincare |
| Area screening | Soft mask (Prop. {prf:ref}`prop-area-law-soft-masking`) | $O(Nk)$ | $O(e^{-9})$ |
| Temperature | Direct computation | $O(N)$ | Exact |
| **Total** | | $O(N(k + L + Rd))$ | Controlled |

With typical values $k = 64$, $L = 8$, $R = 3$, $d = 64$: **O(N · 264)** operations per layer.

**Comparison**: Naive implementation is $O(N^2 d^2) = O(N^2 \cdot 4096)$. For $N = 1000$: speedup factor of $\sim 15,000\times$.

:::

## 08_multiagent/06_full_net.md

:::{prf:definition} Complete Agent Architecture
:label: def-complete-agent-architecture

A **complete agent architecture** is a composition of three neural operators:

$$
\mathcal{A}: \mathcal{X} \xrightarrow{E} \mathcal{Z} \xrightarrow{D} \mathcal{Z} \xrightarrow{P} \mathcal{Y}
$$

where:

1. **Encoder** $E: \mathcal{X} \to \mathcal{Z}$ maps observations $x \in \mathcal{X}$ (e.g., images, sensor readings) to latent representations $z \in \mathcal{Z}$

2. **Latent Dynamics** $D: \mathcal{Z} \times \mathcal{Y} \to \mathcal{Z}$ updates latent state given current state and action, implementing one step of the world model or transition function

3. **Policy/Decoder** $P: \mathcal{Z} \to \mathcal{Y}$ maps latent state to outputs $y \in \mathcal{Y}$ (actions, predictions, reconstructions)

**Notation:**
- $\mathcal{X}$ — observation space (typically $\mathbb{R}^{d_x}$ for pixel values)
- $\mathcal{Z}$ — latent space with bundle structure $\mathcal{Z} = \bigoplus_{i=1}^{n_b} V_i$
- $\mathcal{Y}$ — output space (actions $\mathcal{U}$ or reconstructed observations)

**Sequential composition:** For multi-step rollouts, the dynamics are applied recursively:

$$
z_0 = E(x_0), \quad z_{t+1} = D(z_t, a_t), \quad a_t = P(z_t)
$$

**Units:**
- $[\mathcal{X}] = $ dimensionless (pixel intensities in $[0, 1]$ or normalized sensor values)
- $[\mathcal{Z}] = \sqrt{\text{nat}}$ (latent space has information-theoretic units; $\sqrt{\text{nat}}$ arises from the Fisher-Rao metric on probability distributions, where distances have units of $\sqrt{\text{information}}$)
- $[\mathcal{Y}] = $ task-dependent (e.g., dimensionless for discrete actions, physical units for continuous control)
:::

:::{prf:definition} Gauge-Equivariant Architecture
:label: def-gauge-equivariant-architecture

An agent architecture $\mathcal{A} = P \circ D \circ E$ is **$G$-equivariant** with respect to gauge group $G$ if:

1. **Latent space transforms:** There exists a representation $\rho: G \to \text{GL}(\mathcal{Z})$ such that for $g \in G$, latent states transform as $z \mapsto \rho(g) z$

2. **Encoder invariance:** $E$ maps to gauge-equivalent latent states:
   
   $$
   E(x) \sim \rho(g) E(x) \quad \forall g \in G, x \in \mathcal{X}
   $$
   where $\sim$ denotes equivalence up to gauge choice (physically identical states)

3. **Dynamics equivariance:** $D$ commutes with gauge transformations:
   
   $$
   D(\rho(g) z, a) = \rho(g) D(z, a) \quad \forall g \in G, z \in \mathcal{Z}, a \in \mathcal{Y}
   $$

4. **Decoder covariance:** $P$ produces consistent outputs under gauge transformations:
   
   $$
   P(\rho(g) z) = P(z) \quad \forall g \in G, z \in \mathcal{Z}
   $$
   (output is gauge-invariant if $\mathcal{Y}$ has no gauge structure)

**Interpretation:** Gauge equivariance ensures that changing the internal coordinate frame (latent representation) doesn't change the agent's observable behavior. Two latent states related by $z' = \rho(g) z$ represent the same physical state in different "mental coordinate systems."

**Example:** For $G = SO(d_b)$ (rotations within bundles), if you rotate all feature vectors by the same matrix $R \in SO(d_b)$, the dynamics and output must transform consistently. The agent's behavior is independent of which orthonormal basis you choose for each bundle.

**Remark:** The encoder invariance condition is weaker than full equivariance because the encoder *chooses* a gauge. Different choices lead to gauge-equivalent latent states. This gauge freedom is crucial for achieving universal approximation ({ref}`sec-universal-geometric-network`).
:::

:::{prf:definition} Direct Sum Representation
:label: def-direct-sum-representation

The **direct sum** representation structures the latent space as:

$$
\mathcal{Z} = \mathcal{Z}_C \oplus \mathcal{Z}_L \oplus \mathcal{Z}_Y
$$

where:
- $\mathcal{Z}_C \cong \mathbb{R}^{d_C}$ — color subspace (feature bundles)
- $\mathcal{Z}_L \cong \mathbb{R}^{d_L}$ — weak isospin subspace (observation-action doublet)
- $\mathcal{Z}_Y \cong \mathbb{R}^{d_Y}$ — hypercharge subspace (capacity bound)

**Dimension:** $\dim(\mathcal{Z}) = d_C + d_L + d_Y$ (linear scaling)

**Gauge action:** Block-diagonal:

$$
\rho(g_C, g_L, g_Y) = \begin{pmatrix}
\rho_C(g_C) & 0 & 0 \\
0 & \rho_L(g_L) & 0 \\
0 & 0 & \rho_Y(g_Y)
\end{pmatrix}
$$

Each factor acts independently on its subspace.

**Interpretation:** Gauge quantum numbers are independent labels. The color index, isospin index, and hypercharge are separate attributes of a latent state, analogous to storing (position, velocity, temperature) as separate variables.

**Advantage:** Computational tractability—operations scale linearly with total dimension.

**Limitation:** Cannot represent cross-gauge correlations natively. To model "if color is red AND isospin is up, then..." requires learning explicit coupling through network layers.
:::

:::{prf:definition} Tensor Product Representation
:label: def-tensor-product-representation

The **tensor product** representation structures the latent space as:

$$
\mathcal{Z} = V_C \otimes V_L \otimes V_Y
$$

where $V_C, V_L, V_Y$ are the representation spaces for each gauge factor.

**Dimension:** $\dim(\mathcal{Z}) = \dim(V_C) \times \dim(V_L) \times \dim(V_Y)$ (multiplicative scaling)

**Gauge action:** Kronecker product:

$$
\rho(g_C, g_L, g_Y) = \rho_C(g_C) \otimes \rho_L(g_L) \otimes \rho_Y(g_Y)
$$

**Basis:** Each basis vector $|c, \ell, y\rangle$ carries all three quantum numbers simultaneously. Under gauge transformation:

$$
|c, \ell, y\rangle \mapsto \sum_{c', \ell', y'} [\rho_C]_{c'c} [\rho_L]_{\ell'\ell} [\rho_Y]_{y'y} |c', \ell', y'\rangle
$$

**Interpretation:** This is how quantum states work in particle physics. A quark isn't "red" independently of being "up"—it's a single entity transforming under the representation $(\mathbf{3}, \mathbf{2}, 1/6)$ of $SU(3)_C \times SU(2)_L \times U(1)_Y$.

**Advantage:** Can represent entangled states like $\frac{1}{\sqrt{2}}(|r, \uparrow\rangle + |b, \downarrow\rangle)$ where color and isospin are correlated. Full expressiveness of gauge-invariant functions.

**Limitation:** Dimension explodes. For $d_C = 64, d_L = 8, d_Y = 4$: direct sum gives $76$ dimensions, tensor product gives $2048$ dimensions. This is why quarks work with small representations ($3 \times 2 \times 1 = 6$) while neural networks need hundreds of latent dimensions.

**Factorization:** For low-rank structure (when gauge-invariant couplings are sparse), factored tensor representations can recover efficiency:

$$
W = \sum_{k=1}^r U_C^{(k)} \otimes U_L^{(k)} \otimes U_Y^{(k)}
$$
with $r \ll \dim(V_C) \times \dim(V_L) \times \dim(V_Y)$, requiring only $O(r \sum_i \dim(V_i))$ parameters instead of $O(\prod_i \dim(V_i))$.
:::

:::{prf:definition} Cross-Bundle Interaction Levels
:label: def-cross-bundle-interaction-levels

For a latent space $\mathcal{Z} = \bigoplus_{i=1}^{n_b} V_i$ with bundles $v_i \in V_i \cong \mathbb{R}^{d_b}$, we define three levels of permissible cross-bundle coupling:

**Level 1: Norms Only** (Strict $\prod_i SO(d_b)_i$ equivariance)

Bundles interact only through their magnitudes $\|v_i\|$. Invariant features:

$$
\mathcal{I}_1 = \{\|v_1\|, \|v_2\|, \ldots, \|v_{n_b}\|\} \subset \mathbb{R}^{n_b}
$$

**Implication:** The output of bundle $i$ has the form (by Schur's lemma, Theorem {prf:ref}`thm-equivariant-function-structure`):

$$
f_i(v_1, \ldots, v_{n_b}) = v_i \cdot \phi_i(\|v_1\|, \ldots, \|v_{n_b}\|)
$$
where $\phi_i: \mathbb{R}^{n_b} \to \mathbb{R}$ can be an arbitrary function (e.g., deep MLP).

**Level 2: Gram Matrix** (Global $SO(d_b)$ equivariance)

A single rotation $R \in SO(d_b)$ acts on *all* bundles: $(v_1, \ldots, v_{n_b}) \mapsto (Rv_1, \ldots, Rv_{n_b})$.

Invariant features:

$$
\mathcal{I}_2 = \{G_{ij} = \langle v_i, v_j \rangle : 1 \leq i, j \leq n_b\} \subset \mathbb{R}^{n_b \times n_b}
$$

The Gram matrix $G$ is a symmetric $n_b \times n_b$ matrix with:
- Diagonal: $G_{ii} = \|v_i\|^2$ (energy of bundle $i$)
- Off-diagonal: $G_{ij} = \|v_i\| \|v_j\| \cos\theta_{ij}$ (alignment between bundles $i, j$)

**Implication:** Functions of $G$ can represent any $O(d_b)$-invariant function. Much more expressive than Level 1 (bundles can "see" each other's directions relative to a global frame).

**Level 3: Hybrid / Soft Equivariance**

Use Level 1 (norms) as the primary pathway, add small Level 2 (Gram) or symmetry-breaking terms with regularization:

$$
f = f_{\text{equivariant}} + \lambda \cdot f_{\text{mixing}}
$$

where $\lambda \ll 1$ is learned or regularized (e.g., via L1 penalty).

**Implication:** Network can violate equivariance when necessary for the task, but pays a cost. With strong L1 regularization, symmetry-breaking terms are driven toward zero unless essential.
:::

:::{prf:theorem} Dimensional Scaling
:label: thm-dimensional-scaling

For a gauge group $G = G_C \times G_L \times G_Y$ with representation spaces $V_C, V_L, V_Y$ of dimensions $d_C, d_L, d_Y$ respectively:

**Direct sum:**

$$
\dim(\mathcal{Z}_{\oplus}) = d_C + d_L + d_Y = \sum_{i \in \{C, L, Y\}} d_i
$$

**Tensor product:**

$$
\dim(\mathcal{Z}_{\otimes}) = d_C \times d_L \times d_Y = \prod_{i \in \{C, L, Y\}} d_i
$$

*Proof.* By definition of direct sum and tensor product of vector spaces. $\square$
:::

:::{prf:definition} Level 1: Norms-Only Interaction
:label: def-level1-norms-only

A **norms-only** cross-bundle layer computes outputs solely from bundle magnitudes $\{\|v_i\|\}_{i=1}^{n_b}$.

**Functional form:** For each bundle $i$, the output is:

$$
f_i(v_1, \ldots, v_{n_b}) = v_i \cdot \phi_i(\|v_1\|, \ldots, \|v_{n_b}\|)
$$
where $\phi_i: \mathbb{R}^{n_b} \to \mathbb{R}_+$ is an arbitrary positive function (typically a deep MLP with softplus output).

**Equivariance:** Strictly equivariant under $\prod_{i=1}^{n_b} SO(d_b)_i$ (per-bundle rotations).

**Expressiveness:** Can implement energy-based routing (e.g., "suppress bundle 2 when bundles 1 and 3 are both active") but cannot represent direction-dependent cross-talk.
:::

:::{prf:definition} Level 2: Gram Matrix Interaction
:label: def-level2-gram-matrix

A **Gram matrix** interaction layer uses the full matrix of inner products $G_{ij} = \langle v_i, v_j \rangle$.

**Invariant features:**

$$
\mathcal{I}_2 = \{G_{ij} : 1 \leq i, j \leq n_b\} \subset \mathbb{R}^{n_b \times n_b}
$$

**Equivariance:** Equivariant under global $SO(d_b)$ acting on all bundles simultaneously: $(v_1, \ldots, v_{n_b}) \mapsto (Rv_1, \ldots, Rv_{n_b})$ for single $R \in SO(d_b)$.

**Not equivariant** under per-bundle rotations $(R_1 v_1, \ldots, R_{n_b} v_{n_b})$ with independent $R_i$.

**Expressiveness:** Can encode relative orientations between bundles. Much more expressive than norms-only.
:::

:::{prf:definition} Level 3: Hybrid / Soft Equivariance
:label: def-level3-hybrid

A **hybrid** interaction layer combines:
1. An equivariant pathway (norms-only, Level 1)
2. A mixing pathway (Gram-based or fully learned) with L1 regularization

**Functional form:**

$$
f(z) = f_{\text{equiv}}(z) + f_{\text{mix}}(z)
$$

where $f_{\text{equiv}}$ is strictly equivariant and $f_{\text{mix}}$ has learned weights $W$ penalized by $\lambda_{\text{L1}} \|W\|_1$.

**Equivariance:** Soft—violations are controlled by L1 penalty strength.

**Expressiveness:** Universal (in limit of $\lambda_{\text{L1}} \to 0$), but biased toward equivariant solutions.
:::

:::{prf:theorem} Schur's Lemma for Bundle Representations
:label: thm-schur-bundle

Let $V = \bigoplus_{i=1}^{n_b} V_i$ where each $V_i \cong \mathbb{R}^{d_b}$ is an irreducible representation of $SO(d_b)_i$ (the $i$-th copy of $SO(d_b)$ acting only on $V_i$).

Let $T: V \to V$ be a linear map that is equivariant with respect to $\prod_{i=1}^{n_b} SO(d_b)_i$:

$$
\rho(g) \circ T = T \circ \rho(g) \quad \forall g \in \prod_{i=1}^{n_b} SO(d_b)_i
$$
where $\rho(g)(v_1, \ldots, v_{n_b}) = (g_1 v_1, \ldots, g_{n_b} v_{n_b})$ is the diagonal representation of $g = (g_1, \ldots, g_{n_b})$.

Then $T$ must be **block-diagonal** with each block $T_i: V_i \to V_i$ satisfying $T_i = \lambda_i I_{d_b}$ for some scalar $\lambda_i \in \mathbb{R}$.

*Proof.*

**Step 1 (Block-diagonal structure):** By the direct sum decomposition, $T$ must respect the bundle structure:

$$
T = \begin{pmatrix}
T_{11} & T_{12} & \cdots & T_{1n_b} \\
T_{21} & T_{22} & \cdots & T_{2n_b} \\
\vdots & \vdots & \ddots & \vdots \\
T_{n_b 1} & T_{n_b 2} & \cdots & T_{n_b n_b}
\end{pmatrix}
$$
where $T_{ij}: V_j \to V_i$.

**Step 2 (Off-diagonal blocks vanish):** Consider $g = (I, \ldots, I, g_j, I, \ldots, I)$ where only the $j$-th component is non-identity, and let $v = (0, \ldots, 0, v_j, 0, \ldots, 0)$ with only the $j$-th bundle nonzero.

By linearity, $T(v) = (T_{1j}(v_j), T_{2j}(v_j), \ldots, T_{n_b,j}(v_j))$.

Equivariance requires:

$$
T(g \cdot v) = g \cdot T(v)
$$

The LHS is:

$$
T(0, \ldots, 0, g_j v_j, 0, \ldots, 0) = (T_{1j}(g_j v_j), \ldots, T_{n_b,j}(g_j v_j))
$$

The RHS is:

$$
g \cdot T(v) = (g_1 T_{1j}(v_j), \ldots, g_i T_{ij}(v_j), \ldots, g_{n_b} T_{n_b,j}(v_j))
$$

For $i \neq j$, equating components gives:

$$
T_{ij}(g_j v_j) = g_i T_{ij}(v_j) \quad \forall g_i \in SO(d_b), g_j \in SO(d_b), v_j \in V_j
$$

**Key observation:** For any fixed $g_j$ and $v_j$, the LHS $T_{ij}(g_j v_j)$ is a **fixed vector** in $V_i$. But the RHS $g_i T_{ij}(v_j)$ can be **any rotation** of $T_{ij}(v_j)$ as we vary $g_i$ arbitrarily over $SO(d_b)$. For these to be equal for all choices of $g_i$, we need:

$$
T_{ij}(g_j v_j) = g_i T_{ij}(v_j) \quad \text{for all } g_i \in SO(d_b)
$$

This means $T_{ij}(v_j)$ must be **invariant** under all rotations $g_i$ (i.e., $T_{ij}(v_j)$ lies in the fixed-point set of $SO(d_b)$ acting on $V_i$). For $d_b \geq 2$, the only vector invariant under all rotations is the zero vector. Therefore $T_{ij}(v_j) = 0$ for all $v_j$.

Therefore, $T_{ij} = 0$ for all $i \neq j$.

**Step 3 (Diagonal blocks are scalar multiples):** For each $i$, the block $T_{ii}: V_i \to V_i$ must commute with all $g_i \in SO(d_b)_i$:

$$
T_{ii}(g_i v_i) = g_i T_{ii}(v_i) \quad \forall g_i \in SO(d_b)_i, v_i \in V_i
$$

By Schur's lemma for irreducible representations, $SO(d_b)$ acting on $\mathbb{R}^{d_b}$ is irreducible over $\mathbb{R}$ (for $d_b \geq 2$), so any intertwining operator must be a scalar multiple of the identity:

$$
T_{ii} = \lambda_i I_{d_b}
$$

**Conclusion:**

$$
T = \begin{pmatrix}
\lambda_1 I_{d_b} & 0 & \cdots & 0 \\
0 & \lambda_2 I_{d_b} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_{n_b} I_{d_b}
\end{pmatrix}
$$

$\square$

**Corollary:** Any equivariant linear layer can only scale each bundle independently. No within-bundle mixing, no cross-bundle mixing at the linear level.
:::

:::{prf:theorem} Norm-Based Networks Are NOT Universal
:label: thm-norm-networks-not-universal

A feedforward network with:
- Input: $z = (v_1, \ldots, v_{n_b}) \in \bigoplus_{i=1}^{n_b} \mathbb{R}^{d_b}$
- Layers: Each strictly equivariant under $\prod_i SO(d_b)_i$
- Activations: Applied per-bundle (e.g., norm-gating)

can only approximate functions of the form:

$$
f_i(v_1, \ldots, v_{n_b}) = v_i \cdot \phi_i(\|v_1\|, \ldots, \|v_{n_b}\|)
$$
where $\phi_i: \mathbb{R}^{n_b} \to \mathbb{R}$ is an arbitrary continuous function.

Such networks are **not universal approximators** over continuous functions $f: \mathbb{R}^{n_b \cdot d_b} \to \mathbb{R}^{n_b \cdot d_b}$.

*Proof.*

**Step 1 (Induction setup):** Consider a depth-$L$ network. We prove by induction that each layer output has the form $f_i = v_i \cdot \phi_i(\text{norms})$.

**Base case ($L = 1$):** By Theorem {prf:ref}`thm-schur-bundle`, the first layer can only scale bundles: $z^{(1)}_i = \lambda_i v_i$. If $\lambda_i$ depends on norms $\{\|v_j\|\}$ (via a norm-MLP), we get $z^{(1)}_i = v_i \cdot \phi_i^{(1)}(\|v_1\|, \ldots, \|v_{n_b}\|)$.

**Inductive step:** Suppose layer $\ell$ outputs $z^{(\ell)}_i = v_i \cdot \phi_i^{(\ell)}(\|v_1\|, \ldots, \|v_{n_b}\|)$. Layer $\ell+1$ can only scale:

$$
z^{(\ell+1)}_i = z^{(\ell)}_i \cdot \psi_i^{(\ell+1)}(\|z^{(\ell)}_1\|, \ldots, \|z^{(\ell)}_{n_b}\|)
$$

But $\|z^{(\ell)}_j\| = \|v_j\| \cdot |\phi_j^{(\ell)}(\|v_1\|, \ldots, \|v_{n_b}\|)|$, which is itself a function of norms only. Thus:

$$
z^{(\ell+1)}_i = v_i \cdot \underbrace{\phi_i^{(\ell)}(\{\|v_j\|\}) \cdot \psi_i^{(\ell+1)}(\{\|v_j\| \cdot |\phi_j^{(\ell)}(\{\|v_k\|\})|\})}_{\phi_i^{(\ell+1)}(\{\|v_j\|\})}
$$

The composition is still a function of norms only.

**Step 2 (Counterexample):** Consider the target function:

$$
f(v_1, v_2) = v_{1,1} \cdot v_{2,1}
$$
where $v_{1,1}, v_{2,1}$ denote the first components of bundles 1 and 2.

This function depends on **specific components**, not just norms. For example:
- $v_1 = (1, 0), v_2 = (1, 0)$: $f = 1$
- $v_1 = (1, 0), v_2 = (0, 1)$: $f = 0$

Both inputs have $\|v_1\| = \|v_2\| = 1$, but produce different outputs. Any function $g(\|v_1\|, \|v_2\|)$ must give the same value for both, so $f$ cannot be approximated by norm-based networks.

**Step 3 (Non-density conclusion):** The class of functions representable by norm-based networks is **not dense** in $C(\mathbb{R}^{n_b d_b}, \mathbb{R}^{n_b d_b})$ with respect to the topology of uniform convergence on compact sets. Intuitively, norm-based functions are parametrized by mappings $\mathbb{R}^{n_b} \to \mathbb{R}^{n_b}$ (the norm-to-scale functions $\phi_i$), while general continuous functions on $\mathbb{R}^{n_b d_b}$ form an infinite-dimensional space with vastly more degrees of freedom. The counterexample above (which cannot be approximated) demonstrates a gap in the closure, proving non-density and hence non-universality.

$\square$
:::

:::{prf:theorem} Equivariant Function Structure
:label: thm-equivariant-function-structure

For a function $f: \bigoplus_{i=1}^{n_b} \mathbb{R}^{d_b} \to \bigoplus_{i=1}^{n_b} \mathbb{R}^{d_b}$ to be equivariant under $\prod_{i=1}^{n_b} SO(d_b)_i$, it must satisfy:

$$
f_i(R_1 v_1, \ldots, R_{n_b} v_{n_b}) = R_i f_i(v_1, \ldots, v_{n_b}) \quad \forall R_j \in SO(d_b), v_j \in \mathbb{R}^{d_b}
$$

The **necessary and sufficient** form is:

$$
f_i(v_1, \ldots, v_{n_b}) = v_i \cdot \phi_i(\|v_1\|, \ldots, \|v_{n_b}\|)
$$
where $\phi_i: \mathbb{R}^{n_b} \to \mathbb{R}$ is arbitrary.

*Proof.*

**Sufficiency:** Compute:

$$
f_i(R_1 v_1, \ldots, R_i v_i, \ldots, R_{n_b} v_{n_b}) = R_i v_i \cdot \phi_i(\|R_1 v_1\|, \ldots, \|R_{n_b} v_{n_b}\|)
$$

Since $\|R_j v_j\| = \|v_j\|$ (orthogonal matrices preserve norms):

$$
= R_i v_i \cdot \phi_i(\|v_1\|, \ldots, \|v_{n_b}\|) = R_i \left[ v_i \cdot \phi_i(\|v_1\|, \ldots, \|v_{n_b}\|) \right] = R_i f_i(v_1, \ldots, v_{n_b})
$$

**Necessity:** Fix all bundles except $i$: $v_j = c_j$ for $j \neq i$. Equivariance under $R_i$ gives:

$$
f_i(c_1, \ldots, c_{i-1}, R_i v_i, c_{i+1}, \ldots, c_{n_b}) = R_i f_i(c_1, \ldots, c_{i-1}, v_i, c_{i+1}, \ldots, c_{n_b})
$$

This must hold for all $R_i \in SO(d_b)$ and all $v_i$. By Schur's lemma (irreducibility of $SO(d_b)$ on $\mathbb{R}^{d_b}$), $f_i$ must be proportional to $v_i$:

$$
f_i(\ldots, v_i, \ldots) = v_i \cdot \psi_i(\ldots, v_i, \ldots)
$$

But $\psi_i$ must also be equivariant under $R_i$:

$$
\psi_i(\ldots, R_i v_i, \ldots) = \psi_i(\ldots, v_i, \ldots)
$$

This is only possible if $\psi_i$ depends on $v_i$ through $\|v_i\|$ alone (the only $SO(d_b)$-invariant feature). Extending to all bundles:

$$
f_i = v_i \cdot \phi_i(\|v_1\|, \ldots, \|v_{n_b}\|)
$$

$\square$
:::

:::{prf:proposition} Expressiveness of Norm-Based Networks
:label: prop-norm-network-capabilities

A norm-based equivariant network with $L$ layers and hidden dimension $h$ can approximate any continuous function $\Phi: \mathbb{R}^{n_b} \to \mathbb{R}^{n_b}$ (the norm-to-scale mapping) to arbitrary precision, by the universal approximation theorem for MLPs.

Thus, norm-based networks are **universal** over the restricted class:

$$
\mathcal{F}_{\text{norm}} = \left\{ f: f_i = v_i \cdot \phi_i(\|v_1\|, \ldots, \|v_{n_b}\|), \; \phi_i \in C(\mathbb{R}^{n_b}, \mathbb{R}) \right\}
$$

But $\mathcal{F}_{\text{norm}}$ has measure zero in $C(\mathbb{R}^{n_b d_b}, \mathbb{R}^{n_b d_b})$.
:::

:::{prf:definition} Approximate Equivariance
:label: def-approximate-equivariance

A function $f: \mathcal{Z} \to \mathcal{Z}$ is **$\epsilon$-approximately equivariant** with respect to group $G$ and representation $\rho$ if:

$$
\sup_{g \in G, z \in \mathcal{Z}} \frac{\|f(\rho(g) z) - \rho(g) f(z)\|}{\|z\|} \leq \epsilon
$$

The quantity:

$$
\mathcal{V}(f) := \mathbb{E}_{g \sim G, z \sim \mathcal{Z}} \left[ \|f(\rho(g) z) - \rho(g) f(z)\|^2 \right]
$$
is the **equivariance violation**.

**Remark:** The supremum measure gives a worst-case bound, while $\mathcal{V}(f)$ measures average-case violation used in optimization.
:::

:::{prf:theorem} Approximate Equivariance Bound
:label: thm-approximate-equivariance-bound

Let $f_{\text{equiv}}: \mathcal{Z} \to \mathcal{Z}$ be strictly $G$-equivariant and $f_{\text{break}}: \mathcal{Z} \to \mathcal{Z}$ be an arbitrary symmetry-breaking term.

Define:

$$
f = f_{\text{equiv}} + \lambda f_{\text{break}}
$$

Then the equivariance violation satisfies:

$$
\mathcal{V}(f) = \lambda^2 \mathbb{E}_{g \sim \mu_G, z} \left[ \|f_{\text{break}}(\rho(g) z) - \rho(g) f_{\text{break}}(z)\|^2 \right]
$$
where $\mu_G$ is the Haar measure on $G$ (uniform distribution for compact Lie groups)

*Proof.*

**Step 1:** Compute the violation:

$$
f(\rho(g) z) - \rho(g) f(z) = f_{\text{equiv}}(\rho(g) z) + \lambda f_{\text{break}}(\rho(g) z) - \rho(g) [f_{\text{equiv}}(z) + \lambda f_{\text{break}}(z)]
$$

**Step 2:** Use equivariance of $f_{\text{equiv}}$:

$$
= \rho(g) f_{\text{equiv}}(z) + \lambda f_{\text{break}}(\rho(g) z) - \rho(g) f_{\text{equiv}}(z) - \lambda \rho(g) f_{\text{break}}(z)
$$

$$
= \lambda [f_{\text{break}}(\rho(g) z) - \rho(g) f_{\text{break}}(z)]
$$

**Step 3:** Square and take expectation:

$$
\mathbb{E}_{g, z} \|f(\rho(g) z) - \rho(g) f(z)\|^2 = \lambda^2 \mathbb{E}_{g, z} \|f_{\text{break}}(\rho(g) z) - \rho(g) f_{\text{break}}(z)\|^2
$$

$\square$

**Implication:** The violation scales quadratically with $\lambda$. Small symmetry-breaking terms ($\lambda \ll 1$) lead to small violations.
:::

:::{prf:theorem} L1 and Hierarchies
:label: thm-l1-hierarchies

Consider a network with mixing weights $W \in \mathbb{R}^{n_b \times n_b \times d_b \times d_b}$ (cross-bundle coupling) trained with loss:

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{task}} + \lambda_{\text{L1}} \|W\|_1
$$

Under mild regularity conditions (task loss differentiable, optimizer converges), the following hold at convergence:

1. **Sparsity:** Under L1 regularization, most weights are driven near zero. The expected number of weights with $|W_{ij}^{(k\ell)}| \geq \epsilon$ scales inversely with $\lambda_{\text{L1}}$ (heuristically: stronger regularization → fewer large weights), though the exact bound depends on task gradient structure.

2. **Texture zeros:** Cross-bundle blocks $(i, j)$ where task loss $\mathcal{L}_{\text{task}}$ is insensitive to $W_{ij}$ have $\|W_{ij}\|_F \approx 0$ (driven to zero by L1 with no opposing gradient).

3. **Hierarchy:** Non-zero weights organize into levels with exponential decay (approximately): if $|W^{(1)}| > |W^{(2)}| > \cdots$ are sorted magnitudes, then $|W^{(k+1)}| / |W^{(k)}| \approx \text{const} < 1$.

*Informal proof sketch:*

**(1) Sparsity:** L1 penalty creates a "soft thresholding" effect. Weights with $|W| < \lambda_{\text{L1}} / |\partial \mathcal{L}_{\text{task}} / \partial W|$ are driven to zero (penalty gradient dominates task gradient). As $\lambda_{\text{L1}}$ increases, fewer weights exceed this threshold, though the exact scaling depends on the task loss landscape.

**(2) Texture zeros:** If $\partial \mathcal{L}_{\text{task}} / \partial W_{ij} = 0$ (block irrelevant to task), the total gradient is purely from L1: $\partial \mathcal{L}_{\text{total}} / \partial W_{ij} = \lambda_{\text{L1}} \cdot \text{sign}(W_{ij})$, which pushes $W_{ij} \to 0$.

**(3) Hierarchy:** During training, once a weight crosses zero (becomes active), it can grow under task gradients. But nearby small weights continue to be suppressed by L1. This creates a "rich get richer" dynamic: large weights grow, small weights vanish. The distribution becomes hierarchical.

**Empirical validation required.** This theorem is heuristic; rigorous proof requires analyzing stochastic gradient dynamics.
:::

:::{prf:definition} Universal Geometric Network
:label: def-universal-geometric-network

The **Universal Geometric Network** (UGN) is a three-stage architecture:

$$
\mathcal{A}_{\text{UGN}}: \mathcal{X} \xrightarrow{E} \mathcal{Z} \xrightarrow{D_1, \ldots, D_L} \mathcal{Z} \xrightarrow{P} \mathcal{Y}
$$

**Stage 1: Encoder** (Unconstrained)

$$
E: \mathbb{R}^{d_x} \to \mathbb{R}^{n_b \cdot d_b}
$$
Implemented as:

$$
E(x) = \text{SpectralMLP}(x) = W_2 \sigma(W_1 x)
$$
where $W_1, W_2$ have spectral norm $\|W_i\|_2 \leq 1$ and $\sigma = \text{GELU}$ (smooth, non-polynomial).

**Output structure:** $z = E(x) \in \mathcal{Z}$ with implicit bundle decomposition $z = (v_1, \ldots, v_{n_b})$ where $v_i \in \mathbb{R}^{d_b}$.

**Stage 2: Latent Dynamics** (Soft Equivariant)

Each layer $D_\ell: \mathcal{Z} \to \mathcal{Z}$ has the form:

$$
D_\ell(z) = D_\ell^{\text{equiv}}(z) + D_\ell^{\text{mix}}(z)
$$

where:
- **Equivariant pathway** $D_\ell^{\text{equiv}}$: Strict $\prod_i SO(d_b)$-equivariant (norm-based, Definition {prf:ref}`def-cross-bundle-interaction-levels` Level 1)
- **Mixing pathway** $D_\ell^{\text{mix}}$: Weakly equivariant or symmetry-breaking (Gram-based or learned)

**Regularization:** L1 penalty on mixing pathway weights:

$$
\mathcal{L}_{\text{reg}} = \lambda_{\text{L1}} \sum_{\ell=1}^L \|W_\ell^{\text{mix}}\|_1
$$

**Stage 3: Decoder** (Unconstrained)

$$
P: \mathbb{R}^{n_b \cdot d_b} \to \mathbb{R}^{d_y}
$$
Implemented as:

$$
P(z) = \text{SpectralMLP}(z) = W_4 \sigma(W_3 z)
$$
with spectral normalization $\|W_i\|_2 \leq 1$.

**Total loss:**

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{task}}(y, \hat{y}) + \lambda_{\text{L1}} \mathcal{L}_{\text{reg}} + \lambda_{\text{equiv}} \mathcal{L}_{\text{equiv}}
$$

where:
- $\mathcal{L}_{\text{task}}$ — task loss (e.g., MSE for regression, cross-entropy for classification)
- $\mathcal{L}_{\text{reg}} = \sum_\ell \|W_\ell^{\text{mix}}\|_1$ — L1 regularization on mixing weights
- $\mathcal{L}_{\text{equiv}} = \mathbb{E}_{z, R} \|D(Rz) - RD(z)\|^2$ — equivariance violation penalty (optional)

**Hyperparameters:**
- $n_b$ — number of bundles (typically 4-8)
- $d_b$ — bundle dimension (typically 8-64)
- $L$ — number of latent layers (typically 3-6)
- $\lambda_{\text{L1}}$ — L1 regularization strength (typically 0.001-0.1)
- $\lambda_{\text{equiv}}$ — equivariance penalty (typically 0 or 0.01)

**Implementation note:** When $\lambda_{\text{equiv}} = 0$, equivariance is encouraged only through L1 (which drives mixing weights to zero, making layers effectively equivariant). When $\lambda_{\text{equiv}} > 0$, we explicitly penalize equivariance violations, providing a stronger geometric prior.
:::

:::{prf:theorem} Universal Approximation
:label: thm-ugn-universal-approximation

Let $\mathcal{A}_{\text{UGN}}$ be a Universal Geometric Network with:
- Encoder $E: \mathcal{X} \to \mathcal{Z}$ using SpectralLinear + GELU
- Decoder $P: \mathcal{Z} \to \mathcal{Y}$ using SpectralLinear + GELU
- Latent dynamics $D = D_L \circ \cdots \circ D_1$ with soft-equivariant layers

Then for any continuous function $f: \mathcal{X} \to \mathcal{Y}$ on compact domains and any $\epsilon > 0$, there exists a choice of weights such that:

$$
\sup_{x \in \mathcal{X}} \|P(D(E(x))) - f(x)\| < \epsilon
$$

*Proof.*

**Step 1 (Encoder universality):** By the universal approximation theorem for MLPs with non-polynomial activations (Cybenko 1989, Hornik 1991), the encoder $E$ with GELU activation can approximate any continuous function $g: \mathcal{X} \to \mathcal{Z}$ to arbitrary precision on compact sets. Spectral normalization rescales but doesn't change the function class (can be compensated by adjusting subsequent layer scales).

**Step 2 (Decoder universality):** Similarly, the decoder $P$ can approximate any continuous function $h: \mathcal{Z} \to \mathcal{Y}$.

**Step 3 (Composition):** To approximate $f: \mathcal{X} \to \mathcal{Y}$, consider the decomposition:

$$
f(x) = \underbrace{P}_{\text{decoder}} \left( \underbrace{D}_{\text{latent}} \left( \underbrace{E(x)}_{\text{encoder}} \right) \right)
$$

**Strategy:** Choose:
- $E$ to approximately invert $P^{-1} \circ f$ (if $P$ were bijective)
- $D = \text{identity}$ (or close to identity)
- $P \approx f \circ E^{-1}$ (if $E$ were bijective)

More precisely:

**Sub-step 3a (Anchor points):** For a finite $\epsilon$-net $\{x_i\}_{i=1}^N$ covering $\mathcal{X}$ (exists by compactness), we need:

$$
P(D(E(x_i))) \approx f(x_i) \quad \forall i
$$

**Sub-step 3b (Encoder design):** Let the encoder map $x_i \mapsto z_i$ for arbitrary chosen $z_i \in \mathcal{Z}$ (using encoder's universality).

**Sub-step 3c (Latent passthrough):** With soft equivariance and small $\lambda_{\text{L1}}$, the mixing pathway can be trained to near-zero, making $D \approx \text{identity}$ plus small perturbations. Alternatively, with $L$ latent layers and residual connections, $D$ can implement arbitrary smooth maps via composition.

**Sub-step 3d (Decoder design):** Let the decoder satisfy $P(z_i) \approx f(x_i)$ (using decoder's universality).

**Sub-step 3e (Continuity):** By continuity of $E$, $D$, $P$ and density of $\{x_i\}$ in $\mathcal{X}$, we have:

$$
\sup_{x \in \mathcal{X}} \|P(D(E(x))) - f(x)\| \leq \sup_i \|P(D(E(x_i))) - f(x_i)\| + \underbrace{\text{continuity error}}_{\to 0 \text{ as } N \to \infty}
$$

**Step 4 (Soft equivariance doesn't restrict):** The key observation is that soft equivariance (with $\lambda_{\text{L1}}$ finite) allows the mixing pathway to activate when needed. The L1 penalty is a regularization, not a hard constraint. During training, if the task requires symmetry breaking (i.e., the target $f$ cannot be well-approximated by a strictly equivariant $D$), the optimizer will increase mixing weights, paying the L1 cost but achieving better task loss. The total loss minimization balances:

$$
\min_{\theta} \mathcal{L}_{\text{task}} + \lambda_{\text{L1}} \|W^{\text{mix}}\|_1
$$

For any fixed $\lambda_{\text{L1}} < \infty$ and $\epsilon > 0$, there exist weights achieving $\mathcal{L}_{\text{task}} < \epsilon$ (possibly with large $\|W^{\text{mix}}\|_1$).

Therefore, the UGN is a universal approximator. $\square$

**Remark (Capacity vs Expressiveness):** The theorem guarantees *existence* of approximating weights, not *learnability* via gradient descent. In practice, strong L1 regularization biases the network toward equivariant solutions, which may fail to converge to the universal approximator regime for highly non-equivariant targets. This is a feature, not a bug: the inductive bias toward geometry helps when tasks respect structure, and can be relaxed (by decreasing $\lambda_{\text{L1}}$) when necessary.
:::

:::{prf:theorem} Geometric Consistency
:label: thm-ugn-geometric-consistency

Let $\mathcal{A}_{\text{UGN}}$ be a UGN with $\lambda_{\text{L1}} > 0$. Then:

1. **Approximate capacity bound:** For all layers with spectral normalization, $\|W\|_2 \leq 1$ and approximately 1-Lipschitz activations ensure:
   
   $$
   \|z_{\text{out}}\| \lesssim \|z_{\text{in}}\| + O(\sqrt{L})
   $$
   where $L$ is network depth (approximately non-expansive for fixed depth, consistent with $U(1)_Y$ hypercharge conservation)

2. **Bundle structure preservation:** The latent space maintains decomposition $\mathcal{Z} = \bigoplus_{i=1}^{n_b} V_i$ throughout forward pass (bundles indexed consistently)

3. **Soft equivariance:** Define the **equivariance violation** as:
   
   $$
   \mathcal{V}(D) = \mathbb{E}_{z \sim \mathcal{Z}, R \sim \mu_{SO(d_b)}} \|D(Rz) - RD(z)\|^2
   $$
   where $\mu_{SO(d_b)}$ is the Haar measure on $SO(d_b)$.
   Then:
   
   $$
   \mathcal{V}(D) \leq C \cdot \|W^{\text{mix}}\|_F^2
   $$
   for constant $C$ depending on architecture width. L1 regularization drives $\|W^{\text{mix}}\|_1 \to 0$, which implies $\|W^{\text{mix}}\|_F \to 0$, thus $\mathcal{V}(D) \to 0$.

*Proof.*

**(1) Capacity bound:** By properties of spectral norm (see Section 04, Theorem {prf:ref}`thm-spectral-preserves-hypercharge`):

$$
\|Wz\|_2 \leq \|W\|_2 \|z\|_2 \leq \|z\|_2
$$
Activations (GELU, softplus) are approximately 1-Lipschitz: for large $|x|$, both behave as $\sigma(x) \approx x$, so $\|\sigma(Wz)\|_2 \lesssim \|Wz\|_2 \leq \|z\|_2$ plus bias terms. Composing $L$ spectrally normalized layers with these activations gives:

$$
\|z_{\text{out}}\|_2 \lesssim \|z_{\text{in}}\|_2 + O(\sqrt{L})
$$
where the $O(\sqrt{L})$ term comes from accumulated biases. For fixed depth $L$, this ensures approximate capacity preservation. $\square$

**(2) Bundle structure:** The reshape operations in the forward pass (line `z.view(B, n_bundles, bundle_dim)` and back) maintain the index mapping. Each soft-equivariant layer operates on the bundle structure explicitly (indexing over $i, j \in \{1, \ldots, n_b\}$), so bundles are never "mixed" across their index—only their *contents* are transformed. $\square$

**(3) Soft equivariance bound:**

**Step 1.** Decompose the latent dynamics:

$$
D(z) = D^{\text{equiv}}(z) + D^{\text{mix}}(z)
$$

**Step 2.** The equivariant pathway satisfies $D^{\text{equiv}}(Rz) = R D^{\text{equiv}}(z)$ by construction (norm-based, Theorem {prf:ref}`thm-equivariant-function-structure`).

**Step 3.** The violation comes entirely from the mixing pathway:

$$
D(Rz) - RD(z) = \bigl[ D^{\text{equiv}}(Rz) + D^{\text{mix}}(Rz) \bigr] - R \bigl[ D^{\text{equiv}}(z) + D^{\text{mix}}(z) \bigr]
$$

$$
= \bigl[ RD^{\text{equiv}}(z) + D^{\text{mix}}(Rz) \bigr] - \bigl[ RD^{\text{equiv}}(z) + RD^{\text{mix}}(z) \bigr]
$$

$$
= D^{\text{mix}}(Rz) - RD^{\text{mix}}(z)
$$

**Step 4.** For the mixing pathway implemented as $D^{\text{mix}}(z) = \sum_{i,j} W_{ij} z_j$ (linear in $z$ for each bundle component):

$$
D^{\text{mix}}(Rz) = \sum_{ij} W_{ij} (Rz_j)
$$

$$
RD^{\text{mix}}(z) = R \sum_{ij} W_{ij} z_j
$$

The violation is bounded using operator norm arithmetic. Taking expectation over $R$ and applying Cauchy-Schwarz gives:

$$
\mathbb{E}_R \|D^{\text{mix}}(Rz) - RD^{\text{mix}}(z)\|^2 \leq C_1 \|W^{\text{mix}}\|_F^2 \|z\|^2
$$

Averaging over $z$ with $\|z\|^2$ bounded by capacity constraint:

$$
\mathcal{V}(D) \leq C \|W^{\text{mix}}\|_F^2
$$

**Step 5.** The regularization uses **group lasso** (Frobenius norm per block): $\mathcal{L}_{\text{reg}} = \lambda_{\text{L1}} \sum_{i \neq j} \|W_{ij}\|_F$ where $W_{ij}$ is the $[\text{bundle\_dim} \times \text{bundle\_dim}]$ block coupling bundles $i$ and $j$. This encourages entire blocks to be zero (sparsity at the bundle-interaction level). Since $\|W\|_F \leq \sqrt{n_{\text{blocks}}} \cdot \max_{ij} \|W_{ij}\|_F$, we have:

$$
\lambda_{\text{L1}} \sum_{ij} \|W_{ij}\|_F \to \text{large penalty} \implies \|W_{ij}\|_F \to 0 \text{ for most } (i,j) \implies \|W^{\text{mix}}\|_F \to 0 \implies \mathcal{V}(D) \to 0
$$

Therefore, group lasso regularization enforces soft equivariance with block-wise sparsity. $\square$
:::

:::{prf:proposition} Emergent Gauge Structure from Group Lasso
:label: prop-emergent-gauge-structure

Consider a UGN trained with group lasso regularization $\lambda_{\text{L1}} \sum_{i \neq j} \|W_{ij}\|_F$ (Frobenius norm per block) on a task where the true target function $f^*$ is approximately equivariant (i.e., $f^*(Rx) \approx Rf^*(x)$ for rotations $R$).

Then at convergence, the learned mixing weights $W^{\text{mix}}$ exhibit:

1. **Sparsity:** Most entries $W_{ij}^{(k\ell)}$ (mixing from bundle $j$, component $\ell$ to bundle $i$, component $k$) are driven to zero

2. **Texture zeros:** Specific cross-bundle couplings $(i, j)$ have $\|W_{ij}\|_F \approx 0$ (entire blocks zeroed out), analogous to the CKM/PMNS mixing matrices in particle physics

3. **Hierarchical structure:** Non-zero couplings organize into a hierarchy $|W_{ij}^{(1)}| \gg |W_{ij}^{(2)}| \gg \cdots$, where superscripts index components by magnitude

*Informal justification:*

**Step 1 (Group lasso induces block sparsity):** The group lasso penalty $\sum_{i \neq j} \|W_{ij}\|_F$ (sum of Frobenius norms of blocks) has a non-differentiable minimum at $W_{ij} = 0$ for each block. During gradient descent, small blocks receive gradients pushing them toward zero (block soft thresholding). If the task loss can be minimized without a particular cross-bundle coupling, group lasso drives the entire block to zero.

**Step 2 (Equivariant tasks don't need mixing):** If $f^*$ is equivariant, the optimal network architecture is strictly equivariant ($W^{\text{mix}} = 0$). The equivariant pathway $D^{\text{equiv}}$ can achieve $\mathcal{L}_{\text{task}} \approx 0$ alone. Thus:

$$
\min_{W^{\text{mix}}} \mathcal{L}_{\text{task}}(W^{\text{mix}}) + \lambda \sum_{i \neq j} \|W_{ij}\|_F
$$
has solution $W^{\text{mix}} \approx 0$.

**Step 3 (Symmetry breaking only where needed):** If $f^*$ is *mostly* equivariant but requires small symmetry breaking (e.g., $f^*(Rx) = Rf^*(x) + \epsilon(x, R)$ with $|\epsilon| \ll 1$), the optimizer activates only the minimal set of block couplings needed to capture $\epsilon$. This produces texture zeros: most bundle pairs $(i,j)$ have $\|W_{ij}\|_F \approx 0$ (entire blocks zeroed), only a few are non-zero.

**Step 4 (Hierarchy from optimization dynamics):** The group lasso penalty creates a "rich get richer" dynamic at the block level: once a coupling block $W_{ij}$ is activated (becomes non-zero), further task gradients can flow through it. Blocks that remain near zero get driven to exactly zero by group lasso. This bifurcation produces a hierarchical distribution of block coupling strengths.

**Empirical validation:** This proposition predicts measurable structure in trained models. Diagnostic node 67 (gauge invariance) from the sieve (Chapter 02) can measure $\mathcal{V}(D)$ and the sparsity pattern of $W^{\text{mix}}$. We expect to observe emergent texture zeros without hard-coding them.
:::

:::{prf:observation} BAOAB Structure in Soft Equivariant Layers
:label: obs-baoab-soft-equiv

Each `SoftEquivariantLayer` forward pass can be interpreted as a single BAOAB integration step:

**B (Momentum update):** Equivariant pathway computes geodesic velocity
$$v_i \to v_i \cdot \phi_i(\|v_1\|, \ldots, \|v_{n_b}\|)$$

**A (Position update, first half):** Mixing pathway introduces cross-bundle coupling
$$v_i \to v_i + \sum_j W_{ij} v_j$$

**O (Ornstein-Uhlenbeck thermostat):** Implicit in activation nonlinearity (GELU in norm MLP)

**A (Position update, second half):** Residual connection $z_{\text{out}} = z + \Delta z$

**B (Momentum update):** Next layer repeats

**Correspondence to Section 05:**
- {prf:ref}`def-baoab-attention-heads`: Splitting scheme for Hamiltonian dynamics
- {prf:ref}`def-covariant-qkv-projections`: Multi-head attention = parallel BAOAB chains
:::

:::{prf:definition} Adaptive L1 Schedule
:label: def-adaptive-l1-schedule

The L1 regularization strength adapts based on current equivariance violation:

$$\lambda_{\text{L1}}(t+1) = \lambda_{\text{L1}}(t) \cdot \left(1 + \alpha \cdot (\epsilon(t) - \epsilon_{\text{target}})\right)$$

where:
- $\epsilon(t) = \mathcal{L}_{\text{equiv}}(t)$: Current equivariance violation
- $\epsilon_{\text{target}} \approx 0.22$ nat/step: Proposed target violation (empirical; to be validated)
- $\alpha \in [0.01, 0.1]$: Learning rate for schedule

**Strategy:**
- If $\epsilon(t) > \epsilon_{\text{target}}$: Increase $\lambda_{\text{L1}}$ (more sparsity)
- If $\epsilon(t) < \epsilon_{\text{target}}$: Decrease $\lambda_{\text{L1}}$ (more expressiveness)

This implements **self-tuning** toward the natural symmetry-breaking scale.
:::

## 09_economics/01_pomw.md

:::{prf:definition} The Waste Quotient
:label: def-waste-quotient

For a consensus protocol $\mathcal{P}$, the **Waste Quotient** is:

$$
W_\mathcal{P} := 1 - \frac{\Delta I_{\text{world}}}{\int \dot{\mathcal{M}}(t) \, dt}

$$

where:
- $\Delta I_{\text{world}}$ is the mutual information gained about the world through the computation
- $\dot{\mathcal{M}}(t)$ is the metabolic flux (Definition {prf:ref}`def-metabolic-flux`)

*Units:* $[W_\mathcal{P}] = \text{dimensionless}$.

*Examples:*
- **Bitcoin:** $W_{\text{BTC}} \approx 1$. SHA-256 hashes produce zero structural information about the world: $I(X_{\text{world}}; \text{Hash}) = 0$.
- **Target:** $W_{\text{PoUW}} \to 0$. Energy dissipation equals the reduction in model uncertainty.

:::

:::{prf:theorem} The Cognitive Equivalency Theorem
:label: thm-cognitive-equivalency

Let $\mathcal{C}_{\text{hash}}$ be the computational task of finding a nonce $n$ such that $H(n) < T$ (hash inversion), and let $\mathcal{C}_{\text{grad}}$ be the task of computing a gradient $g = \nabla_\Theta \mathcal{L}(\Theta, D)$ on dataset $D$. Both tasks satisfy the same **Landauer lower bound** on energy expenditure:

$$
E_{\text{min}} \geq k_B T_c \ln 2 \cdot B_{\text{comp}}

$$

where $B_{\text{comp}}$ is the number of irreversible bit operations.

*Proof.*

**Step 1 (Landauer Principle).** By Theorem {prf:ref}`thm-generalized-landauer-bound`, any computation that erases $\Delta H$ nats of information dissipates at least:

$$
\dot{\mathcal{M}} \geq k_B T_c \left| \frac{dH}{ds} \right|

$$

**Step 2 (Hash Computation).** Computing $H(n)$ requires approximately $B_{\text{SHA}} \approx 64 \times 80 = 5120$ irreversible bit operations per hash. The minimum energy is:

$$
E_{\text{hash}} \geq k_B T_c \ln 2 \cdot B_{\text{SHA}} \cdot N_{\text{trials}}

$$

where $N_{\text{trials}} \approx 2^{d}$ for difficulty $d$.

**Step 3 (Gradient Computation).** Computing $g = \nabla_\Theta \mathcal{L}$ via backpropagation requires $O(|\Theta| \cdot |D|)$ multiply-accumulate operations. Each MAC erases intermediate bits, giving:

$$
E_{\text{grad}} \geq k_B T_c \ln 2 \cdot c_{\text{MAC}} \cdot |\Theta| \cdot |D|

$$

for architecture-dependent constant $c_{\text{MAC}}$.

**Step 4 (Equivalence).** Both computations satisfy the same thermodynamic bound. The difference is the *information content* of the output:
- $I(X_{\text{world}}; H(n)) = 0$ (no world knowledge)
- $I(X_{\text{world}}; g) > 0$ (gradient encodes data structure)

Therefore, gradient computation produces **useful information** while satisfying the same energy floor. $\square$

*Consequence:* The security budget of a blockchain can be redirected to train a global model without loss of thermodynamic hardness, provided verification remains tractable.

:::

:::{prf:definition} The Global Model State
:label: def-model-state

The **Global Model State** at block height $h$ is a parameter vector:

$$
\Theta_h \in T_{\bar{z}} \mathcal{Z} \cong \mathbb{R}^D

$$

where:
- $D$ is the model dimension
- $T_{\bar{z}} \mathcal{Z}$ is the tangent space at the current mean belief $\bar{z}$
- The metric on parameter space inherits from the Capacity-Constrained Metric (Theorem {prf:ref}`thm-capacity-constrained-metric-law`)

*Units:* $[\Theta] = [z]$ (latent coordinates).

*Interpretation:* $\Theta_h$ represents the collective belief state of the network—the shared world model encoded in the blockchain.

:::

:::{prf:definition} The Curriculum Block
:label: def-curriculum-block

A **Curriculum Block** $B_h$ at height $h$ is a tuple:

$$
B_h := (\mathcal{H}_{\text{prev}}, \mathcal{H}_D, g_h, \pi_{\text{stake}}, \zeta_h)

$$

where:
- $\mathcal{H}_{\text{prev}} \in \{0,1\}^{256}$ is the hash of the previous block
- $\mathcal{H}_D \in \{0,1\}^{256}$ is the content identifier of training data $D_h$ (e.g., IPFS CID)
- $g_h \in \mathbb{R}^D$ is the **gradient update** computed on $D_h$
- $\pi_{\text{stake}} \in \{0,1\}^{512}$ is the staking proof (signature over stake tokens)
- $\zeta_h \in \mathbb{R}^{d_\zeta}$ is the **Sieve certificate** (validation metadata)

*Units:* $[g_h] = \text{nat}/[z]$ (gradient in latent coordinates).

:::

:::{prf:definition} The Chain Evolution Rule
:label: def-chain-evolution

The global model evolves by **Stochastic Gradient Descent**:

$$
\Theta_{h+1} = \Theta_h - \eta_h \cdot g_h

$$

where $\eta_h > 0$ is the learning rate at height $h$, determined by the difficulty adjustment algorithm (Definition {prf:ref}`def-difficulty-adjustment`).

*Interpretation:* Each block advances the collective belief toward lower loss on the public curriculum. The blockchain is a **thermodynamic record** of this learning process.

:::

:::{prf:definition} The Gradient Mining Puzzle
:label: def-gradient-mining-puzzle

A miner solving block $h$ must:

1. **Fetch Data:** Retrieve training batch $D_h$ from the curriculum queue
2. **Compute Gradient:** Calculate $g = \nabla_\Theta \mathcal{L}(\Theta_{h-1}, D_h)$
3. **Satisfy Sieve Constraints:**
   - **CostBoundCheck (Node 1):** $\|g\|_G \leq E_{\max}$ (bounded energy)
   - **TextureFirewallCheck (Node 29):** $\|\partial_{z_{\text{tex}}} g\| < \epsilon_{\text{tex}}$ (no texture leakage)
   - **CausalEnclosureCheck (Node 53):** $\Delta_{\text{causal}}(g) < \delta_{\text{causal}}$ (causal consistency)
4. **Submit Block:** Broadcast $(B_h, \Theta_h)$ to the network

*Difficulty Adjustment:* See Definition {prf:ref}`def-difficulty-adjustment`.

:::

:::{prf:definition} The Difficulty Adjustment Algorithm
:label: def-difficulty-adjustment

The network **Difficulty** $\mathcal{D}_h$ at height $h$ controls the minimum batch size $|D_h|$ required for valid blocks:

$$
\mathcal{D}_{h+1} = \mathcal{D}_h \cdot \exp\left( -\alpha_{\text{diff}} \left( \frac{t_h - t_{\text{target}}}{t_{\text{target}}} \right) \right)

$$

where:
- $t_h$ is the actual time to mine block $h$
- $t_{\text{target}}$ is the target block time (e.g., 10 minutes)
- $\alpha_{\text{diff}} > 0$ is the adjustment rate

*Units:* $[\mathcal{D}] = \text{samples}$.

*Constraint:* A valid block must satisfy $|D_h| \geq \mathcal{D}_h$.

:::

:::{prf:theorem} Difficulty-Entropy Coupling
:label: thm-difficulty-entropy-coupling

The difficulty adjustment algorithm maintains the **Landauer Invariant**: the minimum energy to produce a valid block is approximately constant:

$$
E_{\min}(B_h) \approx k_B T_c \ln 2 \cdot c_{\text{MAC}} \cdot |\Theta| \cdot \mathcal{D}_h = E_{\text{target}}

$$

*Proof.*

**Step 1.** By the Generalized Landauer Bound (Theorem {prf:ref}`thm-generalized-landauer-bound`), gradient computation costs:

$$
E_{\text{grad}} \geq k_B T_c \ln 2 \cdot c_{\text{MAC}} \cdot |\Theta| \cdot |D_h|

$$

**Step 2.** The difficulty constraint $|D_h| \geq \mathcal{D}_h$ enforces:

$$
E_{\text{grad}} \geq k_B T_c \ln 2 \cdot c_{\text{MAC}} \cdot |\Theta| \cdot \mathcal{D}_h

$$

**Step 3.** The exponential adjustment (Definition {prf:ref}`def-difficulty-adjustment`) stabilizes block time at $t_{\text{target}}$, hence stabilizes energy expenditure rate at $E_{\text{target}} / t_{\text{target}}$.

**Step 4 (Fake Gradient Rejection).** A miner submitting $g' \neq \nabla_\Theta \mathcal{L}(\Theta, D_h)$ violates one of:
- **Directional Check:** Cosine similarity $\cos(g', g_{\text{true}}) < \theta_{\text{min}}$
- **Magnitude Check:** $\|g'\| / \|g_{\text{true}}\| \notin [1-\epsilon, 1+\epsilon]$
- **Causal Check (Node 53):** Interventional gap $\Delta_{\text{causal}}(g') > \delta_{\text{causal}}$

All checks are detectable by spot verification (Section {ref}`sec-holographic-verification`). $\square$

:::

:::{prf:definition} The Boundary Flux Certificate
:label: def-boundary-flux-certificate

The **Boundary Flux Certificate** $\zeta_h$ included in block $B_h$ contains:

$$
\zeta_h := \left( \|g_h\|_G, \, \nabla_{\partial} g_h, \, \text{Tr}(H_h), \, \sigma_{\text{sample}} \right)

$$

where:
- $\|g_h\|_G$ is the gradient norm in the capacity-constrained metric
- $\nabla_{\partial} g_h$ is the boundary gradient (projection onto interface coordinates)
- $\text{Tr}(H_h)$ is the trace of the Hessian (curvature summary)
- $\sigma_{\text{sample}}$ is a random seed for spot-check sampling

*Units:* $[\zeta] = \text{mixed}$ (norm: $\text{nat}/[z]$; trace: $\text{nat}/[z]^2$).

:::

:::{prf:theorem} Holographic Verification Sufficiency
:label: thm-holographic-verification

Let $g$ be a claimed gradient and $\zeta$ its boundary flux certificate. If the boundary data satisfies:

1. **Energy Conservation:** $\|g\|_G^2 \leq \nu_D \cdot \text{Area}(\partial\mathcal{Z}) / \ell_L^{D-1}$ (Causal Information Bound)
2. **Flux Consistency:** $\|\nabla_\partial g - \nabla_\partial g_{\text{spot}}\| < \epsilon_{\text{flux}}$ on spot-check samples
3. **Curvature Bound:** $|\text{Tr}(H)| < \kappa_{\max}$

then with probability $\geq 1 - \delta$, the gradient is valid.

*Proof.*

**Step 1 (Holographic Principle).** By Theorem {prf:ref}`thm-causal-information-bound`, bulk information is bounded by boundary area:

$$
I_{\text{bulk}}(g) \leq \nu_D \cdot \frac{\text{Area}(\partial\mathcal{Z})}{\ell_L^{D-1}} = I_{\max}

$$

**Step 2 (Bulk-Boundary Correspondence).** The gradient $g \in T_\Theta \mathcal{M}$ projects to boundary flux $\nabla_\partial g$ via the restriction map. By the Bulk-Boundary Decoupling Axiom ({prf:ref}`ax-bulk-boundary-decoupling`), the boundary flux determines the bulk gradient up to texture degrees of freedom.

**Step 3 (Spot-Check Amplification).** A fraudulent gradient must differ from the true gradient in some coordinate. The probability of escaping detection in $k$ random spot checks is:

$$
P(\text{escape}) \leq (1 - p_{\text{detect}})^k

$$

where $p_{\text{detect}} \geq \epsilon_{\min}$ is the minimum detection probability per check.

**Step 4 (Energy Conservation).** A gradient claiming to reduce loss by $\Delta \mathcal{L}$ while having energy below the Landauer floor violates:

$$
\|g\|_G^2 < k_B T_c |\Delta H| / \dot{\mathcal{M}}_{\text{claimed}}

$$

This is detectable from the certificate without recomputation.

**Step 5 (Combining).** Setting $k = \log(1/\delta) / \log(1/(1-p_{\text{detect}}))$ spot checks achieves confidence $1-\delta$. $\square$

*Complexity:* Verification requires $O(\sqrt{|D|})$ operations vs $O(|D|)$ for full recomputation.

:::

:::{prf:definition} The Optimistic Verification Protocol
:label: def-optimistic-verification

The network verifies blocks using **Optimistic Acceptance with Challenge Period**:

1. **Submission:** Miner submits block $B_h$ with stake $S_h$
2. **Optimistic Acceptance:** Block is provisionally accepted
3. **Challenge Window:** For duration $T_{\text{challenge}}$, any node may challenge
4. **Challenge:** Challenger computes gradient on random subset $d \subset D_h$ with $|d| = \lceil 0.01 |D_h| \rceil$
5. **Adjudication:** If $\cos(g_h, g_{\text{challenger}}) < \theta_{\text{min}}$, miner is **slashed** (stake burned)
6. **Finalization:** After $T_{\text{challenge}}$ with no successful challenge, block is finalized

:::

:::{prf:definition} The Mining Game
:label: def-mining-game

The **Mining Game** $\Gamma$ is defined by:

- **Players:** $N$ miners indexed by $i \in \{1, \ldots, N\}$
- **Strategy Space:** Each miner chooses $\sigma_i \in \{\text{Honest}, \text{Cheat}\}$
  - **Honest:** Compute true gradient $g_i = \nabla_\Theta \mathcal{L}(\Theta, D)$
  - **Cheat:** Submit fake gradient $g_i' \neq g_{\text{true}}$
- **Payoffs:**
  - Block reward: $R > 0$ (received if block accepted)
  - Stake: $S > 0$ (lost if successfully challenged)
  - Computation cost: $C_{\text{honest}} > C_{\text{cheat}}$

**Key Assumptions:**
1. The detection probability $p_{\text{detect}}$ is exogenous (determined by the spot-check protocol, independent of other miners' strategies)
2. Block rewards are per-miner (not split among winners)
3. Miners play pure strategies (mixed strategies analyzed in Corollary {prf:ref}`cor-stake-reward-ratio`)

:::

:::{prf:theorem} The Verifier's Nash Equilibrium
:label: thm-verifier-nash-equilibrium

In the Mining Game $\Gamma$ with exogenous detection probability $p_{\text{detect}}$ and parameters satisfying:

$$
\frac{S}{R + S} > \frac{C_{\text{honest}} - C_{\text{cheat}}}{R}

$$

**Honest** is a strictly dominant strategy, and $\sigma^* = (\text{Honest}, \ldots, \text{Honest})$ is the unique Nash Equilibrium.

*Proof.*

**Step 1 (Utility Functions).** Define utilities for miner $i$:

$$
U_i(\text{Honest}) = R - C_{\text{honest}}

$$

$$
U_i(\text{Cheat}) = (1 - p_{\text{detect}}) \cdot R + p_{\text{detect}} \cdot (-S) - C_{\text{cheat}}

$$

where $p_{\text{detect}} \in (0, 1]$ is the probability of detection via spot-checking.

**Step 2 (Detection Probability).** By Theorem {prf:ref}`thm-holographic-verification`, detection probability satisfies:

$$
p_{\text{detect}} \geq 1 - (1 - \epsilon_{\min})^k

$$

for $k$ spot-check samples with $\epsilon_{\min} > 0$. Since $p_{\text{detect}}$ is exogenous (determined by the protocol, not other players), each miner faces a constant detection probability regardless of others' strategies.

**Step 3 (Incentive Compatibility).** Honesty is preferred when:

$$
U_i(\text{Honest}) > U_i(\text{Cheat})

$$

$$
R - C_{\text{honest}} > (1 - p_{\text{detect}}) R - p_{\text{detect}} S - C_{\text{cheat}}

$$

Rearranging:

$$
p_{\text{detect}} (R + S) > C_{\text{honest}} - C_{\text{cheat}}

$$

$$
p_{\text{detect}} > \frac{C_{\text{honest}} - C_{\text{cheat}}}{R + S} := p^*

$$

**Step 4 (Equilibrium Condition).** The theorem condition implies:

$$
\frac{S}{R + S} > \frac{C_{\text{honest}} - C_{\text{cheat}}}{R} \implies C_{\text{honest}} - C_{\text{cheat}} < \frac{S \cdot R}{R + S} < R

$$

Therefore $p^* = \frac{C_{\text{honest}} - C_{\text{cheat}}}{R + S} < 1$, ensuring the threshold is achievable with finite spot-checks.

**Step 5 (Dominant Strategy).** Since $p_{\text{detect}}$ is exogenous and independent of other players' strategies, miner $i$'s utility depends only on their own choice. When $p_{\text{detect}} > p^*$:

$$
\Delta U = U(\text{Cheat}) - U(\text{Honest}) = -p_{\text{detect}}(R + S) + (C_{\text{honest}} - C_{\text{cheat}}) < 0

$$

This holds regardless of what other miners do. Thus Honest is a **strictly dominant strategy**, and the unique Nash Equilibrium is all-Honest. $\square$

:::

:::{prf:corollary} The Stake-Reward Ratio
:label: cor-stake-reward-ratio

For the equilibrium to hold with detection probability $p_{\text{detect}} = 0.1$ (10% spot-check rate), the minimum stake-to-reward ratio is:

$$
\frac{S}{R} > \frac{C_{\text{honest}} - C_{\text{cheat}}}{p_{\text{detect}} \cdot R} - 1

$$

For typical gradient computation where $C_{\text{cheat}} = 0.1 C_{\text{honest}}$ (cheating saves 90% of compute), and assuming mining equilibrium where $R \approx C_{\text{honest}}$ (reward covers honest computation cost):

$$
\frac{S}{R} > \frac{0.9 C_{\text{honest}}}{0.1 \cdot R} - 1 = \frac{9 C_{\text{honest}}}{R} - 1 \approx 9 - 1 = 8

$$

*Interpretation:* Miners must stake approximately 8-10x the block reward to make cheating unprofitable with 10% spot-check rate.

:::

:::{prf:definition} The Network Metric Tensor
:label: def-network-metric-tensor

Each validator $i$ maintains a local metric tensor $G^{(i)}$ on the shared latent manifold. The **Network Metric Friction** between chains $\mathcal{C}_A$ and $\mathcal{C}_B$ is:

$$
\mathcal{F}(\mathcal{C}_A, \mathcal{C}_B) := \sum_{i,j} \mathcal{F}_{ij}(\Theta_{\text{head}}^A, \Theta_{\text{head}}^B)

$$

where $\mathcal{F}_{ij}$ is the pairwise metric friction (Definition {prf:ref}`def-metric-friction`).

:::

:::{prf:definition} Metric Friction Consensus
:label: def-metric-friction-consensus

The **Canonical Chain** is selected by minimizing global metric friction:

$$
\mathcal{C}^* = \arg\min_{\mathcal{C}} \sum_{i < j} \mathcal{F}_{ij}(\Theta_{\text{head}}^\mathcal{C})

$$

*Mechanism:*
1. Miners propose competing updates $\{g_A, g_B, \ldots\}$
2. Validators compute local metric tensors $G^{(i)}(\Theta + g_k)$ for each candidate
3. The update minimizing pairwise friction is accepted
4. Ties broken by timestamp (first-seen)

:::

:::{prf:lemma} Gradient Observability
:label: lem-gradient-observability

The gradient $g$ uniquely determines the local metric tensor $G(\Theta + \epsilon g)$ to first order:

$$
G_{ij}(\Theta + \epsilon g) = G_{ij}(\Theta) + \epsilon \, \partial_k G_{ij} \cdot g^k + O(\epsilon^2)

$$

*Proof.* Direct Taylor expansion of the metric tensor. The metric is a smooth function of parameters, and its derivatives are observable from model predictions. $\square$

*Consequence:* Validators can infer each other's metrics from observed gradients without direct communication.

:::

:::{prf:theorem} Minimum Friction Byzantine Fault Tolerance
:label: thm-minimum-friction-bft

The Metric Friction Consensus achieves Byzantine Fault Tolerance against $f < N/3$ adversarial validators for **gradient-poisoning attacks** (adversaries submit incorrect gradients).

**Scope:** This theorem addresses data integrity attacks (model poisoning, fake gradients). Classical BFT attacks (equivocation, censorship) are handled by the underlying stake-based leader election, which is assumed to follow standard PBFT guarantees.

*Proof sketch.*

**Step 1 (Honest Majority Alignment).** By Theorem {prf:ref}`thm-spontaneous-gauge-locking`, honest validators minimizing prediction error on the same data undergo spontaneous gauge locking: $G^{(i)} \to G^{(j)}$ for honest $i, j$.

**Step 2 (Adversarial Inflation).** By Theorem {prf:ref}`thm-adversarial-mass-inflation` (Adversarial Mass Inflation), any gradient $g_{\text{adv}} \neq g_{\text{true}}$ introduces non-zero metric perturbation:

$$
\tilde{G}^{(i)} = G^{(i)} + \alpha_{\text{adv}} \mathcal{G}_{ij}, \quad \alpha_{\text{adv}} = \|g_{\text{adv}} - g_{\text{true}}\|_G > 0

$$

where $\mathcal{G}_{ij}$ is the Game Tensor (Definition {prf:ref}`def-gauge-covariant-game-tensor`). The key insight: *there is no "zero-curvature" way to submit a fake gradient*.

**Step 3 (Friction Separation).** Let $\epsilon$ be the natural gradient variance among honest validators. The pairwise friction satisfies:

- Honest-Honest: $\mathcal{F}_{ij} \leq c_1 \epsilon^2$ (gauge-locked, small noise)
- Honest-Adversarial: $\mathcal{F}_{ik} \geq c_2 \alpha_{\text{adv}}$ (metric mismatch)

For the attack to succeed while evading detection, the adversary requires $\alpha_{\text{adv}} < c_1 \epsilon^2 / c_2$. But such small perturbations have negligible effect on model training—a successful attack requires $\alpha_{\text{adv}} \gg \epsilon$.

**Step 4 (Selection).** The total friction of a chain proposed by honest validators is:

$$
\mathcal{F}_{\text{total}}^{\text{honest}} \leq \binom{N-f}{2} c_1 \epsilon^2 + f(N-f) c_2 \alpha_{\text{adv}}

$$

An adversarial chain has friction at least $\mathcal{F}_{\text{total}}^{\text{adv}} \geq (N-f) c_2 \alpha_{\text{adv}}$.

With $f < N/3$ and $\alpha_{\text{adv}} \gg \epsilon^2$, the honest chain minimizes total friction. $\square$

*Remark:* The $N/3$ threshold matches classical BFT because friction-weighted voting is equivalent to stake-weighted voting when adversarial friction is high.

:::

:::{prf:theorem} Adversarial Geometric Damping
:label: thm-adversarial-geometric-damping

An adversary controlling fraction $\alpha < 1/3$ of validators has influence on consensus bounded by:

$$
\|\Delta \Theta_{\text{adversarial}}\|_G \leq \frac{\alpha}{1 - 2\alpha} \|\Delta \Theta_{\text{honest}}\|_G

$$

*Proof.*

**Step 1.** The consensus update is a friction-weighted average:

$$
\Delta \Theta = \frac{\sum_i w_i \Delta \Theta^{(i)}}{\sum_i w_i}

$$

where weights $w_i = 1/\mathcal{F}_{i,\text{total}}$ penalize high-friction validators.

**Step 2.** Adversarial validators have inflated friction:

$$
w_{\text{adv}} \leq w_{\text{honest}} / (1 + \alpha_{\text{adv}}/\epsilon^2)

$$

**Step 3.** The adversarial contribution is:

$$
\|\Delta \Theta_{\text{adv}}\| \leq \frac{\alpha \cdot w_{\text{adv}}}{(1-\alpha) w_{\text{honest}} + \alpha w_{\text{adv}}} \|\Delta \Theta_{\text{total}}\|

$$

**Step 4.** Taking $w_{\text{adv}} \to 0$ in the limit of high adversarial friction:

$$
\|\Delta \Theta_{\text{adv}}\| \to 0

$$

The adversary is geometrically isolated. $\square$

*Interpretation:* Adversaries are not voted out---they are **geometrically damped**. Their updates carry infinite inertia (Causal Stasis) and cannot influence the consensus trajectory.

:::

:::{prf:definition} The Token Standard
:label: def-token-standard

The $\text{COG}$ token has three fundamental operations:

1. **Minting (Supply).** Tokens are minted when **Ontological Stress** $\Xi$ is reduced:

$$
\Delta \text{Supply} = \kappa_{\text{mint}} \cdot \max(0, -\Delta \Xi_{\text{global}})

$$

where $\kappa_{\text{mint}}$ is the minting coefficient (tokens per nat of stress reduction).

*Interpretation:* Value is created only when the network learns something new.

2. **Burning (Demand).** Tokens are burned to request **Inference**:

$$
\text{Cost}(Q) = \mathfrak{T}_{\text{harvest}}^{-1}(\dot{\mathcal{M}}_Q)

$$

where $\dot{\mathcal{M}}_Q$ is the metabolic cost of answering query $Q$.

3. **Transfer.** Standard ERC-20-like transfers between accounts.

*Units:* $[\text{COG}] = \text{Joules}$ (energy equivalent).

*Value Anchor:* $1 \, \text{COG} \approx 1 \, \text{Joule}$ of useful gradient computation at reference temperature $T_c$.

:::

:::{prf:theorem} Value-Intelligence Coupling
:label: thm-value-intelligence-coupling

The equilibrium token price $P_{\text{COG}}$ is bounded by:

$$
P_{\text{floor}} \leq P_{\text{COG}} \leq P_{\text{ceiling}}

$$

where:

$$
P_{\text{floor}} = C_{\text{electricity}} \cdot J_{\text{per\_COG}}

$$

(cost of electricity to generate one COG worth of computation)

$$
P_{\text{ceiling}} = \frac{V_{\text{inference}}}{J_{\text{per\_query}}}

$$

(value of inference output per Joule)

*Proof.*

**Step 1 (Floor).** If $P_{\text{COG}} < P_{\text{floor}}$, miners cannot profitably produce blocks. Supply decreases until price rises.

**Step 2 (Ceiling).** If $P_{\text{COG}} > P_{\text{ceiling}}$, users won't pay for inference. Demand decreases until price falls.

**Step 3 (Equilibrium).** At equilibrium:

$$
P_{\text{COG}}^* = \sqrt{P_{\text{floor}} \cdot P_{\text{ceiling}}}

$$

(geometric mean under log-linear supply/demand). $\square$

:::

:::{prf:corollary} Intelligence-Price Feedback
:label: cor-intelligence-price-feedback

As the model improves:

1. Inference value $V_{\text{inference}} \uparrow$
2. Ceiling $P_{\text{ceiling}} \uparrow$
3. Equilibrium price $P_{\text{COG}}^* \uparrow$
4. Mining profitability $\uparrow$
5. More compute allocated $\uparrow$
6. Model improves faster $\uparrow$

This creates a **positive feedback loop** between intelligence and economic value.

:::

:::{prf:theorem} Ledger-Memory Screen Isomorphism
:label: thm-ledger-memory-isomorphism

Let $\Xi_T$ be the Memory Screen (Definition {prf:ref}`def-memory-screen`) and $\mathcal{L}_H$ be the blockchain of height $H$. There exists an isomorphism:

$$
\Phi: \mathcal{L}_H \to \Xi_T

$$

given by:

| Blockchain | Memory Screen | Symbol |
|:-----------|:--------------|:-------|
| Block height $h$ | Time coordinate $t$ | $h \leftrightarrow t$ |
| Merkle root $\mathcal{H}_h$ | Boundary state $z_{\partial}$ | $\mathcal{H}_h \leftrightarrow z_{\partial}(t)$ |
| Gradient $g_h$ | Flux $\alpha(t)$ | $g_h \leftrightarrow \alpha(t)$ |
| Chain $\sum_{h=0}^H B_h$ | Screen $\int_0^T \alpha(t) \delta_{\gamma(t)} dt$ | $\mathcal{L}_H \leftrightarrow \Xi_T$ |

*Proof.*

**Step 1.** The Memory Screen (Definition {prf:ref}`def-memory-screen`) is:

$$
\Xi_T = \int_0^T \alpha(t') \, \delta_{\gamma(t')} \, dt'

$$

where $\alpha(t) = J_r(t)$ is the reward flux and $\gamma(t)$ is the trajectory.

**Step 2.** The blockchain is:

$$
\mathcal{L}_H = \sum_{h=0}^{H} B_h = \sum_{h=0}^{H} (g_h, \mathcal{H}_h, \ldots)

$$

**Step 3.** Define the correspondence:
- $t = h \cdot \Delta t$ where $\Delta t$ is block time
- $\alpha(t) = g_h / \Delta t$ (gradient rate)
- $\gamma(h) = \Theta_h$ (parameter trajectory)

**Step 4.** The discrete sum converges to the continuous integral:

$$
\sum_{h=0}^{H} g_h \cdot \mathbb{1}_{\Theta_h} \to \int_0^T \alpha(t) \delta_{\gamma(t)} dt

$$

as $\Delta t \to 0$. $\square$

*Interpretation:* The blockchain is the **frozen boundary** of the network's cognitive trajectory. Each block records a moment of learning; the full chain is the holographic screen encoding the network's history.

:::

:::{prf:corollary} Block Size from Area Law
:label: cor-block-size-area-law

The maximum information in a block is bounded by:

$$
I_{\text{block}} \leq \nu_D \cdot \frac{\text{Area}(\partial \mathcal{Z})}{\ell_L^{D-1}}

$$

where the area is measured in the header's Merkle tree.

*Proof.* Direct application of Theorem {prf:ref}`thm-causal-information-bound` to the block's boundary. $\square$

*Consequence:* Oversized blocks violate the Causal Information Bound. The network enters **Causal Stasis** (Theorem {prf:ref}`thm-causal-stasis`) if blocks exceed capacity—propagation delay exceeds block time.

:::

:::{prf:definition} Chain Renormalization (Pruning)
:label: def-chain-renormalization

Old blocks are **coarse-grained** into **Epoch Blocks** via the Projection Operator:

$$
B_{\text{epoch}} = \Pi\left( \sum_{h \in \text{epoch}} B_h \right)

$$

where $\Pi$ projects onto the low-frequency components of the gradient history.

*Mechanism:*
1. Every $N_{\text{epoch}}$ blocks, compress the epoch into a summary
2. Discard individual block data (retain Merkle proofs)
3. The agent remembers the "gist" but forgets the "noise"

*Thermodynamics:* This is **information erasure** (Landauer cost). It releases storage but maintains the essential learning trajectory.

:::

:::{prf:theorem} 51% Attack Geometric Rejection
:label: thm-51-attack-rejection

An attacker controlling $> 50\%$ of compute cannot rewrite history without triggering **Spontaneous Fission**.

*Proof.*

**Step 1.** The attacker proposes an alternative chain $\mathcal{C}'$ that contradicts the Memory Screen $\Xi_T$ of honest validators.

**Step 2.** By Theorem {prf:ref}`thm-adversarial-mass-inflation`, the attacker's chain has inflated metric:

$$
\tilde{G}_{\text{attack}} = G + \alpha_{\text{adv}} \mathcal{G}

$$

**Step 3.** The Metric Friction between honest and attack chains is:

$$
\mathcal{F}(\mathcal{C}, \mathcal{C}') = \|G - \tilde{G}_{\text{attack}}\|_F^2 \sim O(\alpha_{\text{adv}}^2)

$$

**Step 4.** When $\mathcal{F} > \mathcal{F}_{\text{crit}}$ (Fission Threshold from Theorem {prf:ref}`thm-fission-criterion`), the network undergoes **Spontaneous Fission**:
- The attacker ends up on a high-friction shard
- The honest validators continue on the low-friction chain

**Step 5.** The attacker's shard enters **Causal Stasis** (Theorem {prf:ref}`thm-causal-stasis`)---no one provides data/compute, and it dies. $\square$

*Interpretation:* You cannot buy the network because you cannot buy **geometric alignment**.

:::

:::{prf:theorem} Causal Theft Prevention
:label: thm-causal-theft-prevention

Flash-loan attacks and front-running are rejected by **CausalityViolationCheck (Node 62)**.

*Proof.*

**Step 1.** A flash-loan attack requires: borrow $\to$ manipulate price $\to$ profit $\to$ repay, all in one transaction.

**Step 2.** The profit depends on a price change that **hasn't propagated** in the causal graph at the time of the borrow.

**Step 3.** By the Causal Information Bound (Theorem {prf:ref}`thm-causal-information-bound`), information cannot propagate faster than:

$$
v_{\max} = \frac{d_G(z, z')}{t}

$$

**Step 4.** **Node 62 (CausalityViolationCheck)** detects transactions using information from the future:

$$
\Delta_{\text{causal}} = D_{\text{KL}}(P_{\text{interventional}} \| P_{\text{observational}}) > \delta_{\text{causal}}

$$

**Step 5.** The transaction is rejected as **geometrically impossible**. $\square$

:::

:::{prf:theorem} Corruption Detection via Babel Limit
:label: thm-corruption-babel-detection

Sustained deception by corrupt actors exceeds the **Babel Limit** (Theorem {prf:ref}`thm-babel-limit`) and causes loss of gauge locking.

*Proof.*

**Step 1.** A corrupt actor broadcasts metric $G_{\text{corrupt}}$ claiming to optimize the objective, but their actual gradient flow generates different geometry.

**Step 2.** Maintaining the deception requires transmitting additional fake metric information:

$$
I_{\text{deception}} = H(G_{\text{corrupt}}) - H(G_{\text{true}})

$$

**Step 3.** By Theorem {prf:ref}`thm-babel-limit`, complete gauge locking requires:

$$
\dim(\mathfrak{g}) \cdot H(G) \leq C_{\mathcal{L}}

$$

**Step 4.** The deception increases effective entropy, violating the Babel Limit:

$$
\dim(\mathfrak{g}) \cdot (H(G_{\text{true}}) + I_{\text{deception}}) > C_{\mathcal{L}}

$$

**Step 5.** The corrupt actor loses gauge locking with honest validators. Their words become "noise"---they are **topologically exiled** from consensus. $\square$

*Interpretation:* You cannot lie to the network because you cannot fake the **thermodynamic trace** of your actions.

:::

## 10_appendices/01_derivations.md

:::{prf:definition} A.1.1 (Boundary capacity form)
:label: def-a-boundary-capacity-form

Define the boundary capacity $(n\!-\!1)$-form

$$
\omega_{\partial} := \frac{1}{\eta_\ell}\, dA_G,

$$
so that $C_{\partial}(\partial\mathcal{Z})=\oint_{\partial\mathcal{Z}}\omega_{\partial}$ (Definition 17.1.3).

:::

:::{prf:definition} A.1.2 (Boundary-capacity constraint functional)
:label: def-a-boundary-capacity-constraint-functional

Define the saturation functional

$$
\mathcal{C}[G,V]
:=
\underbrace{\int_{\mathcal{Z}} \rho_I(G,V)\, d\mu_G}_{I_{\text{bulk}}}
\;-\;
\underbrace{\oint_{\partial\mathcal{Z}}\omega_{\partial}}_{C_{\partial}},

$$
where $\rho_I(G,V)$ is an *information density* (nats per unit $d\mu_G$) compatible with the agent's representation scheme (Definition 17.1.2). This $\rho_I$ is distinct from the belief density $p$ used in {ref}`Section 2.11 <sec-variance-value-duality-and-information-conservation>`. When $\rho_I$ is instantiated via the split shutter, the most conservative computable proxy is a global one, $I_{\text{bulk}}\approx \mathbb{E}[I(X;K)]$ (Node 13), and the window theorem (Theorem {prf:ref}`thm-information-stability-window-operational`) supplies the admissible operating range.

:::

:::{prf:definition} A.1.3 (Risk Lagrangian density)
:label: def-a-risk-lagrangian-density

Fix a smooth potential $V\in C^\infty(\mathcal{Z})$. A canonical risk Lagrangian density is the scalar-field functional

$$
\mathcal{L}_{\text{risk}}(V;G) := \frac{1}{2}\,G^{ab}\nabla_a V\,\nabla_b V + U(V),

$$
where $U:\mathbb{R}\to\mathbb{R}$ is a (possibly learned) on-site potential capturing non-gradient costs. (The sign convention is chosen for a Riemannian metric; see e.g. Lee, *Riemannian Manifolds*, 2018, for the variational identities used below.)

:::

:::{prf:definition} A.1.4 (Capacity-constrained curvature functional)
:label: def-a-capacity-constrained-curvature-functional

Let $R(G)$ be the scalar curvature of $G$ and let $\Lambda\in\mathbb{R}$ be a constant. Define the constrained functional

$$
\mathcal{S}[G,V]
:=
\int_{\mathcal{Z}}\left(R(G)-2\Lambda + 2\kappa\,\mathcal{L}_{\text{risk}}(V;G)\right)d\mu_G
\;-\;
2\kappa\oint_{\partial\mathcal{Z}}\omega_{\partial},

$$
with coupling $\kappa\in\mathbb{R}$. The last term is the explicit boundary capacity penalty, and $\Lambda$ is a bulk capacity offset that remains once the boundary is clamped at finite resolution.

*Remark (why $\Lambda$ is allowed).* A constant term in the integrand is the simplest coordinate-invariant scalar density and produces a $\Lambda G_{ij}$ term in the metric Euler–Lagrange equation. Here $\Lambda$ plays the role of a baseline curvature / capacity offset.

:::

:::{prf:lemma} A.3.1 (Divergence-to-boundary conversion)
:label: lem-a-divergence-to-boundary-conversion

For any sufficiently regular information flux field $\mathbf{j}$ on $\mathcal{Z}$,

$$
\int_{\mathcal{Z}} \operatorname{div}_G(\mathbf{j})\, d\mu_G = \oint_{\partial \mathcal{Z}} \langle \mathbf{j}, \mathbf{n}\rangle\, dA_G,

$$
which is the Riemannian divergence theorem underlying the global balance equation in Theorem {prf:ref}`thm-generalized-conservation-of-belief`.

:::

:::{prf:theorem} A.3.2 (Capacity-consistency identity; proof of Theorem {prf:ref}`thm-capacity-constrained-metric-law`)
:label: thm-a-capacity-consistency-identity-proof-of-theorem

Under the hypotheses of Section A.2, stationarity of $\mathcal{S}[G,V]$ with respect to arbitrary variations $\delta G^{ij}$ that vanish on $\partial\mathcal{Z}$ implies the Euler–Lagrange equation

$$
R_{ij} - \frac{1}{2}R\,G_{ij} + \Lambda G_{ij} = \kappa\, T_{ij},

$$
with $T_{ij}$ given by Section A.2.3.

*Proof.* Combine Sections A.2.1–A.2.4:

$$
\delta\mathcal{S} = \int_{\mathcal{Z}}\left[\left(R_{ij}-\frac12 R\,G_{ij}\right) + \Lambda G_{ij} - \kappa T_{ij}\right]\delta G^{ij}\,d\mu_G + \text{(boundary terms)}.

$$
Boundary terms vanish under the clamped boundary condition (or after adding an appropriate boundary term). Because $\delta G^{ij}$ is arbitrary in the interior, the fundamental lemma of the calculus of variations implies the bracketed tensor must vanish pointwise almost everywhere, yielding the stated identity (see e.g. Evans, *Partial Differential Equations*, 2010, for the functional-analytic lemma).

*Interpretation.* The Ricci curvature governs local volume growth; enforcing a boundary-limited bulk information volume forces the metric to stretch/compress coordinates so that information-dense regions (large $\|\nabla_A V\|$ and/or large $U(V)$) do not generate bulk structure that cannot be grounded at the boundary.

*Remark (regularizer).* The squared residual of this identity defines the capacity-consistency loss $\mathcal{L}_{\text{cap-metric}}$; see {ref}`Appendix B <sec-appendix-b-units-parameters-and-coefficients>`.

:::

:::{prf:remark} Physical interpretation
:label: rem-physical-interpretation

The overdamped limit corresponds to:
- **Information geometry:** The "friction" $\gamma$ represents the rate of information dissipation (forgetting). High friction means the system equilibrates quickly to the local gradient.
- **Diffusion models:** Standard score-based diffusion models operate entirely in the overdamped regime, with $\gamma \to \infty$ implicitly.
- **Neural network training:** The geodesic term $\Gamma(\dot{z},\dot{z})$ can be interpreted as a "momentum correction" that accounts for the curvature of the loss landscape. In standard gradient descent (overdamped), this term is ignored.

:::

:::{prf:theorem} Classification as Relaxation
:label: thm-classification-as-relaxation-a

Under the overdamped dynamics with class-conditioned potential $V_y$:

$$
dz = \mathcal{M}_{\text{curl}}\!\left(-G^{-1}(z) \nabla_A V_y(z, K)\right) ds + \sqrt{2T_c}\, G^{-1/2}(z)\, dW_s,

$$
where $\mathcal{M}_{\text{curl}} := (I - \beta_{\text{curl}} G^{-1}\mathcal{F})^{-1}$. Here $\nabla_A V_y := \nabla V_y - A$ (conservative case: $A=0$). When $\mathcal{F}=0$ (conservative case), the curl term vanishes.
the limiting chart assignment satisfies $\lim_{s \to \infty} K(z(s)) \in \mathcal{A}_y$ almost surely, provided the initial condition lies in the basin $\mathcal{B}_y$ and $T_c$ is sufficiently small.

:::

:::{prf:proof}

**Step 1: Lyapunov Function Construction.**

Define the Lyapunov function:

$$
L(z) := V_y(z, K(z)) = -\beta_{\text{class}} \log P(Y=y \mid K(z)) + V_{\text{base}}(z, K(z)).

$$
By construction, $L(z)$ achieves its global minimum on the sub-atlas $\mathcal{A}_y$, where $P(Y=y \mid K) > 1 - \epsilon_{\text{purity}}$, hence $-\log P(Y=y \mid K) < -\log(1 - \epsilon_{\text{purity}})$ is minimized.

**Step 2: Itô Computation.**

Applying Itô's lemma to $L(z(s))$:

$$
dL = \nabla L \cdot dz + \frac{1}{2} \text{tr}(\nabla^2 L \cdot \Sigma)\, ds,

$$
where $\Sigma = 2T_c\, G^{-1}$ is the diffusion covariance.

Substituting the SDE:

$$
dL = \nabla L \cdot \left(-G^{-1} \nabla_A V_y\, ds + \sqrt{2T_c}\, G^{-1/2}\, dW_s\right) + T_c\, \Delta_G L\, ds,

$$
where $\Delta_G L = \text{tr}(G^{-1} \nabla^2 L)$ is the Laplace-Beltrami operator.

Since $L = V_y$, we have $\nabla L = \nabla V_y$, so:

$$
dL = -G^{-1}(\nabla V_y, \nabla_A V_y)\, ds + \sqrt{2T_c}\, \nabla V_y \cdot G^{-1/2}\, dW_s + T_c\, \Delta_G V_y\, ds.

$$
**Step 3: Expected Drift.**

Taking expectations:

$$
\frac{d}{ds}\mathbb{E}[L(z(s))] = -\mathbb{E}[G^{-1}(\nabla V_y, \nabla_A V_y)] + T_c\, \mathbb{E}[\Delta_G V_y].

$$
The first term is non-positive in the conservative case ($A=0$), reducing to $-\|\nabla V_y\|_G^2$. The second term is $O(T_c)$ and bounded if $V_y$ has bounded Hessian.

**Step 4: Low-Temperature Limit.**

For $T_c \to 0$, the drift becomes:

$$
\frac{d}{ds}\mathbb{E}[L] \approx -\mathbb{E}[G^{-1}(\nabla V_y, \nabla_A V_y)],

$$
with equality only at critical points of $V_y$.

**Step 5: Convergence to Attractor Basin.**

By LaSalle's invariance principle (conservative case), the trajectory converges to the largest invariant set where $\|\nabla V_y\|_G = 0$. Since $V_y$ is constructed with:
- A global minimum on $\mathcal{A}_y$ (the class-$y$ sub-atlas)
- Local maxima or saddles in transition regions $\mathcal{A}_i \cap \mathcal{A}_j$

If $z(0) \in \mathcal{B}_y$ (the basin of attraction for $\mathcal{A}_y$), the trajectory cannot escape to other basins (they are separated by energy barriers), hence:

$$
\lim_{s \to \infty} z(s) \in \mathcal{A}_y \quad \text{a.s.}

$$
**Step 6: Chart Assignment.**

*Technical note (Piecewise Continuity).* The Lyapunov function $L(z) = V_y(z, K(z))$ has potential discontinuities at chart boundaries where $K(z)$ changes discretely. However, this does not invalidate the argument because:

1. **Within-chart dynamics:** The SDE governs continuous motion within each chart; chart transitions occur via the jump process ({ref}`Section 20.5 <sec-connection-to-gksl-master-equation>`, WFR reaction term)
2. **Jump consistency:** The class-modulated jump rates (Definition {prf:ref}`def-class-consistent-jump-rate`) ensure that jumps to charts in $\mathcal{A}_y$ are favored when starting from $\mathcal{B}_y$
3. **Effective continuity:** For the soft router weights $w_k(x)$, the "effective chart" is a convex combination, and $\sum_k w_k(z) V_y(z, k)$ is continuous

Since $z(s) \to \mathcal{A}_y$ and the chart assignment $K(z)$ eventually stabilizes (jumps become rare as $z$ approaches the basin interior), we have:

$$
\lim_{s \to \infty} K(z(s)) \in \mathcal{A}_y.

$$
**Quantitative Bound (Low Temperature).**

For small but positive $T_c$, standard results on diffusions in potential wells (Kramers' law {cite}`kramers1940brownian`) give the escape rate from basin $\mathcal{B}_y$:

$$
\text{Rate}_{\text{escape}} \sim e^{-\Delta V / T_c},

$$
where $\Delta V$ is the barrier height. For $T_c \ll \Delta V$, escape is exponentially unlikely, ensuring practical convergence.

This completes the proof. $\square$

:::

:::{prf:remark} Connection to Classification Accuracy
:label: rem-connection-to-classification-accuracy

The theorem provides a geometric interpretation of classification accuracy: a sample $x$ is correctly classified if and only if $\text{Enc}(x) \in \mathcal{B}_{y_{\text{true}}}$. Misclassification occurs when the encoder maps $x$ to the wrong basin—either due to encoder limitations or overlap between class distributions in observation space.

:::

:::{prf:axiom} A.6.0a (Operational Distinguishability)
:label: ax-a-operational-distinguishability

Two probability distributions $p, q \in \mathcal{P}(\mathcal{Z})$ are **operationally distinguishable** if and only if:

$$
D_{\text{KL}}(p \| q) \geq 1 \text{ nat}.

$$
*Justification.* This is an **operational definition**, not a derived fact. The choice of 1 nat as the threshold is grounded in:

1. **Asymptotic error exponent.** For $n$ i.i.d. samples, the optimal Type II error probability at fixed Type I error decays as $\exp(-n \cdot D_{\text{KL}})$ (Stein's lemma). Thus $D_{\text{KL}} = 1$ nat corresponds to error decay rate $e^{-n}$.

2. **Information-theoretic meaning.** 1 nat = log(e) ≈ 1.44 bits represents a "natural unit" of information, where the likelihood ratio $p(x)/q(x)$ has expected log-value 1 under $p$.

3. **Dimensional analysis.** The nat is the natural unit when using natural logarithms; choosing 1 nat as the threshold makes the subsequent formulas dimensionally consistent.

*Remark.* Alternative thresholds (e.g., 1 bit = ln 2 nats) would change the numerical coefficient in the Area Law but not its structure.

:::

:::{prf:theorem} A.6.0b (Chentsov's Uniqueness Theorem)
:label: thm-a-chentsov-uniqueness

The **Fisher Information Metric** is the unique Riemannian metric on statistical manifolds (up to constant scaling) that is invariant under sufficient statistics.

**Statement.** Let $\mathcal{M}$ be a statistical manifold parameterized by $\theta \in \Theta$. Any Riemannian metric $g$ on $\mathcal{M}$ satisfying:
1. **Markov invariance:** $g$ is preserved under Markov morphisms (conditional expectations)
2. **Smoothness:** $g$ varies smoothly with $\theta$

is proportional to the Fisher Information Metric:

$$
g_{ij}(\theta) = c \cdot \mathbb{E}_\theta\left[\frac{\partial \log p(x|\theta)}{\partial \theta^i} \frac{\partial \log p(x|\theta)}{\partial \theta^j}\right]

$$
for some constant $c > 0$.

*Proof.* See Chentsov (1982) {cite}`chentsov1982statistical` and Campbell (1986) {cite}`campbell1986extended`. The proof uses the characterization of Markov morphisms as coarse-grainings and shows that invariance under all such maps forces the metric to be the Fisher metric. $\square$

*Significance.* Chentsov's theorem establishes that the Fisher metric is not a choice but a *necessity*: any geometry on probability space that respects statistical structure must be (proportional to) the Fisher geometry. This grounds our derivation in fundamental statistics, not ad-hoc assumptions.

:::

:::{prf:definition} A.6.0c (Computational Microstate)
:label: def-a-computational-microstate

A **computational microstate** at resolution $\ell$ is a complete specification of the agent's internal configuration $\mu = (\rho, K, \theta)$ where:
- $\rho \in \mathcal{P}(\mathcal{Z})$ is the belief distribution over the latent manifold
- $K \in \{1, \ldots, |\mathcal{K}|\}$ is the active chart assignment
- $\theta$ are the model parameters

discretized at the Levin Length scale: positions resolved to precision $\ell_L$, probabilities resolved to precision $e^{-1}$ in KL divergence.

Two microstates $\mu_1, \mu_2$ are **boundary-distinguishable** if an external observer, receiving only boundary observations $\partial\mathcal{Z}$, can distinguish them with probability $> 1 - e^{-1}$.

*Remark (Analogy to Physics).* In black hole thermodynamics, a microstate is a specific quantum configuration of the horizon degrees of freedom. Here, a microstate is a specific configuration of the agent's belief state. The boundary plays the role of the horizon: internal distinctions not visible at the boundary do not count toward the entropy.

:::

:::{prf:lemma} A.6.0d (Geodesic Distance on the Probability Simplex)
:label: lem-a-geodesic-distance-probability-simplex

On the 1-simplex $\Delta^1 = \{(p, 1-p) : p \in [0,1]\}$ with Fisher Information Metric, the geodesic distance from the uniform distribution $(1/2, 1/2)$ to a vertex $(1, 0)$ is:

$$
d_{\text{Fisher}}\left(\tfrac{1}{2}, 1\right) = \frac{\pi}{2}.

$$
*Proof.* The Fisher metric on $\Delta^1$ is:

$$
ds^2 = \frac{dp^2}{p(1-p)}.

$$
Introduce the angular parameterization $p = \cos^2(\theta/2)$, so that $1-p = \sin^2(\theta/2)$ and:

$$
dp = -\cos(\theta/2)\sin(\theta/2)d\theta = -\frac{1}{2}\sin\theta \, d\theta.

$$
Then:

$$
ds^2 = \frac{\frac{1}{4}\sin^2\theta \, d\theta^2}{\cos^2(\theta/2)\sin^2(\theta/2)} = \frac{\frac{1}{4}\sin^2\theta \, d\theta^2}{\frac{1}{4}\sin^2\theta} = d\theta^2.

$$
The uniform distribution $(1/2, 1/2)$ corresponds to $\theta = \pi/2$. The vertex $(1, 0)$ corresponds to $\theta = 0$. The geodesic distance is:

$$
d = \int_0^{\pi/2} d\theta = \frac{\pi}{2}. \quad \square

$$
*Interpretation.* One bit of information (distinguishing "heads" from "tails") corresponds to geodesic distance $\pi/2$ in Fisher geometry. This is a derived quantity, not an assumption.

:::

:::{prf:lemma} A.6.0e (Curvature Normalization and the Factor of 4)
:label: lem-a-curvature-normalization-factor-4

The Poincare disk model with constant sectional curvature $K = -1$ has metric:

$$
ds^2 = \frac{4(dx^2 + dy^2)}{(1-|z|^2)^2}.

$$
The factor of 4 is uniquely determined by the curvature normalization.

*Proof.* For a 2D Riemannian manifold with conformal metric $ds^2 = \lambda(z)(dx^2 + dy^2)$, the Gaussian curvature is {cite}`docarmo1992riemannian`:

$$
K = -\frac{1}{2\lambda}\Delta(\log \lambda),

$$
where $\Delta = \partial_x^2 + \partial_y^2$ is the flat Laplacian.

For $\lambda = c/(1-r^2)^2$ where $r^2 = x^2 + y^2$ and $c > 0$:

**Step 1:** Compute $\log \lambda = \log c - 2\log(1-r^2)$.

**Step 2:** Compute the Laplacian. Let $f = \log(1-r^2)$. Then:

$$
\partial_x f = \frac{-2x}{1-r^2}.

$$
Applying the quotient rule to $\partial_x f = -2x \cdot (1-r^2)^{-1}$:

$$
\partial_x^2 f = \frac{-2(1-r^2) - (-2x)(-2x)}{(1-r^2)^2} = \frac{-2 + 2r^2 - 4x^2}{(1-r^2)^2}.

$$
Similarly for $y$. Adding:

$$
\Delta f = \frac{(-2 + 2r^2 - 4x^2) + (-2 + 2r^2 - 4y^2)}{(1-r^2)^2} = \frac{-4 + 4r^2 - 4r^2}{(1-r^2)^2} = \frac{-4}{(1-r^2)^2}.

$$
**Step 3:** Therefore $\Delta(\log \lambda) = -2\Delta f = \frac{8}{(1-r^2)^2}$.

**Step 4:** The curvature is:

$$
K = -\frac{1}{2\lambda} \cdot \frac{8}{(1-r^2)^2} = -\frac{(1-r^2)^2}{2c} \cdot \frac{8}{(1-r^2)^2} = -\frac{4}{c}.

$$
**Step 5:** For $K = -1$, we require $c = 4$. $\square$

*Significance.* The choice $K = -1$ is canonical: it sets the "radius of curvature" to unity, making the hyperbolic distance formula $d(0,z) = 2\text{arctanh}|z|$ dimensionless. The factor of 4 in the metric is a *derived consequence* of the curvature normalization, not an assumption.

:::

:::{prf:proposition} A.6.0f (Area of a Minimal Distinguishable Cell)
:label: prop-a-area-minimal-distinguishable-cell

On a 2-dimensional latent manifold with Fisher-compatible geometry (curvature $K = -1$), the Riemannian area of a cell containing exactly one nat of distinguishable information is:

$$
A_{\text{1 nat}} = 4\ell_L^2,

$$
where $\ell_L$ is the Levin Length.

*Proof (Non-Circular Derivation).* The argument proceeds in three independent steps:

**Step 1: Definition of $\ell_L$ (Implementation-Determined).** The Levin Length $\ell_L$ is the fundamental coordinate resolution of the computational manifold, determined by implementation constraints (discretization precision, floating-point resolution, etc.). This is analogous to how the Planck length $\ell_P = \sqrt{\hbar G/c^3}$ is determined by physical constants, not by the form of the area law.

**Step 2: Geodesic-to-Coordinate Relationship (From Fisher Metric).** On the Poincare disk with $K = -1$, the line element at the origin is:

$$
ds = 2 \, dx \quad \text{(from } ds^2 = 4(dx^2 + dy^2) \text{ at } z = 0\text{)}.

$$
A coordinate displacement $\ell_L$ corresponds to geodesic (Riemannian) distance $2\ell_L$.

**Step 3: Information-Geodesic Correspondence (From Chentsov).** By Theorem {prf:ref}`thm-a-chentsov-uniqueness`, the Fisher metric is the unique metric where KL divergence corresponds to squared geodesic distance (locally). Specifically, for nearby distributions $p$ and $q$:

$$
D_{\text{KL}}(p \| q) \approx \frac{1}{2} d_{\text{geo}}(p, q)^2.

$$
Thus, 1 nat of KL divergence corresponds to geodesic distance $\sqrt{2}$.

**Combining:** A coordinate cell of side $\ell_L$ has:
- Coordinate area: $\ell_L^2$
- Riemannian area: $\ell_L^2 \cdot \sqrt{\det G(0)} = \ell_L^2 \cdot 4 = 4\ell_L^2$
- Information capacity: proportional to Riemannian area $/$ (geodesic length per nat)$^2$

The factor of 4 emerges from the conformal factor $\sqrt{\det G(0)} = 4$, which was derived in Lemma {prf:ref}`lem-a-curvature-normalization-factor-4` from the curvature normalization $K = -1$, not from any assumption about information capacity. $\square$

*Remark (Non-Circularity).* In this derivation:
- $\ell_L$ is defined by implementation constraints (Step 1)
- The factor of 4 is derived from $K = -1$ (Lemma A.6.0e)
- The information-geometry correspondence is from Chentsov's theorem (Step 3)

No step assumes the form of the Area Law. Compare with Strominger-Vafa: they derive $S = A/(4\ell_P^2)$ by counting D-brane configurations, where $\ell_P$ is determined by string parameters and the 1/4 emerges from the counting.

:::

:::{prf:theorem} A.6.0g (Boundary Channel Capacity)
:label: thm-a-boundary-channel-capacity

The channel capacity of a 2-dimensional boundary $\partial\mathcal{Z}$ with Riemannian area $A$ is:

$$
C_\partial = \frac{A}{4\ell_L^2} \text{ nats}.

$$
*Proof.*
1. Tile the boundary with minimal distinguishable cells (Proposition {prf:ref}`prop-a-area-minimal-distinguishable-cell`)
2. By Proposition {prf:ref}`prop-a-area-minimal-distinguishable-cell`, each cell with coordinate side $\ell_L$ has Riemannian area $4\ell_L^2$
3. Number of cells: $N_{\text{cells}} = A / (4\ell_L^2)$
4. Each cell encodes 1 nat of information: this follows from the Fisher metric correspondence (Proposition {prf:ref}`prop-a-area-minimal-distinguishable-cell`, Step 3), not by definition
5. By additivity of channel capacity for parallel independent channels:

$$
C_\partial = N_{\text{cells}} \times 1 \text{ nat} = \frac{A}{4\ell_L^2}. \quad \square

$$
*Remark (Dimension Generalization).* For a $(D-1)$-dimensional boundary with $D > 2$, the formula generalizes to:

$$
C_\partial = \nu_D \cdot \frac{A}{\ell_L^{D-1}},

$$
where $\nu_D$ is the Holographic Coefficient (Definition {prf:ref}`def-holographic-coefficient`). The 2D case with $\nu_2 = 1/4$ is the primary focus of this specification.

*Remark (Shannon's Channel Coding Theorem).* This invokes the classical result that the capacity of $N$ parallel channels is additive. The generalization to continuous channels with Fisher geometry follows from rate-distortion theory {cite}`cover2006elements`.

:::

:::{prf:theorem} A.6.0h (Microstate Count and the Area Law)
:label: thm-a-microstate-count-area-law

The number of boundary-distinguishable microstates in the bulk is:

$$
\Omega = \exp\left(\frac{A}{4\ell_L^2}\right),

$$
and the maximum information about bulk configuration, as measured by an external observer, is:

$$
I_{\max} = \ln \Omega = \frac{A}{4\ell_L^2}.

$$
*Proof.*
1. By the **Data Processing Inequality**, information about the bulk cannot exceed the channel capacity of the boundary: $I_{\text{bulk} \to \text{observer}} \leq C_\partial$.

2. The maximum number of distinguishable messages through a channel of capacity $C$ nats is $e^C$ (Shannon's channel coding theorem {cite}`cover2006elements`).

3. Therefore, the number of boundary-distinguishable microstates is bounded:

$$
\Omega \leq e^{C_\partial} = \exp\left(\frac{A}{4\ell_L^2}\right).

$$
4. **Achievability:** The bound is saturated when the boundary is tiled with minimal distinguishable cells, each encoding 1 nat via orthogonal degrees of freedom. This follows from the channel capacity achievability in Shannon's theorem.

5. The maximum information is:

$$
I_{\max} = \ln \Omega = \frac{A}{4\ell_L^2}. \quad \square

$$
*Remark (Non-Circularity).* This derivation uses only:
- Chentsov's uniqueness theorem (statistics)
- Fisher geodesic distance calculation (geometry)
- Curvature normalization $K = -1$ (convention, not assumption)
- Shannon's channel capacity (information theory)

It does **not** invoke the Metric Law (Theorem {prf:ref}`thm-capacity-constrained-metric-law`). The Metric Law is a *dynamical* statement about how the metric responds to information density; the Area Law derived here is a *kinematic* bound on distinguishable states.

:::

:::{prf:lemma} A.6.1 (Bulk-to-Boundary Conversion)
:label: lem-a-bulk-to-boundary-conversion

For a stationary information distribution satisfying the Metric Law, the bulk information integral can be expressed as a boundary integral:

$$
I_{\text{bulk}} = \int_{\mathcal{Z}} \rho_I \, d\mu_G = \frac{1}{\kappa} \oint_{\partial\mathcal{Z}} \text{Tr}(K) \, dA_G,

$$
where $K_{ij}$ is the extrinsic curvature (second fundamental form) of the boundary and $\kappa$ is the coupling constant from the Metric Law.

*Proof.* At stationarity, the information density satisfies the continuity equation $\nabla_i j^i = 0$ where $j^i$ is the information flux. The Metric Law (Theorem {prf:ref}`thm-capacity-constrained-metric-law`) implies:

$$
R - 2\Lambda = \kappa \, T,

$$
where $T = G^{ij}T_{ij}$ is the trace of the stress tensor. For uniform saturation, $T = n \cdot \sigma_{\max}$.

Integrating the Einstein tensor identity over $\mathcal{Z}$ and applying Lemma {prf:ref}`lem-a-divergence-to-boundary-conversion`:

$$
\int_{\mathcal{Z}} R \, d\mu_G = 2 \oint_{\partial\mathcal{Z}} \text{Tr}(K) \, dA_G.

$$
Combining with $R = \kappa T + 2\Lambda$ and noting that the $\Lambda$ term contributes a volume integral that cancels under the capacity constraint, we obtain the stated identity. $\square$

:::

:::{prf:proposition} A.6.2 (Saturation Metric Solution)
:label: prop-a-saturation-metric-solution

Under uniform saturation $T_{ij} = \sigma_{\max} G_{ij}$, the Metric Law reduces to:

$$
\frac{n-2}{r^2}\left(1 - \frac{1}{A(r)}\right) + \frac{n-2}{r} \cdot \frac{A'(r)}{A(r)^2} = \kappa \sigma_{\max} + \Lambda.

$$
The solution is:

$$
A(r) = \left( 1 - \frac{2\mu(r)}{(n-2)r^{n-2}} - \frac{\Lambda_{\text{eff}} r^2}{n(n-1)} \right)^{-1},

$$
where $\mu(r) = \frac{\kappa}{n-2} \int_0^r \sigma_{\max} r'^{n-1} dr'$ is the information mass function and $\Lambda_{\text{eff}} = \Lambda + \kappa \sigma_{\max}$.

*Proof.* This follows from the standard Birkhoff-like analysis for spherically symmetric solutions of Einstein-type equations. The key steps are:

1. Compute the Ricci tensor components for the ansatz
2. Substitute into the Metric Law
3. The radial component of the field equations gives a first-order ODE for $A(r)$
4. Integrate with boundary condition $A(0) = 1$ (regularity at origin)

The integration constant is determined by requiring $\lim_{r \to 0} A(r) = 1$. $\square$

:::

:::{prf:definition} A.6.3 (Information Horizon)
:label: def-a-information-horizon

The **information horizon** $r_h$ is the smallest positive root of:

$$
1 - \frac{2\mu(r_h)}{(n-2)r_h^{n-2}} - \frac{\Lambda_{\text{eff}} r_h^2}{n(n-1)} = 0.

$$
At this radius, $A(r_h) \to \infty$ and $G^{rr}(r_h) \to 0$.

:::

:::{prf:remark} A.6.4a (Connection to Microstate Counting)
:label: rem-a-connection-microstate-counting

The Fisher normalization used here is **not an independent input**. It is the same geometric fact established by:
- Lemma {prf:ref}`lem-a-geodesic-distance-probability-simplex`: Geodesic distance $\pi/2$ for 1 bit
- Lemma {prf:ref}`lem-a-curvature-normalization-factor-4`: Factor of 4 from curvature $K = -1$
- Proposition {prf:ref}`prop-a-area-minimal-distinguishable-cell`: Area $4\ell_L^2$ per nat

The field-theoretic derivation shows that the Metric Law (Theorem {prf:ref}`thm-capacity-constrained-metric-law`) *reproduces* the bound derived from counting—providing a consistency check between the kinematic and dynamic approaches.

:::

:::{prf:lemma} A.6.4 (Geodesic Distance on the Probability Simplex)
:label: lem-a-geodesic-distance-simplex

On the 1-simplex $\Delta^1 = \{(p, 1-p) : p \in [0,1]\}$ with the Fisher Information Metric, the geodesic distance between the uniform distribution $(1/2, 1/2)$ and a vertex $(1, 0)$ is:

$$
d_{\text{Fisher}}\left(\frac{1}{2}, 1\right) = \frac{\pi}{2}.

$$
*Proof.* See Lemma {prf:ref}`lem-a-geodesic-distance-probability-simplex` for the full derivation. $\square$

:::

:::{prf:proposition} A.6.5 (Area of a Minimal Information Cell)
:label: prop-a-area-minimal-cell

On a 2-dimensional Fisher manifold, the area of a cell corresponding to 1 nat of distinguishable information is:

$$
A_{\text{cell}} = 4 \ell_L^2.

$$
*Proof.* See Proposition {prf:ref}`prop-a-area-minimal-distinguishable-cell` for the full derivation. The key steps are:
1. Poincare metric at origin: $G(0) = 4I$ (from curvature normalization $K = -1$)
2. Coordinate cell area $\ell_L^2$ maps to Riemannian area $4\ell_L^2$ $\square$

:::

:::{prf:theorem} A.6.6 (Complete Derivation of the Area Law)
:label: thm-a-complete-derivation-area-law

Combining the above results:

1. **From Lemma {prf:ref}`lem-a-bulk-to-boundary-conversion`:** $I_{\text{bulk}} = \frac{1}{\kappa} \oint_{\partial\mathcal{Z}} \text{Tr}(K) \, dA_G$

2. **At saturation:** The extrinsic curvature $\text{Tr}(K) = (n-1)/r_h$ for an $(n-1)$-sphere boundary.

3. **Boundary area:** $\text{Area}(\partial\mathcal{Z}) = \Omega_{n-1} r_h^{n-1}$ where $\Omega_{n-1}$ is the volume of the unit $(n-1)$-sphere.

4. **Fisher normalization:** $\kappa = 8\pi \ell_L^2$ (fixed by consistency with Proposition {prf:ref}`prop-a-area-minimal-cell`).

Substituting:

$$
I_{\max} = \frac{1}{8\pi \ell_L^{n-1}} \cdot \frac{n-1}{r_h} \cdot \Omega_{n-1} r_h^{n-1} = \frac{(n-1)\Omega_{n-1}}{8\pi} \cdot \frac{\text{Area}(\partial\mathcal{Z})}{\ell_L^{n-1}}.

$$
Identifying the **Holographic Coefficient** $\nu_n := (n-1)\Omega_{n-1}/(8\pi)$ (Definition {prf:ref}`def-holographic-coefficient`), we obtain the **general result**:

$$
\boxed{I_{\max} = \nu_n \cdot \frac{\text{Area}(\partial\mathcal{Z})}{\ell_L^{n-1}}}.

$$

**Special case ($n = 2$, Poincare disk):** With $\Omega_1 = 2\pi$ (circumference of unit circle), we get $\nu_2 = 1/4$. The familiar Bekenstein-Hawking form:

$$
I_{\max} = \frac{\text{Area}(\partial\mathcal{Z})}{4\ell_L^2}

$$
uses $\ell_L^2$ (rather than $\ell_L^{n-1} = \ell_L$) because the Poincare disk metric normalization $G(0) = 4I$ maps coordinate cells to Riemannian areas.

This completes the derivation. The Holographic Coefficient $\nu_n$ arises from the combination of:
- The $1/8\pi$ from the coupling constant $\kappa$
- The geometric factor $(n-1)\Omega_{n-1}$ from sphere surface area
- The Fisher metric normalization

$\square$

:::

:::{prf:corollary} A.6.7 (Dimension-Dependent Coefficient)
:label: cor-a-dimension-dependent-coefficient

For a $D$-dimensional latent manifold with $(D-1)$-sphere boundary, the Causal Information Bound takes the form:

$$
I_{\max}(D) = \nu_D \cdot \frac{\text{Area}(\partial\mathcal{Z})}{\ell_L^{D-1}},

$$
where the Holographic Coefficient $\nu_D$ (Definition {prf:ref}`def-holographic-coefficient`) is:

$$
\nu_D = \frac{(D-1)\Omega_{D-1}}{8\pi} = \frac{(D-1)\pi^{(D-2)/2}}{4\,\Gamma(D/2)},

$$
with $\Omega_{D-1} = 2\pi^{D/2}/\Gamma(D/2)$ the surface area of the unit $(D-1)$-sphere.

**Explicit values:**

| $D$ | $\Omega_{D-1}$ | $\nu_D$    | Numerical |
|-----|----------------|------------|-----------|
| 2   | $2\pi$         | $1/4$      | 0.250     |
| 3   | $4\pi$         | $1$        | 1.000     |
| 4   | $2\pi^2$       | $3\pi/4$   | 2.356     |
| 5   | $8\pi^2/3$     | $4\pi/3$   | 4.189     |
| 6   | $\pi^3$        | $5\pi^2/8$ | 6.169     |

*Remark.* The coefficient $\nu_D$ is **not monotonic** in $D$: it increases from $D=2$ to a peak at $D \approx 9$ ($\nu_9 \approx 9.4$), then decreases toward zero. For typical latent dimensions ($3 \le D \le 20$), $\nu_D > \nu_2 = 1/4$, so using the 2D coefficient **underestimates** capacity. For very high dimensions ($D \gtrsim 22$), $\nu_D < 1/4$, so the 2D coefficient **overestimates** capacity—this is the dangerous case (false safety). Implementers should always use the dimension-appropriate coefficient.

:::

:::{prf:remark} A.6.8 (Gauss-Bonnet Generalization)
:label: rem-a-gauss-bonnet-generalization

The derivation in Lemma {prf:ref}`lem-a-bulk-to-boundary-conversion` uses the **Einstein tensor divergence identity** (also called the contracted Bianchi identity):

$$
\int_{\mathcal{Z}} R \, d\mu_G = 2 \oint_{\partial\mathcal{Z}} \text{Tr}(K) \, dA_G,

$$
which is valid in **arbitrary dimension**. This is more general than the classical 2D Gauss-Bonnet theorem (which relates $\int K \, dA$ to the Euler characteristic $\chi$).

The Chern-Gauss-Bonnet theorem for even-dimensional manifolds computes topological invariants (Euler characteristic) via curvature integrals, but is not required here—we compute information capacity, not topology. The divergence theorem approach generalizes to any $D \geq 2$ without modification.

:::

:::{prf:remark} A.6.9 (Non-Circularity of the Derivation)
:label: rem-a-non-circularity

A potential criticism of this section is circularity: *"The Metric Law encodes the holographic principle, so deriving the Area Law from the Metric Law is question-begging."*

This criticism is addressed by the **two-derivation structure** of this appendix:

1. **Microstate Counting (A.6.0):** Derives $I_{\max} = A/(4\ell_L^2)$ from:
   - Chentsov's uniqueness theorem (Theorem {prf:ref}`thm-a-chentsov-uniqueness`)
   - Fisher geodesic distance calculation (Lemma {prf:ref}`lem-a-geodesic-distance-probability-simplex`)
   - Curvature normalization $K = -1$ (Lemma {prf:ref}`lem-a-curvature-normalization-factor-4`)
   - Shannon's channel capacity (Theorem {prf:ref}`thm-a-boundary-channel-capacity`)

   **This derivation does not invoke the Metric Law.**

2. **Field-Theoretic (A.6.1–A.6.5):** Derives the same bound from the Metric Law dynamics.

The fact that both derivations yield the **same coefficient** $(1/4)$ is a non-trivial consistency check:
- The kinematic bound (from counting) constrains what is *possible*
- The dynamic equations (from the Metric Law) describe what *happens*
- Their agreement shows the Metric Law is *compatible* with holographic constraints—not that it *assumes* them

**Analogy to physics:** In black hole thermodynamics, Hawking derived $S = A/(4\ell_P^2)$ thermodynamically (1975), and Strominger-Vafa derived it microscopically (1996). Neither derivation is circular; their agreement is a profound consistency check on string theory.

Similarly, the microstate counting here is analogous to Strominger-Vafa, while the field-theoretic derivation is analogous to Hawking. The framework admits both perspectives.

:::

## 10_appendices/04_faq.md

:::{prf:axiom} The Bridge Principle
:label: ax-the-bridge-principle

An agent commits to a **response function** $\sigma: \mathcal{O} \to \mathcal{A}$ mapping observations to actions, not a fixed policy $\pi: \mathcal{S} \to \mathcal{A}$ over states. The response function:

1. Is computable given bounded observations
2. Does not require access to opponent internal states or policies
3. Defines the agent's strategic interface at the boundary $\partial\mathcal{X}$

*Consequence:* Strategic interactions reduce to boundary conditions on the response function, eliminating the need for opponent omniscience.
:::

## 10_appendices/05_proofs.md

:::{prf:proof}

Consider the discrete variation $\Delta \mathcal{S} = \mathcal{S}[N_c + 1] - \mathcal{S}[N_c]$. By the definition of the Ontological Action ({ref}`Section 30.3 <sec-the-fission-criterion>`):

$$
\mathcal{S}_{\text{onto}} = -\mathcal{S}_{\text{task}} + \mu_{\text{size}} \cdot N_c,

$$
where $\mathcal{S}_{\text{task}} = \mathbb{E}[\langle V \rangle]$ is the expected task value.

Expanding $\mathcal{S}_{\text{task}}$ via a first-order Taylor approximation in the space of representations:

$$
\mathcal{S}_{\text{task}}[N_c + 1] \approx \mathcal{S}_{\text{task}}[N_c] + \frac{\partial \langle V \rangle}{\partial N_c}.

$$
The marginal utility of a new chart is $\frac{\partial \langle V \rangle}{\partial N_c} = \Delta V_{\text{proj}}$. The complexity cost is $\mu_{\text{size}}$. Therefore:

$$
\Delta \mathcal{S} = -\Delta V_{\text{proj}} + \mu_{\text{size}}.

$$
The transition $N_c \to N_c + 1$ is the global minimizer iff $\Delta \mathcal{S} < 0$, which yields:

$$
\Delta V_{\text{proj}} > \mu_{\text{size}} = \mathcal{C}_{\text{complexity}}.

$$
The condition $\Xi > \Xi_{\text{crit}}$ ensures that the second variation of the texture-entropy functional $\delta^2 H(z_{\text{tex}})$ is negative-definite at the vacuum. This precludes the absorption of the signal into the existing noise floor: if $\Xi \le \Xi_{\text{crit}}$, the texture residual $z_{\text{tex}}$ is truly unpredictable noise, and adding a chart provides no informational benefit. $\square$

:::

:::{prf:proof}

Let $f(\Xi) = \Xi - \Xi_{\text{crit}}$ be the control parameter. By $SO(n)$ symmetry, the Ontological Action can only depend on even powers of $r$ near the origin. We expand in a power series:

$$
\mathcal{S}(r) = \mathcal{S}_0 - \frac{1}{2}f(\Xi)r^2 + \frac{1}{4}\beta r^4 + O(r^6),

$$
where $\beta > 0$ for stability (the quartic term must be positive for bounded energy).

The stationarity condition $\frac{\partial \mathcal{S}}{\partial r} = 0$ yields:

$$
-f(\Xi)r + \beta r^3 = 0 \implies r(f(\Xi) - \beta r^2) = 0.

$$
This has solutions:
1. $r = 0$ (trivial, no new chart)
2. $r^2 = f(\Xi)/\beta$ (symmetry-broken state)

**Analysis of stability:**
- For $f(\Xi) < 0$ (i.e., $\Xi < \Xi_{\text{crit}}$): The Hessian at $r=0$ is $\frac{\partial^2 \mathcal{S}}{\partial r^2}|_{r=0} = -f(\Xi) > 0$. Thus $r=0$ is a stable minimum.
- For $f(\Xi) > 0$ (i.e., $\Xi > \Xi_{\text{crit}}$): The Hessian at $r=0$ becomes $-f(\Xi) < 0$ (unstable). New minima appear at $r^* = \sqrt{f(\Xi)/\beta}$.

Since $r \ge 0$ is a radial coordinate, this constitutes a **supercritical pitchfork bifurcation** where the symmetry-broken state $r^* > 0$ becomes the unique stable equilibrium for $\Xi > \Xi_{\text{crit}}$.

The bifurcation diagram: for $\Xi < \Xi_{\text{crit}}$, the system has a single stable fixed point at $r=0$; for $\Xi > \Xi_{\text{crit}}$, the origin becomes unstable and two symmetric branches (in the full space, a sphere of radius $r^*$) emerge. $\square$

:::

:::{prf:proof}

The time derivative of the Shannon entropy is:

$$
\frac{d}{ds} H(\rho_s) = \frac{d}{ds}\left( -\int_{\mathcal{Z}} \rho \ln \rho \, d\mu_G \right) = -\int_{\mathcal{Z}} (1 + \ln \rho) \partial_s \rho \, d\mu_G.

$$
Substituting the WFR continuity equation:

$$
\frac{d}{ds} H = -\int_{\mathcal{Z}} (1 + \ln \rho)(\rho r - \nabla \cdot (\rho v)) \, d\mu_G.

$$
**Transport term:** Integrating by parts (assuming $\rho v \cdot n|_{\partial\mathcal{Z}} = 0$):

$$
-\int (1 + \ln \rho)(-\nabla \cdot (\rho v)) \, d\mu_G = \int \nabla(1 + \ln \rho) \cdot (\rho v) \, d\mu_G = \int \rho \langle \nabla \ln \rho, v \rangle_G \, d\mu_G.

$$
**Reaction term:**

$$
-\int (1 + \ln \rho) \rho r \, d\mu_G = -\int \rho r \, d\mu_G - \int \rho r \ln \rho \, d\mu_G.

$$
The first integral is the total mass change $\frac{d}{ds}\int \rho \, d\mu_G$. For normalized probabilities, this vanishes if we work in the cone representation. The second integral is bounded by the reaction energy.

**Applying Cauchy-Schwarz:** For the transport term on $(T\mathcal{Z}, G)$:

$$
\left| \int_{\mathcal{Z}} \rho \langle \nabla \ln \rho, v \rangle_G \, d\mu_G \right| \le \left( \int \rho \|\nabla \ln \rho\|_G^2 \, d\mu_G \right)^{1/2} \left( \int \rho \|v\|_G^2 \, d\mu_G \right)^{1/2}.

$$
The first factor is the **Fisher Information** $\mathcal{I}(\rho)$. By the de Bruijn identity for diffusion processes:

$$
\frac{d}{ds} H(\rho_s) = -\frac{1}{2T_c} \mathcal{I}(\rho_s)

$$
under optimal transport scaling $v = -T_c G^{-1}\nabla \ln \rho$.

Combining: $|\dot{H}| \le \frac{1}{T_c}\sqrt{\mathcal{I}(\rho) \cdot \int \rho \|v\|_G^2}$. With $\sigma_{\text{met}} = 1/T_c$, we obtain $\dot{\mathcal{M}} \ge T_c |\dot{H}|$.

The reaction term follows by an identical argument using the $L^2(\rho)$ inner product:

$$
\left| \int \rho r \ln \rho \, d\mu_G \right| \le \|\sqrt{\rho} r\|_{L^2} \|\sqrt{\rho} \ln \rho\|_{L^2}.

$$
Adding both contributions yields the stated bound. $\square$

:::

:::{prf:proof}

Define the deliberation functional:

$$
\mathcal{F}(S) = -\int_{\mathcal{Z}} V(z) \rho(S, z) \, d\mu_G + \int_0^S \dot{\mathcal{M}}(u) \, du.

$$
The necessary condition for an extremum is $\mathcal{F}'(S) = 0$. By the Leibniz integral rule:

$$
\mathcal{F}'(S) = -\int_{\mathcal{Z}} V(z) \partial_s \rho(S, z) \, d\mu_G + \dot{\mathcal{M}}(S).

$$
Using the result that $\partial_s \rho$ is governed by the WFR operator $\mathcal{L}_{\text{WFR}}$:

$$
\mathcal{F}'(S) = -\int_{\mathcal{Z}} V \mathcal{L}_{\text{WFR}}\rho \, d\mu_G + \dot{\mathcal{M}}(S).

$$
By the adjoint property of the WFR operator (the formal $L^2(\rho)$ adjoint):

$$
\int V \mathcal{L}_{\text{WFR}}\rho \, d\mu_G = \int \rho \mathcal{L}_{\text{WFR}}^* V \, d\mu_G,

$$
where $\mathcal{L}_{\text{WFR}}^* V = -\langle \nabla V, v \rangle_G + Vr$ (transport-adjoint plus reaction).

For gradient flows in the covariant case, $v = -G^{-1}\nabla_A V$ with $\nabla_A V := \nabla V - A$:

$$
\mathcal{L}_{\text{WFR}}^* V = G^{-1}(\nabla V, \nabla_A V) + Vr.

$$
Thus:

$$
\mathcal{F}'(S) = -\int \rho \left( G^{-1}(\nabla V, \nabla_A V) + Vr \right) d\mu_G + \dot{\mathcal{M}}(S).

$$
In the conservative case ($A=0$), $G^{-1}(\nabla V, \nabla_A V) = \|\nabla V\|_G^2$, the power dissipated by the value-gradient flow. The stationarity condition $\mathcal{F}'(S^*) = 0$ gives:

$$
\frac{d}{ds} \langle V \rangle_{\rho_s}\bigg|_{s=S^*} = \dot{\mathcal{M}}(S^*).

$$
This states that the optimal stopping time $S^*$ is reached when the power dissipated by the value-gradient flow exactly matches the metabolic cost rate. $\square$

:::

:::{prf:proof}

The Euler-Lagrange equations for the functional are:

$$
\frac{d}{dt} \frac{\partial L}{\partial \dot{z}^k} - \frac{\partial L}{\partial z^k} = 0.

$$
**Computing the momentum:**

$$
\frac{\partial L}{\partial \dot{z}^k} = \frac{\partial}{\partial \dot{z}^k}\left( \frac{1}{2}G_{ij}(z)\dot{z}^i \dot{z}^j \right) = G_{kj}\dot{z}^j = p_k.

$$
**Time derivative of momentum:**

$$
\frac{d}{dt}(G_{kj}\dot{z}^j) = G_{kj}\ddot{z}^j + \frac{\partial G_{kj}}{\partial z^m}\dot{z}^m \dot{z}^j.

$$
**Potential gradient:**

$$
\frac{\partial L}{\partial z^k} = \frac{1}{2}\frac{\partial G_{ij}}{\partial z^k}\dot{z}^i\dot{z}^j - \partial_k V - \beta_{\text{exp}}\partial_k \Psi_{\text{causal}}.

$$
**Euler-Lagrange equation:**

$$
G_{kj}\ddot{z}^j + \frac{\partial G_{kj}}{\partial z^m}\dot{z}^m \dot{z}^j - \frac{1}{2}\frac{\partial G_{ij}}{\partial z^k}\dot{z}^i\dot{z}^j = -\partial_k V - \beta_{\text{exp}}\partial_k \Psi_{\text{causal}}.

$$
Recognizing the Christoffel symbols of the first kind $[ij, k] = \frac{1}{2}(\partial_i G_{jk} + \partial_j G_{ik} - \partial_k G_{ij})$:

$$
G_{kj}\ddot{z}^j + [ij, k]\dot{z}^i\dot{z}^j = -\partial_k V - \beta_{\text{exp}}\partial_k \Psi_{\text{causal}}.

$$
Contracting with $G^{mk}$ and using $\Gamma^m_{ij} = G^{mk}[ij, k]$:

$$
\ddot{z}^m + \Gamma^m_{ij}\dot{z}^i\dot{z}^j = -G^{mk}\partial_k V - \beta_{\text{exp}} G^{mk}\partial_k \Psi_{\text{causal}}.

$$
This is the geodesic equation with forcing terms. In the **overdamped limit** ({ref}`Section 22.3 <sec-the-unified-effective-potential>`), inertia is negligible and the acceleration term vanishes, leaving:

$$
\dot{z}^m = -G^{mk}\partial_k V + \beta_{\text{exp}} G^{mk}\partial_k \Psi_{\text{causal}} = F^m_{\text{total}}.

$$
The drift field $F_{\text{total}}$ is the first-order velocity approximation, proving the additive force of curiosity. $\square$

:::

:::{prf:proof}

We compare the mutual information under the observational measure $P$ and the interventional measure $P_{do(K^{\text{act}})}$.

**Observational case:** By the Causal Enclosure condition ({ref}`Section 2.8 <sec-conditional-independence-and-sufficiency>`):

$$
I(K_{t+1}; Z_{\text{micro}, t} | K_t, K^{\text{act}}_t) = 0 \quad \text{under } P.

$$
This states that the macro-state $K_{t+1}$ is conditionally independent of the micro-texture $Z_{\text{micro}, t}$ given the current macro-state and action.

**Interventional case:** The $do(K^{\text{act}}_t)$ operator performs a graph surgery that removes all incoming edges to $K^{\text{act}}_t$ while preserving all other mechanisms. By Pearl's Causal Markov Condition {cite}`pearl2009causality`:

$$
P(K_{t+1} | K_t, K^{\text{act}}_t, Z_{\text{micro}, t}) \text{ remains invariant under } do(K^{\text{act}}_t).

$$
This is because the mechanism $P(K_{t+1} | \text{parents}(K_{t+1}))$ is a structural equation that does not depend on how $K^{\text{act}}_t$ was generated.

**Combining the conditions:**
If the observational distribution satisfies $I = 0$, then:

$$
P(K_{t+1} | K_t, K^{\text{act}}_t) = P(K_{t+1} | K_t, K^{\text{act}}_t, Z_{\text{micro}, t}) \quad \forall Z_{\text{micro}, t}.

$$
Since the mechanism is invariant under intervention:

$$
P(K_{t+1} | K_t, do(K^{\text{act}}_t)) = P(K_{t+1} | K_t, K^{\text{act}}_t) = P(K_{t+1} | K_t, K^{\text{act}}_t, Z_{\text{micro}, t}).

$$
Therefore, $I(K_{t+1}; Z_{\text{micro}, t} | K_t, do(K^{\text{act}}_t)) = 0$.

**Contrapositive (violation):** If $I > 0$ under $do(K^{\text{act}}_t)$, there exists a back-door path through $Z_{\text{micro}, t}$:

$$
K_t \leftarrow Z_{\text{micro}, t} \to K_{t+1}.

$$
This path was confounded in observational data (the correlation between $Z_{\text{micro}}$ and $K_{t+1}$ was screened by the policy generating $K^{\text{act}}_t$). The intervention breaks this screening, exposing the hidden variable. The remedy is **Ontological Expansion** ({ref}`Section 30 <sec-ontological-expansion-topological-fission-and-the-semantic-vacuum>`): promote the relevant component of $Z_{\text{micro}}$ to a new macro-variable in $K$. $\square$

:::

:::{prf:definition} E.7.1 (The Strategic Metric)
:label: def-e7-strategic-metric

Let $G^{(i)}$ be the capacity-constrained metric on $\mathcal{Z}^{(i)}$ (Theorem {prf:ref}`thm-capacity-constrained-metric-law`). The **Strategic Metric** $\mathbf{g}$ on $\mathcal{M}$ is the block-diagonal sum perturbed by the Game Tensor $\mathcal{G}$ (Definition {prf:ref}`def-the-game-tensor`):

$$
\mathbf{g}(\mathbf{z}) := \bigoplus_{i=1}^N G^{(i)}(z^{(i)}) + \alpha \sum_{i \neq j} \mathcal{G}_{ij}(\mathbf{z}),

$$
where the pullback of the cross-Hessian interaction acts on tangent vectors in the obvious way.

*Assumption 1 (Ellipticity):* We assume $\alpha > 0$ is sufficiently small such that $\mathbf{g}$ remains positive-definite and defines a valid Riemannian structure on $\mathcal{M}$. This is guaranteed when $\|\alpha \mathcal{G}\|_{\text{op}} < \lambda_{\min}(\bigoplus G^{(i)})$.

:::

:::{prf:definition} E.7.2 (The Strategic Hamiltonian)
:label: def-e7-strategic-hamiltonian

The self-adjoint **Strategic Hamiltonian** operator $\hat{H}_\sigma: H^2(\mathcal{M}) \to L^2(\mathcal{M}, d\mu_{\mathbf{g}})$ acts on the joint wave-function $\Psi$:

$$
\hat{H}_\sigma := -\frac{\sigma^2}{2} \Delta_{\mathbf{g}} + \mathcal{V}(\mathbf{z}),

$$
where:
- $\Delta_{\mathbf{g}}$ is the Laplace-Beltrami operator associated with the strategic metric $\mathbf{g}$
- $\mathcal{V}(\mathbf{z}) := \sum_{i=1}^N \Phi^{(i)}_{\text{eff}}(z^{(i)}) + \sum_{i < j} \Phi_{ij}(z^{(i)}, z^{(j)})$ is the joint potential
- $\sigma > 0$ is the cognitive action scale (Definition {prf:ref}`def-cognitive-action-scale`)

*Assumption 2 (Regularity):* $\mathcal{V} \in C^2(\mathcal{M})$ and is bounded below.

:::

:::{prf:definition} E.7.3 (The Forbidden Region and Nash Basins)
:label: def-e7-forbidden-region

Let $E_0 := \inf \text{spec}(\hat{H}_\sigma)$ be the ground state energy. The **Classically Forbidden Region** (Barrier) is:

$$
\mathcal{K} := \{ \mathbf{z} \in \mathcal{M} : \mathcal{V}(\mathbf{z}) > E_0 \}.

$$
Let $\Omega_A, \Omega_B \subset \mathcal{M} \setminus \mathcal{K}$ be disjoint open sets (Nash basins) where $\mathcal{V}(\mathbf{z}) \leq E_0$.

*Geometric interpretation:* $\Omega_A$ and $\Omega_B$ are "potential wells" corresponding to distinct Nash equilibria (Theorem {prf:ref}`thm-nash-ground-state`). The barrier $\mathcal{K}$ separates these wells.

:::

:::{prf:theorem} E.7.1 (Strict Positivity of the Ground State)
:label: thm-e7-ground-state-positivity

Let $\Psi_0$ be the ground state eigenfunction of $\hat{H}_\sigma$ (the eigenfunction with eigenvalue $E_0$). Then:

$$
|\Psi_0(\mathbf{z})| > 0 \quad \forall \mathbf{z} \in \mathcal{M}.

$$
*Consequence:* For any open set $\Omega_B \subset \mathcal{M}$, the probability measure satisfies:

$$
\mu(\Omega_B) = \int_{\Omega_B} |\Psi_0(\mathbf{z})|^2 \, d\mu_{\mathbf{g}}(\mathbf{z}) > 0.

$$
Therefore, if an agent is localized in $\Omega_A$, there is strictly positive probability of finding it in $\Omega_B$.

:::

:::{prf:proof}

**Step 1 (Elliptic Regularity).** Since $\mathbf{g}$ is smooth and positive-definite (Assumption 1), and $\mathcal{V}$ is smooth (Assumption 2), the operator $\hat{H}_\sigma$ is uniformly elliptic. By standard elliptic regularity theory {cite}`gilbarg1977elliptic`, any $L^2$ eigenfunction $\Psi$ satisfying $\hat{H}_\sigma \Psi = E \Psi$ is in $C^\infty(\mathcal{M})$.

**Step 2 (Heat Kernel Positivity).** Consider the heat semigroup $e^{-t\hat{H}_\sigma}$ for $t > 0$. By the **Harnack Inequality** for parabolic equations on manifolds {cite}`li1986parabolic`, the heat kernel $K_t(\mathbf{x}, \mathbf{y}) > 0$ for all $\mathbf{x}, \mathbf{y} \in \mathcal{M}$ and $t > 0$, provided $\mathcal{M}$ is connected.

This implies: for any non-negative, non-zero $f \in L^2(\mathcal{M})$:

$$
(e^{-t\hat{H}_\sigma} f)(\mathbf{x}) = \int_{\mathcal{M}} K_t(\mathbf{x}, \mathbf{y}) f(\mathbf{y}) \, d\mu_{\mathbf{g}}(\mathbf{y}) > 0 \quad \forall \mathbf{x} \in \mathcal{M}.

$$
The heat kernel maps non-negative functions to **strictly positive** functions.

**Step 3 (Perron-Frobenius / Krein-Rutman).** The operator $e^{-t\hat{H}_\sigma}$ is a positivity-improving compact operator on $L^2(\mathcal{M})$. By the **Krein-Rutman Theorem** (the infinite-dimensional generalization of Perron-Frobenius), the spectral radius is a simple eigenvalue with a strictly positive eigenfunction.

Since $e^{-t\hat{H}_\sigma}$ has spectral radius $e^{-tE_0}$ with eigenfunction $\Psi_0$, and this eigenvalue is simple, we conclude:
- $\Psi_0$ can be chosen to be real and non-negative
- By positivity-improving property, $\Psi_0(\mathbf{z}) > 0$ for all $\mathbf{z} \in \mathcal{M}$

**Step 4 (Conclusion).** For any open $\Omega_B \subset \mathcal{M}$:

$$
\mu(\Omega_B) = \int_{\Omega_B} |\Psi_0|^2 \, d\mu_{\mathbf{g}} \geq c \cdot \text{Vol}_{\mathbf{g}}(\Omega_B) > 0,

$$
where $c = \min_{\overline{\Omega}_B} |\Psi_0|^2 > 0$ by continuity and strict positivity. $\square$

:::

:::{prf:definition} E.7.4 (The Agmon Metric)
:label: def-e7-agmon-metric

Inside the barrier $\mathcal{K}$, we define the **Agmon Metric** $\rho_E$, a degenerate conformal rescaling of $\mathbf{g}$:

$$
(\rho_E)_{ij}(\mathbf{z}) := \max\left(0, \mathcal{V}(\mathbf{z}) - E_0\right) \cdot \mathbf{g}_{ij}(\mathbf{z}).

$$
The **Agmon distance** between points $\mathbf{x}, \mathbf{y} \in \mathcal{M}$ is:

$$
d_{\text{Ag}}(\mathbf{x}, \mathbf{y}) := \inf_{\gamma: \mathbf{x} \to \mathbf{y}} \int_0^1 \sqrt{\max(0, \mathcal{V}(\gamma(t)) - E_0)} \cdot \|\dot{\gamma}(t)\|_{\mathbf{g}} \, dt,

$$
where the infimum is over all piecewise smooth paths $\gamma$ from $\mathbf{x}$ to $\mathbf{y}$.

*Properties:*
1. $d_{\text{Ag}}(\mathbf{x}, \mathbf{y}) = 0$ if there exists a path entirely within $\mathcal{M} \setminus \mathcal{K}$ (the "classical" region)
2. $d_{\text{Ag}}(\mathbf{x}, \mathbf{y}) > 0$ if all paths must traverse $\mathcal{K}$ (tunneling required)
3. The Agmon distance is a pseudo-metric (satisfies triangle inequality)

:::

:::{prf:theorem} E.7.2 (Agmon Exponential Decay Bound)
:label: thm-e7-agmon-decay-bound

Let $\Psi_0$ be the ground state of $\hat{H}_\sigma$ with eigenvalue $E_0$. For any $\epsilon > 0$, there exists a constant $C_\epsilon > 0$ (depending on $\mathcal{M}$, $\mathcal{V}$, and $\epsilon$, but not on $\sigma$) such that:

$$
|\Psi_0(\mathbf{z})| \leq C_\epsilon \exp\left( - \frac{1 - \epsilon}{\sigma} d_{\text{Ag}}(\mathbf{z}, \Omega_A) \right) \quad \forall \mathbf{z} \in \mathcal{M},

$$
where $d_{\text{Ag}}(\mathbf{z}, \Omega_A) := \inf_{\mathbf{y} \in \Omega_A} d_{\text{Ag}}(\mathbf{z}, \mathbf{y})$.

*Interpretation:* The wave-function amplitude decays exponentially with rate $1/\sigma$ times the Agmon distance from the classical region. Deeper into the barrier (larger $d_{\text{Ag}}$), the amplitude is exponentially smaller.

:::

:::{prf:proof}

We follow the standard Agmon method {cite}`agmon1982lectures,simon1983semiclassical`.

**Step 1 (Twisted Function).** Define the twisted function:

$$
\phi(\mathbf{z}) := e^{f(\mathbf{z})/\sigma} \Psi_0(\mathbf{z}),

$$
where $f: \mathcal{M} \to \mathbb{R}$ is a Lipschitz weight function to be chosen.

**Step 2 (Agmon Identity).** From the eigenvalue equation $(\hat{H}_\sigma - E_0)\Psi_0 = 0$, we derive:

$$
-\frac{\sigma^2}{2}\Delta_{\mathbf{g}}\phi + (\mathcal{V} - E_0)\phi = \frac{1}{2}\|\nabla_{\mathbf{g}} f\|_{\mathbf{g}}^2 \phi + \sigma \langle \nabla_{\mathbf{g}} f, \nabla_{\mathbf{g}} \phi \rangle_{\mathbf{g}}.

$$
**Step 3 (Energy Estimate).** Multiply by $\bar{\phi}$ and integrate. Using integration by parts:

$$
\frac{\sigma^2}{2} \|\nabla_{\mathbf{g}}\phi\|_{L^2}^2 + \int_{\mathcal{M}} \left(\mathcal{V} - E_0 - \frac{1}{2}\|\nabla_{\mathbf{g}} f\|_{\mathbf{g}}^2\right) |\phi|^2 \, d\mu_{\mathbf{g}} \leq 0.

$$
**Step 4 (Optimal Weight).** Choose $f(\mathbf{z}) = (1-\epsilon) d_{\text{Ag}}(\mathbf{z}, \Omega_A)$. By construction of the Agmon metric:

$$
\|\nabla_{\mathbf{g}} f\|_{\mathbf{g}}^2 \leq (1-\epsilon)^2 (\mathcal{V} - E_0)_+ \quad \text{a.e.}

$$
**Step 5 (Pointwise Bound).** Substituting and using Sobolev embedding on the compact manifold $\mathcal{M}$:

$$
\sup_{\mathbf{z} \in \mathcal{M}} |\phi(\mathbf{z})|^2 \leq C_\epsilon' \|\phi\|_{H^1}^2 \leq C_\epsilon'' \|\Psi_0\|_{L^2}^2 = C_\epsilon''.

$$
Unwinding the twist gives:

$$
|\Psi_0(\mathbf{z})| = e^{-f(\mathbf{z})/\sigma} |\phi(\mathbf{z})| \leq C_\epsilon \exp\left(-\frac{(1-\epsilon)}{\sigma} d_{\text{Ag}}(\mathbf{z}, \Omega_A)\right). \quad \square

$$
:::

:::{prf:corollary} E.7.3 (Adversarial Suppression of Tunneling)
:label: cor-e7-adversarial-suppression

Assume Agent $j$ is adversarial to Agent $i$, so the Game Tensor $\mathcal{G}_{ij}$ is positive semi-definite (Theorem {prf:ref}`thm-adversarial-mass-inflation`). Let:
- $\mathbf{g}_0 := \bigoplus_{i=1}^N G^{(i)}$ be the **non-interacting** (decoupled) metric
- $\mathbf{g}_{\text{adv}} := \mathbf{g}_0 + \alpha \sum_{i \neq j} \mathcal{G}_{ij}$ be the **adversarial** (Game-inflated) metric

Then the Agmon distances satisfy:

$$
d_{\text{Ag}}^{\text{adv}}(\Omega_A, \Omega_B) \geq d_{\text{Ag}}^{0}(\Omega_A, \Omega_B),

$$
and consequently the tunneling probability is exponentially suppressed:

$$
P_{\text{tunnel}}^{\text{adv}} \lesssim \exp\left(-\frac{d_{\text{Ag}}^{\text{adv}}}{\sigma}\right) \leq \exp\left(-\frac{d_{\text{Ag}}^{0}}{\sigma}\right) \lesssim P_{\text{tunnel}}^{0}.

$$
:::

:::{prf:proof}

**Step 1 (Metric Comparison).** Since $\mathcal{G}_{ij} \succeq 0$ (positive semi-definite), for any tangent vector $\mathbf{v} \in T_{\mathbf{z}}\mathcal{M}$:

$$
\mathbf{v}^T \mathbf{g}_{\text{adv}} \mathbf{v} = \mathbf{v}^T \mathbf{g}_0 \mathbf{v} + \alpha \sum_{i \neq j} \mathbf{v}^T \mathcal{G}_{ij} \mathbf{v} \geq \mathbf{v}^T \mathbf{g}_0 \mathbf{v}.

$$
Thus $\mathbf{g}_{\text{adv}} \geq \mathbf{g}_0$ in the sense of quadratic forms.

**Step 2 (Path Length Inequality).** For any path $\gamma: [0,1] \to \mathcal{M}$, the Agmon length satisfies:

$$
L_{\text{Ag}}^{\text{adv}}(\gamma) = \int_0^1 \sqrt{(\mathcal{V} - E_0)_+} \cdot \|\dot{\gamma}\|_{\mathbf{g}_{\text{adv}}} \, dt \geq \int_0^1 \sqrt{(\mathcal{V} - E_0)_+} \cdot \|\dot{\gamma}\|_{\mathbf{g}_0} \, dt = L_{\text{Ag}}^{0}(\gamma).

$$
**Step 3 (Distance Inequality).** Taking the infimum over all paths:

$$
d_{\text{Ag}}^{\text{adv}}(\mathbf{x}, \mathbf{y}) = \inf_{\gamma} L_{\text{Ag}}^{\text{adv}}(\gamma) \geq \inf_{\gamma} L_{\text{Ag}}^{0}(\gamma) = d_{\text{Ag}}^{0}(\mathbf{x}, \mathbf{y}).

$$
**Step 4 (Tunneling Suppression).** By Theorem E.7.2, the ground state amplitude at distance $d$ from $\Omega_A$ scales as $\exp(-d/\sigma)$. Since $d_{\text{Ag}}^{\text{adv}} \geq d_{\text{Ag}}^{0}$:

$$
|\Psi_0^{\text{adv}}(\mathbf{z})|^2 \lesssim \exp\left(-\frac{2 d_{\text{Ag}}^{\text{adv}}}{\sigma}\right) \leq \exp\left(-\frac{2 d_{\text{Ag}}^{0}}{\sigma}\right) \lesssim |\Psi_0^{0}(\mathbf{z})|^2.

$$
The tunneling probability $P_{\text{tunnel}} \approx \int_{\Omega_B} |\Psi_0|^2$ inherits this exponential suppression. $\square$

:::

:::{prf:theorem} E.7.4 (Feynman-Kac Representation)
:label: thm-e7-feynman-kac

Let $(\mathbf{X}_s)_{s \geq 0}$ be Brownian motion on the Riemannian manifold $(\mathcal{M}, \mathbf{g})$, starting at $\mathbf{X}_0 = \mathbf{z}$. Then the ground state $\Psi_0$ admits the representation:

$$
\Psi_0(\mathbf{z}) = \lim_{t \to \infty} e^{E_0 t} \cdot \mathbb{E}_{\mathbf{z}}\left[ \exp\left( -\frac{1}{\sigma^2} \int_0^t \mathcal{V}(\mathbf{X}_s) \, ds \right) \phi(\mathbf{X}_t) \right],

$$
where $\phi \in L^2(\mathcal{M})$ is any function with $\langle \Psi_0, \phi \rangle \neq 0$.

*Remark:* This is rigorous—not a heuristic "path integral." The expectation is over Brownian paths on the manifold.

:::

:::{prf:proof}

**Step 1 (Semigroup Representation).** By the Feynman-Kac-Itô formula for Schrödinger operators on manifolds {cite}`simon1979functional`:

$$
(e^{-t\hat{H}_\sigma/\sigma^2} \phi)(\mathbf{z}) = \mathbb{E}_{\mathbf{z}}\left[ \exp\left( -\frac{1}{\sigma^2} \int_0^t \mathcal{V}(\mathbf{X}_s) \, ds \right) \phi(\mathbf{X}_t) \right].

$$
**Step 2 (Spectral Projection).** As $t \to \infty$, the semigroup projects onto the ground state:

$$
e^{-t\hat{H}_\sigma/\sigma^2} \phi \to e^{-tE_0/\sigma^2} \langle \Psi_0, \phi \rangle \Psi_0.

$$
**Step 3 (Normalization).** Multiplying by $e^{E_0 t/\sigma^2}$ and taking the limit gives the stated formula. $\square$

:::

:::{prf:corollary} E.7.5 (Tunneling via Large Deviations)
:label: cor-e7-large-deviations

The tunneling probability is controlled by the **Large Deviation Principle** for Brownian paths on $(\mathcal{M}, \mathbf{g})$.

The rate function (Freidlin-Wentzell action) is:

$$
I[\gamma] = \frac{1}{2} \int_0^T \|\dot{\gamma}(t)\|_{\mathbf{g}}^2 \, dt,

$$
and paths that cross the barrier $\mathcal{K}$ while minimizing $I[\gamma] + \int_0^T (\mathcal{V}(\gamma) - E_0) \, dt$ are precisely the **instantons** that govern tunneling.

*Interpretation:* Tunneling is realized by rare stochastic fluctuations of the WFR diffusion process that penetrate the high-cost region. The probability of such fluctuations scales as $\exp(-S_{\text{inst}}/\sigma)$ where $S_{\text{inst}}$ is the instanton action—which equals the Agmon distance.

:::

:::{prf:proof}

**Step 1: Express Entropy in terms of $\beta_{\text{ent}}$.**
The entropy of the policy is:

$$
H(\pi) = -\sum_a \pi(a) \ln \pi(a).

$$
Substituting $\ln \pi(a) = \beta_{\text{ent}} Q(a) - \ln Z$:

$$
H(\pi) = -\sum_a \pi(a) [\beta_{\text{ent}} Q(a) - \ln Z] = \ln Z - \beta_{\text{ent}} \mathbb{E}_\pi[Q].

$$
**Step 2: Derivative of Entropy w.r.t. $\beta_{\text{ent}}$.**
Differentiating with respect to $\beta_{\text{ent}}$:

$$
\frac{\partial H}{\partial \beta_{\text{ent}}} = \frac{\partial \ln Z}{\partial \beta_{\text{ent}}} - \mathbb{E}_\pi[Q] - \beta_{\text{ent}} \frac{\partial \mathbb{E}_\pi[Q]}{\partial \beta_{\text{ent}}}.

$$
Using the identity $\frac{\partial \ln Z}{\partial \beta_{\text{ent}}} = \mathbb{E}_\pi[Q]$:

$$
\frac{\partial H}{\partial \beta_{\text{ent}}} = -\beta_{\text{ent}} \frac{\partial \mathbb{E}_\pi[Q]}{\partial \beta_{\text{ent}}} = -\beta_{\text{ent}} \mathrm{Var}_\pi(Q),

$$
where we used $\frac{\partial \mathbb{E}[Q]}{\partial \beta_{\text{ent}}} = \mathrm{Var}(Q)$ (standard fluctuation-response relation).

**Step 3: Relate $\mathrm{Var}(Q)$ to Varentropy.**
Recall $\mathcal{I}(a) = -\ln \pi(a) = -\beta_{\text{ent}} Q(a) + \ln Z$. The variance of the surprisal is:

$$
V_H(\pi) = \mathrm{Var}(\mathcal{I}) = \mathrm{Var}(-\beta_{\text{ent}} Q + \ln Z) = \beta_{\text{ent}}^2 \mathrm{Var}(Q).

$$
**Step 4: Change of variables to $T_c$.**
We have $V_H = \beta_{\text{ent}}^2 \mathrm{Var}(Q)$ and $\frac{\partial H}{\partial \beta_{\text{ent}}} = -\beta_{\text{ent}} \mathrm{Var}(Q)$.
Therefore $V_H = -\beta_{\text{ent}} \frac{\partial H}{\partial \beta_{\text{ent}}}$.

Using the chain rule $\frac{\partial}{\partial T_c} = -\frac{1}{T_c^2} \frac{\partial}{\partial \beta_{\text{ent}}}$:

$$
\frac{\partial H}{\partial T_c} = -\frac{1}{T_c^2} \frac{\partial H}{\partial \beta_{\text{ent}}} = \frac{1}{T_c^2} \cdot \beta_{\text{ent}} \mathrm{Var}(Q) = \frac{V_H}{T_c^2 \cdot \beta_{\text{ent}}} = \frac{V_H}{T_c}.

$$
**Final Result:** Rearranging yields:

$$
V_H(z) = T_c \frac{\partial H(\pi)}{\partial T_c} = \beta_{\text{ent}}^2 \mathrm{Var}(Q) = C_v.

$$
This proves that Varentropy equals the heat capacity and measures the sensitivity of the entropy to temperature fluctuations. $\square$

:::

:::{prf:proof}

**Step 1: Variance of Surprisal Form.**

$$
V_H = \mathbb{E}[\mathcal{I}^2] - (\mathbb{E}[\mathcal{I}])^2.

$$
Since $\mathcal{I} = -\beta Q + \ln Z$, we have $V_H = \beta^2 \mathrm{Var}(Q)$.

**Step 2: Two-Point Statistics.**
Consider two actions $a_1, a_2$ with probabilities $p, 1-p$. The variance of a Bernoulli variable taking values $Q_1, Q_2$ is:

$$
\mathrm{Var}(Q) = p(1-p)(Q_1 - Q_2)^2.

$$
Thus:

$$
V_H = \beta^2 p(1-p) (\Delta Q)^2 = p(1-p) \left( \frac{\Delta Q}{T_c} \right)^2.

$$
For equally weighted modes ($p = 1/2$), this simplifies to:

$$
V_H = \frac{1}{4} \left( \frac{\Delta Q}{T_c} \right)^2.

$$
**Step 3: Interpretation of $\Delta Q$.**
$\Delta Q$ is the value gap between the two modes.

- **Perfect Symmetry (The Ridge):** If $Q_1 = Q_2$ exactly, then $\Delta Q = 0 \implies V_H = 0$.
- **Structural Instability:** When the agent is *slightly* off-center or when sampling includes the *tails*, the effective $\Delta Q > 0$.

**Step 4: Distinguishing Structure from Noise.**
For a distribution with structure (peaks and valleys), $\mathrm{Var}(Q) > 0$. For a flat distribution (noise), $\mathrm{Var}(Q) = 0$.

Specifically, on a ridge, the agent samples $a_{\text{left}}$ and $a_{\text{right}}$ (high $Q$) but also transitively samples the separating region (lower $Q$) during exploration. The variance of $Q$ along the trajectory corresponds to $V_H$:

$$
V_H \propto (\Delta Q_{\text{peak-valley}})^2.

$$
This proves that $V_H$ detects the topological feature (the valley) that distinguishes a fork from a flat plane. $\square$

:::

:::{prf:proof}

**Step 1: Thermodynamic Speed.**
The rate of change of the policy distribution with respect to temperature is measured by the Fisher Information metric $g_{TT}$ on the statistical manifold parameterized by $T_c$:

$$
g_{TT} = \mathbb{E}\left[ \left( \frac{\partial \ln \pi}{\partial T_c} \right)^2 \right].

$$
**Step 2: Relate Fisher Metric to Varentropy.**
Recall $\ln \pi = \frac{Q}{T_c} - \ln Z$. Then:

$$
\frac{\partial \ln \pi}{\partial T_c} = -\frac{Q}{T_c^2} + \frac{\mathbb{E}[Q]}{T_c^2} = -\frac{1}{T_c^2}(Q - \mathbb{E}[Q]).

$$
Substituting into the Fisher definition:

$$
g_{TT} = \frac{1}{T_c^4} \mathbb{E}\left[ (Q - \mathbb{E}[Q])^2 \right] = \frac{\mathrm{Var}(Q)}{T_c^4}.

$$
Using $V_H = \frac{\mathrm{Var}(Q)}{T_c^2}$ (from Proof E.8):

$$
g_{TT} = \frac{V_H}{T_c^2}.

$$
**Step 3: Thermodynamic Length.**
The "distance" traversed in probability space for a small temperature change $dT_c$ is $ds^2 = g_{TT} dT_c^2$:

$$
ds = \sqrt{g_{TT}} |dT_c| = \frac{\sqrt{V_H}}{T_c} |dT_c|.

$$
**Step 4: Adiabatic Condition.**
For the system to relax to equilibrium (stay in the basin of attraction), the speed of change in distribution space must be bounded:

$$
\left| \frac{ds}{dt} \right| \leq C \cdot \tau_{\text{relax}}^{-1}.

$$
Substituting $ds/dt$:

$$
\frac{\sqrt{V_H}}{T_c} \left| \frac{dT_c}{dt} \right| \leq C.

$$
Solving for the cooling rate:

$$
\left| \frac{dT_c}{dt} \right| \leq C \frac{T_c}{\sqrt{V_H}}.

$$
**Conclusion:** When Varentropy $V_H$ is large (phase transition/critical point), the permissible cooling rate goes to zero. The Governor must apply the "Varentropy Brake" to prevent quenching the system into a suboptimal metastable state. $\square$

:::

:::{prf:proof}

**Step 1: Definition of EIG.**

$$
\text{EIG}(z, a) = I(\theta; z' | z, a) = H(z' | z, a) - \mathbb{E}_{\theta} [ H(z' | z, a, \theta) ].

$$
This is the **Total Predictive Entropy** minus the **Expected Aleatoric Entropy**.

**Step 2: Decomposition of Uncertainty.**
For the "noisy TV" case (outcomes are stochastic noise independent of $\theta$):

$$
H(z' | z, a, \theta) \approx H(z' | z, a) \implies \text{EIG} \approx 0.

$$
**Step 3: Varentropy as Structure Detector.**
The varentropy $V_H(z' | z, a)$ measures the variance of log-probabilities.

- **Uniform noise:** $V_H^{\text{noise}} \to 0$ (all outcomes equally likely).
- **Structured uncertainty:** $V_H^{\text{structured}} > 0$ (some outcomes much more likely).

**Step 4: Connection to Multimodality.**
If the model is uncertain about structure ($\theta$), the predictive distribution $p(z')$ is a mixture of distinct hypotheses $p(z'|\theta_1), p(z'|\theta_2)$. As established in Proof E.9, a mixture of distinct modes has high Varentropy compared to a broad unimodal distribution (noise).

**Step 5: Operational Equivalence.**
Thus, maximizing EIG is functionally equivalent to maximizing the **Varentropy of the expected outcome**, provided the aleatoric noise floor is constant:

$$
\nabla \Psi_{\text{causal}} \propto \nabla \mathrm{Var}_{z' \sim p(z'|z,a)} [ -\ln p(z'|z,a) ].

$$
**Conclusion:** The agent should seek states where the World Model's prediction has high Varentropy (conflicting hypotheses), as these offer the maximum potential for falsification (reduction of parameter variance). $\square$

:::

:::{prf:proof}

**Step 1: Bellman Recursion with Temporal Structure.**

The standard Bellman equation assumes instantaneous value propagation:

$$
V(z, t) = r(z)\Delta t + \gamma \mathbb{E}_{z' \sim P(\cdot|z,a)}[V(z', t + \Delta t)]

$$

where $\gamma = e^{-\kappa_t \Delta t}$ is the temporal discount factor with $\kappa_t = -\ln\gamma / \Delta t$ having units $[\kappa_t] = 1/[\text{time}]$.

**Step 2: Second-Order Taylor Expansion.**

Expand $V(z', t + \Delta t)$ to second order in both space and time. Let $z' = z + \delta z$ where $\delta z$ is the state transition:

$$
V(z', t + \Delta t) = V(z, t) + \partial_t V \cdot \Delta t + \frac{1}{2}\partial_t^2 V \cdot (\Delta t)^2 + \nabla V \cdot \delta z + \frac{1}{2}(\delta z)^\top \nabla^2 V (\delta z) + \partial_t \nabla V \cdot \Delta t \cdot \delta z + O(3)

$$

**Step 3: Expectations Under Diffusion.**

For a diffusion process with drift $b(z)$ and diffusion tensor $\Sigma = 2T_c G^{-1}$:

$$
\mathbb{E}[\delta z] = b \Delta t, \quad \mathbb{E}[(\delta z)(\delta z)^\top] = \Sigma \Delta t

$$

Taking expectations:

$$
\mathbb{E}[V(z', t + \Delta t)] = V + \partial_t V \Delta t + \frac{1}{2}\partial_t^2 V (\Delta t)^2 + \nabla_A V \cdot b \Delta t + T_c \text{Tr}(G^{-1}\nabla^2 V) \Delta t + O((\Delta t)^{3/2})

$$

The trace term is the Laplace-Beltrami operator: $\text{Tr}(G^{-1}\nabla^2 V) = \Delta_G V$.

**Step 4: Substitution into Bellman.**

Substituting into the Bellman equation:

$$
V = r \Delta t + (1 - \kappa_t \Delta t)\left(V + \partial_t V \Delta t + \frac{1}{2}\partial_t^2 V (\Delta t)^2 + \nabla_A V \cdot b \Delta t + T_c \Delta_G V \Delta t\right)

$$

**Step 5: Instantaneous Limit (Elliptic Case).**

Dividing by $\Delta t$ and taking $\Delta t \to 0$ while keeping only $O(\Delta t)$ terms:

$$
0 = r - \kappa_t V + \partial_t V + \nabla_A V \cdot b + T_c \Delta_G V

$$

For stationary states ($\partial_t V = 0$) with zero drift ($b = 0$):

$$
-T_c \Delta_G V + \kappa_t V = r

$$

This is the **Helmholtz equation** (Theorem {prf:ref}`thm-the-hjb-helmholtz-correspondence`). Note that $\kappa_t$ here has temporal units.

**Step 6: Finite Propagation Speed (Hyperbolic Case).**

The key insight is that the above derivation assumes **instantaneous** information propagation: the value at time $t + \Delta t$ depends on rewards and transitions known at time $t$. When information propagates at finite speed $c_{\text{info}}$, two modifications occur:

**(i) Retardation of Spatial Coupling:** Rewards at spatial distance $\ell$ are received with delay $\tau = \ell / c_{\text{info}}$. This is handled by the retarded potential $\Phi_{ij}^{\text{ret}}$.

**(ii) Wave Propagation of Value:** The value function itself propagates as a wave, not instantaneously. The characteristic timescale for value changes over spatial scale $\ell$ is $\tau_\ell = \ell / c_{\text{info}}$.

To derive the wave equation, we must retain the **second-order time derivative**. Define the **spatial screening mass**:

$$
\kappa := \kappa_t / c_{\text{info}} = -\ln\gamma / (c_{\text{info}} \Delta t)

$$

with units $[\kappa] = 1/[\text{length}]$.

**Step 7: Wave Equation Derivation.**

Consider the characteristic scales:
- Temporal: $\Delta t \sim \ell / c_{\text{info}}$ (time for information to traverse distance $\ell$)
- Spatial: $\ell$ (characteristic length scale)

The ratio $(\Delta t)^2 / \ell^2 \sim 1/c_{\text{info}}^2$ is no longer negligible. Retaining the $(\Delta t)^2$ term in the expansion:

$$
\frac{1}{c_{\text{info}}^2}\partial_t^2 V + \partial_t V / c_{\text{info}} = r - \kappa^2 V + \Delta_G V + \text{(coupling terms)}

$$

In the **stationary wave regime** where $\partial_t V \ll c_{\text{info}} \partial_t^2 V / \kappa$, the first-order time derivative is negligible compared to the second-order term, yielding:

$$
\frac{1}{c_{\text{info}}^2}\partial_t^2 V - \Delta_G V + \kappa^2 V = \rho_r + \sum_j \rho^{\text{ret}}_{ij}

$$

This is the **Klein-Gordon equation** with mass $\kappa$.

**Step 8: Physical Interpretation.**

The transition from Helmholtz (elliptic) to Klein-Gordon (hyperbolic) parallels the transition in electromagnetism:

| Regime | Equation | Value Propagation |
|:-------|:---------|:------------------|
| $c_{\text{info}} \to \infty$ | Helmholtz: $(-\Delta_G + \kappa^2)V = \rho_r$ | Instantaneous |
| $c_{\text{info}} < \infty$ | Klein-Gordon: $(\frac{1}{c^2}\partial_t^2 - \Delta_G + \kappa^2)V = \rho_r$ | Wave at speed $c$ |

The screening mass $\kappa$ determines the characteristic decay length $\ell_\gamma = 1/\kappa$: the distance over which value influence diminishes by factor $e$.

**Step 9: Dimensional Verification.**

- $[\partial_t^2 V] = [\text{nat}]/[\text{time}]^2$
- $[c_{\text{info}}^{-2}\partial_t^2 V] = [\text{nat}]/[\text{length}]^2$
- $[\Delta_G V] = [\text{nat}]/[\text{length}]^2$
- $[\kappa^2 V] = [\text{nat}]/[\text{length}]^2$ (since $[\kappa] = 1/[\text{length}]$)
- $[\rho_r] = [\text{nat}]/[\text{length}]^2$

All terms have consistent units. $\square$

:::

:::{prf:proof}

**Step 1: Polar Decomposition.**

Let the belief wave-function have polar form:

$$
\psi = R \, e^{i\phi}, \quad R := \sqrt{\rho}, \quad \phi := V/\sigma

$$

where $\rho = |\psi|^2$ is the belief density and $V$ is the value function. The parameter $\sigma > 0$ is the cognitive action scale (Definition {prf:ref}`def-cognitive-action-scale`).

**Step 2: Compute Time Derivative.**

$$
\partial_s \psi = \partial_s(R e^{i\phi}) = (\partial_s R) e^{i\phi} + R \cdot i(\partial_s \phi) e^{i\phi} = \left(\frac{\partial_s R}{R} + i \partial_s \phi\right)\psi

$$

Since $R = \sqrt{\rho}$, we have $\partial_s R / R = \partial_s \rho / (2\rho)$. Thus:

$$
\partial_s \psi = \left(\frac{\partial_s \rho}{2\rho} + \frac{i}{\sigma}\partial_s V\right)\psi

$$

**Step 3: Compute the covariant Laplacian of $\psi$.**

Let $D_i := \nabla_i - \frac{i}{\sigma}A_i$ and $\nabla_A V := \nabla V - A$. Then:

$$
D_i \psi = \left(\nabla_i R + \frac{i}{\sigma} R (\nabla_A V)_i\right) e^{i\phi}

$$
and

$$
D^i D_i \psi = \left[\Delta_G R + \frac{2i}{\sigma} G^{-1}(\nabla R, \nabla_A V) + \frac{i}{\sigma} R \nabla_G \cdot (G^{-1}\nabla_A V) - \frac{1}{\sigma^2} R \|\nabla_A V\|_G^2\right] e^{i\phi}.

$$

Dividing by $\psi = R e^{i\phi}$:

$$
\frac{D^i D_i \psi}{\psi} = \frac{\Delta_G R}{R} + \frac{2i}{\sigma} \frac{G^{-1}(\nabla R, \nabla_A V)}{R} + \frac{i}{\sigma}\nabla_G \cdot (G^{-1}\nabla_A V) - \frac{1}{\sigma^2}\|\nabla_A V\|_G^2.

$$

**Step 4: Define the Bohm Potential.**

Using $R = \sqrt{\rho}$, we have:

$$
\frac{\Delta_G R}{R} = \frac{\Delta_G \sqrt{\rho}}{\sqrt{\rho}}

$$

Define the **Bohm quantum potential**:

$$
Q_B := -\frac{\sigma^2}{2} \frac{\Delta_G \sqrt{\rho}}{\sqrt{\rho}} = -\frac{\sigma^2}{2} \frac{\Delta_G R}{R}

$$

**Step 5: Inference Schrödinger Equation.**

The Inference Schrödinger Equation is:

$$
i\sigma \partial_s \psi = \hat{H}_{\text{inf}} \psi, \quad \hat{H}_{\text{inf}} = -\frac{\sigma^2}{2} D^i D_i + \Phi_{\text{eff}} + Q_B - \frac{i\sigma}{2}r

$$

Substituting our expressions:

$$
i\sigma \partial_s \psi = i\sigma\left(\frac{\partial_s \rho}{2\rho} + \frac{i}{\sigma}\partial_s V\right)\psi = \left(\frac{i\sigma \partial_s \rho}{2\rho} - \partial_s V\right)\psi

$$

$$
\hat{H}_{\text{inf}}\psi = \left[-\frac{\sigma^2}{2}\frac{D^i D_i \psi}{\psi} + \Phi_{\text{eff}} + Q_B - \frac{i\sigma}{2}r\right]\psi

$$

**Step 6: Separate Real and Imaginary Parts.**

Expanding $-\frac{\sigma^2}{2}\frac{D^i D_i \psi}{\psi}$:

$$
-\frac{\sigma^2}{2}\frac{D^i D_i \psi}{\psi} = -\frac{\sigma^2}{2}\frac{\Delta_G R}{R} - i\sigma \frac{G^{-1}(\nabla R, \nabla_A V)}{R} - \frac{i\sigma}{2}\nabla_G \cdot (G^{-1}\nabla_A V) + \frac{1}{2}\|\nabla_A V\|_G^2

$$

$$
= Q_B - i\sigma \frac{G^{-1}(\nabla \sqrt{\rho}, \nabla_A V)}{\sqrt{\rho}} - \frac{i\sigma}{2}\nabla_G \cdot (G^{-1}\nabla_A V) + \frac{1}{2}\|\nabla_A V\|_G^2

$$

The Schrödinger equation $i\sigma \partial_s \psi = \hat{H}_{\text{inf}}\psi$ becomes:

$$
\frac{i\sigma \partial_s \rho}{2\rho} - \partial_s V = Q_B + Q_B - i\sigma \frac{G^{-1}(\nabla \sqrt{\rho}, \nabla_A V)}{\sqrt{\rho}} - \frac{i\sigma}{2}\nabla_G \cdot (G^{-1}\nabla_A V) + \frac{1}{2}\|\nabla_A V\|_G^2 + \Phi_{\text{eff}} - \frac{i\sigma}{2}r

$$

Wait—there's a double $Q_B$. Let me redo this more carefully. The $Q_B$ in $\hat{H}_{\text{inf}}$ cancels with the $-\frac{\sigma^2}{2}\frac{\Delta_G R}{R}$ from the kinetic term:

$$
\hat{H}_{\text{inf}}\psi = \left[- i\sigma \frac{G^{-1}(\nabla \sqrt{\rho}, \nabla_A V)}{\sqrt{\rho}} - \frac{i\sigma}{2}\nabla_G \cdot (G^{-1}\nabla_A V) + \frac{1}{2}\|\nabla_A V\|_G^2 + \Phi_{\text{eff}} - \frac{i\sigma}{2}r\right]\psi

$$

**Real part (coefficient of $\psi$):**

$$
-\partial_s V = \frac{1}{2}\|\nabla_A V\|_G^2 + \Phi_{\text{eff}}

$$

This is the **Hamilton-Jacobi-Bellman equation**:

$$
\partial_s V + \frac{1}{2}\|\nabla_A V\|_G^2 + \Phi_{\text{eff}} = 0 \quad \checkmark

$$

**Imaginary part (coefficient of $i\psi$):**

$$
\frac{\sigma \partial_s \rho}{2\rho} = -\sigma \frac{G^{-1}(\nabla \sqrt{\rho}, \nabla_A V)}{\sqrt{\rho}} - \frac{\sigma}{2}\nabla_G \cdot (G^{-1}\nabla_A V) - \frac{\sigma}{2}r

$$

Simplifying: $\frac{G^{-1}(\nabla \sqrt{\rho}, \nabla_A V)}{\sqrt{\rho}} = \frac{G^{-1}(\nabla \rho, \nabla_A V)}{2\rho}$. Thus:

$$
\frac{\partial_s \rho}{2\rho} = -\frac{G^{-1}(\nabla \rho, \nabla_A V)}{2\rho} - \frac{1}{2}\nabla_G \cdot (G^{-1}\nabla_A V) - \frac{1}{2}r

$$

Multiplying by $2\rho$:

$$
\partial_s \rho = -G^{-1}(\nabla \rho, \nabla_A V) - \rho \nabla_G \cdot (G^{-1}\nabla_A V) - \rho r

$$

Using the velocity field $\mathbf{v} = -G^{-1}\nabla_A V$ (conservative case: $A=0$) and the identity $\nabla_G \cdot (\rho \mathbf{v}) = G^{-1}(\nabla \rho, \mathbf{v}) + \rho \nabla_G \cdot \mathbf{v}$:

$$
\partial_s \rho = G^{-1}(\nabla \rho, \mathbf{v}) + \rho \nabla_G \cdot \mathbf{v} - \rho r = \nabla_G \cdot (\rho \mathbf{v}) - \rho r

$$

This is the **WFR continuity equation** (unbalanced):

$$
\partial_s \rho + \nabla_G \cdot (\rho \mathbf{v}) = \rho r \quad \checkmark

$$

**Conclusion:** The Madelung transform $\psi = \sqrt{\rho} e^{iV/\sigma}$ is an exact equivalence between:
- The Inference Schrödinger Equation for $\psi$
- The coupled WFR-HJB system for $(\rho, V)$

The Bohm potential $Q_B$ emerges naturally from the kinetic energy operator acting on the amplitude $R = \sqrt{\rho}$. $\square$

:::

:::{prf:proof}

**Step 1: Define the Information Content.**

Let $\mathcal{I}_t$ denote the total information available to the system at time $t$:

$$
\mathcal{I}_t := \sigma(z^{(N)}_\tau, a^{(N)}_\tau, r^{(N)}_\tau : \tau \leq t)

$$
where $\sigma(\cdot)$ denotes the sigma-algebra generated by the random variables.

**Step 2: Causal Factorization.**

Under the finite information speed $c_{\text{info}}$, define the **causal past** of agent $i$ at time $t$:

$$
\mathcal{C}^{(i)}_t := \{(j, \tau) : \tau \leq t - d_{\mathcal{E}}(i,j)/c_{\text{info}}\}

$$
This is the set of (agent, time) pairs that can causally influence agent $i$ at time $t$.

The transition kernel factorizes:

$$
P(z^{(i)}_{t+\Delta t} | \mathcal{I}_t) = P(z^{(i)}_{t+\Delta t} | z^{(i)}_t, \{z^{(j)}_\tau : (j,\tau) \in \mathcal{C}^{(i)}_t\})

$$

**Step 3: Memory Screen as Sufficient Statistic.**

The Memory Screen $\Xi^{(i)}_{<t}$ is defined (Definition {prf:ref}`def-memory-screen`) as a compression of the causal past:

$$
\Xi^{(i)}_{<t} := f^{(i)}(\{z^{(j)}_\tau : (j,\tau) \in \mathcal{C}^{(i)}_t\})

$$
where $f^{(i)}$ is a sufficient statistic for predicting $z^{(i)}_{t+\Delta t}$.

**Claim:** $\Xi^{(i)}_{<t}$ satisfies the **sufficiency condition**:

$$
P(z^{(i)}_{t+\Delta t} | z^{(i)}_t, \Xi^{(i)}_{<t}, \Xi^{(i)}_{<t'}) = P(z^{(i)}_{t+\Delta t} | z^{(i)}_t, \Xi^{(i)}_{<t}) \quad \forall t' < t

$$

**Step 4: Proof of Sufficiency.**

By the definition of causal structure:
1. Events at $(j, \tau)$ with $\tau < t - d_{\mathcal{E}}(i,j)/c_{\text{info}}$ are already incorporated into $\Xi^{(i)}_{<t}$
2. Events at $(j, \tau)$ with $\tau \geq t - d_{\mathcal{E}}(i,j)/c_{\text{info}}$ cannot yet influence agent $i$

Thus, all information from $\Xi^{(i)}_{<t'}$ for $t' < t$ that is relevant to $z^{(i)}_{t+\Delta t}$ is already contained in $\Xi^{(i)}_{<t}$ (by the nested structure of causal cones).

**Step 5: Joint Markov Property.**

Define the joint augmented state:

$$
\mathbf{X}_t := (z^{(N)}_t, \Xi_{<t}) \in \mathcal{Z}_{\text{causal}}

$$

The transition kernel for the augmented state is:

$$
P(\mathbf{X}_{t+\Delta t} | \mathbf{X}_t, \mathbf{X}_{t-\Delta t}, \ldots) = P(\mathbf{X}_{t+\Delta t} | \mathbf{X}_t)

$$

This follows because:
- The current positions $z^{(N)}_t$ determine the local dynamics
- The memory screens $\Xi_{<t}$ contain all causally relevant history
- No additional information from $\mathbf{X}_{t-\Delta t}, \ldots$ can improve prediction beyond what $\mathbf{X}_t$ provides

**Step 6: Formal Verification (Chapman-Kolmogorov).**

The augmented process satisfies the Chapman-Kolmogorov equation:

$$
P(\mathbf{X}_{t+s} | \mathbf{X}_t) = \int P(\mathbf{X}_{t+s} | \mathbf{X}_{t+r}) P(\mathbf{X}_{t+r} | \mathbf{X}_t) \, d\mathbf{X}_{t+r}

$$
for all $0 < r < s$, which characterizes Markov processes. $\square$

:::

:::{prf:proof}

**Step 1: Standing Wave Ansatz.**

Consider the coupled Klein-Gordon system for $N$ agents:

$$
\left(\frac{1}{c^2}\partial_t^2 - \Delta_{G^{(i)}} + \kappa^2\right)V^{(i)} = \rho_r^{(i)} + \sum_{j \neq i} \rho^{\text{ret}}_{ij}

$$

Seek standing wave solutions of the form:

$$
V^{(i)}(z, t) = \bar{V}^{(i)}(z) + \sum_{n=1}^\infty \left[a_n^{(i)}(z) \cos(\omega_n t) + b_n^{(i)}(z) \sin(\omega_n t)\right]

$$
where $\bar{V}^{(i)}$ is the time-averaged component.

**Step 2: Boundary Conditions.**

On the product manifold $\mathcal{Z}^{(N)} = \prod_i \mathcal{Z}^{(i)}$, impose:
- **Dirichlet at sensors:** $V^{(i)}|_{\partial_{\text{in}}} = V_{\text{obs}}$ (observations fix boundary values)
- **Neumann at motors:** $\nabla_n V^{(i)}|_{\partial_{\text{out}}} = 0$ (no value flux at action boundary)

These boundary conditions create a "cavity" that supports discrete eigenfrequencies.

**Step 3: Eigenmode Expansion.**

The D'Alembertian $\square_G = \frac{1}{c^2}\partial_t^2 - \Delta_G$ on the bounded domain has discrete spectrum. Let $\{\phi_n\}$ be the eigenfunctions of $-\Delta_G + \kappa^2$ with eigenvalues $\lambda_n$:

$$
(-\Delta_G + \kappa^2)\phi_n = \lambda_n \phi_n, \quad \lambda_1 \leq \lambda_2 \leq \cdots

$$

The standing wave frequencies are $\omega_n = c\sqrt{\lambda_n}$.

**Step 4: Time-Averaged Stationarity Implies Nash.**

**Definition (Time-Averaged Nash):** A configuration $\mathbf{z}^* = (z^{(1)*}, \ldots, z^{(N)*})$ is a time-averaged Nash equilibrium if:

$$
\langle \mathbf{J}^{(i)} \rangle_T := \frac{1}{T}\int_0^T \mathbf{J}^{(i)}(z^{(i)*}, t) \, dt = 0 \quad \forall i

$$
where $\mathbf{J}^{(i)} = -\rho^{(i)} G^{-1} \nabla_A V^{(i)}$ is the probability current.

**Claim:** At a standing wave equilibrium, $\langle \mathbf{J}^{(i)} \rangle_T = 0$.

*Proof of Claim:* For the standing wave ansatz:

$$
\nabla_A V^{(i)} = \nabla_A \bar{V}^{(i)} + \sum_n \left[\nabla_A a_n^{(i)} \cos(\omega_n t) + \nabla_A b_n^{(i)} \sin(\omega_n t)\right]

$$

Time-averaging over period $T \gg 2\pi/\omega_1$:

$$
\langle \nabla_A V^{(i)} \rangle_T = \nabla_A \bar{V}^{(i)}

$$
since $\langle \cos(\omega_n t) \rangle_T = \langle \sin(\omega_n t) \rangle_T = 0$.

At a stationary point of $\bar{V}^{(i)}$, we have $\nabla \bar{V}^{(i)} = 0$, hence $\langle \mathbf{J}^{(i)} \rangle_T = 0$.

**Step 5: Connection to Game-Theoretic Nash.**

The Nash equilibrium condition is:

$$
V^{(i)}(z^{(i)*}, z^{(-i)*}) \geq V^{(i)}(z^{(i)}, z^{(-i)*}) \quad \forall z^{(i)}, \forall i

$$

This is equivalent to $z^{(i)*}$ being a local maximum of $V^{(i)}(\cdot, z^{(-i)*})$, requiring:
1. **First-order:** $\nabla_{z^{(i)}} V^{(i)}|_{z^*} = 0$
2. **Second-order:** $\nabla^2_{z^{(i)}} V^{(i)}|_{z^*} \preceq 0$ (negative semi-definite Hessian)

The standing wave equilibrium satisfies the first-order condition via $\langle \nabla_A V^{(i)} \rangle_T = 0$.

**Step 6: Ground State Correspondence.**

The **ground state** (lowest eigenvalue $\lambda_1$) corresponds to:
- Minimal oscillation energy
- Longest wavelength mode
- Most stable equilibrium

Higher modes ($n > 1$) are metastable—small perturbations can cause transitions to lower modes. The stable Nash equilibrium corresponds to the ground state of the coupled system. $\square$

:::

:::{prf:definition} Strategic Jacobian
:label: def-strategic-jacobian

The **Strategic Jacobian** $\mathcal{J}_{ji} \in \mathbb{R}^{d \times d}$ is the derivative of agent $j$'s best response with respect to agent $i$'s position:

$$
\mathcal{J}_{ji} := \frac{\partial BR_j(z^{(-j)})}{\partial z^{(i)}} = \frac{\partial z^{(j)*}}{\partial z^{(i)}}\bigg|_{BR}

$$
where $z^{(j)*} = BR_j(z^{(-j)})$.
:::

:::{prf:proof}

**Step 1: Best-Response Correspondence.**

In a multi-agent system, each agent $j$ has a **best-response correspondence**:

$$
BR_j(z^{(-j)}) := \arg\max_{z^{(j)}} V^{(j)}(z^{(j)}, z^{(-j)})

$$
where $z^{(-j)}$ denotes the positions of all agents except $j$.

**Assumption (Smooth Best-Response):** Assume $BR_j$ is single-valued and $C^1$ in a neighborhood of equilibrium. This holds when:
- The value function $V^{(j)}$ is strictly concave in $z^{(j)}$
- The equilibrium is isolated (non-degenerate Hessian)

**Step 2: Strategic Jacobian Definition.**

:::{prf:definition} Strategic Jacobian
:label: def-strategic-jacobian

The **Strategic Jacobian** $\mathcal{J}_{ji} \in \mathbb{R}^{d \times d}$ is the derivative of agent $j$'s best response with respect to agent $i$'s position:

$$
\mathcal{J}_{ji} := \frac{\partial BR_j(z^{(-j)})}{\partial z^{(i)}} = \frac{\partial z^{(j)*}}{\partial z^{(i)}}\bigg|_{BR}

$$
where $z^{(j)*} = BR_j(z^{(-j)})$.
:::

**Step 3: Implicit Function Theorem Derivation.**

At a best response, the first-order condition is:

$$
\nabla_{z^{(j)}} V^{(j)}(z^{(j)*}, z^{(-j)}) = 0

$$

Differentiating with respect to $z^{(i)}$ using the implicit function theorem:

$$
\nabla^2_{z^{(j)}z^{(j)}} V^{(j)} \cdot \frac{\partial z^{(j)*}}{\partial z^{(i)}} + \nabla^2_{z^{(j)}z^{(i)}} V^{(j)} = 0

$$

Solving for the Strategic Jacobian:

$$
\mathcal{J}_{ji} = -\left(\nabla^2_{z^{(j)}z^{(j)}} V^{(j)}\right)^{-1} \nabla^2_{z^{(j)}z^{(i)}} V^{(j)}

$$

**Step 4: Second-Order Value Variation.**

When agent $i$ moves by $\delta z^{(i)}$, agent $j$ responds with $\delta z^{(j)} \approx \mathcal{J}_{ji} \delta z^{(i)}$.

The second-order variation of agent $i$'s value is:

$$
\delta^2 V^{(i)} = (\delta z^{(i)})^\top \underbrace{\nabla^2_{z^{(i)}z^{(i)}} V^{(i)}}_{\text{direct curvature}} (\delta z^{(i)}) + (\delta z^{(i)})^\top \underbrace{\nabla^2_{z^{(i)}z^{(j)}} V^{(i)} \cdot \mathcal{J}_{ji}}_{\text{strategic back-reaction}} (\delta z^{(i)})

$$

**Step 5: Game Tensor as Effective Curvature.**

Define the **Game Tensor** as the strategic contribution to curvature:

$$
\mathcal{G}_{ij}^{kl} := \frac{\partial^2 V^{(i)}}{\partial z^{(j)}_k \partial z^{(j)}_l}\bigg|_{z^{(j)*}}

$$

The **perceived Hessian** including strategic back-reaction is:

$$
\tilde{H}^{(i)}_{kl} = \frac{\partial^2 V^{(i)}}{\partial z^{(i)}_k \partial z^{(i)}_l} + \sum_{j \neq i} \frac{\partial^2 V^{(i)}}{\partial z^{(i)}_k \partial z^{(j)}_m} (\mathcal{J}_{ji})^m_l

$$

**Step 6: Metric Modification.**

The agent's perceived geometry is modified by the Game Tensor. Under the Capacity-Constrained Metric Law (Theorem {prf:ref}`thm-capacity-constrained-metric-law`), risk increases effective metric:

$$
\tilde{G}^{(i)}_{kl} = G^{(i)}_{kl} + \sum_{j \neq i} \beta_{ij} \mathcal{G}_{ij,kl}

$$

where:
- $\beta_{ij} > 0$ for adversarial agents (opponents increase perceived curvature)
- $\beta_{ij} = 0$ for neutral agents
- $\beta_{ij} < 0$ for cooperative agents (allies reduce perceived curvature)

The lowered-index Game Tensor is:

$$
\mathcal{G}_{ij,kl} = G^{(i)}_{km} G^{(i)}_{ln} \mathcal{G}_{ij}^{mn}

$$

**Physical Interpretation:** The Game Tensor measures how "curved" agent $i$'s value landscape appears due to agent $j$'s presence. High $\|\mathcal{G}_{ij}\|$ regions are strategically volatile—small movements create large value changes. $\square$

:::

:::{prf:proof}

**Step 1: Jacobi Identity for Covariant Derivatives.**

The covariant derivatives satisfy the Jacobi identity:

$$
[[D_\mu, D_\nu], D_\rho] + [[D_\nu, D_\rho], D_\mu] + [[D_\rho, D_\mu], D_\nu] = 0

$$

**Step 2: Commutator in Terms of Field Strength.**

From Theorem {prf:ref}`thm-curvature-commutator`:

$$
[D_\mu, D_\nu] = -ig\mathcal{F}_{\mu\nu}

$$
where $\mathcal{F}_{\mu\nu}$ acts on fields in the appropriate representation.

**Step 3: Action on a Test Field.**

Let $\psi$ be a field in the fundamental representation. Apply the Jacobi identity:

$$
[[D_\mu, D_\nu], D_\rho]\psi + \text{cyclic} = 0

$$

Compute the first term:

$$
[[D_\mu, D_\nu], D_\rho]\psi = [D_\mu, D_\nu](D_\rho \psi) - D_\rho([D_\mu, D_\nu]\psi)

$$

$$
= -ig\mathcal{F}_{\mu\nu}(D_\rho \psi) - D_\rho(-ig\mathcal{F}_{\mu\nu}\psi)

$$

$$
= -ig\mathcal{F}_{\mu\nu}D_\rho \psi + ig D_\rho(\mathcal{F}_{\mu\nu}\psi)

$$

$$
= -ig\mathcal{F}_{\mu\nu}D_\rho \psi + ig (D_\rho \mathcal{F}_{\mu\nu})\psi + ig \mathcal{F}_{\mu\nu}D_\rho \psi

$$

$$
= ig (D_\rho \mathcal{F}_{\mu\nu})\psi

$$

**Step 4: Covariant Derivative of Field Strength.**

The covariant derivative acts on $\mathcal{F}_{\mu\nu}$ (an adjoint-valued 2-form) as:

$$
D_\rho \mathcal{F}_{\mu\nu} = \partial_\rho \mathcal{F}_{\mu\nu} - ig[A_\rho, \mathcal{F}_{\mu\nu}]

$$

**Step 5: Cyclic Sum.**

From the Jacobi identity:

$$
ig(D_\mu \mathcal{F}_{\nu\rho} + D_\nu \mathcal{F}_{\rho\mu} + D_\rho \mathcal{F}_{\mu\nu})\psi = 0

$$

Since this holds for arbitrary $\psi$:

$$
D_\mu \mathcal{F}_{\nu\rho} + D_\nu \mathcal{F}_{\rho\mu} + D_\rho \mathcal{F}_{\mu\nu} = 0

$$

**Step 6: Component Form Verification.**

In components, with $\mathcal{F}_{\mu\nu}^a = \partial_\mu A_\nu^a - \partial_\nu A_\mu^a + g f^{abc} A_\mu^b A_\nu^c$:

$$
D_\rho \mathcal{F}_{\mu\nu}^a = \partial_\rho \mathcal{F}_{\mu\nu}^a + g f^{abc} A_\rho^b \mathcal{F}_{\mu\nu}^c

$$

The cyclic sum:

$$
D_{[\mu}\mathcal{F}_{\nu\rho]}^a = \partial_{[\mu}\mathcal{F}_{\nu\rho]}^a + g f^{abc} A_{[\mu}^b \mathcal{F}_{\nu\rho]}^c

$$

The first term vanishes by the Jacobi identity for ordinary derivatives (applied to the definition of $\mathcal{F}$):

$$
\partial_{[\mu}\mathcal{F}_{\nu\rho]} = \partial_{[\mu}(\partial_\nu A_{\rho]} - \partial_\rho A_{\nu]}) + g f^{abc} \partial_{[\mu}(A_\nu^b A_{\rho]}^c) = 0

$$

The second term vanishes by antisymmetry:

$$
f^{abc} A_{[\mu}^b \mathcal{F}_{\nu\rho]}^c = f^{abc} \cdot \frac{1}{6}(A_\mu^b \mathcal{F}_{\nu\rho}^c + \text{5 cyclic permutations}) = 0

$$

by the Jacobi identity for structure constants and antisymmetry of $\mathcal{F}$. $\square$

:::

:::{prf:proof}

**Step 1: Higgs Potential Minimization.**

The Higgs potential is:

$$
V(\Phi) = \mu^2 |\Phi|^2 + \lambda |\Phi|^4

$$

For $\mu^2 > 0$: Minimum at $\Phi = 0$ (symmetric phase).

For $\mu^2 < 0$: The potential has the "Mexican hat" shape. Setting $\partial V / \partial |\Phi| = 0$:

$$
2\mu^2 |\Phi| + 4\lambda |\Phi|^3 = 0

$$

$$
|\Phi|^2 = -\frac{\mu^2}{2\lambda} =: \frac{v^2}{2}

$$

The vacuum expectation value (VEV) is:

$$
\langle \Phi \rangle = \frac{v}{\sqrt{2}}, \quad v = \sqrt{-\frac{\mu^2}{\lambda}}

$$

**Step 2: Fluctuations Around the VEV.**

Expand around the vacuum:

$$
\Phi(z) = \frac{1}{\sqrt{2}}(v + h(z))e^{i\theta(z)/v}

$$

where:
- $h(z)$ is the **Higgs boson** (radial fluctuation, physical degree of freedom)
- $\theta(z)$ is the **Goldstone mode** (angular fluctuation, will be "eaten")

For small fluctuations, linearize:

$$
\Phi \approx \frac{1}{\sqrt{2}}(v + h + i\theta)

$$

**Step 3: Gauge Boson Mass Generation.**

The kinetic term for the Higgs field is:

$$
|D_\mu \Phi|^2 = |(\partial_\mu - igA_\mu)\Phi|^2

$$

Substituting $\Phi = (v + h)/\sqrt{2}$ (unitary gauge, $\theta = 0$):

$$
D_\mu \Phi = \frac{1}{\sqrt{2}}(\partial_\mu h - igA_\mu(v + h))

$$

$$
|D_\mu \Phi|^2 = \frac{1}{2}(\partial_\mu h)^2 + \frac{g^2}{2}(v + h)^2 A_\mu A^\mu - \frac{ig}{\sqrt{2}}(v+h)(A_\mu \partial^\mu h - \partial_\mu h A^\mu)

$$

The mass term for the gauge field emerges from the $(v^2)$ contribution:

$$
|D_\mu \Phi|^2 \supset \frac{g^2 v^2}{2} A_\mu A^\mu

$$

Comparing with the standard mass term $\frac{1}{2}m_A^2 A_\mu A^\mu$:

$$
m_A = gv

$$

**Step 4: Goldstone Boson Absorption.**

In the unitary gauge, the Goldstone mode $\theta$ is absorbed into the longitudinal component of the massive gauge boson. The gauge field gains a third polarization state (longitudinal), as required for a massive spin-1 particle.

**Counting degrees of freedom:**
- Before SSB: 2 (massless gauge) + 2 (complex Higgs) = 4
- After SSB: 3 (massive gauge) + 1 (real Higgs $h$) = 4 ✓

**Step 5: Matter Field Mass Generation (Yukawa).**

The Yukawa coupling is:

$$
\mathcal{L}_{\text{Yukawa}} = -y_{ij}\bar{\psi}^{(i)}\Phi\psi^{(j)}

$$

After SSB, substituting $\Phi = (v + h)/\sqrt{2}$:

$$
\mathcal{L}_{\text{Yukawa}} = -\frac{y_{ij}}{\sqrt{2}}(v + h)\bar{\psi}^{(i)}\psi^{(j)}

$$

$$
= -\frac{y_{ij} v}{\sqrt{2}}\bar{\psi}^{(i)}\psi^{(j)} - \frac{y_{ij}}{\sqrt{2}}h\bar{\psi}^{(i)}\psi^{(j)}

$$

The first term is a mass term with:

$$
m_{ij} = \frac{y_{ij} v}{\sqrt{2}}

$$

For diagonal Yukawa ($y_{ij} = y_i \delta_{ij}$):

$$
m_i = \frac{y_i v}{\sqrt{2}}

$$

**Step 6: Symmetry Breaking Pattern.**

The original symmetry group $G$ is broken to a subgroup $H$ that leaves the VEV invariant:

$$
U \langle \Phi \rangle = \langle \Phi \rangle \quad \text{for } U \in H

$$

The number of massive gauge bosons equals $\dim(G) - \dim(H)$ (the number of broken generators).

**Example:** For $G = SO(D)$ broken to $H = SO(D-1)$:
- Broken generators: $D - 1$
- Each broken generator → one massive gauge boson
- Remaining $SO(D-1)$ gauge bosons stay massless $\square$

:::

:::{prf:proof}

**Step 1: WKB/Semiclassical Ansatz.**

For small $\sigma$, seek solutions of the form:

$$
\Psi(\mathbf{z}) = A(\mathbf{z}) \exp\left(-\frac{S(\mathbf{z})}{\sigma}\right)

$$

where $S(\mathbf{z}) \geq 0$ is the "action" and $A(\mathbf{z})$ is a slowly-varying amplitude.

**Step 2: Substitution into Schrödinger.**

The Strategic Hamiltonian acting on $\Psi$:

$$
\hat{H}_{\text{strat}}\Psi = \left[-\frac{\sigma^2}{2}\Delta_{\tilde{G}} + \Phi_{\text{eff}}\right]\Psi

$$

Compute the Laplacian of the WKB ansatz:

$$
\Delta_{\tilde{G}}(Ae^{-S/\sigma}) = e^{-S/\sigma}\left[\Delta_{\tilde{G}} A - \frac{2}{\sigma}\tilde{G}^{-1}(\nabla A, \nabla S) - \frac{A}{\sigma}\Delta_{\tilde{G}} S + \frac{A}{\sigma^2}\|\nabla S\|_{\tilde{G}}^2\right]

$$

**Step 3: Leading Order ($O(\sigma^{-2})$).**

The leading term gives:

$$
-\frac{\sigma^2}{2} \cdot \frac{A}{\sigma^2}\|\nabla S\|_{\tilde{G}}^2 = -\frac{A}{2}\|\nabla S\|_{\tilde{G}}^2

$$

For the ground state (minimum energy), we need:

$$
E_0 = \frac{1}{2}\|\nabla S\|_{\tilde{G}}^2 + \Phi_{\text{eff}}

$$

This is minimized when $\|\nabla S\|_{\tilde{G}}^2 = 0$ and $\Phi_{\text{eff}}$ is minimized.

**Step 4: Concentration on Critical Points.**

The condition $\nabla S = 0$ implies that $S$ is constant along directions where the wave-function has support. The wave-function $|\Psi|^2 = |A|^2 e^{-2S/\sigma}$ concentrates exponentially on the **minimum of $S$**.

For the ground state, $S(\mathbf{z}) = S_0 + \frac{1}{2}(\mathbf{z} - \mathbf{z}^*)^\top H (\mathbf{z} - \mathbf{z}^*) + O(|\mathbf{z} - \mathbf{z}^*|^3)$

where $\mathbf{z}^*$ is the minimum and $H$ is the Hessian.

**Step 5: Gaussian Approximation.**

Near the minimum:

$$
|\Psi(\mathbf{z})|^2 \approx |A(\mathbf{z}^*)|^2 \exp\left(-\frac{(\mathbf{z} - \mathbf{z}^*)^\top H (\mathbf{z} - \mathbf{z}^*)}{\sigma}\right)

$$

This is a Gaussian with width $\sim \sqrt{\sigma}$. As $\sigma \to 0$:

$$
|\Psi(\mathbf{z})|^2 \to \delta(\mathbf{z} - \mathbf{z}^*)

$$

**Step 6: Identification with Nash Equilibrium.**

The minimum of $\Phi_{\text{eff}}(\mathbf{z})$ is the Nash equilibrium by definition:
- $\Phi_{\text{eff}}^{(i)}(z^{(i)}, z^{(-i)}) = -V^{(i)}(z^{(i)}, z^{(-i)})$ (negative value = cost)
- Nash: each agent maximizes their own value → minimizes their own cost
- Joint minimum: $\nabla_{z^{(i)}} \Phi_{\text{eff}}^{(i)} = 0$ for all $i$

**Step 7: Energy Correction.**

The ground state energy is:

$$
E_0 = \Phi_{\text{eff}}(\mathbf{z}^*) + O(\sigma)

$$

The $O(\sigma)$ correction comes from zero-point energy:

$$
E_0 = \Phi_{\text{eff}}(\mathbf{z}^*) + \frac{\sigma}{2}\text{Tr}(\sqrt{H \tilde{G}^{-1}}) + O(\sigma^2)

$$

This is the sum of $\frac{\sigma \omega_n}{2}$ over all normal mode frequencies $\omega_n = \sqrt{\lambda_n}$ where $\lambda_n$ are eigenvalues of $H \tilde{G}^{-1}$.

**Step 8: Stability from Spectral Gap.**

The Nash equilibrium is **stable** if $H \succ 0$ (positive definite Hessian at the minimum). This ensures:
1. The ground state is unique
2. There is a spectral gap $\Delta = E_1 - E_0 > 0$
3. The concentration is exponentially tight in $\sigma$

Unstable critical points (saddles) have $H$ with negative eigenvalues, leading to **excited states** rather than ground states. $\square$

:::

## 10_appendices/06_losses.md

:::{prf:definition} F.1.1 (Reconstruction Loss)
:label: def-f-reconstruction-loss

$$
\mathcal{L}_{\text{recon}} = (x - \hat{x})^i \, G_{ij}^{\text{obs}}(x) \, (x - \hat{x})^j
$$

**Parameters:**
- $x, \hat{x}$ – original and reconstructed observations
- $G_{ij}^{\text{obs}}(x)$ – metric tensor on observation space (learned or fixed)

**Purpose:** Ensures charted latents collectively preserve information for reconstruction, with
metric-weighted distances.

**Units:** $[\mathrm{nat}]$ (when scaled appropriately) or metric-weighted MSE.

**Flat limit:** When $G_{ij}^{\text{obs}} = \delta_{ij}$ (identity), recovers standard MSE:
$\|x - \hat{x}\|^2$.

**Source:** {ref}`Section 3.2 <sec-loss-function-enforcing-macro-micro-separation>`,
Definition {prf:ref}`def-total-disentangled-loss`

:::

:::{prf:definition} F.1.2 (Vector Quantization Loss)
:label: def-f-vq-loss

$$
\mathcal{L}_{\text{vq}} = (z_q - z_e)^i \, G_{ij}(z_e) \, (z_q - z_e)^j + \beta \, (z_e - z_q)^i \, G_{ij}(z_q) \, (z_e - z_q)^j
$$

**Parameters:**
- $z_e$ – encoder output (pre-quantization)
- $z_q$ – quantized code embedding $e_{K}$ (per-chart)
- $G_{ij}(z)$ – metric tensor on latent space
- $\beta$ – commitment weight (default 0.25)

**Purpose:** Stabilizes per-chart codebooks. The first term updates code vectors; the second term
encourages the encoder to commit to nearby codes.

**Flat limit:** When $G_{ij} = \delta_{ij}$, recovers standard VQ:
$\|z_q - z_e\|^2 + \beta\|z_e - z_q\|^2$.

**Source:** {ref}`Section 3.2 <sec-architecture-the-disentangled-vq-vae-rnn>`,
Definition {prf:ref}`def-total-disentangled-loss`

:::

:::{prf:definition} F.1.3 (Routing Entropy Loss)
:label: def-f-closure-loss

$$
\mathcal{L}_{\text{entropy}} = -\frac{1}{B}\sum_{b=1}^{B}\sum_{k=1}^{N_c} w_{bk}\,\log(w_{bk} + \epsilon)
$$

**Parameters:**
- $w_{bk}$ – router weights over charts
- $N_c$ – number of charts

**Purpose:** Penalizes diffuse routing. Lower entropy corresponds to sharper chart assignments.

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 3.2 <sec-loss-function-enforcing-macro-micro-separation>`

:::

:::{prf:definition} F.1.4 (Consistency Loss)
:label: def-f-slowness-loss

$$
\mathcal{L}_{\text{consistency}} = \frac{1}{B}\sum_{b=1}^{B} \sum_{k=1}^{N_c} w^{\text{enc}}_{bk}\,\log\left(\frac{w^{\text{enc}}_{bk}+\epsilon}{w^{\text{dec}}_{bk}+\epsilon}\right)
$$

**Parameters:**
- $w^{\text{enc}}$ – encoder router weights
- $w^{\text{dec}}$ – decoder router weights

**Purpose:** Aligns encoder and decoder chart usage.

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 3.2 <sec-loss-function-enforcing-macro-micro-separation>`

:::

:::{prf:definition} F.1.5 (Window Loss / Grounding)
:label: def-f-nuisance-kl-loss

$$
\mathcal{L}_{\text{window}} = \max\left(0, \epsilon_{\text{ground}} - I(X;K)\right)^2
$$

**Parameters:**
- $I(X;K) = H(K) - H(K|X)$ – mutual information between input and chart assignment
- $\epsilon_{\text{ground}}$ – grounding threshold

**Purpose:** Enforces the stable learning window by requiring chart assignments to carry
information about inputs.

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 3.2 <sec-loss-function-enforcing-macro-micro-separation>`

:::

:::{prf:definition} F.1.6 (Per-Chart Code Entropy Loss)
:label: def-f-texture-kl-loss

$$
\mathcal{L}_{\text{code}} = \frac{1}{N_c}\sum_{k=1}^{N_c} \left(\log K - H(C\mid K=k)\right)
$$

**Parameters:**
- $C$ – code index within each chart
- $K$ – number of codes per chart

**Purpose:** Encourages each chart to use its codebook uniformly rather than collapsing to a
subset.

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 3.2 <sec-loss-function-enforcing-macro-micro-separation>`

:::

:::{prf:definition} F.1.7 (Total TopoEncoder Loss)
:label: def-f-total-disentangled-loss

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{recon}} + \mathcal{L}_{\text{vq}} +
\lambda_{\text{ent}}\,\mathcal{L}_{\text{entropy}} +
\lambda_{\text{cons}}\,\mathcal{L}_{\text{consistency}} +
\sum_{i \in \text{tiers}} \lambda_i \mathcal{L}_i +
\lambda_{\text{jump}}\,\mathcal{L}_{\text{jump}} +
\lambda_{\text{sup}}\,\mathcal{L}_{\text{sup}}
$$

**Purpose:** Compound loss enforcing sharp routing, charted quantization, and stable geometry.

**Source:** {ref}`Section 3.2 <sec-loss-function-enforcing-macro-micro-separation>`,
Definition {prf:ref}`def-total-disentangled-loss`

:::

:::{prf:definition} F.1.8 (Jump Consistency Loss)
:label: def-f-overlap-consistency

$$
\mathcal{L}_{\text{jump}} = \mathbb{E}_{i \ne j}\left[\|\,z_n^{(j)} - \mathcal{J}_{i \to j}(z_n^{(i)})\,\|^2\right]
$$

**Parameters:**
- $z_n^{(i)}$ – nuisance coordinate from chart $i$
- $\mathcal{J}_{i \to j}$ – learned jump operator from chart $i$ to chart $j$

**Purpose:** Enforces consistency in chart overlaps by learning transitions between chart-local
nuisance coordinates.

**Units:** Dimensionless (metric-weighted embedding distance).

**Source:** {ref}`Section 7 <sec-the-overlap-consistency-loss>`

:::

:::{prf:definition} F.2.1 (Purity Loss / Conditional Entropy)
:label: def-f-purity-loss

$$
\mathcal{L}_{\text{purity}} = \sum_{k=1}^{N_c} P(K=k) \cdot H(Y \mid K=k) = H(Y \mid K)
$$

**Parameters:**
- $P(K=k) = \mathbb{E}_{x \sim \mathcal{D}}[w_k(x)]$ – marginal chart probability
- $H(Y \mid K=k) = -\sum_y P(Y=y \mid K=k) \log P(Y=y \mid K=k)$ – class entropy within chart $k$

**Purpose:** Measures how well charts separate classes. Low purity loss means each chart is associated with a single class. Equivalent to maximizing mutual information $I(K; Y)$ since $\mathcal{L}_{\text{purity}} = H(Y) - I(K; Y)$.

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 25.4 <sec-the-supervised-topology-loss>`, Definition {prf:ref}`def-purity-loss`

:::

:::{prf:definition} F.2.2 (Load Balance Loss)
:label: def-f-balance-loss

$$
\mathcal{L}_{\text{balance}} = D_{\text{KL}}\left(\bar{w} \;\|\; \text{Uniform}(N_c)\right)
$$

**Parameters:**
- $\bar{w} = \mathbb{E}_{x \sim \mathcal{D}}[w(x)]$ – average router weight vector
- $N_c$ – number of charts

**Purpose:** Prevents "dead charts" (collapse to few charts). Encourages all charts to be used productively, addressing the expert-collapse problem in mixture-of-experts systems.

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 25.4 <sec-load-balance-loss>`, Definition {prf:ref}`def-balance-loss`

:::

:::{prf:definition} F.2.3 (Metric Contrastive Loss)
:label: def-f-contrastive-loss

$$
\mathcal{L}_{\text{metric}} = \frac{1}{|\mathcal{P}|} \sum_{(i,j) \in \mathcal{P}: y_i \neq y_j} w_i^\top w_j \cdot \max(0, m - d_G^{\text{jump}}(z_i, z_j))^2
$$

**Parameters:**
- $\mathcal{P}$ – set of sample pairs in batch
- $w_i, w_j$ – router weight vectors
- $m > 0$ – margin (minimum desired geodesic separation)
- $d_G^{\text{jump}}(z_i, z_j)$ – minimum geodesic jump cost between samples under metric $G$

**Purpose:** Enforces that different-class samples are geometrically far apart in geodesic jump distance. The weighting $w_i^\top w_j$ focuses penalty on hard examples (high routing overlap despite different classes). The geodesic distance respects the curved manifold structure.

**Units:** $[\mathrm{nat}]$

**Flat limit:** When $G_{ij} = \delta_{ij}$, reduces to Euclidean jump distance.

**Source:** {ref}`Section 25.4 <sec-metric-contrastive-loss>`, Definition {prf:ref}`def-contrastive-loss`

:::

:::{prf:definition} F.2.4 (Route Alignment Loss)
:label: def-f-route-alignment-loss

$$
\mathcal{L}_{\text{route}} = \mathbb{E}_{x, y_{\text{true}}}\left[\text{CE}\left(\sum_k w_k(x) \cdot P(Y=\cdot \mid K=k), \; y_{\text{true}}\right)\right]
$$

**Parameters:**
- $w_k(x)$ – router weights for sample $x$ and chart $k$
- $P(Y=\cdot \mid K=k)$ – per-chart class distributions
- $\text{CE}$ – cross-entropy loss

**Purpose:** Primary classification loss. The predicted class distribution (router-weighted average of per-chart distributions) must match the true label.

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 25.4 <sec-route-alignment-loss>`, Definition {prf:ref}`def-route-alignment-loss`

:::

:::{prf:definition} F.2.5 (Combined Supervised Topology Loss)
:label: def-f-total-supervised-loss

$$
\mathcal{L}_{\text{sup-topo}} = \mathcal{L}_{\text{route}} + \lambda_{\text{pur}} \mathcal{L}_{\text{purity}} + \lambda_{\text{bal}} \mathcal{L}_{\text{balance}} + \lambda_{\text{met}} \mathcal{L}_{\text{metric}}
$$

**Typical hyperparameters:**
| Weight | Typical Value | Role |
|--------|---------------|------|
| $\lambda_{\text{pur}}$ | 0.1 | Chart purity |
| $\lambda_{\text{bal}}$ | 0.01 | Load balancing |
| $\lambda_{\text{met}}$ | 0.01 | Metric separation |

**Purpose:** Weighted combination enforcing chart purity, balanced usage, geometric separation, and prediction accuracy.

**Source:** {ref}`Section 25.4 <sec-combined-supervised-topology-loss>`, Definition {prf:ref}`def-total-loss`

:::

:::{prf:definition} F.2.6 (Hierarchical Supervised Loss)
:label: def-f-hierarchical-loss

$$
\mathcal{L}_{\text{hier}} = \sum_{\ell=0}^{L} \alpha_\ell \left(\mathcal{L}_{\text{route}}^{(\ell)} + \lambda_{\text{pur}} \mathcal{L}_{\text{purity}}^{(\ell)}\right)
$$

**Parameters:**
- $\ell \in \{0, \ldots, L\}$ – scale levels (bulk to boundary)
- $\alpha_\ell$ – per-scale weights (often $\alpha_\ell = 1$ or decaying)
- $\mathcal{Y}_\ell$ – label space at scale $\ell$ (coarse to fine)

**Purpose:** Enforces classification at multiple scales via stacked TopoEncoders. Coarse (bulk) layers distinguish broad categories; fine (boundary) layers distinguish leaf categories.

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 25.6 <sec-hierarchical-classification-via-scale-decomposition>`, Definition {prf:ref}`def-hierarchical-supervised-loss`

:::

:::{prf:definition} F.3.1 (Cumulative Cost Functional)
:label: def-f-cumulative-cost

$$
\mathcal{S} = \int \Big(\mathcal{L}_{\text{control}} + C(z_t, a_t)\Big) \, dt
$$

**Parameters:**
- $\mathcal{L}_{\text{control}}$ – control/effort cost (KL penalty, action magnitude)
- $C(z_t, a_t)$ – task cost

**Purpose:** General optimal control objective under information/effort constraints. Specializes to KL-control and entropy-regularized RL.

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 2 <sec-the-control-loop-representation-and-control>`

:::

:::{prf:definition} F.3.2 (Instantaneous Regularized Objective)
:label: def-f-instantaneous-objective

$$
F_t := V(Z_t) + \beta_K\big(-\log p_\psi(K_t)\big) + \beta_n D_{\mathrm{KL}}(q(z_{n,t} \mid x_t) \| p(z_n)) + \beta_{\mathrm{tex}} D_{\mathrm{KL}}(q(z_{\mathrm{tex},t} \mid x_t) \| p(z_{\mathrm{tex}})) + T_c D_{\mathrm{KL}}(\pi(\cdot \mid K_t) \| \pi_0(\cdot \mid K_t))
$$

**Parameters:**
- $V(Z_t)$ – task-aligned cost-to-go (critic estimate)
- $\beta_K(-\log p_\psi(K_t))$ – macro codelength penalty (Occam's razor for discrete state)
- $\beta_n, \beta_{\text{tex}}$ – residual regularization weights
- $T_c$ – cognitive temperature
- $\pi_0$ – prior policy

**Purpose:** Trades off task cost, representation complexity, and control effort, all in consistent units (nats).

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 3.2 <sec-the-entropy-regularized-objective-functional>`, Definition {prf:ref}`def-instantaneous-objective`

:::

:::{prf:definition} F.3.3 (Monotonicity Surrogate Loss)
:label: def-f-monotonicity-loss

$$
\mathcal{L}_{\downarrow F} := \mathbb{E}\left[\mathrm{ReLU}(F_{t+1} - F_t)^2\right]
$$

**Purpose:** Penalizes increases in the instantaneous objective $F_t$ from one step to the next, encouraging trajectories that smoothly descend the objective landscape.

**Units:** $[\mathrm{nat}^2]$

**Source:** {ref}`Section 3.2 <sec-the-entropy-regularized-objective-functional>`

:::

:::{prf:definition} F.3.4 (Closure Ratio Diagnostic)
:label: def-f-closure-ratio

$$
\text{Closure Ratio} = \frac{\mathbb{E}[-\log p_\psi(K_{t+1} \mid K_t, a_t)]}{\mathbb{E}[-\log p_{\text{base}}(K_{t+1})]} = \frac{H(K_{t+1} \mid K_t, a_t)}{H(K_{t+1})}
$$

**Interpretation:**
| Ratio | Meaning | Action |
|-------|---------|--------|
| $\ll 1$ | Strong predictive law learned | Success |
| $\approx 1$ | No predictive law | Increase model capacity |
| $> 1$ | Worse than baseline | Bug/degeneracy |

**Purpose:** Measures how much better the macro dynamics model predicts $K_{t+1}$ compared to a marginal baseline. The gap estimates predictive information $I(K_{t+1}; K_t, a_t)$.

**Units:** Dimensionless.

**Source:** {ref}`Section 3.2 <sec-runtime-diagnostics-the-closure-ratio>`, Definition {prf:ref}`def-closure-ratio`

:::

:::{prf:definition} F.3.5 (Causal Information Potential)
:label: def-f-causal-info-potential

$$
\Psi_{\text{causal}}(z, a) := \mathbb{E}_{z' \sim \bar{P}(\cdot | z, a)} \left[ D_{\text{KL}} \left( p(\theta_W | z, a, z') \| p(\theta_W | z, a) \right) \right]
$$

**Parameters:**
- $z, a$ – current state and action
- $z'$ – next state sampled from world model
- $\theta_W$ – world model parameters
- $\bar{P}$ – world model transition distribution

**Purpose:** Measures the **Expected Information Gain** about world model parameters from executing action $a$ at state $z$. High $\Psi_{\text{causal}}$ indicates the outcome will resolve significant uncertainty about dynamics. Drives intrinsic motivation for exploration via Bayesian experimental design.

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 29 <sec-the-causal-information-potential>`, Definition {prf:ref}`def-causal-information-potential`

:::

:::{prf:definition} F.4.1 (Hodge Decomposition of Reward)
:label: def-f-hodge-decomposition

$$
\mathcal{R} = \underbrace{d\Phi}_{\text{Gradient}} + \underbrace{\delta \Psi}_{\text{Solenoidal}} + \underbrace{\eta}_{\text{Harmonic}}
$$

**Components:**
- $d\Phi$ – Gradient/conservative component (optimizable via value function $V = \Phi$)
- $\delta\Psi$ – Solenoidal/rotational component (cyclic reward structure)
- $\eta$ – Harmonic component (topological cycles from manifold holes)

**Purpose:** Decomposes reward 1-form into orthogonal components. Separates optimizable value from inherently cyclic structure (e.g., Rock-Paper-Scissors).

**Units:** $[\Phi] = \mathrm{nat}$, $[\Psi] = \mathrm{nat} \cdot [\text{length}]^2$, $[\eta] = \mathrm{nat}/[\text{length}]$.

**Source:** {ref}`Section 18.2 <sec-hodge-decomposition-of-value>`, Theorem {prf:ref}`thm-hodge-decomposition`

:::

:::{prf:definition} F.4.2 (Value Curl / Vorticity)
:label: def-f-value-curl

$$
\mathcal{F}_{ij} := \partial_i \mathcal{R}_j - \partial_j \mathcal{R}_i = d\mathcal{R}
$$

**Properties:**
- Antisymmetric: $\mathcal{F}_{ij} = -\mathcal{F}_{ji}$
- Satisfies Bianchi identity: $d\mathcal{F} = 0$
- Gauge-invariant under $\mathcal{R} \to \mathcal{R} + d\chi$

**Purpose:** Detects non-conservative reward structure. Non-zero curl indicates orbiting strategies may be optimal. Diagnostic: $\oint_\gamma \mathcal{R} = \int_\Sigma \mathcal{F} \, d\Sigma \neq 0$ implies non-conservative rewards.

**Units:** $[\mathcal{F}] = \mathrm{nat}/[\text{length}]^2$

**Source:** {ref}`Section 18.2 <sec-hodge-decomposition-of-value>`, Definition {prf:ref}`def-value-curl`

:::

:::{prf:definition} F.4.3 (Class-Conditioned Potential)
:label: def-f-class-potential

$$
V_y(z, K) := -\beta_{\text{class}} \log P(Y=y \mid K) + V_{\text{base}}(z, K)
$$

**Parameters:**
- $P(Y=y \mid K) = \text{softmax}(\Theta_{K,:})_y$ – learnable chart-to-class affinities
- $V_{\text{base}}(z, K)$ – unconditioned critic
- $\beta_{\text{class}} > 0$ – class temperature (inverse semantic diffusion)

**Purpose:** Shapes potential landscape so class-$y$ regions become energy minima. Used for both classification (relaxation inference) and generation (Langevin sampling).

**Units:** $[V_y] = \mathrm{nat}$

**Source:** {ref}`Section 25.2 <sec-the-semantic-potential>`, Definition {prf:ref}`def-class-conditioned-potential`

:::

:::{prf:definition} F.5.1 (Synchronization Potential)
:label: def-f-sync-potential

$$
\mathcal{L}_{\text{sync}} = \beta \Psi_{\text{sync}} = \beta \int_{\partial\Omega} \mathcal{F}_{AB}^{\mu\nu} \mathcal{F}_{AB\,\mu\nu} \, dA
$$

**Parameters:**
- $\beta$ – coupling strength
- $\mathcal{F}_{AB}$ – Locking curvature (geometric disagreement between agents $A$ and $B$)

**Purpose:** Penalizes disagreement in representations between agents. Drives gauge locking (synchronized metrics). In the strong coupling limit ($\beta \to \infty$), forces $\mathcal{F}_{AB} \to 0$ (perfect synchronization).

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 37 <sec-the-inter-subjective-metric-gauge-locking-and-the-emergence-of-objective-reality>`

:::

:::{prf:definition} F.6.1 (Ontological Stress)
:label: def-f-ontological-stress

$$
\Xi = \sum_{\ell=1}^{L} \left( z_{\text{tex}}^{(\ell)} \right)^i G_{ij}^{(\ell)} \left( z_{\text{tex}}^{(\ell)} \right)^j
$$

**Parameters:**
- $z_{\text{tex}}^{(\ell)}$ – texture embedding at scale $\ell$
- $G_{ij}^{(\ell)}$ – metric tensor at scale $\ell$

**Purpose:** Measures predictability *within* texture across scales. High stress indicates ontological inadequacy---texture contains compressible structure that should have been captured by macro/nuisance. Dual to closure defect: closure measures micro-to-macro leakage; ontological stress measures within-texture predictability.

**Units:** Dimensionless (metric-weighted embedding norm).

**Flat limit:** When $G_{ij}^{(\ell)} = \delta_{ij}$, recovers $\sum_\ell \|z_{\text{tex}}^{(\ell)}\|^2$.

**Source:** {ref}`Section 33 <sec-ontological-expansion-topological-fission-and-the-semantic-vacuum>`

:::

:::{prf:definition} F.7.1 (Gradient Penalty Loss)
:label: def-f-gradient-penalty

$$
\mathcal{L}_{GP} = \mathbb{E}_{\hat{s}} \left[\left(\|\nabla_A V\|_G - K\right)^2\right], \qquad \|\nabla_A V\|_G^2 := G^{ij}(\hat{s}) \, (\partial_i V - A_i) \, (\partial_j V - A_j)
$$

**Parameters:**
- $\hat{s}$ – interpolated samples between real and generated
- $V(\hat{s})$ – critic value at sample
- $G^{ij}(\hat{s})$ – inverse metric tensor (contravariant) at sample
- $K$ – target gradient norm (typically 1)

**Purpose:** Enforces Lipschitz constraint on the critic using the metric-induced norm. The covariant gradient norm $\|\nabla_A V\|_G$ measures the gauge-invariant rate of change along geodesics. Prevents vanishing gradients in flat value regions (BarrierGap) and ensures smooth value landscape for stable learning.

**Units:** Dimensionless.

**Flat limit:** When $G^{ij} = \delta^{ij}$ and $A=0$, recovers $(\|\nabla_A V\|_2 - K)^2$.

**Source:** {ref}`Section 4 <sec-barrier-implementation-details>`

:::

:::{prf:definition} F.7.2 (Information-Control Loss)
:label: def-f-info-control

$$
\mathcal{L}_{\text{InfoControl}} = \underbrace{\beta_K \mathbb{E}[-\log p_\psi(K)] + \beta_n D_{\mathrm{KL}}(q(z_n \mid x) \| p(z_n)) + \beta_{\mathrm{tex}} D_{\mathrm{KL}}(q(z_{\mathrm{tex}} \mid x) \| p(z_{\mathrm{tex}}))}_{\text{Compression (Rate)}} + \underbrace{\gamma \mathbb{E}[\mathfrak{D}(Z, A)]}_{\text{Control Effort}}
$$

**Parameters:**
- $\beta_K, \beta_n, \beta_{\text{tex}}$ – compression weights for macro/nuisance/texture
- $\mathfrak{D}(Z, A)$ – actuation cost (KL-control or action norm)
- $\gamma$ – control effort weight

**Purpose:** Balances the Information-Control Tradeoff (BarrierScat vs BarrierCap). High compression removes details needed for fine control; this loss finds the Pareto frontier.

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 4 <sec-b-cross-barrier-regularization>`

:::

:::{prf:definition} F.7.3 (Elastic Weight Consolidation)
:label: def-f-ewc

$$
\mathcal{L}_{\text{EWC}} = \sum_i F_i (\theta_i - \theta^*_{i,\text{old}})^2
$$

**Parameters:**
- $F_i$ – diagonal Fisher Information for parameter $i$
- $\theta_i$ – current parameter value
- $\theta^*_{i,\text{old}}$ – parameter value from previous task

**Purpose:** Addresses the Stability-Plasticity Dilemma (BarrierVac vs BarrierPZ). High-sensitivity weights (large $F_i$) are constrained to preserve past learning; low-sensitivity weights can adapt freely.

**Units:** Dimensionless (parameter space distance, Fisher-weighted).

**Source:** {ref}`Section 4 <sec-b-cross-barrier-regularization>`

:::

:::{prf:definition} F.7.4 (Bode Magnitude Loss)
:label: def-f-bode

$$
\mathcal{L}_{\text{Bode}} = \|\mathcal{F}(e_t) \cdot W(\omega)\|^2
$$

**Parameters:**
- $\mathcal{F}(e_t)$ – Fourier transform of error signal
- $W(\omega)$ – frequency weighting function

**Purpose:** Addresses the Bode Sensitivity Integral (BarrierBode). Suppressing error in one frequency band amplifies it in another (waterbed effect). This loss explicitly chooses where to be sensitive vs. blind.

**Units:** $[\mathrm{nat}^2]$

**Source:** {ref}`Section 4 <sec-b-cross-barrier-regularization>`

:::

:::{prf:definition} F.8.1 (Quantum Speed Limit Loss)
:label: def-f-qsl

$$
\mathcal{L}_{\text{QSL}} := \mathrm{ReLU}\left(d_G(z_{t+1}, z_t) - v_{\max}\right)^2
$$

**Parameters:**
- $d_G(z_{t+1}, z_t)$ – geodesic distance traveled in one step
- $v_{\max}$ – maximum allowed velocity in latent space

**Purpose:** Enforces the Quantum Speed Limit: belief cannot change faster than the Mandelstam-Tamm bound allows. Prevents unrealistic jumps in belief state.

**Units:** $[\mathrm{nat}^2]$

**Source:** {ref}`Section 11 <sec-belief-dynamics-prediction-update-projection>`

:::

:::{prf:definition} F.8.2 (Joint Prediction Loss)
:label: def-f-joint-prediction

$$
\mathcal{L}_{\text{joint}} = d_G(\hat{x}_{t+1}^A, x_{t+1})^2 + d_G(\hat{x}_{t+1}^B, x_{t+1})^2 + \beta \Psi_{\text{sync}}
$$

Expanded in coordinates:

$$
d_G(\hat{x}, x)^2 = (\hat{x} - x)^i \, G_{ij}^{\text{obs}}(x) \, (\hat{x} - x)^j
$$

**Parameters:**
- $\hat{x}_{t+1}^A, \hat{x}_{t+1}^B$ – predictions from agents $A$ and $B$
- $x_{t+1}$ – actual next observation
- $G_{ij}^{\text{obs}}$ – metric tensor on observation space
- $\Psi_{\text{sync}}$ – synchronization potential
- $\beta$ – coupling strength

**Purpose:** Multi-agent world model training. Both agents must predict accurately (measured under the observation-space metric), and their representations must synchronize (gauge lock).

**Units:** Metric-weighted prediction error + $[\mathrm{nat}]$.

**Flat limit:** When $G_{ij}^{\text{obs}} = \delta_{ij}$, recovers $\|\hat{x}^A - x\|^2 + \|\hat{x}^B - x\|^2 + \beta\Psi_{\text{sync}}$.

**Source:** {ref}`Section 37 <sec-the-inter-subjective-metric-gauge-locking-and-the-emergence-of-objective-reality>`

:::

:::{prf:definition} F.9.1 (VICReg Loss)
:label: def-f-vicreg

$$
\mathcal{L}_{\text{VICReg}} = \lambda \mathcal{L}_{\text{inv}} + \mu \mathcal{L}_{\text{var}} + \nu \mathcal{L}_{\text{cov}}
$$

**Components:**

$$
\begin{aligned}
\mathcal{L}_{\text{inv}} &= d_G(z, z')^2 = (z - z')^i \, G_{ij}(z) \, (z - z')^j & \text{(invariance)} \\
\mathcal{L}_{\text{var}} &= \frac{1}{d} \sum_{j=1}^{d} \max\left(0, \gamma - \sqrt{\text{Var}_G(z^j) + \epsilon}\right) & \text{(variance)} \\
\mathcal{L}_{\text{cov}} &= \frac{1}{d} \sum_{i \neq j} \left[G^{-1/2} \text{Cov}(z) \, G^{-1/2}\right]_{ij}^2 & \text{(covariance)}
\end{aligned}
$$

**Parameters:**
- $z, z'$ – embeddings of two augmented views of the same input
- $G_{ij}(z)$ – metric tensor on embedding space
- $\gamma$ – variance threshold (typically 1)
- $\lambda, \mu, \nu$ – component weights (typically $\lambda = 25$, $\mu = \nu = 1$)

**Purpose:** Prevents representation collapse without negative samples. Invariance pulls augmented views together (geodesic distance); variance prevents dimension collapse (metric-aware); covariance decorrelates dimensions (whitened by metric).

**Units:** Dimensionless.

**Flat limit:** When $G_{ij} = \delta_{ij}$, recovers standard VICReg with $\|z - z'\|^2$.

**Source:** {ref}`Section 3 <sec-diagnostics-stability-checks>` (GeomCheck, Node 6)

:::

:::{prf:definition} F.9.2 (InfoNCE Loss)
:label: def-f-infonce

$$
\mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp\left(-d_G(z_t, z_{t+k})^2 / \tau\right)}{\sum_{j} \exp\left(-d_G(z_t, z_j)^2 / \tau\right)}
$$

**Parameters:**
- $z_t, z_{t+k}$ – embeddings at current and future timesteps
- $z_j$ – negative samples (other timesteps or other sequences)
- $d_G(z, z')$ – geodesic distance under metric $G$
- $\tau$ – temperature parameter (squared distance scale)

**Purpose:** Contrastive predictive coding with geodesic similarity. Anchors macro latents to temporal structure by maximizing mutual information between present and future representations. The geodesic kernel $k_G(z, z') = \exp(-d_G(z,z')^2/\tau)$ respects the curved geometry of the latent manifold.

**Units:** $[\mathrm{nat}]$

**Flat limit:** When $G_{ij} = \delta_{ij}$ and using $\text{sim}(z,z') = -\|z-z'\|^2$, recovers standard InfoNCE with Gaussian kernel.

**Source:** {ref}`Section 3 <sec-diagnostics-stability-checks>` (GeomCheck, Node 6)

:::

:::{prf:definition} F.10.1 (Behavior Cloning Loss)
:label: def-f-bc

$$
\mathcal{L}_{\text{BC}} = \mathbb{E}_{(s, a^*) \sim \mathcal{D}_{\text{expert}}}[-\log \pi(a^* \mid s)]
$$

**Parameters:**
- $(s, a^*)$ – state-action pairs from expert demonstrations
- $\mathcal{D}_{\text{expert}}$ – expert demonstration dataset
- $\pi(a \mid s)$ – learned policy

**Purpose:** Supervised policy learning. Trains the policy to match expert actions via maximum likelihood.

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 25 <sec-supervised-topology-semantic-potentials-and-metric-segmentation>`

:::

:::{prf:definition} F.11.1 (WFR Consistency Loss)
:label: def-f-wfr-consistency

$$
\mathcal{L}_{\text{WFR}} = \left\| \sqrt{\rho_{t+1}} - \sqrt{\rho_t} - \frac{\Delta t}{2\sqrt{\rho_t}}\left(\rho_t r_t - \nabla \cdot (\rho_t v_t)\right) \right\|_{L^2}^2
$$

**Parameters:**
- $\rho_t$ – belief density at time $t$
- $r_t$ – reaction rate (birth/death)
- $v_t$ – transport velocity field
- $\Delta t$ – timestep

**Purpose:** Enforces Wasserstein-Fisher-Rao consistency. Penalizes deviations from the unbalanced continuity equation in cone-space formulation.

**Units:** $[\mathrm{nat}^2]$

**Source:** {ref}`Section 20 <sec-wfr-dynamics-with-memory-sources>`

:::

:::{prf:definition} F.11.2 (Critic TD Loss with PDE Regularization)
:label: def-f-critic-td

$$
\mathcal{L}_{\text{critic}} = \|\text{TD-Error}\|^2 + \lambda_{\text{PDE}} \| -\Delta_G V + \kappa^2 V - \rho_r \|^2
$$

**Parameters:**
- TD-Error $= r + \gamma V(s') - V(s)$ – temporal difference error
- $\Delta_G$ – Laplace-Beltrami operator on manifold
- $\kappa^2 = -\ln \gamma$ – screening mass from discount factor
- $\rho_r$ – reward density

**Purpose:** Combines TD learning with Helmholtz PDE regularization. The PDE term enforces that the critic satisfies the continuum Bellman equation.

**Units:** $[\mathrm{nat}^2]$

**Source:** {ref}`Section 18 <sec-the-reward-field-value-forms-and-hodge-geometry>`

:::

:::{prf:definition} F.12.1 (Waste Quotient)
:label: def-f-waste-quotient

$$
W_\mathcal{P} := 1 - \frac{\Delta I_{\text{world}}}{\int \dot{\mathcal{M}}(t) \, dt}
$$

**Parameters:**
- $\Delta I_{\text{world}}$ – mutual information gained about world
- $\dot{\mathcal{M}}(t)$ – metabolic flux (energy dissipation rate)

**Interpretation:**
| Protocol | Waste Quotient | Meaning |
|----------|----------------|---------|
| Bitcoin PoW | $W_{\text{BTC}} \approx 1$ | Energy produces zero world knowledge |
| Target PoUW | $W_{\text{PoUW}} \to 0$ | Energy produces useful learning |

**Purpose:** Measures efficiency of consensus protocol. Low waste quotient means energy dissipation produces useful information gain.

**Units:** Dimensionless.

**Source:** {ref}`Section 38.1 <sec-the-thermodynamic-inefficiency-of-nakamoto-consensus>`, Definition {prf:ref}`def-waste-quotient`

:::

:::{prf:definition} F.13.1 (Governor Training Regret)
:label: def-f-governor-regret

$$
J(\phi) = \mathbb{E}_{\mathcal{T} \sim P(\mathcal{T})} \left[ \sum_{t=0}^T \left( \mathcal{L}_{\text{task}}(\theta_t) + \gamma_{\text{viol}} \sum_{k=1}^K \text{ReLU}(C_k(\theta_t))^2 \right) \right]
$$

**Parameters:**
- $\phi$ – Governor parameters
- $\mathcal{T}$ – task from task distribution
- $\mathcal{L}_{\text{task}}(\theta_t)$ – task loss at training step $t$
- $C_k(\theta_t)$ – constraint $k$ value (negative when satisfied)
- $\gamma_{\text{viol}}$ – constraint violation penalty weight

**Purpose:** Meta-learning objective for the Governor. Minimizes cumulative task loss (convergence speed) plus squared constraint violations (feasibility). The Governor learns to set hyperparameters $\Lambda$ that lead to fast, stable training across diverse tasks.

**Units:** $[\mathrm{nat}]$

**Source:** {ref}`Section 26 <sec-bilevel-optimization-objective>`, Definition {prf:ref}`def-outer-problem-governor-optimization`

:::

## 10_appendices/07_architecture.md

:::{prf:definition} G.1.1 (TopoEncoderConfig)
:label: def-g-disentangled-config

**Class signature:**
```python
@dataclass
class TopoEncoderConfig:
    input_dim: int = 784
    hidden_dim: int = 32
    latent_dim: int = 2
    num_charts: int = 10
    codes_per_chart: int = 32
    covariant_attn: bool = True
    covariant_attn_tensorization: str = "full"
    covariant_attn_rank: int = 8
    covariant_attn_tau_min: float = 1e-2
    covariant_attn_denom_min: float = 1e-3
    covariant_attn_use_transport: bool = True
    covariant_attn_transport_eps: float = 1e-3
    vision_preproc: bool = False
    soft_equiv_metric: bool = False
    soft_equiv_temperature: float = 1.0
```

**Purpose:** Configuration dataclass for the TopoEncoder benchmark in
`src/experiments/topoencoder_2d.py`. Controls chart routing, codebook sizes, and optional
covariant/soft-equivariant components.

**Key parameters:**
- `num_charts`, `codes_per_chart` – atlas resolution
- `covariant_attn_*` – routing tensorization and transport
- `vision_preproc` – CovariantRetina feature extractor toggle
- `soft_equiv_metric`, `soft_equiv_temperature` – per-chart metric control

**Source:** {ref}`Section 3.2 <sec-architecture-the-disentangled-vq-vae-rnn>`, `topoencoder_2d.py`.
:::

:::{prf:definition} G.1.2 (PrimitiveAttentiveAtlasEncoder)
:label: def-g-encoder

**Class signature:**
```python
class PrimitiveAttentiveAtlasEncoder(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int, latent_dim: int, num_charts: int, codes_per_chart: int, ...):
        ...

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, ...]:
        ...
```

**Input/Output:**
- Input: `x` shape `[B, D_in]` or `[B, C, H, W]`
- Output (ordered): `K_chart`, `K_code`, `z_n`, `z_tex`, `router_weights`, `z_geo`,
  `vq_loss`, `indices_stack`, `z_n_all_charts`, `c_bar`

**Purpose:** Encodes inputs into charted VQ latents with typed residuals. Uses chart routing to
select per-chart codebooks and produces geometry `z_geo` and texture `z_tex`.

**Key parameters:** `num_charts`, `codes_per_chart`, `covariant_attn_*`, `vision_preproc`,
`soft_equiv_metric`.

**Source:** {ref}`Section 3.2 <sec-architecture-the-disentangled-vq-vae-rnn>`, `atlas.py`.
:::

:::{prf:definition} G.1.3 (CovariantChartRouter)
:label: def-g-vector-quantizer

**Class signature:**
```python
class CovariantChartRouter(nn.Module):
    def __init__(self, latent_dim: int, key_dim: int, num_charts: int, feature_dim: int | None = None, ...):
        ...

    def forward(self, z: torch.Tensor, features: torch.Tensor | None = None, chart_tokens: torch.Tensor | None = None) -> Tuple[torch.Tensor, torch.Tensor]:
        ...
```

**Input/Output:**
- Input: `z` shape `[B, D]`, optional `features` shape `[B, H]`
- Output: `(router_weights, K_chart)` where `router_weights` is `[B, N_c]`

**Purpose:** Gauge-covariant chart routing with Wilson-line transport and metric-aware temperature.

**Key parameters:** `tensorization`, `rank`, `tau_min`, `tau_denom_min`, `use_transport`,
`transport_eps`.

**Source:** {ref}`Section 3.2 <sec-architecture-the-disentangled-vq-vae-rnn>`, `atlas.py`.
:::

:::{prf:definition} G.1.4 (PrimitiveTopologicalDecoder)
:label: def-g-decoder

**Class signature:**
```python
class PrimitiveTopologicalDecoder(nn.Module):
    def __init__(self, latent_dim: int, hidden_dim: int, num_charts: int, output_dim: int, ...):
        ...

    def forward(self, z_geo: torch.Tensor, z_tex: torch.Tensor | None = None, chart_index: torch.Tensor | None = None) -> Tuple[torch.Tensor, torch.Tensor]:
        ...
```

**Input/Output:**
- Input: `z_geo` shape `[B, D]`, optional `z_tex` shape `[B, D]`
- Output: `(x_hat, router_weights)` where `x_hat` is `[B, D_out]`

**Purpose:** Decodes charted geometry latents into reconstructions using chart projectors, a shared
renderer, and an optional texture residual path.

**Source:** {ref}`Section 3.2 <sec-architecture-the-disentangled-vq-vae-rnn>`, `atlas.py`.
:::

:::{prf:definition} G.1.5 (TopoEncoderPrimitives)
:label: def-g-macro-dynamics-model

**Class signature:**
```python
class TopoEncoderPrimitives(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int, latent_dim: int, num_charts: int, codes_per_chart: int, ...):
        ...

    def forward(self, x: torch.Tensor, use_hard_routing: bool = False) -> Tuple[torch.Tensor, ...]:
        ...
```

**Input/Output:**
- Input: `x` shape `[B, D_in]`
- Output: `(x_recon, vq_loss, enc_weights, dec_weights, K_chart, z_geo, z_n, c_bar)`

**Purpose:** Wrapper that couples encoder and decoder; exposes consistency loss and chart usage
perplexity helpers.

**Source:** {ref}`Section 3.2 <sec-architecture-the-disentangled-vq-vae-rnn>`, `atlas.py`.
:::

:::{prf:definition} G.1.6 (HierarchicalAtlasStack)
:label: def-g-disentangled-agent

**Purpose:** Multi-scale atlas stack that extends the TopoEncoder with multiple charted codebooks,
as defined in {ref}`Section 3.2 <sec-advanced-hierarchical-multi-scale-latents>` and Definition
{prf:ref}`def-hierarchical-latent`.

**Implementation sketch:**
- Shared feature extractor, multiple chart routers and codebooks
- Coarser levels update more slowly than fine levels
- Jump operator links charts across levels when enabled
:::

:::{prf:definition} G.1.7 (TopoEncoderAttachments)
:label: def-g-hierarchical-disentangled

Optional modules frequently attached to the TopoEncoder stack:

- `CovariantRetina` (feature extractor) – {prf:ref}`def-g-covariant-retina`
- `SoftEquivariantLayer` (metric) – {prf:ref}`def-g-soft-equivariant-layer`
- `FactorizedJumpOperator` (chart transitions)
- `InvariantChartClassifier` (detached readout)
:::

:::{prf:definition} G.2.1 (SupervisedTopologyLoss)
:label: def-g-supervised-topology-loss

**Class signature:**
```python
class SupervisedTopologyLoss(nn.Module):
    def __init__(
        self,
        num_charts: int,
        num_classes: int,
        lambda_purity: float = 0.1,
        lambda_balance: float = 0.01,
        lambda_metric: float = 0.01,
        margin: float = 1.0,
        temperature: float = 1.0,
    ):
        ...

    def forward(
        self,
        chart_assignments: torch.Tensor,  # [B, N_c] soft assignments
        class_labels: torch.Tensor,        # [B] ground truth classes
        embeddings: torch.Tensor,          # [B, D] latent embeddings
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        ...
```

**Input/Output:**
- Input:
  - `chart_assignments` shape `[B, N_c]` – Soft chart assignments (router weights)
  - `class_labels` shape `[B]` – Ground truth class labels
  - `embeddings` shape `[B, D]` – Latent embeddings
- Output: `(total_loss, loss_dict)` where `loss_dict` contains individual loss terms

**Purpose:** Enforces that each chart is dominated by a single class (purity), charts are used roughly equally (balance), and same-class samples are metrically closer (separation).

**Key parameters:**
- `num_charts` – Number of atlas charts $N_c$
- `num_classes` – Number of semantic classes $C$
- `lambda_purity` – Weight for chart purity loss (Definition {prf:ref}`def-purity-loss`)
- `lambda_balance` – Weight for chart balance loss
- `lambda_metric` – Weight for metric contrastive loss

**Learnable parameters:**
- `chart_to_class` shape `[N_c, C]` – Logits mapping charts to class probabilities

**Loss components:**
1. **Chart Purity:** $\mathcal{L}_{\text{purity}} = -\sum_k \max_c p(c|k) \log \max_c p(c|k)$
2. **Chart Balance:** $\mathcal{L}_{\text{balance}} = D_{\text{KL}}(\bar{p}(k) \| \text{Uniform})$
3. **Metric Contrastive:** Encourages intra-class proximity, inter-class separation

**Source:** {ref}`Section 25.4 <sec-the-supervised-topology-loss>`, Definition {prf:ref}`def-total-loss`, line 680.
:::

:::{prf:definition} G.2.2 (class_modulated_jump_rate)
:label: def-g-class-modulated-jump-rate

**Function signature:**
```python
def class_modulated_jump_rate(
    lambda_base: torch.Tensor,     # [N_c, N_c] base jump rates
    chart_to_class: torch.Tensor,  # [N_c, C] learnable logits
    gamma_sep: float = 5.0,        # Separation strength
) -> torch.Tensor:
    ...
```

**Input/Output:**
- Input:
  - `lambda_base` shape `[N_c, N_c]` – Base jump rate matrix
  - `chart_to_class` shape `[N_c, C]` – Chart-to-class mapping logits
  - `gamma_sep` – Separation strength coefficient
- Output: `lambda_sup` shape `[N_c, N_c]` – Class-modulated jump rates

**Purpose:** Computes class-consistent jump rates that suppress transitions between charts of different dominant classes, implementing the class-modulated rate from Definition {prf:ref}`def-class-consistent-jump-rate`.

**Mathematical operation:**
$$\lambda_{kk'}^{\text{sup}} = \lambda_{kk'}^{\text{base}} \cdot \exp(-\gamma_{\text{sep}} \cdot D_{\text{class}}(k, k'))$$

where $D_{\text{class}}(k, k') = 1$ if charts $k$ and $k'$ have different dominant classes, else $0$.

**Key parameters:**
- `gamma_sep` – Controls how strongly cross-class jumps are suppressed (higher = stronger suppression)

**Source:** {ref}`Section 25.3 <sec-metric-segmentation-via-jump-rate-modulation>`, Definition {prf:ref}`def-class-consistent-jump-rate`, line 445.
:::

:::{prf:definition} G.3.1 (LorentzianConfig)
:label: def-g-lorentzian-config

**Class signature:**
```python
@dataclass
class LorentzianConfig:
    d_model: int = 256        # Model dimension [nat]
    d_latent: int = 64        # Latent space dimension
    n_heads: int = 4          # Number of attention heads
    c_info: float = 1.0       # Information speed (latent units per timestep)
    T_c: float = 0.1          # Cognitive temperature [nat/step]
    gamma_friction: float = 1.0  # Friction coefficient for O-step
    dt: float = 0.01          # Integration timestep
```

**Purpose:** Configuration for Lorentzian memory attention with causal structure.

**Key parameters:**
- `c_info` – Information speed $c_{\text{info}}$ defining the light cone (Definition {prf:ref}`def-information-speed-recap`)
- `d_latent` – Dimension of the latent manifold $\mathcal{Z}$

**Units:** `d_model` and `d_latent` in [nat], `c_info` in [latent units/timestep], `T_c` in [nat/step].

**Source:** {ref}`Section 33 <sec-covariant-memory-attention-architecture>`, line 864.
:::

:::{prf:definition} G.3.2 (LorentzianMetric)
:label: def-g-lorentzian-metric

**Class signature:**
```python
class LorentzianMetric(nn.Module):
    def __init__(self, config: LorentzianConfig, epsilon: float = 1e-6):
        ...

    def conformal_factor(self, z: torch.Tensor) -> torch.Tensor:
        ...

    def geodesic_distance(self, z1: torch.Tensor, z2: torch.Tensor) -> torch.Tensor:
        ...

    def spacetime_interval(self, z: torch.Tensor, t: torch.Tensor,
                           z_mem: torch.Tensor, t_mem: torch.Tensor) -> torch.Tensor:
        ...

    def temperature(self, z: torch.Tensor, d_k: int) -> torch.Tensor:
        ...
```

**Input/Output:**
- `conformal_factor`: Input `z` shape `[B, d]` → Output `[B, 1]`
- `geodesic_distance`: Input `z1` shape `[B, d]`, `z2` shape `[B, N, d]` → Output `[B, N]`
- `spacetime_interval`: Input positions and times → Output `[B, N]` intervals
- `temperature`: Input `z` shape `[B, d]` → Output `[B, 1]`

**Purpose:** Implements the Lorentzian metric on the memory manifold $\mathcal{M} = \mathbb{R} \times \mathcal{Z}$ with signature $(-,+,\ldots,+)$.

**Key methods:**
- `conformal_factor`: $\lambda(z) = 2/(1-|z|^2)$ (Poincaré disk)
- `geodesic_distance`: $d_G(z, z') = \operatorname{arcosh}(1 + 2|z-z'|^2/((1-|z|^2)(1-|z'|^2)))$
- `spacetime_interval`: $\Delta s^2_{\text{eff}} = -c_{\text{info}}^2(t-t')^2 + d_G^2$ (Definition {prf:ref}`def-spacetime-interval`)
- `temperature`: $\tau(z) = \sqrt{d_k}/\lambda(z)$ (Theorem {prf:ref}`thm-metric-temperature-correspondence`)

**Source:** {ref}`Section 33 <sec-covariant-memory-attention-architecture>`, Definition {prf:ref}`def-lorentzian-memory-manifold`, line 886.
:::

:::{prf:definition} G.3.3 (CausalMask)
:label: def-g-causal-mask

**Class signature:**
```python
class CausalMask(nn.Module):
    def __init__(self, config: LorentzianConfig):
        ...

    def forward(
        self,
        z: torch.Tensor,       # [B, d] query position
        t: torch.Tensor,       # [B, 1] query time
        z_mem: torch.Tensor,   # [B, N, d] memory positions
        t_mem: torch.Tensor,   # [B, N, 1] memory times
    ) -> torch.Tensor:
        ...
```

**Input/Output:**
- Input: Query spacetime position $(z, t)$ and memory positions $(z_{\text{mem}}, t_{\text{mem}})$
- Output: `mask` shape `[B, N]` – Binary mask (1 = causal, 0 = acausal)

**Purpose:** Computes the causal mask from the light cone structure, enforcing that attention is zero outside the causal past $J^-(z, t)$.

**Mathematical operation:**
$$M_{\text{causal}}(z, t; z', t') = \mathbf{1}\left[ t' < t \text{ and } d_G(z, z') \leq c_{\text{info}}(t - t') \right]$$

**Key insight:** This is spacetime causality, not just temporal ordering. Events must be both in the past *and* within the light cone defined by the information speed $c_{\text{info}}$.

**Source:** {ref}`Section 33 <sec-covariant-memory-attention-architecture>`, Definition {prf:ref}`def-causal-past-light-cone`, line 978.
:::

:::{prf:definition} G.3.4 (TemporalChristoffelQuery)
:label: def-g-temporal-christoffel-query

**Class signature:**
```python
class TemporalChristoffelQuery(nn.Module):
    def __init__(self, d_in: int, d_out: int, d_latent: int):
        ...

    def forward(
        self,
        x: torch.Tensor,        # [B, d_in]
        z: torch.Tensor,        # [B, d_latent]
        t: torch.Tensor,        # [B, 1]
        v_feat: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        ...
```

**Input/Output:**
- Input: Features `x`, position `z`, time `t`, optional velocity features
- Output: `Q` shape `[B, d_out]` – Geodesic Query vector

**Purpose:** Extends the geodesic Query projection to include temporal Christoffel terms for the Lorentzian metric.

**Mathematical operation:**
$$Q_{\text{geo}}(x, z, t, v) = W_Q x + W_{Qz} z + W_{Qt} t + W_{Qv} v + W_{Q,\Gamma}(z, z) + W_{Q,t}(t, t) + W_{Q,zt}(z, t)$$

**Christoffel structure:** For the Lorentzian metric $g_{\mu\nu} = \text{diag}(-c^2\lambda^2, \lambda^2 I_d)$:
- Spatial: $\Gamma^k_{ij} = \frac{2}{1-|z|^2}(\delta^k_i z_j + \delta^k_j z_i - \delta_{ij} z^k)$
- Time-time-space: $\Gamma^0_{0j} = \frac{2z_j}{1-|z|^2}$
- Space-time-time: $\Gamma^k_{00} = \frac{2c^2 z_k}{1-|z|^2}$

**Source:** {ref}`Section 33 <sec-covariant-memory-attention-architecture>`, Definition {prf:ref}`def-temporal-christoffel-encoding`, line 1021.
:::

:::{prf:definition} G.3.5 (LorentzianMemoryAttention)
:label: def-g-lorentzian-memory-attention

**Class signature:**
```python
class LorentzianMemoryAttention(nn.Module):
    def __init__(self, config: LorentzianConfig):
        ...

    def forward(
        self,
        x: torch.Tensor,         # [B, d_model] current state features
        z: torch.Tensor,         # [B, d_latent] current position
        t: torch.Tensor,         # [B, 1] current time
        x_mem: torch.Tensor,     # [B, N, d_model] memory features
        z_mem: torch.Tensor,     # [B, N, d_latent] memory positions
        t_mem: torch.Tensor,     # [B, N, 1] memory times
        v_feat: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        ...
```

**Input/Output:**
- Input: Current state $(x, z, t)$ and memory bank $(x_{\text{mem}}, z_{\text{mem}}, t_{\text{mem}})$
- Output: `(output, weights)` where:
  - `output` shape `[B, d_model]` – Attended memory representation
  - `weights` shape `[B, N]` – Attention weights (for diagnostics)

**Purpose:** Full Lorentzian memory attention combining covariant self-attention with causal mask. Implements Definition {prf:ref}`def-covariant-self-attention-causal` and Definition {prf:ref}`def-lorentzian-cross-attention`.

**Components:**
- `metric` – LorentzianMetric for conformal factor and geodesic distance
- `causal_mask` – CausalMask for light cone enforcement
- `query` – TemporalChristoffelQuery for geodesic Query
- `wilson_scale` – Learnable Wilson line approximation scale

**Key properties:**
1. **Causality:** Attention weight is zero outside $J^-(z, t)$
2. **Gauge covariance:** Wilson line preprocessing ensures gauge invariance
3. **Metric-encoded temperature:** $\tau(z) = \sqrt{d_k}/\lambda(z)$

**Diagnostic nodes:** Monitor with Nodes 71-73 (CausalMaskCheck, RetardedPotentialCheck, LorentzianSignatureCheck).

**Source:** {ref}`Section 33 <sec-covariant-memory-attention-architecture>`, line 1095.
:::

:::{prf:definition} G.4.1 (GeodesicConfig)
:label: def-g-geodesic-config

**Class signature:**
```python
@dataclass
class GeodesicConfig:
    d_model: int = 256         # Model dimension [nat]
    d_latent: int = 64         # Latent space dimension
    n_heads: int = 1           # Number of attention heads per BAOAB step
    T_c: float = 0.1           # Cognitive temperature [nat/step]
    gamma_friction: float = 1.0  # Friction coefficient for O-step
    dt: float = 0.01           # Integration timestep
    g_s: float = 1.0           # Binding coupling strength
    g_2: float = 0.5           # Error field coupling
    g_1: float = 0.3           # Opportunity field coupling
    use_learned_thermostat: bool = False  # Enable learned thermostat residual
    thermostat_residual_scale: float = 0.1  # Scale for learned residual
```

**Purpose:** Configuration for the gauge-covariant geodesic cross-attention world model.

**Key parameters:**
- `g_s` – $SU(N_f)_C$ binding coupling (confinement)
- `g_2` – $SU(2)_L$ error field coupling (chirality)
- `g_1` – $U(1)_Y$ opportunity field coupling (hypercharge)
- `use_learned_thermostat` – If True, adds a learnable thermostat head; otherwise uses closed-form OU

**Units:** Couplings $g_s, g_2, g_1$ are dimensionless.

**Source:** {ref}`Section 35 <sec-covariant-cross-attention-architecture>`, line 1146.
:::

:::{prf:definition} G.4.2 (WilsonLineApprox)
:label: def-g-wilson-line-approx

**Class signature:**
```python
class WilsonLineApprox(nn.Module):
    def __init__(self, config: GeodesicConfig, d_k: int):
        ...

    def forward(
        self,
        z_query: torch.Tensor,  # [B, d_latent]
        z_key: torch.Tensor,    # [B, N, d_latent]
    ) -> torch.Tensor:
        ...
```

**Input/Output:**
- Input: Query position `z_query` and key positions `z_key`
- Output: `U` shape `[B, N, d_k, d_k]` – Transformation matrices for each key

**Purpose:** Computes the linearized Wilson line $U(z, z') \approx I - i A_\mu(z)(z - z')^\mu$ for parallel transport in attention.

**Learnable parameters:**
- `theta_binding` – $SU(N_f)_C$ connection coefficients
- `theta_error` – $SU(2)_L$ connection coefficients
- `theta_opportunity` – $U(1)_Y$ connection coefficient

**Mathematical operation:**
$$U(z, z') \approx I - i\Theta(z) \cdot (z - z')$$

where $\Theta$ encodes the total gauge connection $A_\mu = g_s G_\mu + g_2 W_\mu + g_1 B_\mu$.

**Source:** {ref}`Section 35 <sec-covariant-cross-attention-architecture>`, Proposition {prf:ref}`prop-wilson-line-approximation`, line 1176.
:::

:::{prf:definition} G.4.3 (ConformalMetric)
:label: def-g-conformal-metric

**Class signature:**
```python
class ConformalMetric(nn.Module):
    def __init__(self, epsilon: float = 1e-6):
        ...

    def conformal_factor(self, z: torch.Tensor) -> torch.Tensor:
        ...

    def metric(self, z: torch.Tensor) -> torch.Tensor:
        ...

    def metric_inv(self, z: torch.Tensor) -> torch.Tensor:
        ...

    def temperature(self, z: torch.Tensor, d_k: int) -> torch.Tensor:
        ...
```

**Input/Output:**
- `conformal_factor`: `z` shape `[B, d]` → `[B, 1]`
- `metric`: `z` shape `[B, d]` → `[B, d, d]`
- `metric_inv`: `z` shape `[B, d]` → `[B, d, d]`
- `temperature`: `z` shape `[B, d]` → `[B, 1]`

**Purpose:** Computes the Poincaré disk metric and its derived quantities.

**Key formulas:**
- Conformal factor: $\lambda(z) = 2/(1-|z|^2)$
- Metric: $G_{ij}(z) = \lambda(z)^2 \delta_{ij}$
- Inverse metric: $G^{ij}(z) = \lambda(z)^{-2} \delta^{ij}$
- Temperature: $\tau(z) = \sqrt{d_k}/\lambda(z)$

**Boundary behavior:** As $|z| \to 1$, $\lambda \to \infty$ and $\tau \to 0$, making attention infinitely sharp and preventing boundary crossing.

**Source:** {ref}`Section 35 <sec-covariant-cross-attention-architecture>`, Definition {prf:ref}`def-poincare-metric-recap`, line 1248.
:::

:::{prf:definition} G.4.4 (ChristoffelQuery)
:label: def-g-christoffel-query

**Class signature:**
```python
class ChristoffelQuery(nn.Module):
    def __init__(self, d_in: int, d_out: int, d_latent: int):
        ...

    def forward(
        self,
        x: torch.Tensor,         # [B, d_in] feature vector
        z_geom: torch.Tensor,    # [B, d_latent] position
        v_feat: Optional[torch.Tensor] = None,  # velocity features
        v_geom: Optional[torch.Tensor] = None,  # velocity
    ) -> torch.Tensor:
        ...
```

**Input/Output:**
- Input: Features `x`, position `z_geom`, optional velocity features
- Output: `Q` shape `[B, d_out]` – Geodesic Query vector

**Purpose:** Implements the geodesic Query projection encoding Christoffel symbols via linear + quadratic terms.

**Mathematical operation:**
$$Q_{\text{geo}}(x, z, v) = W_Q x + W_{Qz} z + W_{Qv} v_{\text{feat}} + W_{Q,\Gamma}(z, z) + W_{Qzv}(z, v)$$

**Learnable parameters:**
- `W_Q` – Feature projection
- `W_Qz` – Position projection (captures linear part of $\Gamma$)
- `W_Qv` – Velocity feature projection
- `W_Q_gamma` – Quadratic tensor for Christoffel encoding
- `W_Qzv` – Position-velocity coupling

**Initialization:** `W_Q_gamma` is initialized with Poincaré-inspired structure to approximate $\Gamma^k_{ij} \propto (\delta^k_i z_j + \delta^k_j z_i - \delta_{ij} z^k)$.

**Source:** {ref}`Section 35 <sec-covariant-cross-attention-architecture>`, Definition {prf:ref}`def-geodesic-query-projection`, line 1311.
:::

:::{prf:definition} G.4.5 (ChiralProjector)
:label: def-g-chiral-projector

**Class signature:**
```python
class ChiralProjector(nn.Module):
    def __init__(self, d_latent: int):
        ...

    def forward(
        self,
        psi_doublet: torch.Tensor,  # [B, 2, d] observation-action doublet
        grad_V: torch.Tensor,        # [B, d_latent] value gradient
    ) -> torch.Tensor:
        ...
```

**Input/Output:**
- Input: Doublet `psi_doublet` shape `[B, 2, d]` and value gradient `grad_V`
- Output: Gated projected doublet shape `[B, 2*d]`

**Purpose:** Implements the $SU(2)_L$ chiral projector that extracts committed actions from the observation-action doublet using the value gradient direction.

**Mathematical operation:**
$$\hat{n}(z) = \frac{P \nabla_A V}{\|P \nabla_A V\|}, \quad \Pi_{\text{chirality}} = \frac{1}{2}(I_2 + \hat{n} \cdot \vec{\tau})$$

where $\vec{\tau} = (\tau_1, \tau_2, \tau_3)$ are Pauli matrices.

**Key insight:** The projection extracts the component of the doublet aligned with the value gradient—the direction of improvement. When $\nabla_A V \approx 0$ (flat landscape), the projector is degenerate, encoding decision ambiguity.

**Gauge covariance:** The commitment strength $c(z) = \Psi_L^\dagger \Pi \Psi_L$ is $SU(2)$-invariant (Theorem {prf:ref}`thm-gauge-covariance-chiral-projection`).

**Source:** {ref}`Section 35 <sec-covariant-cross-attention-architecture>`, Definition {prf:ref}`def-chiral-projector-value-gradient`, line 1384.
:::

:::{prf:definition} G.4.6 (AreaLawScreening)
:label: def-g-area-law-screening

**Class signature:**
```python
class AreaLawScreening(nn.Module):
    def __init__(self, config: GeodesicConfig):
        ...

    def string_area(
        self,
        z_query: torch.Tensor,
        z_key: torch.Tensor,
        lambda_z: torch.Tensor,
    ) -> torch.Tensor:
        ...

    def forward(
        self,
        attention: torch.Tensor,  # [B, N] attention scores
        z_query: torch.Tensor,
        z_key: torch.Tensor,
        lambda_z: torch.Tensor,
        level: int = 0,
    ) -> torch.Tensor:
        ...
```

**Input/Output:**
- Input: Attention weights, positions, conformal factor, hierarchy level
- Output: Screened attention shape `[B, N]`

**Purpose:** Implements $SU(N_f)_C$ area law screening for texture confinement. Suppresses attention between positions at different representation levels.

**Mathematical operation:**
$$\alpha_{\text{screened}} = \alpha \cdot \exp(-\sigma(\ell) \cdot A_{\text{string}})$$

where:
- $\sigma(\ell) = \sigma_0 \cdot e^{-\ell/L}$ is the level-dependent string tension
- $A_{\text{string}} \approx \frac{\lambda^2}{2}|z - z'|^2$ is the minimal string area

**Asymptotic freedom:** At texture level ($\ell = L$), $\sigma \to 0$ and features interact freely. At macro level ($\ell = 0$), $\sigma$ is large and texture is confined.

**Source:** {ref}`Section 35 <sec-covariant-cross-attention-architecture>`, Definition {prf:ref}`def-area-law-screening-attention`, Theorem {prf:ref}`thm-texture-confinement-area-law`, line 1438.
:::

:::{prf:definition} G.4.7 (CovariantAttention)
:label: def-g-covariant-attention

**Class signature:**
```python
class CovariantAttention(nn.Module):
    def __init__(
        self,
        config: GeodesicConfig,
        use_chirality: bool = False,
        use_screening: bool = False,
        head_type: str = 'generic',  # 'B', 'A', 'O', or 'generic'
    ):
        ...

    def forward(
        self,
        z_query: torch.Tensor,
        z_key: torch.Tensor,
        x_query: torch.Tensor,
        x_key: torch.Tensor,
        x_value: torch.Tensor,
        v_query: Optional[torch.Tensor] = None,
        v_query_geom: Optional[torch.Tensor] = None,
        grad_V: Optional[torch.Tensor] = None,
        level: int = 0,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        ...
```

**Input/Output:**
- Input: Query/key positions and features, optional velocity and value gradient
- Output: `(output, attention)` where:
  - `output` shape `[B, d_model]` – Attention output
  - `attention` shape `[B, N]` – Attention weights

**Purpose:** Single covariant attention head combining all gauge structures: Wilson lines, position-dependent temperature, Christoffel Query, chiral projection, and area law screening.

**Components:**
- `query` – ChristoffelQuery
- `wilson` – WilsonLineApprox
- `metric` – ConformalMetric
- `chiral` – ChiralProjector (optional)
- `screening` – AreaLawScreening (optional)

**Attention computation:**
1. Compute $Q$ with geodesic Query projection
2. Compute $K$ and apply Wilson line: $K_{\text{transported}} = U \cdot K$
3. Score: $s = Q^T K_{\text{transported}} / \tau(z)$
4. Softmax and optional screening
5. Weighted sum of $V$, optional chiral projection

**Source:** {ref}`Section 35 <sec-covariant-cross-attention-architecture>`, line 1505.
:::

:::{prf:definition} G.4.8 (GeodesicCrossAttention)
:label: def-g-geodesic-cross-attention

**Class signature:**
```python
class GeodesicCrossAttention(nn.Module):
    def __init__(self, config: GeodesicConfig):
        ...

    def forward(
        self,
        z: torch.Tensor,             # [B, d_latent] current position
        p: torch.Tensor,             # [B, d_latent] current momentum
        context_z: torch.Tensor,     # [B, N, d_latent] context positions
        context_x: torch.Tensor,     # [B, N, d_model] context features
        context_force: torch.Tensor, # [B, N, d_latent] force/gradient bank
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        ...
```

**Input/Output:**
- Input: Current phase space state $(z, p)$ and context banks
- Output: `(z_next, p_next)` – Updated position and momentum

**Purpose:** Full geodesic world model implementing Boris-BAOAB integration via four attention heads (B-A-A-B) plus a closed-form OU thermostat (or optional learned thermostat head).

**BAOAB Steps:**
1. **Head 1 (B-step):** First half-kick from force bank
2. **Head 2 (A-step):** First half-drift + attention correction
3. **OU step:** Ornstein-Uhlenbeck thermostat (closed-form, or learned residual)
4. **Head 4 (A-step):** Second half-drift + attention correction
5. **Head 5 (B-step):** Second half-kick from force bank

**OU coefficients:**
$$c_1 = e^{-\gamma h}, \quad c_2 = \sqrt{(1-c_1^2)T_c}$$

**Boltzmann preservation:** Preserves $\rho(z, p) \propto \exp(-\Phi_{\text{eff}}/T_c - \|p\|_G^2/(2T_c))$ to $O(h^2)$ (Theorem {prf:ref}`thm-baoab-attention-boltzmann`).

**Diagnostic nodes:** Monitor with Nodes 67-70 (gauge, temperature, chirality, confinement).

**Source:** {ref}`Section 35 <sec-covariant-cross-attention-architecture>`, Definition {prf:ref}`def-baoab-attention-heads`, line 1616.
:::

:::{prf:definition} G.5.1 (SpectralLinear)
:label: def-g-spectral-linear

**Class signature:**
```python
class SpectralLinear(nn.Module):
    def __init__(self, in_features: int, out_features: int, bias: bool = False):
        ...
```

**Input/Output:**
- Input: `x` shape `[B, in_features]` – Feature vectors
- Output: `y` shape `[B, out_features]` – Transformed features

**Purpose:** Linear layer with spectral normalization $\sigma_{\max}(W) \leq 1$. Ensures capacity bound and light cone preservation for causal structure.

**Key parameters:**
- `in_features` – Input dimension [nat]
- `out_features` – Output dimension [nat]
- `bias` – Typically `False` for gauge invariance (breaks tangent bundle structure)

**Mathematical operation:**
$$y = W_{\text{normalized}} \cdot x \quad \text{where} \quad \sigma_{\max}(W_{\text{normalized}}) \leq 1$$

**Key properties:**
- Contraction: $\|y\| \leq \|x\|$ (no unbounded amplification)
- Light cone preservation: $d(Wz_1, Wz_2) \leq c_{\text{info}} \Delta t$ whenever inputs are causally connected
- No bias term (gauge invariance requirement)

**Diagnostic node:** Node 62 (CausalityViolationCheck) verifies $\sigma_{\max}(W) \leq 1 + \epsilon$ during training.

**Source:** {ref}`Section 04 <sec-dnn-blocks>`, Definition {prf:ref}`def-spectral-linear`, line 569.
:::

:::{prf:definition} G.5.2 (NormGatedActivation)
:label: def-g-norm-gated-activation

**Function signature:**
```python
def norm_gated_activation(v: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """
    Args:
        v: Bundle vectors [B, n_bundles, bundle_dim]
        b: Bias scalars [n_bundles]
    Returns:
        Gated vectors [B, n_bundles, bundle_dim]
    """
    norms = torch.norm(v, dim=-1, keepdim=True)  # [B, n_bundles, 1]
    gates = F.gelu(norms.squeeze(-1) + b)        # [B, n_bundles]
    return v * gates.unsqueeze(-1) / (norms + 1e-8)
```

**Input/Output:**
- Input: `v` shape `[B, n_bundles, d_b]` – Bundle vectors
- Output: Gated vectors shape `[B, n_bundles, d_b]` – Energy-filtered output

**Purpose:** $SO(d_b)$-equivariant activation using radial symmetry. Gates signal based on energy $\|v\|$ exceeding threshold $-b$.

**Mathematical operation:**
$$f(v_i) = v_i \cdot g(\|v_i\| + b_i)$$

where:
- $\|v_i\| = \sqrt{v_i^T v_i}$ is the Euclidean norm (rotation-invariant)
- $g: \mathbb{R} \to \mathbb{R}$ is GELU or another smooth scalar function
- $b_i$ is the learnable activation potential (energy barrier)

**Key properties:**
- **$SO(d_b)$ equivariance:** $f(Rv) = R f(v)$ for all $R \in SO(d_b)$
- **Physical interpretation:** Energy barrier—gate opens when $\|v\| > -b$
- **Direction independence:** Gate decision depends only on magnitude, not orientation

**GELU rationale:**
- $C^\infty$ smoothness (compatible with WFR metric)
- Linear growth at large arguments: $g(x) \approx x$ for $x \gg 1$
- Controlled Lipschitz constant $L_g \approx 1.129$
- Empirically effective (validated in transformers)

**Alternative activations:** Softplus ($C^\infty$, always positive), Sigmoid/Tanh (saturate, reduced dynamic range).

**Source:** {ref}`Section 04 <sec-dnn-blocks>`, Definition {prf:ref}`def-norm-gated-activation`, line 714.
:::

:::{prf:definition} G.5.3 (IsotropicBlock)
:label: def-g-isotropic-block

**Class signature:**
```python
class IsotropicBlock(nn.Module):
    def __init__(
        self,
        in_dim: int,
        out_dim: int,
        bundle_size: int = 16,
        exact: bool = False
    ):
        ...
```

**Input/Output:**
- Input: `z` shape `[B, in_dim]` – Input features
- Output: `z_out` shape `[B, out_dim]` – Transformed features

**Purpose:** Atomic gauge-covariant building block combining SpectralLinear, Reshape, and NormGate in sequence.

**Architecture:**
$$\text{IsotropicBlock}(z) = \text{NormGate}(\text{Reshape}(\text{SpectralLinear}(z)))$$

**Key parameters:**
- `in_dim` – Input dimension [nat]
- `out_dim` – Output dimension (must be divisible by `bundle_size`) [nat]
- `bundle_size` – Dimension of each bundle $d_b$ [nat]
- `exact` – If `True`, uses scalar blocks $W_i = \lambda_i I_{d_b}$ for exact equivariance; if `False` (default), uses block-diagonal for approximate equivariance

**Equivariance modes:**
- **Exact mode** (`exact=True`): Strictly $\prod_{i=1}^{n_b} SO(d_b)$ equivariant via scalar blocks
  - Weight matrix: $W = \text{diag}(\lambda_1 I, \ldots, \lambda_{n_b} I)$
  - Limited expressiveness (can only scale bundles)
  - Zero equivariance violation

- **Approximate mode** (`exact=False`): Bounded equivariance violation, greater expressiveness
  - Weight matrix: Block-diagonal with general $d_b \times d_b$ blocks
  - Each block spectrally normalized: $\sigma_{\max}(W_i) \leq 1$
  - Can learn within-bundle transformations

**Mathematical constraint (exact mode):**
By Schur's lemma, any linear map commuting with all $g \in SO(d_b)$ must be a scalar multiple of identity:
$$W_i \cdot g_i = g_i \cdot W_i \quad \forall g_i \in SO(d_b) \quad \Rightarrow \quad W_i = \lambda_i I_{d_b}$$

**Diagnostic nodes:** Node 67 (GaugeInvarianceCheck), Node 62 (CausalityViolationCheck), Node 40 (PurityCheck).

**Source:** {ref}`Section 04 <sec-dnn-blocks>`, Definition {prf:ref}`def-isotropic-block`, line 803.
:::

:::{prf:definition} G.5.4 (GaugeInvarianceCheck)
:label: def-g-gauge-invariance-check

**Class signature:**
```python
class GaugeInvarianceCheck(DiagnosticNode):
    def __init__(self, layer: nn.Module, group: str = "SO(d)"):
        ...

    def check(self, z: torch.Tensor) -> Dict[str, float]:
        ...
```

**Input/Output:**
- Input: `z` shape `[B, d]` – Latent state
- Output: Dictionary with `gauge_violation`, `threshold`, `passed` keys

**Purpose:** Diagnostic node (Node 67) that verifies $G$-equivariance by sampling random group transformations and measuring violation.

**Mathematical test:**
$$\delta_{\text{gauge}} = \|f(g \cdot z) - g \cdot f(z)\| < \epsilon_{\text{gauge}}$$

where $g$ is a randomly sampled group element (e.g., rotation matrix for $SO(d)$).

**Key parameters:**
- `layer` – The module to test
- `group` – Symmetry group ("SO(d)" for rotations)
- Threshold: $\epsilon_{\text{gauge}} = 10^{-4}$ (exact equivariance) or $\epsilon_{\text{gauge}} \approx 0.1$ (soft equivariance)

**Failure modes:**
- Large violation ($\delta > 0.1$): Symmetry breaking without L1 regularization
- Asymmetric violation: Equivariant under some $g$ but not others (indicates partial symmetry)

**Source:** {ref}`Section 04 <sec-dnn-blocks>`, line 2908.
:::

:::{prf:definition} G.5.5 (CovariantRetina)
:label: def-g-covariant-retina

**Class signature:**
```python
class CovariantRetina(nn.Module):
    def __init__(
        self,
        in_channels: int = 3,
        out_dim: int = 512,
        num_rotations: int = 8,
        kernel_size: int = 5
    ):
        ...
```

**Input/Output:**
- Input: `x` shape `[B, C, H, W]` – RGB images
- Output: `z` shape `[B, out_dim]` – Latent features

**Purpose:** $SO(2)$-equivariant vision encoder using steerable convolutions (via E2CNN library). Ensures rotation equivariance for visual inputs.

**Architecture:**
1. **Lifting layer:** Maps trivial representation (standard image) to regular representation on $SE(2)$
2. **Steerable convolutions:** 3 layers with expanding channels (32 → 64 → 64)
3. **Group pooling:** Max over rotation group to extract rotation-invariant features
4. **Spatial pooling:** Adaptive average pooling to fixed size
5. **Linear projection:** Spectral-normalized fully connected layer to latent dimension

**Key parameters:**
- `in_channels` – Input channels (3 for RGB)
- `out_dim` – Output latent dimension [nat]
- `num_rotations` – Discretization of $SO(2)$ (typically 8 or 16)
- `kernel_size` – Convolutional kernel size [pixels]

**Equivariance guarantee:**
$$\text{Conv}(R_\theta \cdot I) = D^{(\ell)}(\theta) \cdot \text{Conv}(I)$$

where $R_\theta$ is a rotation by angle $\theta$ and $D^{(\ell)}$ is the representation matrix.

**Diagnostic node:** Node 68 (RotationEquivarianceCheck) verifies $\|f(R \cdot I) - R \cdot f(I)\| < \epsilon$ for random rotations.

**Source:** {ref}`Section 04 <sec-dnn-blocks>`, line 1464.
:::

:::{prf:definition} G.6.1 (UGNConfig / BundleConfig)
:label: def-g-ugn-config

**Class signatures:**
```python
@dataclass
class BundleConfig:
    name: str              # Semantic label (e.g., "charge", "lepton")
    dim: int               # Bundle dimension d_b [dimensionless]
    semantic_role: str = ""  # Physical interpretation

@dataclass
class UGNConfig:
    input_dim: int         # Input dimension [dimensionless]
    output_dim: int        # Output dimension [dimensionless]
    bundles: List[BundleConfig]  # Bundle specifications
    n_latent_layers: int = 4     # Number of soft equivariant layers
    encoder_hidden_dim: int = 256
    decoder_hidden_dim: int = 256
    lambda_l1: float = 0.01      # L1 regularization strength
    lambda_equiv: float = 0.0    # Equivariance penalty
    use_spectral_norm: bool = True
```

**Purpose:** Configuration dataclasses for the three-stage Universal Geometric Network architecture.

**Key properties:**
- `n_bundles` – Number of gauge bundles (computed from `bundles` list)
- `total_latent_dim` – $\sum_{i=1}^{n_b} d_i$
- `bundle_dims` – List of bundle dimensions $[d_1, \ldots, d_{n_b}]$

**Typical bundle structure:**
```python
bundles = [
    BundleConfig(name="color", dim=64, semantic_role="Binding/texture confinement"),
    BundleConfig(name="isospin", dim=8, semantic_role="Error field/chirality"),
    BundleConfig(name="hypercharge", dim=4, semantic_role="Opportunity field/capacity"),
]
```

**Units:** All dimensions [nat] or [dimensionless], loss weights [dimensionless].

**Source:** {ref}`Section 06 <sec-universal-geometric-network>`, lines 1180, 1873.
:::

:::{prf:definition} G.6.2 (SoftEquivariantLayer)
:label: def-g-soft-equivariant-layer

**Class signature:**
```python
class SoftEquivariantLayer(nn.Module):
    def __init__(
        self,
        bundle_dims: List[int],
        hidden_dim: int = 64,
        use_spectral_norm: bool = True
    ):
        ...
```

**Input/Output:**
- Input: `z` shape `[B, sum(bundle_dims)]` – Latent state
- Output: `z_out` shape `[B, sum(bundle_dims)]` – Updated latent state

**Purpose:** Core latent dynamics layer combining equivariant and mixing pathways with L1 regularization for emergent structure discovery.

**Architecture:**
$$z_{\text{out}} = z + f_{\text{equiv}}(z) + g \cdot f_{\text{mix}}(z)$$

where:
- **Equivariant pathway:** $f_{\text{equiv}}(z) = v_i \cdot \phi_i(\|v_1\|, \ldots, \|v_{n_b}\|)$
  - Uses only bundle norms → strictly $\prod_i SO(d_i)$ equivariant
  - Implemented via norm MLP: $\mathbb{R}^{n_b} \to \mathbb{R}^{n_b}$

- **Mixing pathway:** $f_{\text{mix}}(z) = \sum_{i,j} W_{ij} v_j$
  - Cross-bundle interactions with learnable weights
  - L1 penalized: $\mathcal{L}_{\text{L1}} = \sum_{i,j} \|W_{ij}\|_1$
  - Encouraged to be sparse (emergent texture zeros)

**Key parameters:**
- `bundle_dims` – List $[d_1, \ldots, d_{n_b}]$ of bundle dimensions
- `hidden_dim` – Hidden dimension for norm MLP
- `use_spectral_norm` – Apply spectral normalization to all linear layers

**Learnable parameters:**
- Norm MLP weights: $O(n_b \cdot h + h^2)$ parameters
- Mixing weights $W_{ij}$: $O(n_b^2 d_{\max}^2)$ parameters (largest memory consumer)
- Gate biases: $n_b$ scalars

**L1 loss:**
```python
def l1_loss(self) -> torch.Tensor:
    return sum(
        torch.sum(torch.abs(self.mixing_weights[i][j]))
        for i in range(n_b) for j in range(n_b)
    )
```

**Diagnostic methods:**
- `mixing_strength()` – Total Frobenius norm of mixing weights (measures symmetry breaking)

**Source:** {ref}`Section 06 <sec-universal-geometric-network>`, lines 1219 (simplified), 1970 (production).
:::

:::{prf:definition} G.6.3 (UniversalGeometricNetwork)
:label: def-g-universal-geometric-network

**Class signature:**
```python
class UniversalGeometricNetwork(nn.Module):
    def __init__(self, config: UGNConfig):
        ...
```

**Input/Output:**
- Input: `x` shape `[B, input_dim]` – Raw observations
- Output: `y` shape `[B, output_dim]` – Predictions/actions

**Purpose:** Three-stage architecture achieving both universal approximation and geometric consistency.

**Architecture:**
1. **Encoder** (unconstrained, universal):
   - $E: \mathbb{R}^{d_{\text{in}}} \to \bigoplus_i V_i$
   - 2-3 spectral-normalized linear layers with GELU
   - **Chooses gauge** for latent representation

2. **Latent Dynamics** (soft equivariant):
   - $D_1, \ldots, D_L: \bigoplus_i V_i \to \bigoplus_i V_i$
   - Stack of `SoftEquivariantLayer` modules
   - **Respects bundle structure** via equivariant pathway + L1-regularized mixing

3. **Decoder** (unconstrained, universal):
   - $P: \bigoplus_i V_i \to \mathbb{R}^{d_{\text{out}}}$
   - 2-3 spectral-normalized linear layers with GELU
   - **Interprets gauge** to extract observables

**Key methods:**
```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    z = self.encode(x)      # Encoder
    z = self.dynamics(z)    # Latent layers
    y = self.decode(z)      # Decoder
    return y

def regularization_loss(self) -> torch.Tensor:
    # L1 penalty on all mixing weights
    return sum(layer.l1_loss() for layer in self.latent_layers)

def equivariance_violation(self, z=None, n_samples=16) -> torch.Tensor:
    # Measure ||D(Rz) - RD(z)||² for random rotations
    ...
```

**Total loss:**
$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{task}} + \lambda_{\text{L1}} \mathcal{L}_{\text{L1}} + \lambda_{\text{equiv}} \mathcal{L}_{\text{equiv}}$$

**Key theorems:**
- Universal approximation (encoder/decoder handle arbitrary functions)
- Geometric consistency (latent dynamics respect bundle structure)
- Emergent gauge structure (L1 discovers texture zeros)

**Source:** {ref}`Section 06 <sec-universal-geometric-network>`, lines 1335 (simplified), 2154 (production).
:::

:::{prf:definition} G.6.4 (FactoredTensorLayer)
:label: def-g-factored-tensor-layer

**Class signature:**
```python
class FactoredTensorLayer(nn.Module):
    def __init__(
        self,
        d_C: int,
        d_L: int,
        d_Y: int,
        rank: int,
        d_out: int
    ):
        ...
```

**Input/Output:**
- Input: `(z_C, z_L, z_Y)` shapes `[B, d_C]`, `[B, d_L]`, `[B, d_Y]`
- Output: `y` shape `[B, d_out]`

**Purpose:** Low-rank factorization of tensor product interaction for cross-gauge coupling.

**Mathematical operation:**
$$W = \sum_{k=1}^r U_C^{(k)} \otimes U_L^{(k)} \otimes U_Y^{(k)}$$

instead of full tensor $W \in \mathbb{R}^{(d_C d_L d_Y) \times d_{\text{out}}}$.

**Parameter count:**
- Factored: $r(d_C + d_L + d_Y + d_{\text{out}})$
- Full tensor: $(d_C \times d_L \times d_Y) \times d_{\text{out}}$

**Example reduction:**
For $d_C=64, d_L=8, d_Y=4, d_{\text{out}}=64, r=16$:
- Factored: 2,240 parameters
- Full: 131,072 parameters
- **58.5× reduction**

**Use case:** Specific cross-gauge interactions when low-rank structure is empirically justified. Not used in default UGN (uses direct sum instead).

**Source:** {ref}`Section 06 <sec-universal-geometric-network>`, line 381.
:::

:::{prf:definition} G.6.5 (NormInteractionLayer)
:label: def-g-norm-interaction-layer

**Class signature:**
```python
class NormInteractionLayer(nn.Module):
    def __init__(self, n_bundles: int, hidden_dim: int = 64):
        ...
```

**Input/Output:**
- Input: `z` shape `[B, n_bundles, bundle_dim]` – Bundle representation
- Output: `z_out` shape `[B, n_bundles, bundle_dim]` – Scaled bundles

**Purpose:** Level 1 cross-bundle interaction using only bundle norms (strictly equivariant).

**Mathematical operation:**
$$f_i(v_1, \ldots, v_{n_b}) = v_i \cdot \phi_i(\|v_1\|, \ldots, \|v_{n_b}\|)$$

where $\phi: \mathbb{R}^{n_b} \to \mathbb{R}_+$ is an MLP with Softplus output.

**Equivariance:** Strictly $\prod_{i=1}^{n_b} SO(d_b)_i$ equivariant (per-bundle rotations).

**Expressiveness:** Limited—can only scale bundles based on energy, cannot represent direction-dependent interactions.

**Computational cost:** $O(n_b d_b + h^2)$ where $h$ is MLP hidden dimension.

**Source:** {ref}`Section 06 <sec-universal-geometric-network>`, line 446.
:::

:::{prf:definition} G.6.6 (GramInteractionLayer)
:label: def-g-gram-interaction-layer

**Class signature:**
```python
class GramInteractionLayer(nn.Module):
    def __init__(self, n_bundles: int, hidden_dim: int = 64):
        ...
```

**Input/Output:**
- Input: `z` shape `[B, n_bundles, bundle_dim]` – Bundle representation
- Output: `z_out` shape `[B, n_bundles, bundle_dim]` – Scaled bundles

**Purpose:** Level 2 cross-bundle interaction using Gram matrix $G_{ij} = \langle v_i, v_j \rangle$ (encodes relative orientations).

**Mathematical operation:**
$$G = z \cdot z^T \quad \text{(Gram matrix)}$$
$$\text{scales} = \phi(G_{\text{flat}}) \quad \text{(MLP)}$$
$$z_{\text{out}} = z \cdot \text{scales}$$

**Equivariance:** Equivariant under **global** $SO(d_b)$ (same rotation applied to all bundles), **not** under per-bundle rotations.

**Expressiveness:** High—can encode relative orientations between bundles.

**Computational cost:** $O(n_b^2 d_b + h^2)$.

**Source:** {ref}`Section 06 <sec-universal-geometric-network>`, line 494.
:::

:::{prf:definition} G.6.7 (L1Scheduler / AdaptiveL1Scheduler)
:label: def-g-l1-scheduler

**Class signature:**
```python
class AdaptiveL1Scheduler:
    def __init__(
        self,
        initial_lambda: float = 0.01,
        target_violation: float = 0.22,
        learning_rate: float = 0.05,
        min_lambda: float = 1e-4,
        max_lambda: float = 1.0
    ):
        ...

    def step(self, current_violation: float) -> float:
        ...
```

**Purpose:** Adaptive scheduler for L1 regularization strength $\lambda_{\text{L1}}$ that targets a specific equivariance violation level.

**Update rule:**
$$\lambda_{\text{L1}}(t+1) = \lambda_{\text{L1}}(t) \cdot \left(1 + \alpha \cdot (\epsilon(t) - \epsilon_{\text{target}})\right)$$

where:
- $\epsilon(t) = \mathcal{L}_{\text{equiv}}(t)$ is current equivariance violation
- $\epsilon_{\text{target}} \approx 0.22$ nat/step (proposed target)
- $\alpha$ is adaptation rate (typically 0.01-0.1)

**Strategy:**
- If $\epsilon(t) > \epsilon_{\text{target}}$: Increase $\lambda_{\text{L1}}$ (more sparsity, less mixing)
- If $\epsilon(t) < \epsilon_{\text{target}}$: Decrease $\lambda_{\text{L1}}$ (more expressiveness, more mixing)

**Key parameters:**
- `initial_lambda` – Starting $\lambda_{\text{L1}}$ value
- `target_violation` – Desired equivariance violation $\epsilon_{\text{target}}$ [nat/step]
- `learning_rate` – Adaptation rate $\alpha$
- `min_lambda` / `max_lambda` – Clamping bounds to prevent collapse or over-sparsity

**Training protocol:**
1. **Warmup (epochs 1-10):** Low $\lambda_{\text{L1}} = 0.001$, let network explore
2. **Ramp up (epochs 10-50):** Gradually increase $\lambda_{\text{L1}}$
3. **Adaptive (epochs 50+):** Use `AdaptiveL1Scheduler` to maintain target violation
4. **Fine-tune:** Fix $\lambda_{\text{L1}}$, early stopping on validation

**Source:** {ref}`Section 06 <sec-universal-geometric-network>`, lines 955, 2577.
:::

:::{prf:definition} G.6.8 (CovariantAttentionLayer)
:label: def-g-covariant-attention-layer

**Class signature:**
```python
class CovariantAttentionLayer(nn.Module):
    def __init__(
        self,
        bundle_dims: List[int],
        n_heads: int = 4,
        use_wilson_lines: bool = True
    ):
        ...
```

**Input/Output:**
- Input: `z` shape `[B, sum(bundle_dims)]`, optional `context` shape `[B, T, sum(bundle_dims)]`
- Output: `z_out` shape `[B, sum(bundle_dims)]`

**Purpose:** Covariant cross-attention for explicit world modeling and trajectory prediction. Alternative to `SoftEquivariantLayer` when planning is required.

**Architecture:**
- Multi-head attention per bundle
- Wilson lines for gauge-covariant Q/K/V projections
- Position-dependent temperature $\tau(z) = \sqrt{d_k}/\lambda(z)$
- Geometric Query terms with Christoffel symbols

**Use cases:**
- **SoftEquivariantLayer:** Default latent dynamics, implicit world model
- **CovariantAttentionLayer:** Explicit trajectory prediction, planning, memory retrieval

**Multi-stage pipeline example:**
1. Encoder → latent $Z$
2. SoftEquivariantLayer (×2) for geometric regularization
3. CovariantAttentionLayer for trajectory rollout
4. SoftEquivariantLayer (×2) for policy extraction
5. Decoder → action $Y$

**Source:** {ref}`Section 06 <sec-universal-geometric-network>`, line 2445. See also {ref}`Section 05 <sec-covariant-cross-attention-architecture>` for full derivation.
:::

## intro_agent.md

:::{prf:theorem} The RL Degeneracy Theorem
:label: thm-rl-degeneracy

Standard Reinforcement Learning is recovered from the Fragile Agent framework under the joint limit:

$$
\text{Standard RL} = \lim_{\substack{G \to I \\ |\mathcal{K}| \to \infty \\ \Xi_{\text{crit}} \to \infty}} \text{Fragile Agent}
$$
where:
1. **Flat Geometry** ($G \to I$): The state-space metric becomes Euclidean, eliminating coordinate-invariant updates
2. **Infinite Capacity** ($|\mathcal{K}| \to \infty$): No information bottleneck, continuous state space without quantization
3. **No Safety Constraints** ($\Xi_{\text{crit}} \to \infty$): The Sieve is disabled, all actions permitted

*Proof.* Each of the 37 Connection boxes below demonstrates a specific reduction. The composite limit follows from the independence of the five degeneracy conditions. $\square$
:::
