## 32. Causal Discovery: Interventional Geometry and the Singularity of Action

*Abstract.* We formalize the process of causal induction as a topological surgery on the latent transition kernel. We define an **Intervention** as a singular operator $\mathfrak{I}$ that decouples the latent state from the environment's Dirichlet boundary conditions (Perception) and replaces it with a forced Neumann condition (Action). We prove that the agent's "Curiosity" is a vector field $\mathbf{f}_{\text{exp}}$ generated by the gradient of a **Causal Information Potential** $\Psi_{\text{causal}}$, which measures the epistemic volatility of the World Model. We characterize Causal Discovery as a variational search for the transition law $\bar{P}$ that minimizes the Interventional Gap, thereby transforming observational correlations into structural causal manifolds.

:::{admonition} Researcher Bridge: Curiosity as a Vector Field (Not a Scalar)
:class: tip
:name: rb-curiosity-vector
Standard curiosity-driven RL (like RND) uses a scalar reward bonus to encourage exploration. We reframe "Curiosity" as a **Riemannian Force Field**. It is defined by the **Interventional Gap** - the discrepancy between what the model predicts through passive observation vs. active $do$-sampling. Curiosity is not an "incentive" you add to the reward; it is a vector that physically steers the agent toward states where its causal model is most likely to be proven wrong.
:::

*Cross-references:* This section builds on the symplectic boundary framework (Section 23.1), the World Model dynamics (Section 3.2), and the Causal Enclosure condition (Section 2.8). It connects to Ontological Expansion (Section 30) via the interventional closure theorem.

*Literature:* Causal inference {cite}`pearl2009causality`; causal discovery {cite}`spirtes2000causation`; expected information gain {cite}`lindley1956measure`; optimal experimental design {cite}`chaloner1995bayesian`; intrinsic motivation {cite}`schmidhuber2010formal,oudeyer2007intrinsic`; curiosity-driven exploration {cite}`pathak2017curiosity,houthooft2016vime`.



(sec-the-interventional-operator-as-manifold-surgery)=
### 32.1 The Interventional Operator as Manifold Surgery

In passive interaction, the agent's state is constrained by the environment (Section 23.1). Causal discovery requires the active breaking of this constraint.

:::{prf:definition} The Interventional Surgery
:label: def-the-interventional-surgery

Let $P(z_{t+1} | z_t, a_t)$ be the transition kernel on the latent manifold $\mathcal{Z}$. We define the **Interventional Operator** $\mathfrak{I}: \mathcal{P}(\mathcal{Z} \times \mathcal{A} \times \mathcal{Z}) \to \mathcal{P}(\mathcal{Z} \times \mathcal{A} \times \mathcal{Z})$—equivalent to Pearl's $do(a_t)$ {cite}`pearl2009causality`—as a surgery on the joint distribution that cuts the incoming edges to the action variable.

Geometrically, $\mathfrak{I}$ transforms the symplectic interface (Section 23.1) from a **Coupled Dirichlet state** (where $z_t$ is clamped by the observation $x_t$) to a **Forced Neumann state** (where $z_{t+1}$ is driven purely by the agent's internal motor impulse $u_\pi$).

Formally, the operator acts by truncated factorization:

$$
P(z' | z, do(a)) := P(z' | z, a),
$$
where the structural mechanism $P(z' | z, a)$ is preserved but $a$ is no longer a function of $z$. For marginal interventional queries:

$$
P(z' | do(a)) = \int_{\mathcal{Z}} P(z' | \tilde{z}, a) P_{\text{pre}}(\tilde{z}) \, d\mu_G(\tilde{z}),
$$
where $P_{\text{pre}}(\tilde{z})$ is the pre-intervention distribution over latent states.

:::
:::{prf:lemma} The Interventional Singularity
:label: lem-the-interventional-singularity

An intervention at state $z$ is a point-source singularity in the field theory. It imposes a non-natural boundary condition that forces the system to explore the off-equilibrium response of the environment law $P_\partial$ (Section 1.1.1).

*Proof sketch.* Under passive observation, the agent samples from the equilibrium distribution $P_{\text{eq}}(z' | z, a)$ determined by the environment's Dirichlet boundary $\partial\mathcal{Z}$. The $do$-operator breaks this equilibrium by injecting an external impulse $u_\pi$ that does not arise from the natural dynamics. In PDE terms, this corresponds to introducing a Dirac delta source $\delta(z - z_0)$ at the intervention point, creating a Green's function response that propagates through the causal graph. The "singularity" is geometric: the intervention point has infinite curvature in the causal manifold because all causal arrows pointing into it are severed. $\square$

*Remark (Surgery vs. Conditioning).* The key distinction from Bayesian conditioning is that $P(z' | do(a)) \neq P(z' | a)$ in general. Conditioning updates beliefs given evidence; intervention changes the generating mechanism. The former is reversible; the latter is a topological surgery.

:::



(sec-the-causal-information-potential)=
### 32.2 The Causal Information Potential

To motivate the agent to perform experiments, we define a potential based on the uncertainty of the World Model $\bar{P}$.

:::{prf:definition} Causal Information Potential
:label: def-causal-information-potential

Recall the World Model scaling coefficient $\gamma$ (Section 3.2). We define the **Causal Information Potential** $\Psi_{\text{causal}}: \mathcal{Z} \times \mathcal{A} \to \mathbb{R}_{\ge 0}$ as the Expected Information Gain (EIG) {cite}`lindley1956measure` regarding the transition parameters $\theta_W$ at state-action pair $(z, a)$:

$$
\Psi_{\text{causal}}(z, a) := \mathbb{E}_{z' \sim \bar{P}(\cdot | z, a)} \left[ D_{\text{KL}} \left( p(\theta_W | z, a, z') \| p(\theta_W | z, a) \right) \right].
$$
Units: $[\Psi_{\text{causal}}] = \text{nat}$.

*Physical interpretation:* $\Psi_{\text{causal}}(z, a)$ measures how much the agent expects to learn about the World Model parameters $\theta_W$ by executing action $a$ from state $z$. High $\Psi_{\text{causal}}$ indicates that the outcome $z'$ is highly informative about the transition dynamics—the agent is uncertain about what will happen, and observing the outcome will resolve significant uncertainty. This is the foundation of Bayesian experimental design {cite}`chaloner1995bayesian`.

:::

::::{note} Connection to RL #16: Entropy Maximization as Causal-Blind Exploration
**The General Law (Fragile Agent):**
The agent explores via the **Causal Information Potential** $\Psi_{\text{causal}}$ (Definition {prf:ref}`def-causal-information-potential`):

$$
\Psi_{\text{causal}}(z, a) := \mathbb{E}_{z' \sim \bar{P}(\cdot | z, a)} \left[ D_{\text{KL}} \left( p(\theta_W | z, a, z') \| p(\theta_W | z, a) \right) \right].
$$
This measures the **Expected Information Gain** about the world model—the agent seeks actions that maximally resolve uncertainty about causal dynamics.

**The Degenerate Limit:**
Remove the causal/interventional structure: $do(a) \to \text{just take } a$. Replace model-based EIG with model-free entropy.

**The Special Case (Standard RL - Maximum Entropy):**
Standard MaxEnt RL maximizes action entropy without considering *what* the entropy is about:

$$
\max_\pi \mathbb{E}\left[ \sum_t r_t + \alpha H(\pi(\cdot | s_t)) \right].
$$
This encourages diverse actions but is **causally blind** -- it cannot distinguish correlation from causation, confounded from unconfounded observations.

**Result:** Shannon entropy maximization is the $\Psi_{\text{causal}} \to H(\pi)$ limit where the causal graph is ignored.

**What the generalization offers:**
- **Causal targeting**: $\Psi_{\text{causal}}$ guides the agent toward experiments that resolve *specific* uncertainties about dynamics
- **Interventional semantics**: The $do(\cdot)$ operator distinguishes observations from interventions (Definition **Def: Interventional Operator**)
- **Causal Deficit detection**: $\Delta_{\text{causal}}$ (Theorem {prf:ref}`thm-the-interventional-gap`) diagnoses where correlations fail as causal predictors
- **Principled exploration-exploitation**: $\beta_{\text{exp}}$ trades off curiosity force vs utility force (Theorem {prf:ref}`thm-augmented-drift-law`)
::::

:::{prf:theorem} The Interventional Gap
:label: thm-the-interventional-gap

Let $P_{\text{obs}}(z' | z, a)$ be the conditional density obtained via passive observation, and $P_{\text{int}}(z' | do(z, a))$ be the density under intervention. We define the **Causal Deficit** $\Delta_{\text{causal}}: \mathcal{Z} \times \mathcal{A} \to \mathbb{R}_{\ge 0}$ as:

$$
\Delta_{\text{causal}}(z, a) := D_{\text{KL}} \left( P_{\text{int}}(z' | do(z, a)) \| P_{\text{obs}}(z' | z, a) \right).
$$
*Interpretation:* The Causal Deficit measures the discrepancy between interventional and observational predictions. If $\Delta_{\text{causal}} = 0$, the observational model is causally correct -- correlations reflect true causal mechanisms. If $\Delta_{\text{causal}} > 0$, the agent has mistaken a correlation for a causal link (confounding) or vice versa.

*Proof.* By the properties of KL-divergence, $\Delta_{\text{causal}} \ge 0$ with equality iff $P_{\text{int}} = P_{\text{obs}}$ almost everywhere. The agent's "Causal Ignorance" is the volume of states where $\Delta_{\text{causal}} > 0$:

$$
\text{Vol}_{\text{ignorant}} := \int_{\mathcal{Z} \times \mathcal{A}} \mathbb{I}[\Delta_{\text{causal}}(z, a) > 0] \, d\mu_G(z) \, da.
$$
This volume represents the region of state-action space where the agent's observational model fails to predict interventional outcomes. $\square$

:::
:::{prf:corollary} The Epistemic Curiosity Filter
:label: cor-epistemic-curiosity-filter

The Causal Information Potential $\Psi_{\text{causal}}$ (Definition {prf:ref}`def-causal-information-potential`) is maximized in regions of high **posterior varentropy**, not merely high entropy.

**Key Insight:** Let $V_H[P(\theta_W | z, a, z')]$ denote the Varentropy of the posterior over World Model parameters after observing transition $(z, a) \to z'$. Then:

$$
\nabla \Psi_{\text{causal}} \propto \nabla \mathbb{E}_{z'} \left[ V_H [P(\theta_W | z, a, z')] \right].
$$
*Units:* nat (for $\Psi_{\text{causal}}$), $\mathrm{nat}^2$ (for $V_H$).

**Operational Significance:**
The Curiosity Force $\mathbf{f}_{\text{exp}}$ (Theorem {prf:ref}`thm-augmented-drift-law`) should be weighted by the Varentropy of the World Model's prediction, not just its Entropy.

1. **High Entropy, Low Varentropy:** The World Model is confidently predicting "I don't know" (White Noise). The gradient $\nabla \Psi \approx 0$. The agent ignores this region (solves the "Noisy TV" problem).
2. **High Entropy, High Varentropy:** The World Model oscillates between distinct causal hypotheses ($H_1$: "Object falls", $H_2$: "Object floats"). The gradient $\nabla \Psi$ is maximal. The agent is strongly attracted to this state to resolve the structural ambiguity.

**Implementation:** The Experimental Sieve (Algorithm 32.5.1) selects interventions $do(a)$ that maximize the **Varentropy of the expected outcome distribution**.

*Proof:* See Appendix {ref}`E.11 <sec-appendix-e-proof-of-corollary-epistemic-curiosity-filter>`.

:::



(sec-the-force-of-curiosity-geodesic-experimentation)=
### 32.3 The Force of Curiosity: Geodesic Experimentation

The agent does not only move toward reward; it moves toward **Causal Clarity**.

:::{prf:theorem} Augmented Drift Law
:label: thm-augmented-drift-law

The Equation of Motion (Section 22.2) is extended by the **Interventional Force** $\mathbf{f}_{\text{exp}}$:

$$
F_{\text{total}} = \underbrace{-G^{-1} \nabla_G V}_{\text{Utility Force}} + \underbrace{\beta_{\text{exp}} \mathbf{f}_{\text{exp}}}_{\text{Curiosity Force}},
$$
where:
- $\mathbf{f}_{\text{exp}} := G^{-1} \nabla_z \Psi_{\text{causal}}$ is the gradient of the causal potential
- $\beta_{\text{exp}} \ge 0$ is the **exploration coefficient** balancing exploitation vs. exploration

*Proof.* We define a combined action functional $\mathcal{S}_{\text{total}} = \int_0^T \left[ \frac{1}{2}\|\dot{z}\|_G^2 - V(z) - \beta_{\text{exp}} \Psi_{\text{causal}}(z) \right] dt$. The Euler-Lagrange equations on $(\mathcal{Z}, G)$ yield:

$$
\frac{d}{dt}\left( G_{kj} \dot{z}^j \right) - \frac{1}{2} \partial_k G_{ij} \dot{z}^i \dot{z}^j = -\partial_k V - \beta_{\text{exp}} \partial_k \Psi_{\text{causal}}.
$$
Expanding the left-hand side and identifying the Christoffel symbols of the first kind $[ij, k] = \frac{1}{2}(\partial_i G_{jk} + \partial_j G_{ik} - \partial_k G_{ij})$:

$$
G_{kj} \ddot{z}^j + [ij, k] \dot{z}^i \dot{z}^j = -\partial_k V - \beta_{\text{exp}} \partial_k \Psi_{\text{causal}}.
$$
Contracting with $G^{mk}$ and using $\Gamma^m_{ij} = G^{mk}[ij, k]$:

$$
\ddot{z}^m + \Gamma^m_{ij} \dot{z}^i \dot{z}^j = -G^{mk} \partial_k V - \beta_{\text{exp}} G^{mk} \partial_k \Psi_{\text{causal}}.
$$
In the overdamped limit (Section 22.3), the acceleration term vanishes and the drift field is $F_{\text{total}} = -G^{-1}\nabla V + \beta_{\text{exp}} G^{-1}\nabla\Psi_{\text{causal}}$. See Appendix E.5 for the full derivation. $\square$

*Physical interpretation:* The curiosity force $\mathbf{f}_{\text{exp}}$ pulls the agent toward regions of high epistemic uncertainty about the transition dynamics. This is the geometric formulation of **intrinsic motivation** {cite}`schmidhuber2010formal,oudeyer2007intrinsic`: the agent is rewarded for reducing its causal ignorance, independent of external task reward. This connects to curiosity-driven exploration in reinforcement learning {cite}`pathak2017curiosity,houthooft2016vime`.

:::
:::{prf:corollary} Scientific Method as Geodesic
:label: cor-scientific-method-as-geodesic

In the absence of task reward ($V = \text{const}$), the agent behaves as a "Pure Scientist," traversing the latent manifold to minimize the total epistemic entropy of the World Model.

*Proof.* Setting $V = \text{const}$ implies $\nabla V = 0$. The equation of motion reduces to $\ddot{z}^m + \Gamma^m_{ij} \dot{z}^i \dot{z}^j = \beta_{\text{exp}} G^{mk} \partial_k \Psi_{\text{causal}}$. The agent follows geodesics modified by the curiosity potential, exploring the manifold to maximize $\Psi_{\text{causal}}$ (i.e., to find maximally informative experiments). $\square$

:::



(sec-causal-enclosure-and-interventional-stability)=
### 32.4 Causal Enclosure and Interventional Stability

We refine the **Causal Enclosure** condition (Section 2.8) to account for interventions.

:::{prf:theorem} Interventional Closure
:label: thm-interventional-closure

The macro-ontology $K$ is **Interventionally Closed** if and only if the predictability of the macro-state is invariant under $do$-operations:

$$
I(K_{t+1} ; Z_{\text{micro}, t} | K_t, do(A_t)) = 0.
$$
*Interpretation:* If an agent moves an object (intervention), and the resulting macro-state $K_{t+1}$ depends on micro-texture $z_{\text{tex}}$ that was previously labeled "noise," the ontology has failed. The intervention has **exposed a hidden variable**, triggering **Ontological Expansion** (Section 30).

*Proof sketch.* We compare the mutual information $I(K_{t+1}; Z_{\text{micro}, t} | K_t)$ under the observational measure $P$ and the interventional measure $P_{do(A)}$. Causal enclosure (Section 2.8) guarantees the condition for $P$. Because the $do(A)$ operator is a surgery that only removes incoming edges to $A$ (Pearl's Causal Markov Condition {cite}`pearl2009causality`), it leaves the mechanism $P(K_{t+1} | K_t, A_t, Z_{\text{micro}, t})$ invariant.

If the observational distribution is closed ($I = 0$), and the mechanism is invariant, the interventional distribution is necessarily closed. A violation ($I > 0$ under $do$) implies the existence of a back-door path through $Z_{\text{micro}}$ that was previously unobserved, necessitating a topological expansion of $K$ to include the confounding variable. See Appendix E.6 for the full proof. $\square$

*Remark (Interventional Debugging).* Theorem {prf:ref}`thm-interventional-closure` provides a diagnostic for ontological adequacy: if the agent's predictions fail specifically under intervention but succeed under observation, the ontology contains a hidden confounder. This is the geometric manifestation of Simpson's paradox {cite}`pearl2009causality`. Algorithmic approaches to discovering such confounders are developed in the causal discovery literature {cite}`spirtes2000causation`.

:::



(sec-implementation-the-experimental-sieve)=
### 32.5 Implementation: The Experimental Sieve

The following algorithm implements curiosity-driven exploration via the Causal Information Potential, connecting to active inference {cite}`friston2017active` and information-directed sampling.

**Algorithm 32.5.1 (Active Interventional Sampling).**
For each interaction step $t$:
1. **Monitor Volatility:** Track $\gamma(z)$ (World Model scaling coefficient) across the manifold.
2. **Generate Hypothesis:** Identify a region $U \subset \mathcal{Z}$ where $\Delta_{\text{causal}}$ is high.
3. **Execute do-operation:** Inject a Neumann impulse $u_\pi$ to drive the state into $U$.
4. **Update Kernel:** Correct $\bar{P}$ using the interventional feedback, reducing $\Psi_{\text{causal}}$.

(node-53)=
**Node 53: InterventionalGapCheck (CausalEnclosureCheck)**

| **#**  | **Name**                   | **Component** | **Type**         | **Interpretation**                   | **Proxy**                                                                       | **Cost**                             |
|--------|----------------------------|---------------|------------------|--------------------------------------|---------------------------------------------------------------------------------|--------------------------------------|
| **53** | **InterventionalGapCheck** | World Model   | Causal Soundness | Does observation match intervention? | $\Delta_{\text{causal}} := D_{\text{KL}}(P_{\text{int}} \lVert P_{\text{obs}})$ | $O(\lvert\mathcal{A}\rvert \cdot d)$ |

**Interpretation:** Monitors the Interventional Gap (Theorem {prf:ref}`thm-the-interventional-gap`). High $\Delta_{\text{causal}}$ indicates the agent is "surprised" by the results of its own actions—its model of "how things work" is purely correlational, not causal.

**Threshold:** $\Delta_{\text{causal}} < \Delta_{\max}$ (typical default $\Delta_{\max} = 0.5$ nat).

**Trigger conditions:**
- **High InterventionalGapCheck:** The agent's observational model is confounded. Predictions under passive observation do not match outcomes under active intervention.
- **Low InterventionalGapCheck:** The agent understands the causal manifold—interventional and observational predictions coincide.

**Remediation:**
- If $\Delta_{\text{causal}}$ is persistently high: Increase $\beta_{\text{exp}}$ to prioritize causal exploration.
- If $\Delta_{\text{causal}}$ spikes after ontological change: The new macro-variables may have introduced confounders. Run Ontological Stress analysis (Node 49).

*Cross-reference:* Node 53 complements the TextureFirewallCheck (Node 29) by detecting causal leakage rather than representational leakage.



(sec-summary-table-the-hierarchy-of-interaction)=
### 32.6 Summary Table: The Hierarchy of Interaction

**Table 32.6.1 (Interaction Mode Summary).**

| Mode             | Operator              | Boundary Condition  | Information Goal       |
|:-----------------|:----------------------|:--------------------|:-----------------------|
| **Observation**  | $P(z' \mid z, a)$     | Dirichlet (Clamped) | Information Extraction |
| **Retrieval**    | $\mathcal{R}(\omega)$ | Symplectic Bridge   | Semantic Alignment     |
| **Intervention** | $do(a)$               | Neumann (Forced)    | **Causal Induction**   |

**Key Results:**
1. **Interventional Surgery (Definition {prf:ref}`def-the-interventional-surgery`):** $do(a)$ severs incoming causal arrows, creating a singularity in the causal graph.
2. **Causal Potential (Definition {prf:ref}`def-causal-information-potential`):** $\Psi_{\text{causal}}$ measures expected information gain about transition dynamics.
3. **Curiosity Force (Theorem {prf:ref}`thm-augmented-drift-law`):** $\mathbf{f}_{\text{exp}} = G^{-1}\nabla\Psi_{\text{causal}}$ drives exploration toward causally informative states.
4. **Interventional Closure (Theorem {prf:ref}`thm-interventional-closure`):** Ontological adequacy requires invariance of macro-predictability under $do$-operations.

**Conclusion.** Causal Discovery is the final layer of the Fragile Agent's intelligence. By modeling actions as singular surgeries on the transition kernel and defining Curiosity as a Riemannian force field, we provide a rigorous mechanism for the agent to actively simplify the world. The agent is no longer a passive observer of a Markov process, but an **active topologist** who prunes the latent space into a robust, causal architecture.



(sec-causal-information-bound)=
