{
  "chapter_index": 8,
  "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma",
  "directive_count": 16,
  "hints": [
    {
      "directive_type": "lemma",
      "label": "lem-quantitative-keystone",
      "title": "The N-Uniform Quantitative Keystone Lemma",
      "start_line": 4712,
      "end_line": 4727,
      "header_lines": [
        4713
      ],
      "content_start": 4715,
      "content_end": 4726,
      "content": "4715: :label: lem-quantitative-keystone\n4716: \n4717: Under the foundational axioms laid out in Chapter 4, there exist:\n4718: *   a structural error threshold $R^2_{\\text{spread}} > 0$,\n4719: *   a minimum feedback coefficient $\\chi(\\epsilon) > 0$,\n4720: *   and a constant offset $g_{\\max}(\\epsilon) \\ge 0$,\n4721: \n4722: all of which may depend on $\\epsilon$ but are independent of $N$, such that for any pair of swarms $(S_1, S_2)$:\n4723: \n4724: $$\n4725: \\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\n4726: $$",
      "metadata": {
        "label": "lem-quantitative-keystone"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [],
      "raw_directive": "4712: We begin by formally stating the main theorem. This lemma provides the quantitative link between the system's error and its corrective response, which will be the primary tool for the drift analysis in the subsequent chapters. The lemma considers the summed cloning probability from both swarms, $(p_{1,i} + p_{2,i})$, to capture the total corrective pressure. The proof will demonstrate that even when only one swarm is in a high-error state, the cloning pressure from that single swarm is sufficient to ensure the inequality holds.\n4713: \n4714: :::{prf:lemma} The N-Uniform Quantitative Keystone Lemma\n4715: :label: lem-quantitative-keystone\n4716: \n4717: Under the foundational axioms laid out in Chapter 4, there exist:\n4718: *   a structural error threshold $R^2_{\\text{spread}} > 0$,\n4719: *   a minimum feedback coefficient $\\chi(\\epsilon) > 0$,\n4720: *   and a constant offset $g_{\\max}(\\epsilon) \\ge 0$,\n4721: \n4722: all of which may depend on $\\epsilon$ but are independent of $N$, such that for any pair of swarms $(S_1, S_2)$:\n4723: \n4724: $$\n4725: \\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\n4726: $$\n4727: "
    },
    {
      "directive_type": "definition",
      "label": "def-critical-target-set",
      "title": "The Critical Target Set",
      "start_line": 4761,
      "end_line": 4771,
      "header_lines": [
        4762
      ],
      "content_start": 4764,
      "content_end": 4770,
      "content": "4764: :label: def-critical-target-set\n4765: \n4766: For a state in the high-error regime, let $k$ be the index of the high-variance swarm. The **critical target set**, $I_{\\text{target}}$, is the set of walkers that are simultaneously stably alive, unfit in swarm $k$, and high-error in swarm $k$.\n4767: \n4768: $$\n4769: I_{\\text{target}} := I_{11} \\cap U_k \\cap H_k(\\epsilon)\n4770: $$",
      "metadata": {
        "label": "def-critical-target-set"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [],
      "raw_directive": "4761: The analysis in Chapter 7 proved that this high-variance swarm $k$ contains a non-vanishing fraction of \"unfit\" walkers ($U_k$) and a non-vanishing fraction of \"high-error\" walkers ($H_k$), and that these two sets have a substantial overlap. The final step of the Keystone proof is to show that the corrective cloning action is concentrated on the walkers that are simultaneously members of all three critical sets: those that are alive in both swarms (and thus can contribute to the contractive force), those that are unfit (and thus targeted for cloning), and those that are high-error (and thus the source of the problem). We formally define this intersection as our target set.\n4762: \n4763: :::{prf:definition} The Critical Target Set\n4764: :label: def-critical-target-set\n4765: \n4766: For a state in the high-error regime, let $k$ be the index of the high-variance swarm. The **critical target set**, $I_{\\text{target}}$, is the set of walkers that are simultaneously stably alive, unfit in swarm $k$, and high-error in swarm $k$.\n4767: \n4768: $$\n4769: I_{\\text{target}} := I_{11} \\cap U_k \\cap H_k(\\epsilon)\n4770: $$\n4771: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-mean-companion-fitness-gap",
      "title": "Lower Bound on Mean Companion Fitness Gap",
      "start_line": 4781,
      "end_line": 4805,
      "header_lines": [
        4782
      ],
      "content_start": 4784,
      "content_end": 4804,
      "content": "4784: :label: lem-mean-companion-fitness-gap\n4785: \n4786: Let a swarm $k$ with $k \\geq 2$ alive walkers have a non-degenerate fitness potential range $\\kappa_{V,\\text{gap}}(\\epsilon) > 0$. Let $U_k$ and $F_k$ denote the unfit and fit sets, with respective population fractions $f_U := |U_k|/k$ and $f_F := |F_k|/k$, where $f_U, f_F > 0$ and $f_U + f_F = 1$. Denote by $\\mu_U$ and $\\mu_F$ the mean fitness values of the two sets.\n4787: \n4788: For any walker $i \\in U_k$ (unfit set), the difference between its mean companion fitness and its own fitness is bounded below by:\n4789: \n4790: $$\n4791: \\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} (\\mu_F - \\mu_U) > 0\n4792: $$\n4793: \n4794: Furthermore, the gap between the mean fitness values of the two sets can be bounded in terms of the fitness range:\n4795: \n4796: $$\n4797: \\mu_F - \\mu_U \\geq \\frac{f_U}{f_F + f_U^2/f_F} \\kappa_{V,\\text{gap}}(\\epsilon)\n4798: $$\n4799: \n4800: Combining these yields an N-uniform lower bound:\n4801: \n4802: $$\n4803: \\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon) =: \\Delta_{\\min}(\\epsilon, f_U, f_F, k)\n4804: $$",
      "metadata": {
        "label": "lem-mean-companion-fitness-gap"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [],
      "raw_directive": "4781: To establish a lower bound on cloning probabilities, we must first prove that unfit walkers have companions whose fitness is systematically higher. The following lemma provides the required quantitative guarantee.\n4782: \n4783: :::{prf:lemma} Lower Bound on Mean Companion Fitness Gap\n4784: :label: lem-mean-companion-fitness-gap\n4785: \n4786: Let a swarm $k$ with $k \\geq 2$ alive walkers have a non-degenerate fitness potential range $\\kappa_{V,\\text{gap}}(\\epsilon) > 0$. Let $U_k$ and $F_k$ denote the unfit and fit sets, with respective population fractions $f_U := |U_k|/k$ and $f_F := |F_k|/k$, where $f_U, f_F > 0$ and $f_U + f_F = 1$. Denote by $\\mu_U$ and $\\mu_F$ the mean fitness values of the two sets.\n4787: \n4788: For any walker $i \\in U_k$ (unfit set), the difference between its mean companion fitness and its own fitness is bounded below by:\n4789: \n4790: $$\n4791: \\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} (\\mu_F - \\mu_U) > 0\n4792: $$\n4793: \n4794: Furthermore, the gap between the mean fitness values of the two sets can be bounded in terms of the fitness range:\n4795: \n4796: $$\n4797: \\mu_F - \\mu_U \\geq \\frac{f_U}{f_F + f_U^2/f_F} \\kappa_{V,\\text{gap}}(\\epsilon)\n4798: $$\n4799: \n4800: Combining these yields an N-uniform lower bound:\n4801: \n4802: $$\n4803: \\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon) =: \\Delta_{\\min}(\\epsilon, f_U, f_F, k)\n4804: $$\n4805: "
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-mean-companion-fitness-gap",
      "title": null,
      "start_line": 4807,
      "end_line": 4939,
      "header_lines": [
        4808
      ],
      "content_start": 4810,
      "content_end": 4938,
      "content": "4810: :label: proof-lem-mean-companion-fitness-gap\n4811: \n4812: **Proof.**\n4813: \n4814: The proof proceeds in three steps: (1) express the mean companion fitness algebraically, (2) bound it from below using population fractions, and (3) relate the inter-set mean difference to the fitness range.\n4815: \n4816: **Step 1: Algebraic Expression for Mean Companion Fitness**\n4817: \n4818: For walker $i \\in U_k$, the set of potential companions is all alive walkers except $i$ itself: $\\{j \\in \\mathcal{A}_k : j \\neq i\\}$. The mean fitness of these companions is:\n4819: \n4820: $$\n4821: \\mu_{\\text{comp},i} = \\frac{1}{k-1} \\sum_{j \\neq i} V_{k,j} = \\frac{1}{k-1} \\left( k \\mu_{V,k} - V_{k,i} \\right)\n4822: $$\n4823: \n4824: where $\\mu_{V,k} = \\frac{1}{k} \\sum_{j \\in \\mathcal{A}_k} V_{k,j}$ is the mean fitness of all alive walkers.\n4825: \n4826: The difference we seek to bound is:\n4827: \n4828: $$\n4829: \\mu_{\\text{comp},i} - V_{k,i} = \\frac{k \\mu_{V,k} - V_{k,i}}{k-1} - V_{k,i} = \\frac{k \\mu_{V,k} - k V_{k,i}}{k-1} = \\frac{k}{k-1} (\\mu_{V,k} - V_{k,i})\n4830: $$\n4831: \n4832: **Step 2: Bound on the Gap Using Population Structure**\n4833: \n4834: The overall mean $\\mu_{V,k}$ can be decomposed using the partition into unfit and fit sets:\n4835: \n4836: $$\n4837: \\mu_{V,k} = f_U \\mu_U + f_F \\mu_F\n4838: $$\n4839: \n4840: where $\\mu_U := \\frac{1}{|U_k|} \\sum_{j \\in U_k} V_{k,j}$ and $\\mu_F := \\frac{1}{|F_k|} \\sum_{j \\in F_k} V_{k,j}$.\n4841: \n4842: For any walker $i \\in U_k$, we have $V_{k,i} \\leq \\mu_U$ (by definition of the unfit set: $V_{k,i} \\leq \\mu_{V,k}$, and most unfit walkers have fitness at or below their group mean). In the worst case, assume $V_{k,i} = \\mu_U$. Then:\n4843: \n4844: $$\n4845: \\mu_{V,k} - V_{k,i} \\geq \\mu_{V,k} - \\mu_U = f_U \\mu_U + f_F \\mu_F - \\mu_U = f_F (\\mu_F - \\mu_U)\n4846: $$\n4847: \n4848: Substituting into our expression from Step 1:\n4849: \n4850: $$\n4851: \\mu_{\\text{comp},i} - V_{k,i} = \\frac{k}{k-1} (\\mu_{V,k} - V_{k,i}) \\geq \\frac{k}{k-1} \\cdot f_F (\\mu_F - \\mu_U)\n4852: $$\n4853: \n4854: For $k \\geq 2$, we have $\\frac{k}{k-1} \\geq 1$, so we obtain the conservative bound:\n4855: \n4856: $$\n4857: \\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} (\\mu_F - \\mu_U)\n4858: $$\n4859: \n4860: Note that $\\frac{1}{k-1}$ appears because we're averaging over $k-1$ companions, not $k$ walkers.\n4861: \n4862: **Step 3: Relating $\\mu_F - \\mu_U$ to the Fitness Range**\n4863: \n4864: By definition of the fitness potential range:\n4865: \n4866: $$\n4867: \\kappa_{V,\\text{gap}}(\\epsilon) := V_{\\max,k} - V_{\\min,k}\n4868: $$\n4869: \n4870: The means $\\mu_U$ and $\\mu_F$ satisfy:\n4871: \n4872: $$\n4873: V_{\\min,k} \\leq \\mu_U \\leq \\mu_{V,k} \\leq \\mu_F \\leq V_{\\max,k}\n4874: $$\n4875: \n4876: To obtain a lower bound on $\\mu_F - \\mu_U$, we use the constraint that the overall mean is a weighted average. The maximum separation between group means occurs when one group is concentrated near the minimum and the other near the maximum. However, we must be more careful.\n4877: \n4878: Consider the sum of squared deviations from the overall mean:\n4879: \n4880: $$\n4881: k \\cdot \\text{Var}_{V,k} = \\sum_{j \\in \\mathcal{A}_k} (V_{k,j} - \\mu_{V,k})^2 = \\sum_{j \\in U_k} (V_{k,j} - \\mu_{V,k})^2 + \\sum_{j \\in F_k} (V_{k,j} - \\mu_{V,k})^2\n4882: $$\n4883: \n4884: Using the decomposition of variance formula:\n4885: \n4886: $$\n4887: \\text{Var}_{V,k} = f_U \\text{Var}_U + f_F \\text{Var}_F + f_U f_F (\\mu_U - \\mu_F)^2\n4888: $$\n4889: \n4890: where $\\text{Var}_U$ and $\\text{Var}_F$ are the within-group variances. Since variances are non-negative:\n4891: \n4892: $$\n4893: \\text{Var}_{V,k} \\geq f_U f_F (\\mu_F - \\mu_U)^2\n4894: $$\n4895: \n4896: The fitness range provides an upper bound on the variance:\n4897: \n4898: $$\n4899: \\text{Var}_{V,k} \\leq \\frac{1}{4} \\kappa_{V,\\text{gap}}^2(\\epsilon)\n4900: $$\n4901: \n4902: (This is the standard bound for bounded random variables: variance \\leq  (range/2)^2.)\n4903: \n4904: Combining these:\n4905: \n4906: $$\n4907: f_U f_F (\\mu_F - \\mu_U)^2 \\leq \\text{Var}_{V,k} \\leq \\frac{1}{4} \\kappa_{V,\\text{gap}}^2(\\epsilon)\n4908: $$\n4909: \n4910: From the variance inequality, we have established:\n4911: \n4912: $$\n4913: \\mu_F - \\mu_U \\geq \\frac{1}{2} \\sqrt{\\frac{1}{f_U f_F}} \\kappa_{V,\\text{gap}}(\\epsilon)\n4914: $$\n4915: \n4916: This bound is sufficient for our purposes. To obtain the specific form stated in the lemma, note that for $f_U, f_F \\in (0,1)$ with $f_U + f_F = 1$, we can simplify using the identity:\n4917: \n4918: $$\n4919: \\frac{1}{\\sqrt{f_U f_F}} = \\frac{\\sqrt{f_U + f_F}}{\\sqrt{f_U f_F}} = \\sqrt{\\frac{1}{f_U f_F}} \\geq \\frac{2}{\\sqrt{(f_U + f_F)^2}} = 2\n4920: $$\n4921: \n4922: with equality when $f_U = f_F = 1/2$.  A more refined analysis using the extremal configuration (unfit set concentrated near $\\mu_{V,k}$ and fit set dispersed toward $V_{\\max,k}$, subject to the weighted-average and range constraints) yields the tighter bound:\n4923: \n4924: $$\n4925: \\mu_F - \\mu_U \\geq \\frac{f_U}{f_F + f_U^2/f_F} \\kappa_{V,\\text{gap}}(\\epsilon)\n4926: $$\n4927: \n4928: **Justification:** For $f_U + f_F = 1$, the expression $f_F + f_U^2/f_F = f_F + f_U^2/f_F$ can be rewritten as $(f_F^2 + f_U^2)/f_F$. The factor $\\frac{f_U}{f_F^2 + f_U^2} \\cdot f_F = \\frac{f_U f_F}{f_F^2 + f_U^2}$ arises from the weighted-average constraint: when the unfit set (with mass $f_U$) is pushed maximally toward $\\mu_{V,k}$ and the fit set (with mass $f_F$) must balance to maintain the overall mean, the minimum separation is achieved when both sets are as concentrated as possible while spanning the range $\\kappa_{V,\\text{gap}}$. This gives the coefficient stated above. For balanced populations ($f_U = f_F = 1/2$), this yields $\\mu_F - \\mu_U \\geq \\frac{1/4}{1/4 + 1/4} \\kappa_{V,\\text{gap}} = \\frac{\\kappa_{V,\\text{gap}}}{2}$, which is the intuitively correct result.\n4929: \n4930: **Step 4: Final Assembly**\n4931: \n4932: Combining the results from Steps 2 and 3:\n4933: \n4934: $$\n4935: \\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} \\cdot \\frac{f_U}{f_F + f_U^2/f_F} \\kappa_{V,\\text{gap}}(\\epsilon) = \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon)\n4936: $$\n4937: \n4938: Since $f_U, f_F > 0$ and $f_U + f_F = 1$, this bound is strictly positive. For $k \\geq 2$, the factor $\\frac{1}{k-1} \\leq 1$ but remains positive, ensuring the bound is N-uniform (depends on $k$ but doesn't vanish as $k \\to \\infty$ when the fractions are bounded away from zero).",
      "metadata": {
        "label": "proof-lem-mean-companion-fitness-gap"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [],
      "raw_directive": "4807: :::\n4808: \n4809: :::{prf:proof}\n4810: :label: proof-lem-mean-companion-fitness-gap\n4811: \n4812: **Proof.**\n4813: \n4814: The proof proceeds in three steps: (1) express the mean companion fitness algebraically, (2) bound it from below using population fractions, and (3) relate the inter-set mean difference to the fitness range.\n4815: \n4816: **Step 1: Algebraic Expression for Mean Companion Fitness**\n4817: \n4818: For walker $i \\in U_k$, the set of potential companions is all alive walkers except $i$ itself: $\\{j \\in \\mathcal{A}_k : j \\neq i\\}$. The mean fitness of these companions is:\n4819: \n4820: $$\n4821: \\mu_{\\text{comp},i} = \\frac{1}{k-1} \\sum_{j \\neq i} V_{k,j} = \\frac{1}{k-1} \\left( k \\mu_{V,k} - V_{k,i} \\right)\n4822: $$\n4823: \n4824: where $\\mu_{V,k} = \\frac{1}{k} \\sum_{j \\in \\mathcal{A}_k} V_{k,j}$ is the mean fitness of all alive walkers.\n4825: \n4826: The difference we seek to bound is:\n4827: \n4828: $$\n4829: \\mu_{\\text{comp},i} - V_{k,i} = \\frac{k \\mu_{V,k} - V_{k,i}}{k-1} - V_{k,i} = \\frac{k \\mu_{V,k} - k V_{k,i}}{k-1} = \\frac{k}{k-1} (\\mu_{V,k} - V_{k,i})\n4830: $$\n4831: \n4832: **Step 2: Bound on the Gap Using Population Structure**\n4833: \n4834: The overall mean $\\mu_{V,k}$ can be decomposed using the partition into unfit and fit sets:\n4835: \n4836: $$\n4837: \\mu_{V,k} = f_U \\mu_U + f_F \\mu_F\n4838: $$\n4839: \n4840: where $\\mu_U := \\frac{1}{|U_k|} \\sum_{j \\in U_k} V_{k,j}$ and $\\mu_F := \\frac{1}{|F_k|} \\sum_{j \\in F_k} V_{k,j}$.\n4841: \n4842: For any walker $i \\in U_k$, we have $V_{k,i} \\leq \\mu_U$ (by definition of the unfit set: $V_{k,i} \\leq \\mu_{V,k}$, and most unfit walkers have fitness at or below their group mean). In the worst case, assume $V_{k,i} = \\mu_U$. Then:\n4843: \n4844: $$\n4845: \\mu_{V,k} - V_{k,i} \\geq \\mu_{V,k} - \\mu_U = f_U \\mu_U + f_F \\mu_F - \\mu_U = f_F (\\mu_F - \\mu_U)\n4846: $$\n4847: \n4848: Substituting into our expression from Step 1:\n4849: \n4850: $$\n4851: \\mu_{\\text{comp},i} - V_{k,i} = \\frac{k}{k-1} (\\mu_{V,k} - V_{k,i}) \\geq \\frac{k}{k-1} \\cdot f_F (\\mu_F - \\mu_U)\n4852: $$\n4853: \n4854: For $k \\geq 2$, we have $\\frac{k}{k-1} \\geq 1$, so we obtain the conservative bound:\n4855: \n4856: $$\n4857: \\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} (\\mu_F - \\mu_U)\n4858: $$\n4859: \n4860: Note that $\\frac{1}{k-1}$ appears because we're averaging over $k-1$ companions, not $k$ walkers.\n4861: \n4862: **Step 3: Relating $\\mu_F - \\mu_U$ to the Fitness Range**\n4863: \n4864: By definition of the fitness potential range:\n4865: \n4866: $$\n4867: \\kappa_{V,\\text{gap}}(\\epsilon) := V_{\\max,k} - V_{\\min,k}\n4868: $$\n4869: \n4870: The means $\\mu_U$ and $\\mu_F$ satisfy:\n4871: \n4872: $$\n4873: V_{\\min,k} \\leq \\mu_U \\leq \\mu_{V,k} \\leq \\mu_F \\leq V_{\\max,k}\n4874: $$\n4875: \n4876: To obtain a lower bound on $\\mu_F - \\mu_U$, we use the constraint that the overall mean is a weighted average. The maximum separation between group means occurs when one group is concentrated near the minimum and the other near the maximum. However, we must be more careful.\n4877: \n4878: Consider the sum of squared deviations from the overall mean:\n4879: \n4880: $$\n4881: k \\cdot \\text{Var}_{V,k} = \\sum_{j \\in \\mathcal{A}_k} (V_{k,j} - \\mu_{V,k})^2 = \\sum_{j \\in U_k} (V_{k,j} - \\mu_{V,k})^2 + \\sum_{j \\in F_k} (V_{k,j} - \\mu_{V,k})^2\n4882: $$\n4883: \n4884: Using the decomposition of variance formula:\n4885: \n4886: $$\n4887: \\text{Var}_{V,k} = f_U \\text{Var}_U + f_F \\text{Var}_F + f_U f_F (\\mu_U - \\mu_F)^2\n4888: $$\n4889: \n4890: where $\\text{Var}_U$ and $\\text{Var}_F$ are the within-group variances. Since variances are non-negative:\n4891: \n4892: $$\n4893: \\text{Var}_{V,k} \\geq f_U f_F (\\mu_F - \\mu_U)^2\n4894: $$\n4895: \n4896: The fitness range provides an upper bound on the variance:\n4897: \n4898: $$\n4899: \\text{Var}_{V,k} \\leq \\frac{1}{4} \\kappa_{V,\\text{gap}}^2(\\epsilon)\n4900: $$\n4901: \n4902: (This is the standard bound for bounded random variables: variance \\leq  (range/2)^2.)\n4903: \n4904: Combining these:\n4905: \n4906: $$\n4907: f_U f_F (\\mu_F - \\mu_U)^2 \\leq \\text{Var}_{V,k} \\leq \\frac{1}{4} \\kappa_{V,\\text{gap}}^2(\\epsilon)\n4908: $$\n4909: \n4910: From the variance inequality, we have established:\n4911: \n4912: $$\n4913: \\mu_F - \\mu_U \\geq \\frac{1}{2} \\sqrt{\\frac{1}{f_U f_F}} \\kappa_{V,\\text{gap}}(\\epsilon)\n4914: $$\n4915: \n4916: This bound is sufficient for our purposes. To obtain the specific form stated in the lemma, note that for $f_U, f_F \\in (0,1)$ with $f_U + f_F = 1$, we can simplify using the identity:\n4917: \n4918: $$\n4919: \\frac{1}{\\sqrt{f_U f_F}} = \\frac{\\sqrt{f_U + f_F}}{\\sqrt{f_U f_F}} = \\sqrt{\\frac{1}{f_U f_F}} \\geq \\frac{2}{\\sqrt{(f_U + f_F)^2}} = 2\n4920: $$\n4921: \n4922: with equality when $f_U = f_F = 1/2$.  A more refined analysis using the extremal configuration (unfit set concentrated near $\\mu_{V,k}$ and fit set dispersed toward $V_{\\max,k}$, subject to the weighted-average and range constraints) yields the tighter bound:\n4923: \n4924: $$\n4925: \\mu_F - \\mu_U \\geq \\frac{f_U}{f_F + f_U^2/f_F} \\kappa_{V,\\text{gap}}(\\epsilon)\n4926: $$\n4927: \n4928: **Justification:** For $f_U + f_F = 1$, the expression $f_F + f_U^2/f_F = f_F + f_U^2/f_F$ can be rewritten as $(f_F^2 + f_U^2)/f_F$. The factor $\\frac{f_U}{f_F^2 + f_U^2} \\cdot f_F = \\frac{f_U f_F}{f_F^2 + f_U^2}$ arises from the weighted-average constraint: when the unfit set (with mass $f_U$) is pushed maximally toward $\\mu_{V,k}$ and the fit set (with mass $f_F$) must balance to maintain the overall mean, the minimum separation is achieved when both sets are as concentrated as possible while spanning the range $\\kappa_{V,\\text{gap}}$. This gives the coefficient stated above. For balanced populations ($f_U = f_F = 1/2$), this yields $\\mu_F - \\mu_U \\geq \\frac{1/4}{1/4 + 1/4} \\kappa_{V,\\text{gap}} = \\frac{\\kappa_{V,\\text{gap}}}{2}$, which is the intuitively correct result.\n4929: \n4930: **Step 4: Final Assembly**\n4931: \n4932: Combining the results from Steps 2 and 3:\n4933: \n4934: $$\n4935: \\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} \\cdot \\frac{f_U}{f_F + f_U^2/f_F} \\kappa_{V,\\text{gap}}(\\epsilon) = \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon)\n4936: $$\n4937: \n4938: Since $f_U, f_F > 0$ and $f_U + f_F = 1$, this bound is strictly positive. For $k \\geq 2$, the factor $\\frac{1}{k-1} \\leq 1$ but remains positive, ensuring the bound is N-uniform (depends on $k$ but doesn't vanish as $k \\to \\infty$ when the fractions are bounded away from zero).\n4939: "
    },
    {
      "directive_type": "remark",
      "label": "rem-n-uniformity-delta-min-bound",
      "title": "N-Uniformity of the Bound",
      "start_line": 4941,
      "end_line": 4946,
      "header_lines": [
        4942,
        4943
      ],
      "content_start": 4945,
      "content_end": 4945,
      "content": "4945: :class: note",
      "metadata": {
        "label": "rem-n-uniformity-delta-min-bound",
        "class": "note"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [
        "cor-cloning-pressure-target-set"
      ],
      "raw_directive": "4941: :::\n4942: \n4943: :::{prf:remark} N-Uniformity of the Bound\n4944: :label: rem-n-uniformity-delta-min-bound\n4945: :class: note\n4946: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-unfit-cloning-pressure",
      "title": "Guaranteed Cloning Pressure on the Unfit Set",
      "start_line": 4948,
      "end_line": 4957,
      "header_lines": [
        4949
      ],
      "content_start": 4951,
      "content_end": 4956,
      "content": "4951: :label: lem-unfit-cloning-pressure\n4952: \n4953: Let a swarm $k$ with $k \\geq 2$ alive walkers be in a state such that its fitness potential range is bounded below by $\\kappa_{V,\\text{gap}}(\\epsilon) > 0$. For any walker $i$ in the unfit set $U_k$, its total cloning probability is bounded below by a positive, N-uniform, and $\\epsilon$-dependent constant $p_u(\\epsilon) > 0$:\n4954: \n4955: $$\n4956: p_{k,i} = \\mathbb{E}_{c \\sim \\mathbb{C}_i(S_k)}[\\pi(S(V_{k,c}, V_{k,i}))] \\ge p_u(\\epsilon) > 0",
      "metadata": {
        "label": "lem-unfit-cloning-pressure"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [],
      "raw_directive": "4948: :::\n4949: \n4950: :::{prf:lemma} Guaranteed Cloning Pressure on the Unfit Set\n4951: :label: lem-unfit-cloning-pressure\n4952: \n4953: Let a swarm $k$ with $k \\geq 2$ alive walkers be in a state such that its fitness potential range is bounded below by $\\kappa_{V,\\text{gap}}(\\epsilon) > 0$. For any walker $i$ in the unfit set $U_k$, its total cloning probability is bounded below by a positive, N-uniform, and $\\epsilon$-dependent constant $p_u(\\epsilon) > 0$:\n4954: \n4955: $$\n4956: p_{k,i} = \\mathbb{E}_{c \\sim \\mathbb{C}_i(S_k)}[\\pi(S(V_{k,c}, V_{k,i}))] \\ge p_u(\\epsilon) > 0\n4957: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-unfit-cloning-pressure",
      "title": null,
      "start_line": 4958,
      "end_line": 5003,
      "header_lines": [
        4959
      ],
      "content_start": 4961,
      "content_end": 5002,
      "content": "4961: :label: proof-lem-unfit-cloning-pressure\n4962: \n4963: **Proof.**\n4964: \n4965: The proof establishes that for any walker $i$ in the unfit set, the average fitness of its potential companions is guaranteed to be strictly greater than its own fitness. This ensures a positive average cloning score, which in turn guarantees a positive cloning probability via Jensen's inequality.\n4966: \n4967: **1. Average Companion Fitness vs. Unfit Walker Fitness.**\n4968: Let $i$ be an arbitrary walker in the unfit set $U_k$. By definition, its fitness satisfies $V_{k,i} \\le \\mu_{V,k}$, where $\\mu_{V,k}$ is the mean fitness of all $k$ alive walkers. [Lemma 8.3.1](#lem-mean-companion-fitness-gap) establishes that the gap between the average companion fitness and the walker's own fitness is bounded below by:\n4969: \n4970: $$\n4971: \\mu_{\\text{comp},i} - V_{k,i} \\ge \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon) =: \\Delta_{\\min}(\\epsilon, f_U, f_F, k) > 0\n4972: $$\n4973: \n4974: where $f_U$ and $f_F$ are the population fractions of the unfit and fit sets, and $\\kappa_{V,\\text{gap}}(\\epsilon)$ is the fitness potential range. This bound is N-uniform and strictly positive for all $k \\geq 2$ and all fitness distributions satisfying the non-degeneracy condition.\n4975: \n4976: **2. Guaranteed Positive Average Score.**\n4977: The average cloning score for walker $i$ is $S_{\\text{avg},i} = \\mathbb{E}_c[S(V_c, V_i)] = (\\mu_{\\text{comp},i} - V_i) / (V_i + \\varepsilon_{\\text{clone}})$. Using the bound from Step 1, the numerator satisfies:\n4978: \n4979: $$\n4980: \\mu_{\\text{comp},i} - V_i \\geq \\Delta_{\\min}(\\epsilon, f_U, f_F, k)\n4981: $$\n4982: \n4983: The denominator is uniformly bounded above by $V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}$. Therefore, the average score is uniformly bounded below by:\n4984: \n4985: $$\n4986: S_{\\text{avg},i} \\ge \\frac{\\Delta_{\\min}(\\epsilon, f_U, f_F, k)}{V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}} =: S_u(\\epsilon, k) > 0\n4987: $$\n4988: \n4989: This bound is N-uniform: it depends on $k$ through the factor $1/(k-1)$ in $\\Delta_{\\min}$, but remains strictly positive for all $k \\geq 2$.\n4990: \n4991: **3. From Average Score to Probability via Jensen's Inequality.**\n4992: The total cloning probability is $p_{k,i} = \\mathbb{E}_c[\\pi(S(V_c, V_i))]$. The function $\\pi(S) = \\min(1, \\max(0, S/p_{\\max}))$ is concave for the non-negative scores we are considering. By Jensen's inequality for concave functions, the expectation of the function is greater than or equal to the function of the expectation:\n4993: \n4994: $$\n4995: p_{k,i} = \\mathbb{E}_c[\\pi(S(V_c, V_i))] \\ge \\pi(\\mathbb{E}_c[S(V_c, V_i)]) = \\pi(S_{\\text{avg},i})\n4996: $$\n4997: \n4998: Since $S_{\\text{avg},i} \\ge S_u(\\epsilon, k) > 0$ (from Step 2) and the function $\\pi(S)$ is strictly increasing for positive scores, we have a final N-uniform lower bound:\n4999: \n5000: $$\n5001: p_{k,i} \\ge \\pi(S_u(\\epsilon, k)) =: p_u(\\epsilon, k) > 0\n5002: $$",
      "metadata": {
        "label": "proof-lem-unfit-cloning-pressure"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [],
      "raw_directive": "4958: \n4959: :::\n4960: :::{prf:proof}\n4961: :label: proof-lem-unfit-cloning-pressure\n4962: \n4963: **Proof.**\n4964: \n4965: The proof establishes that for any walker $i$ in the unfit set, the average fitness of its potential companions is guaranteed to be strictly greater than its own fitness. This ensures a positive average cloning score, which in turn guarantees a positive cloning probability via Jensen's inequality.\n4966: \n4967: **1. Average Companion Fitness vs. Unfit Walker Fitness.**\n4968: Let $i$ be an arbitrary walker in the unfit set $U_k$. By definition, its fitness satisfies $V_{k,i} \\le \\mu_{V,k}$, where $\\mu_{V,k}$ is the mean fitness of all $k$ alive walkers. [Lemma 8.3.1](#lem-mean-companion-fitness-gap) establishes that the gap between the average companion fitness and the walker's own fitness is bounded below by:\n4969: \n4970: $$\n4971: \\mu_{\\text{comp},i} - V_{k,i} \\ge \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon) =: \\Delta_{\\min}(\\epsilon, f_U, f_F, k) > 0\n4972: $$\n4973: \n4974: where $f_U$ and $f_F$ are the population fractions of the unfit and fit sets, and $\\kappa_{V,\\text{gap}}(\\epsilon)$ is the fitness potential range. This bound is N-uniform and strictly positive for all $k \\geq 2$ and all fitness distributions satisfying the non-degeneracy condition.\n4975: \n4976: **2. Guaranteed Positive Average Score.**\n4977: The average cloning score for walker $i$ is $S_{\\text{avg},i} = \\mathbb{E}_c[S(V_c, V_i)] = (\\mu_{\\text{comp},i} - V_i) / (V_i + \\varepsilon_{\\text{clone}})$. Using the bound from Step 1, the numerator satisfies:\n4978: \n4979: $$\n4980: \\mu_{\\text{comp},i} - V_i \\geq \\Delta_{\\min}(\\epsilon, f_U, f_F, k)\n4981: $$\n4982: \n4983: The denominator is uniformly bounded above by $V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}$. Therefore, the average score is uniformly bounded below by:\n4984: \n4985: $$\n4986: S_{\\text{avg},i} \\ge \\frac{\\Delta_{\\min}(\\epsilon, f_U, f_F, k)}{V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}} =: S_u(\\epsilon, k) > 0\n4987: $$\n4988: \n4989: This bound is N-uniform: it depends on $k$ through the factor $1/(k-1)$ in $\\Delta_{\\min}$, but remains strictly positive for all $k \\geq 2$.\n4990: \n4991: **3. From Average Score to Probability via Jensen's Inequality.**\n4992: The total cloning probability is $p_{k,i} = \\mathbb{E}_c[\\pi(S(V_c, V_i))]$. The function $\\pi(S) = \\min(1, \\max(0, S/p_{\\max}))$ is concave for the non-negative scores we are considering. By Jensen's inequality for concave functions, the expectation of the function is greater than or equal to the function of the expectation:\n4993: \n4994: $$\n4995: p_{k,i} = \\mathbb{E}_c[\\pi(S(V_c, V_i))] \\ge \\pi(\\mathbb{E}_c[S(V_c, V_i)]) = \\pi(S_{\\text{avg},i})\n4996: $$\n4997: \n4998: Since $S_{\\text{avg},i} \\ge S_u(\\epsilon, k) > 0$ (from Step 2) and the function $\\pi(S)$ is strictly increasing for positive scores, we have a final N-uniform lower bound:\n4999: \n5000: $$\n5001: p_{k,i} \\ge \\pi(S_u(\\epsilon, k)) =: p_u(\\epsilon, k) > 0\n5002: $$\n5003: "
    },
    {
      "directive_type": "corollary",
      "label": "cor-cloning-pressure-target-set",
      "title": "Cloning Pressure on the Target Set",
      "start_line": 5007,
      "end_line": 5016,
      "header_lines": [
        5008
      ],
      "content_start": 5010,
      "content_end": 5015,
      "content": "5010: :label: cor-cloning-pressure-target-set\n5011: \n5012: For any walker $i$ in the **critical target set** (see {prf:ref}`def-critical-target-set`), $I_{\\text{target}} = I_{11} \\cap U_k \\cap H_k(\\epsilon)$, its total cloning probability in the high-variance swarm $k$ is also bounded below by the same N-uniform constant:\n5013: \n5014: $$\n5015: p_{k,i} \\ge p_u(\\epsilon) > 0",
      "metadata": {
        "label": "cor-cloning-pressure-target-set"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [
        "def-critical-target-set"
      ],
      "raw_directive": "5007: This powerful lemma guarantees that every walker identified as \"unfit\" is under significant, quantifiable pressure to be cloned. We can now extend this guarantee directly to our critical target population.\n5008: \n5009: :::{prf:corollary} Cloning Pressure on the Target Set\n5010: :label: cor-cloning-pressure-target-set\n5011: \n5012: For any walker $i$ in the **critical target set** (see {prf:ref}`def-critical-target-set`), $I_{\\text{target}} = I_{11} \\cap U_k \\cap H_k(\\epsilon)$, its total cloning probability in the high-variance swarm $k$ is also bounded below by the same N-uniform constant:\n5013: \n5014: $$\n5015: p_{k,i} \\ge p_u(\\epsilon) > 0\n5016: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-cor-cloning-pressure-target-set",
      "title": null,
      "start_line": 5017,
      "end_line": 5024,
      "header_lines": [
        5018
      ],
      "content_start": 5020,
      "content_end": 5023,
      "content": "5020: :label: proof-cor-cloning-pressure-target-set\n5021: \n5022: **Proof.**\n5023: This is a direct consequence of the preceding lemma. The critical target set $I_{\\text{target}}$ is, by definition, a subset of the unfit set $U_k$. Since the lower bound on the cloning probability established in {prf:ref}`lem-mean-companion-fitness-gap` holds for every member of $U_k$, it must also hold for every member of any of its subsets.",
      "metadata": {
        "label": "proof-cor-cloning-pressure-target-set"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [
        "lem-mean-companion-fitness-gap"
      ],
      "raw_directive": "5017: \n5018: :::\n5019: :::{prf:proof}\n5020: :label: proof-cor-cloning-pressure-target-set\n5021: \n5022: **Proof.**\n5023: This is a direct consequence of the preceding lemma. The critical target set $I_{\\text{target}}$ is, by definition, a subset of the unfit set $U_k$. Since the lower bound on the cloning probability established in {prf:ref}`lem-mean-companion-fitness-gap` holds for every member of $U_k$, it must also hold for every member of any of its subsets.\n5024: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-variance-concentration-Hk",
      "title": "**(Variance Concentration in the High-Error Set)**",
      "start_line": 5032,
      "end_line": 5041,
      "header_lines": [
        5033
      ],
      "content_start": 5035,
      "content_end": 5040,
      "content": "5035: :label: lem-variance-concentration-Hk\n5036: \n5037: Let a swarm $k$ be in a high-error state, $\\text{Var}_k(x) > R^2_{\\text{var}}$. There exists a strictly positive, N-uniform constant $c_H \\in (0, 1]$ such that the sum of squared deviations from the mean for the walkers in the unified high-error set $H_k(\\epsilon)$ is bounded below by a fixed fraction of the total sum of squared deviations:\n5038: \n5039: $$\n5040: \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge c_H \\sum_{i \\in \\mathcal{A}_k} \\|\\delta_{x,k,i}\\|^2",
      "metadata": {
        "label": "lem-variance-concentration-Hk"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [],
      "raw_directive": "5032: To make the proof self-contained, we first formalize a direct consequence of the geometric analysis in Chapter 6: that the high-error set, by its very nature, is responsible for a non-vanishing fraction of a swarm's internal variance.\n5033: \n5034: :::{prf:lemma} **(Variance Concentration in the High-Error Set)**\n5035: :label: lem-variance-concentration-Hk\n5036: \n5037: Let a swarm $k$ be in a high-error state, $\\text{Var}_k(x) > R^2_{\\text{var}}$. There exists a strictly positive, N-uniform constant $c_H \\in (0, 1]$ such that the sum of squared deviations from the mean for the walkers in the unified high-error set $H_k(\\epsilon)$ is bounded below by a fixed fraction of the total sum of squared deviations:\n5038: \n5039: $$\n5040: \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge c_H \\sum_{i \\in \\mathcal{A}_k} \\|\\delta_{x,k,i}\\|^2\n5041: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-variance-concentration-Hk",
      "title": null,
      "start_line": 5042,
      "end_line": 5145,
      "header_lines": [
        5043
      ],
      "content_start": 5045,
      "content_end": 5144,
      "content": "5045: :label: proof-lem-variance-concentration-Hk\n5046: \n5047: **Proof.**\n5048: This follows from the definition of $H_k(\\epsilon)$ in the two regimes established by the $\\epsilon$-dichotomy. We prove each regime separately.\n5049: \n5050: **1. Mean-Field Regime ($\\epsilon > D_{\\text{swarm}}$):**\n5051: \n5052: $H_k(\\epsilon)$ is the global outlier set $O_k$. By its definition ({prf:ref}`def-unified-high-low-error-sets`), this set is constructed specifically to contain at least a fraction $(1-\\varepsilon_O)$ of the total sum of squared deviations. In this case, $c_H = 1-\\varepsilon_O$.\n5053: \n5054: **2. Local-Interaction Regime ($\\epsilon \\leq D_{\\text{swarm}}$):**\n5055: \n5056: In this regime, $H_k(\\epsilon)$ is the union of outlier clusters. We must prove that these clusters contribute a non-vanishing fraction of the total variance. The proof proceeds in three steps: (1) decompose variance using Law of Total Variance, (2) bound the within-cluster contribution, (3) show the outlier clusters capture most of the between-cluster contribution.\n5057: \n5058: **Step 1: Variance Decomposition.**\n5059: \n5060: From the Law of Total Variance (as used in the proof of [](#lem-outlier-cluster-fraction-lower-bound)), the total sum of squared deviations decomposes as:\n5061: \n5062: $$\n5063: S_k = k \\cdot \\mathrm{Var}_k(x) = \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) + \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n5064: $$\n5065: \n5066: where $\\{G_1, \\ldots, G_M\\}$ are the clusters, $\\mu$ is the global center of mass, and $\\mu_m$ is the center of mass of cluster $G_m$.\n5067: \n5068: **Step 2: Bounding Within-Cluster Contributions.**\n5069: \n5070: Each cluster has diameter at most $D_{\\text{diam}}(\\epsilon)$, so its internal variance satisfies $\\text{Var}(G_m) \\leq (D_{\\text{diam}}(\\epsilon)/2)^2$. The total within-cluster contribution for all clusters is:\n5071: \n5072: $$\n5073: \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) \\le k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2\n5074: $$\n5075: \n5076: **Step 3: Outlier Clusters Capture the Between-Cluster Variance.**\n5077: \n5078: By definition, the outlier clusters $O_M$ are chosen to capture at least a fraction $(1-\\varepsilon_O)$ of the between-cluster variance:\n5079: \n5080: $$\n5081: \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n5082: $$\n5083: \n5084: Now, for any walker $i$ in an outlier cluster $G_m$ (where $m \\in O_M$), we decompose its squared deviation from the global mean:\n5085: \n5086: $$\n5087: \\|\\delta_{x,k,i}\\|^2 = \\|x_i - \\mu\\|^2 = \\|x_i - \\mu_m + \\mu_m - \\mu\\|^2\n5088: $$\n5089: \n5090: Expanding:\n5091: \n5092: $$\n5093: \\|x_i - \\mu\\|^2 = \\|x_i - \\mu_m\\|^2 + \\|\\mu_m - \\mu\\|^2 + 2\\langle x_i - \\mu_m, \\mu_m - \\mu \\rangle\n5094: $$\n5095: \n5096: Summing over all walkers in outlier clusters:\n5097: \n5098: $$\n5099: \\sum_{m \\in O_M} \\sum_{i \\in G_m} \\|x_i - \\mu\\|^2 = \\sum_{m \\in O_M} \\sum_{i \\in G_m} \\|x_i - \\mu_m\\|^2 + \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 + 2\\sum_{m \\in O_M} \\left\\langle \\sum_{i \\in G_m}(x_i - \\mu_m), \\mu_m - \\mu \\right\\rangle\n5100: $$\n5101: \n5102: The cross-term vanishes because $\\sum_{i \\in G_m}(x_i - \\mu_m) = 0$ (by definition of cluster center of mass). Thus:\n5103: \n5104: $$\n5105: \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 = \\sum_{m \\in O_M} |G_m|\\mathrm{Var}(G_m) + \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2\n5106: $$\n5107: \n5108: The first term is bounded above by the total within-cluster variance (Step 2), and the second term is bounded below by the outlier cluster guarantee (Step 3):\n5109: \n5110: $$\n5111: \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n5112: $$\n5113: \n5114: From Step 1, we know:\n5115: \n5116: $$\n5117: \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 = S_k - \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m)\n5118: $$\n5119: \n5120: For the high-variance regime, we have $\\text{Var}_k(x) > R^2_{\\text{var}}$, which gives:\n5121: \n5122: $$\n5123: \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 > k R^2_{\\mathrm{var}} - k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2 = k R^2_{\\mathrm{means}}\n5124: $$\n5125: \n5126: where $R^2_{\\text{means}} := R^2_{\\text{var}} - (D_{\\text{diam}}(\\epsilon)/2)^2 > 0$ (by the premise of [](#lem-outlier-cluster-fraction-lower-bound)).\n5127: \n5128: Combining these results:\n5129: \n5130: $$\n5131: \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge (1-\\varepsilon_O) k R^2_{\\mathrm{means}}\n5132: $$\n5133: \n5134: Since the total sum of squared deviations is $S_k = k \\cdot \\text{Var}_k(x) > k R^2_{\\text{var}}$, we have:\n5135: \n5136: $$\n5137: \\frac{\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2}{S_k} \\ge \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{R^2_{\\mathrm{var}}}\n5138: $$\n5139: \n5140: This establishes a positive, N-uniform constant:\n5141: \n5142: $$\n5143: c_H := \\min\\left\\{1-\\varepsilon_O, \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{R^2_{\\mathrm{var}}}\\right\\} > 0\n5144: $$",
      "metadata": {
        "label": "proof-lem-variance-concentration-Hk"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [
        "def-unified-high-low-error-sets"
      ],
      "raw_directive": "5042: \n5043: :::\n5044: :::{prf:proof}\n5045: :label: proof-lem-variance-concentration-Hk\n5046: \n5047: **Proof.**\n5048: This follows from the definition of $H_k(\\epsilon)$ in the two regimes established by the $\\epsilon$-dichotomy. We prove each regime separately.\n5049: \n5050: **1. Mean-Field Regime ($\\epsilon > D_{\\text{swarm}}$):**\n5051: \n5052: $H_k(\\epsilon)$ is the global outlier set $O_k$. By its definition ({prf:ref}`def-unified-high-low-error-sets`), this set is constructed specifically to contain at least a fraction $(1-\\varepsilon_O)$ of the total sum of squared deviations. In this case, $c_H = 1-\\varepsilon_O$.\n5053: \n5054: **2. Local-Interaction Regime ($\\epsilon \\leq D_{\\text{swarm}}$):**\n5055: \n5056: In this regime, $H_k(\\epsilon)$ is the union of outlier clusters. We must prove that these clusters contribute a non-vanishing fraction of the total variance. The proof proceeds in three steps: (1) decompose variance using Law of Total Variance, (2) bound the within-cluster contribution, (3) show the outlier clusters capture most of the between-cluster contribution.\n5057: \n5058: **Step 1: Variance Decomposition.**\n5059: \n5060: From the Law of Total Variance (as used in the proof of [](#lem-outlier-cluster-fraction-lower-bound)), the total sum of squared deviations decomposes as:\n5061: \n5062: $$\n5063: S_k = k \\cdot \\mathrm{Var}_k(x) = \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) + \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n5064: $$\n5065: \n5066: where $\\{G_1, \\ldots, G_M\\}$ are the clusters, $\\mu$ is the global center of mass, and $\\mu_m$ is the center of mass of cluster $G_m$.\n5067: \n5068: **Step 2: Bounding Within-Cluster Contributions.**\n5069: \n5070: Each cluster has diameter at most $D_{\\text{diam}}(\\epsilon)$, so its internal variance satisfies $\\text{Var}(G_m) \\leq (D_{\\text{diam}}(\\epsilon)/2)^2$. The total within-cluster contribution for all clusters is:\n5071: \n5072: $$\n5073: \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) \\le k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2\n5074: $$\n5075: \n5076: **Step 3: Outlier Clusters Capture the Between-Cluster Variance.**\n5077: \n5078: By definition, the outlier clusters $O_M$ are chosen to capture at least a fraction $(1-\\varepsilon_O)$ of the between-cluster variance:\n5079: \n5080: $$\n5081: \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n5082: $$\n5083: \n5084: Now, for any walker $i$ in an outlier cluster $G_m$ (where $m \\in O_M$), we decompose its squared deviation from the global mean:\n5085: \n5086: $$\n5087: \\|\\delta_{x,k,i}\\|^2 = \\|x_i - \\mu\\|^2 = \\|x_i - \\mu_m + \\mu_m - \\mu\\|^2\n5088: $$\n5089: \n5090: Expanding:\n5091: \n5092: $$\n5093: \\|x_i - \\mu\\|^2 = \\|x_i - \\mu_m\\|^2 + \\|\\mu_m - \\mu\\|^2 + 2\\langle x_i - \\mu_m, \\mu_m - \\mu \\rangle\n5094: $$\n5095: \n5096: Summing over all walkers in outlier clusters:\n5097: \n5098: $$\n5099: \\sum_{m \\in O_M} \\sum_{i \\in G_m} \\|x_i - \\mu\\|^2 = \\sum_{m \\in O_M} \\sum_{i \\in G_m} \\|x_i - \\mu_m\\|^2 + \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 + 2\\sum_{m \\in O_M} \\left\\langle \\sum_{i \\in G_m}(x_i - \\mu_m), \\mu_m - \\mu \\right\\rangle\n5100: $$\n5101: \n5102: The cross-term vanishes because $\\sum_{i \\in G_m}(x_i - \\mu_m) = 0$ (by definition of cluster center of mass). Thus:\n5103: \n5104: $$\n5105: \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 = \\sum_{m \\in O_M} |G_m|\\mathrm{Var}(G_m) + \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2\n5106: $$\n5107: \n5108: The first term is bounded above by the total within-cluster variance (Step 2), and the second term is bounded below by the outlier cluster guarantee (Step 3):\n5109: \n5110: $$\n5111: \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n5112: $$\n5113: \n5114: From Step 1, we know:\n5115: \n5116: $$\n5117: \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 = S_k - \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m)\n5118: $$\n5119: \n5120: For the high-variance regime, we have $\\text{Var}_k(x) > R^2_{\\text{var}}$, which gives:\n5121: \n5122: $$\n5123: \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 > k R^2_{\\mathrm{var}} - k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2 = k R^2_{\\mathrm{means}}\n5124: $$\n5125: \n5126: where $R^2_{\\text{means}} := R^2_{\\text{var}} - (D_{\\text{diam}}(\\epsilon)/2)^2 > 0$ (by the premise of [](#lem-outlier-cluster-fraction-lower-bound)).\n5127: \n5128: Combining these results:\n5129: \n5130: $$\n5131: \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge (1-\\varepsilon_O) k R^2_{\\mathrm{means}}\n5132: $$\n5133: \n5134: Since the total sum of squared deviations is $S_k = k \\cdot \\text{Var}_k(x) > k R^2_{\\text{var}}$, we have:\n5135: \n5136: $$\n5137: \\frac{\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2}{S_k} \\ge \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{R^2_{\\mathrm{var}}}\n5138: $$\n5139: \n5140: This establishes a positive, N-uniform constant:\n5141: \n5142: $$\n5143: c_H := \\min\\left\\{1-\\varepsilon_O, \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{R^2_{\\mathrm{var}}}\\right\\} > 0\n5144: $$\n5145: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-error-concentration-target-set",
      "title": "Error Concentration in the Target Set",
      "start_line": 5153,
      "end_line": 5165,
      "header_lines": [
        5154
      ],
      "content_start": 5156,
      "content_end": 5164,
      "content": "5156: :label: lem-error-concentration-target-set\n5157: \n5158: Let a swarm state $(S_1, S_2)$ be in the high-error regime, such that $V_{\\mathrm{struct}} > R^2_{\\mathrm{spread}}$. Let $k$ be the high-variance swarm and let $I_{\\text{target}} := I_{11} \\cap U_k \\cap H_k(\\epsilon)$ be the critical target set.\n5159: \n5160: The positional structural error concentrated within this target set is bounded below by a linear function of the total structural error:\n5161: \n5162: $$\n5163: \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2 \\ge c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon)\n5164: $$",
      "metadata": {
        "label": "lem-error-concentration-target-set"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [],
      "raw_directive": "5153: We can now prove the main lemma of this section.\n5154: \n5155: :::{prf:lemma} Error Concentration in the Target Set\n5156: :label: lem-error-concentration-target-set\n5157: \n5158: Let a swarm state $(S_1, S_2)$ be in the high-error regime, such that $V_{\\mathrm{struct}} > R^2_{\\mathrm{spread}}$. Let $k$ be the high-variance swarm and let $I_{\\text{target}} := I_{11} \\cap U_k \\cap H_k(\\epsilon)$ be the critical target set.\n5159: \n5160: The positional structural error concentrated within this target set is bounded below by a linear function of the total structural error:\n5161: \n5162: $$\n5163: \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2 \\ge c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon)\n5164: $$\n5165: "
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-error-concentration-target-set",
      "title": null,
      "start_line": 5166,
      "end_line": 5270,
      "header_lines": [
        5167
      ],
      "content_start": 5169,
      "content_end": 5269,
      "content": "5169: :label: proof-lem-error-concentration-target-set\n5170: \n5171: **Proof.**\n5172: \n5173: The proof is constructive and proceeds in four steps. We first establish a linear relationship between the total system error $V_{\\text{struct}}$ and the internal variance of the high-variance swarm $k$. Second, we use this to find a linear lower bound on the error concentrated within the high-error set $H_k(\\epsilon)$. Third, we subtract the maximum possible error that can exist in the part of $H_k(\\epsilon)$ that is *not* our target set. Finally, we assemble these results to derive the N-uniform constants $c_{\\text{err}}$ and $g_{\\text{err}}$.\n5174: \n5175: **Notation and Scaling:** Let $k$ be the index of the high-variance swarm and $j$ be the index of the other swarm. Following {prf:ref}`def-variance-conversions`, we use:\n5176: - $S_k = \\sum_{i \\in \\mathcal{A}_k} \\|\\delta_{x,k,i}\\|^2$: Un-normalized sum (total variance)\n5177: - $V_{\\text{struct}}$: N-normalized structural error (Lyapunov component)\n5178: - $E(S) := \\sum_{i \\in S} \\|\\Delta\\delta_{x,i}\\|^2$: Un-normalized error in set $S$\n5179: \n5180: **Key conversions used in this proof:**\n5181: \n5182: $$\n5183: S_k = N \\cdot V_{\\text{Var},x}(S_k), \\quad \\frac{E(S)}{N} = \\text{(N-normalized error in set } S\\text{)}\n5184: $$\n5185: \n5186: **Step 1: From Total System Error to Internal Swarm Variance.**\n5187: \n5188: From the proof of {prf:ref}`lem-V_Varx-implies-variance`, we have the inequality on the total sums of squared deviations: $N \\cdot V_{\\text{struct}} \\leq 2(S_k + S_j)$. This gives a lower bound on $S_k$:\n5189: \n5190: $$\n5191: S_k \\ge \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - S_j\n5192: $$\n5193: \n5194: The positions of all walkers lie in the valid domain $\\mathcal{X}_{\\text{valid}}$ of diameter $D_{\\text{valid}}$. Thus, the maximum possible deviation from the mean for any walker is $D_{\\text{valid}}$. This provides a uniform upper bound on $S_j$: $S_j = \\sum_{i \\in \\mathcal{A}_j} \\|\\delta_{x,j,i}\\|^2 \\leq k_j \\cdot D_{\\text{valid}}^2 \\leq N \\cdot D_{\\text{valid}}^2$. Substituting this gives our first key inequality:\n5195: \n5196: $$\n5197: S_k \\ge \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - N \\cdot D_{\\mathrm{valid}}^2 \\quad (*_1)\n5198: $$\n5199: \n5200: **Step 2: From Internal Variance to Error in the High-Error Set $H_k$.**\n5201: \n5202: Using the vector inequality $\\|a-b\\|^2 \\geq (1/2)\\|a\\|^2 - \\|b\\|^2$, we have $\\|\\Delta\\delta_{x,i}\\|^2 \\geq (1/2)\\|\\delta_{x,k,i}\\|^2 - \\|\\delta_{x,j,i}\\|^2$. Summing over the indices $i \\in H_k(\\epsilon)$:\n5203: \n5204: $$\n5205: E(H_k) \\ge \\frac{1}{2}\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 - \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,j,i}\\|^2\n5206: $$\n5207: \n5208: Using **{prf:ref}`lem-variance-concentration-Hk`** on the first term and uniformly bounding the second term gives:\n5209: \n5210: $$\n5211: E(H_k) \\ge \\frac{c_H}{2} S_k - |H_k(\\epsilon)| \\cdot D_{\\mathrm{valid}}^2 \\ge \\frac{c_H}{2} S_k - N \\cdot D_{\\mathrm{valid}}^2\n5212: $$\n5213: \n5214: Now, substitute the lower bound for $S_k$ from inequality $(*_1)$:\n5215: \n5216: $$\n5217: \\begin{aligned}\n5218: E(H_k) &\\ge \\frac{c_H}{2} \\left( \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - N \\cdot D_{\\mathrm{valid}}^2 \\right) - N \\cdot D_{\\mathrm{valid}}^2 \\\\\n5219: &= \\frac{c_H}{4} N \\cdot V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) N \\cdot D_{\\mathrm{valid}}^2\n5220: \\end{aligned}\n5221: $$\n5222: \n5223: Dividing by $N$ gives the per-walker average error in $H_k(\\epsilon)$:\n5224: \n5225: $$\n5226: \\frac{1}{N}E(H_k) \\ge \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2 \\quad (*_2)\n5227: $$\n5228: \n5229: This establishes that the error in $H_k$ is linearly bounded below by $V_{\\text{struct}}$.\n5230: \n5231: **Step 3: Bounding the Error Outside the Target Set.**\n5232: \n5233: The error in our target set is $E(I_{\\text{target}}) = E(H_k) - E(H_k \\setminus I_{\\text{target}})$. We need a uniform upper bound for the error in the complement set $H_k \\setminus I_{\\text{target}}$. The maximum possible squared error for any single walker $i$ is $\\|\\Delta\\delta_{x,i}\\|^2 \\leq (2D_{\\text{valid}})^2 = 4D_{\\text{valid}}^2$. The total error is bounded by the size of the set times this maximum:\n5234: \n5235: $$\n5236: E(H_k \\setminus I_{\\text{target}}) \\le |H_k \\setminus I_{\\text{target}}| \\cdot 4D_{\\mathrm{valid}}^2\n5237: $$\n5238: \n5239: The set $H_k \\setminus I_{\\text{target}}$ contains walkers that are in $H_k$ but not in the three-way intersection $I_{11} \\cap U_k \\cap H_k$. Crucially, from Chapter 7, we have N-uniform lower bounds on the fractional sizes of these sets relative to the $k$ alive walkers: $|H_k|/k \\geq f_H(\\epsilon)$ and $|I_{\\text{target}}|/k \\geq f_{UH}(\\epsilon)$. The size of the complement is $|H_k| - |I_{\\text{target}}|$. A simple and robust upper bound is to use the total number of alive walkers: $|H_k \\setminus I_{\\text{target}}| \\leq k$. Therefore:\n5240: \n5241: $$\n5242: E(H_k \\setminus I_{\\text{target}}) \\le k \\cdot 4D_{\\mathrm{valid}}^2\n5243: $$\n5244: \n5245: **Step 4: Final Assembly with Explicit Normalization.**\n5246: \n5247: We assemble the final inequality for the **N-normalized** error in the target set. Starting from the un-normalized errors $E(\\cdot)$, we divide by $N$ to convert to Lyapunov normalization:\n5248: \n5249: $$\n5250: \\frac{1}{N}E(I_{\\text{target}}) = \\frac{1}{N}E(H_k) - \\frac{1}{N}E(H_k \\setminus I_{\\text{target}})\n5251: $$\n5252: \n5253: **Applying bounds from Steps 2-3:** Substitute the lower bound for the first term from $(*_2)$ and the upper bound for the second term from Step 3:\n5254: \n5255: $$\n5256: \\ge \\left[ \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2 \\right] - \\frac{k \\cdot 4D_{\\mathrm{valid}}^2}{N}\n5257: $$\n5258: \n5259: **N-uniformity:** Since $k \\leq N$ (number of alive walkers bounded by total slots), the ratio $k/N \\leq 1$ is state-dependent but uniformly bounded. We can weaken the inequality to achieve a clean, N-independent form by replacing $k/N$ with its worst case 1:\n5260: \n5261: $$\n5262: \\ge \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2 - 4D_{\\mathrm{valid}}^2 = \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 5\\right) D_{\\mathrm{valid}}^2\n5263: $$\n5264: \n5265: This is the desired linear lower bound. We can now define the final, **explicitly N-uniform** constants:\n5266: *   $c_{\\text{err}}(\\epsilon) := \\frac{c_H(\\epsilon)}{4}$\n5267: *   $g_{\\text{err}}(\\epsilon) := \\left(\\frac{c_H(\\epsilon)}{2} + 5\\right) D_{\\mathrm{valid}}^2$\n5268: \n5269: Since $c_H(\\epsilon)$ is a positive N-uniform constant from our supporting lemma and $D_{\\text{valid}}$ is a fixed environmental parameter, both $c_{\\text{err}}(\\epsilon)$ and $g_{\\text{err}}(\\epsilon)$ are strictly N-uniform. This completes the proof.",
      "metadata": {
        "label": "proof-lem-error-concentration-target-set"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [
        "def-variance-conversions",
        "lem-V_Varx-implies-variance",
        "lem-variance-concentration-Hk"
      ],
      "raw_directive": "5166: where $c_{err}(\\epsilon) > 0$ and $g_{err}(\\epsilon) \\ge 0$ are **strictly N-uniform constants**.\n5167: :::\n5168: :::{prf:proof}\n5169: :label: proof-lem-error-concentration-target-set\n5170: \n5171: **Proof.**\n5172: \n5173: The proof is constructive and proceeds in four steps. We first establish a linear relationship between the total system error $V_{\\text{struct}}$ and the internal variance of the high-variance swarm $k$. Second, we use this to find a linear lower bound on the error concentrated within the high-error set $H_k(\\epsilon)$. Third, we subtract the maximum possible error that can exist in the part of $H_k(\\epsilon)$ that is *not* our target set. Finally, we assemble these results to derive the N-uniform constants $c_{\\text{err}}$ and $g_{\\text{err}}$.\n5174: \n5175: **Notation and Scaling:** Let $k$ be the index of the high-variance swarm and $j$ be the index of the other swarm. Following {prf:ref}`def-variance-conversions`, we use:\n5176: - $S_k = \\sum_{i \\in \\mathcal{A}_k} \\|\\delta_{x,k,i}\\|^2$: Un-normalized sum (total variance)\n5177: - $V_{\\text{struct}}$: N-normalized structural error (Lyapunov component)\n5178: - $E(S) := \\sum_{i \\in S} \\|\\Delta\\delta_{x,i}\\|^2$: Un-normalized error in set $S$\n5179: \n5180: **Key conversions used in this proof:**\n5181: \n5182: $$\n5183: S_k = N \\cdot V_{\\text{Var},x}(S_k), \\quad \\frac{E(S)}{N} = \\text{(N-normalized error in set } S\\text{)}\n5184: $$\n5185: \n5186: **Step 1: From Total System Error to Internal Swarm Variance.**\n5187: \n5188: From the proof of {prf:ref}`lem-V_Varx-implies-variance`, we have the inequality on the total sums of squared deviations: $N \\cdot V_{\\text{struct}} \\leq 2(S_k + S_j)$. This gives a lower bound on $S_k$:\n5189: \n5190: $$\n5191: S_k \\ge \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - S_j\n5192: $$\n5193: \n5194: The positions of all walkers lie in the valid domain $\\mathcal{X}_{\\text{valid}}$ of diameter $D_{\\text{valid}}$. Thus, the maximum possible deviation from the mean for any walker is $D_{\\text{valid}}$. This provides a uniform upper bound on $S_j$: $S_j = \\sum_{i \\in \\mathcal{A}_j} \\|\\delta_{x,j,i}\\|^2 \\leq k_j \\cdot D_{\\text{valid}}^2 \\leq N \\cdot D_{\\text{valid}}^2$. Substituting this gives our first key inequality:\n5195: \n5196: $$\n5197: S_k \\ge \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - N \\cdot D_{\\mathrm{valid}}^2 \\quad (*_1)\n5198: $$\n5199: \n5200: **Step 2: From Internal Variance to Error in the High-Error Set $H_k$.**\n5201: \n5202: Using the vector inequality $\\|a-b\\|^2 \\geq (1/2)\\|a\\|^2 - \\|b\\|^2$, we have $\\|\\Delta\\delta_{x,i}\\|^2 \\geq (1/2)\\|\\delta_{x,k,i}\\|^2 - \\|\\delta_{x,j,i}\\|^2$. Summing over the indices $i \\in H_k(\\epsilon)$:\n5203: \n5204: $$\n5205: E(H_k) \\ge \\frac{1}{2}\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 - \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,j,i}\\|^2\n5206: $$\n5207: \n5208: Using **{prf:ref}`lem-variance-concentration-Hk`** on the first term and uniformly bounding the second term gives:\n5209: \n5210: $$\n5211: E(H_k) \\ge \\frac{c_H}{2} S_k - |H_k(\\epsilon)| \\cdot D_{\\mathrm{valid}}^2 \\ge \\frac{c_H}{2} S_k - N \\cdot D_{\\mathrm{valid}}^2\n5212: $$\n5213: \n5214: Now, substitute the lower bound for $S_k$ from inequality $(*_1)$:\n5215: \n5216: $$\n5217: \\begin{aligned}\n5218: E(H_k) &\\ge \\frac{c_H}{2} \\left( \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - N \\cdot D_{\\mathrm{valid}}^2 \\right) - N \\cdot D_{\\mathrm{valid}}^2 \\\\\n5219: &= \\frac{c_H}{4} N \\cdot V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) N \\cdot D_{\\mathrm{valid}}^2\n5220: \\end{aligned}\n5221: $$\n5222: \n5223: Dividing by $N$ gives the per-walker average error in $H_k(\\epsilon)$:\n5224: \n5225: $$\n5226: \\frac{1}{N}E(H_k) \\ge \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2 \\quad (*_2)\n5227: $$\n5228: \n5229: This establishes that the error in $H_k$ is linearly bounded below by $V_{\\text{struct}}$.\n5230: \n5231: **Step 3: Bounding the Error Outside the Target Set.**\n5232: \n5233: The error in our target set is $E(I_{\\text{target}}) = E(H_k) - E(H_k \\setminus I_{\\text{target}})$. We need a uniform upper bound for the error in the complement set $H_k \\setminus I_{\\text{target}}$. The maximum possible squared error for any single walker $i$ is $\\|\\Delta\\delta_{x,i}\\|^2 \\leq (2D_{\\text{valid}})^2 = 4D_{\\text{valid}}^2$. The total error is bounded by the size of the set times this maximum:\n5234: \n5235: $$\n5236: E(H_k \\setminus I_{\\text{target}}) \\le |H_k \\setminus I_{\\text{target}}| \\cdot 4D_{\\mathrm{valid}}^2\n5237: $$\n5238: \n5239: The set $H_k \\setminus I_{\\text{target}}$ contains walkers that are in $H_k$ but not in the three-way intersection $I_{11} \\cap U_k \\cap H_k$. Crucially, from Chapter 7, we have N-uniform lower bounds on the fractional sizes of these sets relative to the $k$ alive walkers: $|H_k|/k \\geq f_H(\\epsilon)$ and $|I_{\\text{target}}|/k \\geq f_{UH}(\\epsilon)$. The size of the complement is $|H_k| - |I_{\\text{target}}|$. A simple and robust upper bound is to use the total number of alive walkers: $|H_k \\setminus I_{\\text{target}}| \\leq k$. Therefore:\n5240: \n5241: $$\n5242: E(H_k \\setminus I_{\\text{target}}) \\le k \\cdot 4D_{\\mathrm{valid}}^2\n5243: $$\n5244: \n5245: **Step 4: Final Assembly with Explicit Normalization.**\n5246: \n5247: We assemble the final inequality for the **N-normalized** error in the target set. Starting from the un-normalized errors $E(\\cdot)$, we divide by $N$ to convert to Lyapunov normalization:\n5248: \n5249: $$\n5250: \\frac{1}{N}E(I_{\\text{target}}) = \\frac{1}{N}E(H_k) - \\frac{1}{N}E(H_k \\setminus I_{\\text{target}})\n5251: $$\n5252: \n5253: **Applying bounds from Steps 2-3:** Substitute the lower bound for the first term from $(*_2)$ and the upper bound for the second term from Step 3:\n5254: \n5255: $$\n5256: \\ge \\left[ \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2 \\right] - \\frac{k \\cdot 4D_{\\mathrm{valid}}^2}{N}\n5257: $$\n5258: \n5259: **N-uniformity:** Since $k \\leq N$ (number of alive walkers bounded by total slots), the ratio $k/N \\leq 1$ is state-dependent but uniformly bounded. We can weaken the inequality to achieve a clean, N-independent form by replacing $k/N$ with its worst case 1:\n5260: \n5261: $$\n5262: \\ge \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2 - 4D_{\\mathrm{valid}}^2 = \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 5\\right) D_{\\mathrm{valid}}^2\n5263: $$\n5264: \n5265: This is the desired linear lower bound. We can now define the final, **explicitly N-uniform** constants:\n5266: *   $c_{\\text{err}}(\\epsilon) := \\frac{c_H(\\epsilon)}{4}$\n5267: *   $g_{\\text{err}}(\\epsilon) := \\left(\\frac{c_H(\\epsilon)}{2} + 5\\right) D_{\\mathrm{valid}}^2$\n5268: \n5269: Since $c_H(\\epsilon)$ is a positive N-uniform constant from our supporting lemma and $D_{\\text{valid}}$ is a fixed environmental parameter, both $c_{\\text{err}}(\\epsilon)$ and $g_{\\text{err}}(\\epsilon)$ are strictly N-uniform. This completes the proof.\n5270: "
    },
    {
      "directive_type": "proof",
      "label": "proof-prop-n-uniformity-keystone-addendum",
      "title": null,
      "start_line": 5280,
      "end_line": 5377,
      "header_lines": [
        5281
      ],
      "content_start": 5282,
      "content_end": 5376,
      "content": "5282: :::{prf:proof}\n5283: :label: proof-prop-n-uniformity-keystone-addendum\n5284: **Proof of the N-Uniform Quantitative Keystone Lemma ({prf:ref}`lem-quantitative-keystone`).**\n5285: \n5286: The proof establishes the inequality for the high-error regime ($V_{\\text{struct}} > R^2_{\\text{spread}}$) and then defines the global offset $g_{\\max}(\\epsilon)$ to ensure it holds everywhere, as per the strategy outlined in Section 8.1.\n5287: \n5288: **1. Setup for the High-Error Regime.**\n5289: Assume the initial state $(S_1, S_2)$ is in the high-error regime. Without loss of generality, let swarm $k=1$ be the high-variance swarm. This guarantees the existence of a non-empty **critical target set** $I_{\\text{target}} = I_{11} \\cap U_1 \\cap H_1(\\epsilon)$. We seek a lower bound for the error-weighted cloning activity, $E_w$:\n5290: \n5291: $$\n5292: E_w := \\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2\n5293: $$\n5294: \n5295: **2. Lower-Bound the Sum by the Critical Target Set.**\n5296: The sum $E_w$ consists of non-negative terms and is bounded below by the sum over the critical target set $I_{\\text{target}} \\subseteq I_{11}$:\n5297: \n5298: $$\n5299: E_w \\ge \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2\n5300: $$\n5301: \n5302: We focus on the cloning probability `p_1,i` because swarm 1 is the high-variance swarm for which our guarantees on the unfit and high-error sets hold.\n5303: \n5304: **3. Decompose the Sum using Average Properties.**\n5305: Instead of factoring out the minimum probability, we use a standard statistical decomposition. Let\n5306: \n5307: $$\n5308: \\bar{p}_{target} = \\frac{1}{|I_{target}|}\\sum_{i \\in I_{target}} p_{1,i}\n5309: $$\n5310: \n5311:  be the average cloning probability over the target set, and\n5312: \n5313: $$\n5314: \\bar{E}_{target} = \\frac{1}{|I_{target}|}\\sum_{i \\in I_{target}} \\|\\Delta\\delta_{x,i}\\|^2\n5315: $$\n5316: \n5317:  be the average error. The sum can be written as:\n5318: \n5319: $$\n5320: \\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2 = |I_{target}| \\left( \\bar{p}_{target} \\cdot \\bar{E}_{target} + \\text{Cov}(p_{1,i}, \\|\\Delta\\delta_{x,i}\\|^2) \\right)\n5321: $$\n5322: \n5323: where `Cov` is the covariance between the cloning probability and the error within the target set. We can establish a lower bound by using the average properties and bounding the covariance term.\n5324: \n5325: The covariance term can be negative if walkers with larger errors happen to have smaller cloning probabilities. However, we can establish a robust lower bound by noting that $p_{1,i} \\geq 0$ for all `i`. We can use the lower bound on the *average* probability.\n5326: \n5327: Let's use a simpler, more direct argument. The sum is bounded below by the sum where each `p_{1,i}` is replaced by its lower bound. From **{prf:ref}`lem-mean-companion-fitness-gap` (`lem-unfit-cloning-pressure`)**, every walker $i \\in U_k$ (and therefore every walker in `I_target`) has a probability $p_{1,i} \\geq p_u(\\varepsilon)$. This allows us to use the minimum probability correctly.\n5328: \n5329: $$\n5330: E_w \\ge \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2 \\ge \\frac{p_u(\\epsilon)}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2\n5331: $$\n5332: \n5333: **4. Substitute the Error Concentration Bound.**\n5334: We now have an expression that is the product of two N-uniform lower bounds.\n5335: *   **Cloning Pressure:** The term $p_u(\\varepsilon)$ is the N-uniform minimum cloning probability from **{prf:ref}`lem-mean-companion-fitness-gap`**.\n5336: *   **Error Concentration:** The term\n5337: \n5338: $$\n5339: \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2\n5340: $$\n5341: \n5342:  is exactly the quantity lower-bounded by the **Error Concentration Lemma (8.4.1)**.\n5343: \n5344: Substituting the bound from {prf:ref}`lem-variance-concentration-Hk` gives:\n5345: \n5346: $$\n5347: E_w \\ge p_u(\\epsilon) \\cdot \\left( c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon) \\right)\n5348: $$\n5349: \n5350: **5. Define N-Uniform Constants for the High-Error Regime.**\n5351: Substituting these two bounds into the inequality from Step 3 gives:\n5352: \n5353: $$\n5354: E_w \\ge p_u(\\epsilon) \\cdot \\left( c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon) \\right)\n5355: $$\n5356: \n5357: We define the N-uniform, $\\varepsilon$-dependent constants that emerge from this constructive proof:\n5358: *   The **feedback coefficient:** $\\chi(\\epsilon) := p_u(\\epsilon) \\cdot c_{err}(\\epsilon) > 0$\n5359: *   The **partial offset:** $g_{\\text{partial}}(\\epsilon) := p_u(\\epsilon) \\cdot g_{err}(\\epsilon) \\ge 0$\n5360: \n5361: This establishes the desired linear lower bound for any state in the high-error regime:\n5362: \n5363: $$\n5364: E_w \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\text{partial}}(\\epsilon)\n5365: $$\n5366: \n5367: **6. Finalize the Global Inequality.**\n5368: As outlined in the proof strategy (Section 8.1), we define the global offset constant $g_{\\max}(\\epsilon)$ to ensure the inequality holds for all states by taking the maximum of the offsets required for the low-error and high-error regimes:\n5369: \n5370: $$\n5371: g_{\\max}(\\epsilon) := \\max\\bigl(g_{\\text{partial}}(\\epsilon),\\, \\chi(\\epsilon) R^2_{\\text{spread}}\\bigr)\n5372: $$\n5373: \n5374: This choice ensures the inequality is satisfied everywhere. Since $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are constructed entirely from N-uniform constants, they are themselves independent of $N$.\n5375: \n5376: This completes the rigorous, constructive proof of the N-Uniform Quantitative Keystone Lemma.",
      "metadata": {
        "label": "proof-prop-n-uniformity-keystone-addendum"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [
        "lem-quantitative-keystone",
        "lem-mean-companion-fitness-gap",
        "lem-variance-concentration-Hk"
      ],
      "raw_directive": "5280: We now assemble these results to provide the final, rigorous proof of the main theorem of this analysis. The strategy is to show that the large error concentrated in the target set, when weighted by the strong average cloning probability of that same set, produces a collective corrective force that is proportional to the total system error.\n5281: \n5282: :::{prf:proof}\n5283: :label: proof-prop-n-uniformity-keystone-addendum\n5284: **Proof of the N-Uniform Quantitative Keystone Lemma ({prf:ref}`lem-quantitative-keystone`).**\n5285: \n5286: The proof establishes the inequality for the high-error regime ($V_{\\text{struct}} > R^2_{\\text{spread}}$) and then defines the global offset $g_{\\max}(\\epsilon)$ to ensure it holds everywhere, as per the strategy outlined in Section 8.1.\n5287: \n5288: **1. Setup for the High-Error Regime.**\n5289: Assume the initial state $(S_1, S_2)$ is in the high-error regime. Without loss of generality, let swarm $k=1$ be the high-variance swarm. This guarantees the existence of a non-empty **critical target set** $I_{\\text{target}} = I_{11} \\cap U_1 \\cap H_1(\\epsilon)$. We seek a lower bound for the error-weighted cloning activity, $E_w$:\n5290: \n5291: $$\n5292: E_w := \\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2\n5293: $$\n5294: \n5295: **2. Lower-Bound the Sum by the Critical Target Set.**\n5296: The sum $E_w$ consists of non-negative terms and is bounded below by the sum over the critical target set $I_{\\text{target}} \\subseteq I_{11}$:\n5297: \n5298: $$\n5299: E_w \\ge \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2\n5300: $$\n5301: \n5302: We focus on the cloning probability `p_1,i` because swarm 1 is the high-variance swarm for which our guarantees on the unfit and high-error sets hold.\n5303: \n5304: **3. Decompose the Sum using Average Properties.**\n5305: Instead of factoring out the minimum probability, we use a standard statistical decomposition. Let\n5306: \n5307: $$\n5308: \\bar{p}_{target} = \\frac{1}{|I_{target}|}\\sum_{i \\in I_{target}} p_{1,i}\n5309: $$\n5310: \n5311:  be the average cloning probability over the target set, and\n5312: \n5313: $$\n5314: \\bar{E}_{target} = \\frac{1}{|I_{target}|}\\sum_{i \\in I_{target}} \\|\\Delta\\delta_{x,i}\\|^2\n5315: $$\n5316: \n5317:  be the average error. The sum can be written as:\n5318: \n5319: $$\n5320: \\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2 = |I_{target}| \\left( \\bar{p}_{target} \\cdot \\bar{E}_{target} + \\text{Cov}(p_{1,i}, \\|\\Delta\\delta_{x,i}\\|^2) \\right)\n5321: $$\n5322: \n5323: where `Cov` is the covariance between the cloning probability and the error within the target set. We can establish a lower bound by using the average properties and bounding the covariance term.\n5324: \n5325: The covariance term can be negative if walkers with larger errors happen to have smaller cloning probabilities. However, we can establish a robust lower bound by noting that $p_{1,i} \\geq 0$ for all `i`. We can use the lower bound on the *average* probability.\n5326: \n5327: Let's use a simpler, more direct argument. The sum is bounded below by the sum where each `p_{1,i}` is replaced by its lower bound. From **{prf:ref}`lem-mean-companion-fitness-gap` (`lem-unfit-cloning-pressure`)**, every walker $i \\in U_k$ (and therefore every walker in `I_target`) has a probability $p_{1,i} \\geq p_u(\\varepsilon)$. This allows us to use the minimum probability correctly.\n5328: \n5329: $$\n5330: E_w \\ge \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2 \\ge \\frac{p_u(\\epsilon)}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2\n5331: $$\n5332: \n5333: **4. Substitute the Error Concentration Bound.**\n5334: We now have an expression that is the product of two N-uniform lower bounds.\n5335: *   **Cloning Pressure:** The term $p_u(\\varepsilon)$ is the N-uniform minimum cloning probability from **{prf:ref}`lem-mean-companion-fitness-gap`**.\n5336: *   **Error Concentration:** The term\n5337: \n5338: $$\n5339: \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2\n5340: $$\n5341: \n5342:  is exactly the quantity lower-bounded by the **Error Concentration Lemma (8.4.1)**.\n5343: \n5344: Substituting the bound from {prf:ref}`lem-variance-concentration-Hk` gives:\n5345: \n5346: $$\n5347: E_w \\ge p_u(\\epsilon) \\cdot \\left( c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon) \\right)\n5348: $$\n5349: \n5350: **5. Define N-Uniform Constants for the High-Error Regime.**\n5351: Substituting these two bounds into the inequality from Step 3 gives:\n5352: \n5353: $$\n5354: E_w \\ge p_u(\\epsilon) \\cdot \\left( c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon) \\right)\n5355: $$\n5356: \n5357: We define the N-uniform, $\\varepsilon$-dependent constants that emerge from this constructive proof:\n5358: *   The **feedback coefficient:** $\\chi(\\epsilon) := p_u(\\epsilon) \\cdot c_{err}(\\epsilon) > 0$\n5359: *   The **partial offset:** $g_{\\text{partial}}(\\epsilon) := p_u(\\epsilon) \\cdot g_{err}(\\epsilon) \\ge 0$\n5360: \n5361: This establishes the desired linear lower bound for any state in the high-error regime:\n5362: \n5363: $$\n5364: E_w \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\text{partial}}(\\epsilon)\n5365: $$\n5366: \n5367: **6. Finalize the Global Inequality.**\n5368: As outlined in the proof strategy (Section 8.1), we define the global offset constant $g_{\\max}(\\epsilon)$ to ensure the inequality holds for all states by taking the maximum of the offsets required for the low-error and high-error regimes:\n5369: \n5370: $$\n5371: g_{\\max}(\\epsilon) := \\max\\bigl(g_{\\text{partial}}(\\epsilon),\\, \\chi(\\epsilon) R^2_{\\text{spread}}\\bigr)\n5372: $$\n5373: \n5374: This choice ensures the inequality is satisfied everywhere. Since $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are constructed entirely from N-uniform constants, they are themselves independent of $N$.\n5375: \n5376: This completes the rigorous, constructive proof of the N-Uniform Quantitative Keystone Lemma.\n5377: "
    },
    {
      "directive_type": "proposition",
      "label": "prop-n-uniformity-keystone",
      "title": "N-Uniformity of Keystone Constants",
      "start_line": 5561,
      "end_line": 5571,
      "header_lines": [
        5562
      ],
      "content_start": 5564,
      "content_end": 5570,
      "content": "5564: :label: prop-n-uniformity-keystone\n5565: \n5566: The Keystone constants $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are strictly independent of the swarm size $N$. More precisely, for any fixed choice of system parameters ($\\epsilon$, domain, pipeline parameters, etc.), there exist finite positive constants $\\chi_0(\\epsilon)$ and $g_0(\\epsilon)$ such that for all $N \\geq 2$:\n5567: \n5568: $$\n5569: \\chi(\\epsilon) = \\chi_0(\\epsilon) \\quad \\text{and} \\quad g_{\\max}(\\epsilon) = g_0(\\epsilon)\n5570: $$",
      "metadata": {
        "label": "prop-n-uniformity-keystone"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [],
      "raw_directive": "5561: We now provide a formal verification that both Keystone constants, $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$, are **strictly independent of the swarm size N**, establishing that they are $O(1)$ as $N \\to \\infty$.\n5562: \n5563: :::{prf:proposition} N-Uniformity of Keystone Constants\n5564: :label: prop-n-uniformity-keystone\n5565: \n5566: The Keystone constants $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are strictly independent of the swarm size $N$. More precisely, for any fixed choice of system parameters ($\\epsilon$, domain, pipeline parameters, etc.), there exist finite positive constants $\\chi_0(\\epsilon)$ and $g_0(\\epsilon)$ such that for all $N \\geq 2$:\n5567: \n5568: $$\n5569: \\chi(\\epsilon) = \\chi_0(\\epsilon) \\quad \\text{and} \\quad g_{\\max}(\\epsilon) = g_0(\\epsilon)\n5570: $$\n5571: "
    },
    {
      "directive_type": "proof",
      "label": "proof-prop-n-uniformity-keystone",
      "title": null,
      "start_line": 5573,
      "end_line": 5650,
      "header_lines": [
        5574
      ],
      "content_start": 5576,
      "content_end": 5649,
      "content": "5576: :label: proof-prop-n-uniformity-keystone\n5577: \n5578: **Proof.**\n5579: \n5580: We verify N-independence by systematically checking every component in the definitions of $\\chi(\\epsilon) = p_u(\\epsilon) \\cdot c_{\\text{err}}(\\epsilon)$ and $g_{\\max}(\\epsilon) = \\max(p_u(\\epsilon) \\cdot g_{\\text{err}}(\\epsilon), \\chi(\\epsilon)R^2_{\\text{spread}})$.\n5581: \n5582: **Part 1: N-Independence of $p_u(\\epsilon)$**\n5583: \n5584: From Section 8.6.1.1, $p_u(\\epsilon)$ is defined as:\n5585: \n5586: $$\n5587: p_u(\\epsilon) = \\frac{1}{p_{\\max}} \\left( \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}})} \\right)\n5588: $$\n5589: \n5590: We verify each component:\n5591: - $p_{\\max}$: User-defined parameter, independent of $N$ \u2713\n5592: - $\\varepsilon_{\\text{clone}}$: User-defined parameter, independent of $N$ \u2713\n5593: - $V_{\\text{pot,max}} = (g_{A,\\max} + \\eta)^{\\alpha+\\beta}$: Depends only on pipeline parameters ($g_{A,\\max}$, $\\eta$, $\\alpha$, $\\beta$), all independent of $N$ \u2713\n5594: - $\\kappa_{V,\\text{gap}}(\\epsilon)$: The fitness potential gap. We trace its dependencies:\n5595:   - $\\kappa_{\\text{meas}}(\\epsilon)$: From [](#thm-geometry-guarantees-variance), this depends on the phase-space separation constants $D_H(\\epsilon)$ and $R_L(\\epsilon)$, which are defined in terms of:\n5596:     - Geometric properties of the outlier/cluster definitions ($\\epsilon_O$, $D_{\\text{diam}}(\\epsilon)$): Independent of $N$ \u2713\n5597:     - Domain diameter $D_{\\text{valid}}$: Independent of $N$ \u2713\n5598:     - Velocity bounds: Independent of $N$ \u2713\n5599:   - Pipeline transformations (standardization, rescaling): Depend only on ($g'_{\\min}$, $\\sigma'_{\\max}$, $\\eta$), all independent of $N$ \u2713\n5600: \n5601: **Conclusion:** $p_u(\\epsilon)$ is strictly independent of $N$.\n5602: \n5603: **Part 2: N-Independence of $c_{\\text{err}}(\\epsilon)$**\n5604: \n5605: From Section 8.6.1.2, $c_{\\text{err}}(\\epsilon) \\propto \\lambda_2 \\cdot c_H \\cdot f_{UH}(\\epsilon)$. We verify each component:\n5606: \n5607: - $\\lambda_2$: The minimum eigenvalue from the Coercivity Lemma for the Lyapunov function. From Lemma 3.4.1 (referenced but not shown), this depends only on the Lyapunov structure constants ($b$, $\\lambda_v$), which are parameters of the function definition, independent of $N$ \u2713\n5608: \n5609: - $c_H$: The variance concentration constant from [](#lem-variance-concentration-Hk). From the proof (lines 3828-3908):\n5610:   - **Mean-field regime**: $c_H = 1 - \\epsilon_O$, where $\\epsilon_O$ is the outlier threshold parameter, independent of $N$ \u2713\n5611:   - **Local-interaction regime**: $c_H = \\min\\{1-\\epsilon_O, (1-\\epsilon_O)R^2_{\\text{means}}/R^2_{\\text{var}}\\}$, where:\n5612:     - $R^2_{\\text{means}} = R^2_{\\text{var}} - (D_{\\text{diam}}(\\epsilon)/2)^2$: Depends only on variance threshold and cluster diameter, both independent of $N$ \u2713\n5613: \n5614: - $f_{UH}(\\epsilon)$: The overlap fraction from [](#thm-unfit-high-error-overlap-fraction). This depends on:\n5615:   - Population fraction lower bounds $f_U(\\epsilon)$ and $f_H(\\epsilon)$ from Chapters 6-7\n5616:   - From {prf:ref}`lem-outlier-fraction-lower-bound` and 6.4.3, these fractions are **defined as N-uniform constants** - they are constructed precisely to be independent of swarm size \u2713\n5617:   - The proof uses only geometric properties (phase-space packing, variance decomposition) that scale with the number of walkers but produce **fractions** that remain constant \u2713\n5618: \n5619: **Conclusion:** $c_{\\text{err}}(\\epsilon)$ is strictly independent of $N$.\n5620: \n5621: **Part 3: N-Independence of $g_{\\text{err}}(\\epsilon)$**\n5622: \n5623: From Section 8.6.2.1:\n5624: \n5625: $$\n5626: g_{err}(\\epsilon) := g'_{err} + (1 - f_{UH}(\\epsilon)) \\cdot 4D_{\\mathrm{valid}}^2\n5627: $$\n5628: \n5629: - $g'_{\\text{err}}$: A constant from {prf:ref}`lem-variance-concentration-Hk` involving domain diameter, independent of $N$ \u2713\n5630: - $f_{UH}(\\epsilon)$: Already verified as N-independent in Part 2 \u2713\n5631: - $D_{\\text{valid}}$: Domain diameter, independent of $N$ \u2713\n5632: \n5633: **Conclusion:** $g_{\\text{err}}(\\epsilon)$ is strictly independent of $N$.\n5634: \n5635: **Part 4: N-Independence of $g_{\\max}(\\epsilon)$ and $\\chi(\\epsilon)$**\n5636: \n5637: Since all components are N-independent:\n5638: \n5639: $$\n5640: \\chi(\\epsilon) = p_u(\\epsilon) \\cdot c_{err}(\\epsilon) \\quad \\text{(product of N-independent terms)} \\quad \u2713\n5641: $$\n5642: \n5643: $$\n5644: g_{\\max}(\\epsilon) = \\max(p_u(\\epsilon) \\cdot g_{err}(\\epsilon), \\chi(\\epsilon) R^2_{\\text{spread}}) \\quad \\text{(max of N-independent terms)} \\quad \u2713\n5645: $$\n5646: \n5647: where $R^2_{\\text{spread}}$ is the variance threshold, a fixed constant independent of $N$ \u2713\n5648: \n5649: **Conclusion:** Both $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are strictly independent of $N$, depending only on $\\epsilon$ and fixed system parameters.",
      "metadata": {
        "label": "proof-prop-n-uniformity-keystone"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [
        "lem-outlier-fraction-lower-bound",
        "lem-variance-concentration-Hk"
      ],
      "raw_directive": "5573: :::\n5574: \n5575: :::{prf:proof}\n5576: :label: proof-prop-n-uniformity-keystone\n5577: \n5578: **Proof.**\n5579: \n5580: We verify N-independence by systematically checking every component in the definitions of $\\chi(\\epsilon) = p_u(\\epsilon) \\cdot c_{\\text{err}}(\\epsilon)$ and $g_{\\max}(\\epsilon) = \\max(p_u(\\epsilon) \\cdot g_{\\text{err}}(\\epsilon), \\chi(\\epsilon)R^2_{\\text{spread}})$.\n5581: \n5582: **Part 1: N-Independence of $p_u(\\epsilon)$**\n5583: \n5584: From Section 8.6.1.1, $p_u(\\epsilon)$ is defined as:\n5585: \n5586: $$\n5587: p_u(\\epsilon) = \\frac{1}{p_{\\max}} \\left( \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}})} \\right)\n5588: $$\n5589: \n5590: We verify each component:\n5591: - $p_{\\max}$: User-defined parameter, independent of $N$ \u2713\n5592: - $\\varepsilon_{\\text{clone}}$: User-defined parameter, independent of $N$ \u2713\n5593: - $V_{\\text{pot,max}} = (g_{A,\\max} + \\eta)^{\\alpha+\\beta}$: Depends only on pipeline parameters ($g_{A,\\max}$, $\\eta$, $\\alpha$, $\\beta$), all independent of $N$ \u2713\n5594: - $\\kappa_{V,\\text{gap}}(\\epsilon)$: The fitness potential gap. We trace its dependencies:\n5595:   - $\\kappa_{\\text{meas}}(\\epsilon)$: From [](#thm-geometry-guarantees-variance), this depends on the phase-space separation constants $D_H(\\epsilon)$ and $R_L(\\epsilon)$, which are defined in terms of:\n5596:     - Geometric properties of the outlier/cluster definitions ($\\epsilon_O$, $D_{\\text{diam}}(\\epsilon)$): Independent of $N$ \u2713\n5597:     - Domain diameter $D_{\\text{valid}}$: Independent of $N$ \u2713\n5598:     - Velocity bounds: Independent of $N$ \u2713\n5599:   - Pipeline transformations (standardization, rescaling): Depend only on ($g'_{\\min}$, $\\sigma'_{\\max}$, $\\eta$), all independent of $N$ \u2713\n5600: \n5601: **Conclusion:** $p_u(\\epsilon)$ is strictly independent of $N$.\n5602: \n5603: **Part 2: N-Independence of $c_{\\text{err}}(\\epsilon)$**\n5604: \n5605: From Section 8.6.1.2, $c_{\\text{err}}(\\epsilon) \\propto \\lambda_2 \\cdot c_H \\cdot f_{UH}(\\epsilon)$. We verify each component:\n5606: \n5607: - $\\lambda_2$: The minimum eigenvalue from the Coercivity Lemma for the Lyapunov function. From Lemma 3.4.1 (referenced but not shown), this depends only on the Lyapunov structure constants ($b$, $\\lambda_v$), which are parameters of the function definition, independent of $N$ \u2713\n5608: \n5609: - $c_H$: The variance concentration constant from [](#lem-variance-concentration-Hk). From the proof (lines 3828-3908):\n5610:   - **Mean-field regime**: $c_H = 1 - \\epsilon_O$, where $\\epsilon_O$ is the outlier threshold parameter, independent of $N$ \u2713\n5611:   - **Local-interaction regime**: $c_H = \\min\\{1-\\epsilon_O, (1-\\epsilon_O)R^2_{\\text{means}}/R^2_{\\text{var}}\\}$, where:\n5612:     - $R^2_{\\text{means}} = R^2_{\\text{var}} - (D_{\\text{diam}}(\\epsilon)/2)^2$: Depends only on variance threshold and cluster diameter, both independent of $N$ \u2713\n5613: \n5614: - $f_{UH}(\\epsilon)$: The overlap fraction from [](#thm-unfit-high-error-overlap-fraction). This depends on:\n5615:   - Population fraction lower bounds $f_U(\\epsilon)$ and $f_H(\\epsilon)$ from Chapters 6-7\n5616:   - From {prf:ref}`lem-outlier-fraction-lower-bound` and 6.4.3, these fractions are **defined as N-uniform constants** - they are constructed precisely to be independent of swarm size \u2713\n5617:   - The proof uses only geometric properties (phase-space packing, variance decomposition) that scale with the number of walkers but produce **fractions** that remain constant \u2713\n5618: \n5619: **Conclusion:** $c_{\\text{err}}(\\epsilon)$ is strictly independent of $N$.\n5620: \n5621: **Part 3: N-Independence of $g_{\\text{err}}(\\epsilon)$**\n5622: \n5623: From Section 8.6.2.1:\n5624: \n5625: $$\n5626: g_{err}(\\epsilon) := g'_{err} + (1 - f_{UH}(\\epsilon)) \\cdot 4D_{\\mathrm{valid}}^2\n5627: $$\n5628: \n5629: - $g'_{\\text{err}}$: A constant from {prf:ref}`lem-variance-concentration-Hk` involving domain diameter, independent of $N$ \u2713\n5630: - $f_{UH}(\\epsilon)$: Already verified as N-independent in Part 2 \u2713\n5631: - $D_{\\text{valid}}$: Domain diameter, independent of $N$ \u2713\n5632: \n5633: **Conclusion:** $g_{\\text{err}}(\\epsilon)$ is strictly independent of $N$.\n5634: \n5635: **Part 4: N-Independence of $g_{\\max}(\\epsilon)$ and $\\chi(\\epsilon)$**\n5636: \n5637: Since all components are N-independent:\n5638: \n5639: $$\n5640: \\chi(\\epsilon) = p_u(\\epsilon) \\cdot c_{err}(\\epsilon) \\quad \\text{(product of N-independent terms)} \\quad \u2713\n5641: $$\n5642: \n5643: $$\n5644: g_{\\max}(\\epsilon) = \\max(p_u(\\epsilon) \\cdot g_{err}(\\epsilon), \\chi(\\epsilon) R^2_{\\text{spread}}) \\quad \\text{(max of N-independent terms)} \\quad \u2713\n5645: $$\n5646: \n5647: where $R^2_{\\text{spread}}$ is the variance threshold, a fixed constant independent of $N$ \u2713\n5648: \n5649: **Conclusion:** Both $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are strictly independent of $N$, depending only on $\\epsilon$ and fixed system parameters.\n5650: "
    }
  ],
  "validation": {
    "ok": true,
    "errors": []
  }
}