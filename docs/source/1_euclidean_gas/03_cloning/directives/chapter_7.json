{
  "chapter_index": 7,
  "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
  "directive_count": 36,
  "hints": [
    {
      "directive_type": "theorem",
      "label": "thm-geometry-guarantees-variance",
      "title": "Geometric Structure Guarantees Measurement Variance",
      "start_line": 3327,
      "end_line": 3338,
      "header_lines": [
        3328
      ],
      "content_start": 3330,
      "content_end": 3337,
      "content": "3330: :label: thm-geometry-guarantees-variance\n3331: \n3332: Let the `Sequential Stochastic Greedy Pairing Operator` be defined as in Definition 5.1.2. There exists a positional variance threshold $R^2_{\\mathrm{var}} > 0$ and a positive, $\\varepsilon$-dependent constant $\\kappa_{\\text{meas}}(\\epsilon) > 0$ such that for any swarm with $k \\geq 2$ alive walkers:\n3333: \n3334: If the internal positional variance of the swarm is large, $\\mathrm{Var}(x) \\ge R^2_{\\mathrm{var}}$, then the expected empirical variance of the raw distance-to-companion measurements is uniformly bounded below:\n3335: \n3336: $$\n3337: \\mathbb{E}[\\operatorname{Var}(d)] \\ge \\kappa_{\\text{meas}}(\\epsilon) > 0",
      "metadata": {
        "label": "thm-geometry-guarantees-variance"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3327: \n3328: #### 7.2.1 Guaranteed Measurement Variance from Geometric Structure\n3329: :::{prf:theorem} Geometric Structure Guarantees Measurement Variance\n3330: :label: thm-geometry-guarantees-variance\n3331: \n3332: Let the `Sequential Stochastic Greedy Pairing Operator` be defined as in Definition 5.1.2. There exists a positional variance threshold $R^2_{\\mathrm{var}} > 0$ and a positive, $\\varepsilon$-dependent constant $\\kappa_{\\text{meas}}(\\epsilon) > 0$ such that for any swarm with $k \\geq 2$ alive walkers:\n3333: \n3334: If the internal positional variance of the swarm is large, $\\mathrm{Var}(x) \\ge R^2_{\\mathrm{var}}$, then the expected empirical variance of the raw distance-to-companion measurements is uniformly bounded below:\n3335: \n3336: $$\n3337: \\mathbb{E}[\\operatorname{Var}(d)] \\ge \\kappa_{\\text{meas}}(\\epsilon) > 0\n3338: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-thm-geometry-guarantees-variance",
      "title": null,
      "start_line": 3339,
      "end_line": 3473,
      "header_lines": [
        3340
      ],
      "content_start": 3342,
      "content_end": 3472,
      "content": "3342: :label: proof-thm-geometry-guarantees-variance\n3343: \n3344: **Proof.**\n3345: \n3346: The proof is constructive and proceeds in three stages. First, we invoke the proven geometric consequences for a high-variance swarm, which guarantee a separation in the *expected* distance measurements between the high-error and low-error subpopulations. Second, we prove that this separation between subpopulation means necessitates a non-zero variance in the set of all individual expected distances. Finally, we use the Law of Total Variance to show that this provides a direct lower bound for the total expected measurement variance.\n3347: \n3348: **1. Invoking Proven Guarantees on Expected Distances in the `d_alg` Metric.**\n3349: \n3350: The premise of the theorem is that $Var_x \\geq R^{2}_var$. From the results established in Chapter 6, this premise has two direct consequences:\n3351: \n3352: *   **Geometric Structure (Corollary 6.4.4 & Lemma 6.5.1):** The swarm's alive set `A_k` is guaranteed to contain a **unified high-error set** `H_k` and a **low-error set** `L_k = A_k \\ H_k`. The fractional sizes of these sets, `f_H = |H_k|/k` and `f_L = |L_k|/k`, are bounded below by positive, N-uniform constants. Furthermore, these sets possess distinct geometric separation properties in the **algorithmic phase-space metric (`d_alg`)**, as quantified by the constants $D_H(\\varepsilon)$ and $R_L(\\varepsilon)$.\n3353: \n3354: *   **Algorithmic Perception (Lemma 5.1.3):** The `Sequential Stochastic Greedy Pairing Operator`, when applied to this guaranteed geometric structure in `d_alg`, produces a statistical separation in the expected raw distance measurements for these two populations. Let $\\mu_d(H_k) = \\text{E}[d_i | i \\in H_k]$ be the mean expected distance for a high-error walker and $\\mu_d(L_k) = \\text{E}[d_j | j \\in L_k]$ be the mean for a low-error walker.\n3355: \n3356:     From Lemma 5.1.3, we have the bounds $\\mu_d(H_k) \\geq D_H(\\varepsilon)$ and $\\mu_d(L_k) \\leq R_L(\\varepsilon) + C_tail(\\varepsilon)$, where $C_tail(\\varepsilon)$ is a small, exponentially decaying error term accounting for boundary effects. As the separation $D_H(\\varepsilon) > R_L(\\varepsilon)$ is a required condition for a well-posed system (guaranteed by the Unified Condition from Section 6.5.4), we can choose parameters such that $D_H(\\varepsilon) - R_L(\\varepsilon)$ is large enough to dominate $C_tail(\\varepsilon)$.\n3357: \n3358:     We therefore define the guaranteed positive gap:\n3359: \n3360: \n3361: $$\n3362: \\kappa'_{\\text{gap}}(\\epsilon) := D_H(\\epsilon) - R_L(\\epsilon) - C_{\\text{tail}}(\\epsilon) > 0\n3363: $$\n3364: \n3365:     This ensures:\n3366: \n3367: \n3368: $$\n3369: \\mu_d(H_k) - \\mu_d(L_k) \\ge \\kappa'_{\\text{gap}}(\\epsilon) > 0\n3370: $$\n3371: \n3372: **2. From Subpopulation Mean Gap to Variance of Expectations.**\n3373: \n3374: Let `E_d` be the set of individual expected distances for all `k` alive walkers: `E_d = {E[d\u2081], E[d\u2082], ..., E[d_k]}`. We now prove that the gap between the subpopulation means, established above, forces the variance of this entire set, `Var(E_d)`, to be non-zero.\n3375: \n3376: The variance of a set partitioned into two subsets (`H_k`, `L_k`) is bounded below by the squared difference of their means, weighted by their population fractions. This follows from the Law of Total Variance, which states that for any partition:\n3377: \n3378: $$\n3379: \\operatorname{Var}(X) = \\operatorname{Var}_{\\text{within}}(X) + \\operatorname{Var}_{\\text{between}}(X)\n3380: $$\n3381: \n3382: where the within-group variance `Var_within(X)` is always non-negative. Therefore, the total variance is bounded below by the between-group variance:\n3383: \n3384: $$\n3385: \\operatorname{Var}(E_d) \\ge \\operatorname{Var}_{\\text{between}}(E_d) = f_H f_L (\\mu_d(H_k) - \\mu_d(L_k))^2\n3386: $$\n3387: \n3388: Substituting the guaranteed bounds from Step 1, we get a uniform lower bound on the variance of the *expected* raw distances:\n3389: \n3390: $$\n3391: \\operatorname{Var}(E_d) \\ge f_H f_L (\\kappa'_{\\text{gap}}(\\epsilon))^2 > 0\n3392: $$\n3393: \n3394: **3. From Variance of Expectations to Expected Variance (The Key Inequality).**\n3395: \n3396: The final step is to prove the key inequality connecting the variance of the *expectations* to the expectation of the *variance*: $\\text{E}[\\text{Var}(d)] \\geq \\text{Var}(E_d)$.\n3397: \n3398: Let `d_i` denote the random distance measurement for walker `i`, and let $\\mu_i = \\text{E}[d_i]$ be its expectation. The empirical variance of the measurements is:\n3399: \n3400: $$\n3401: \\operatorname{Var}(d) = \\frac{1}{k}\\sum_{i=1}^k (d_i - \\bar{d})^2\n3402: $$\n3403: \n3404: where $bar{d} = (1/k) \\Sigma d_i$ is the sample mean.\n3405: \n3406: Taking expectations and using the fact that $\\text{E}[d_i] = \\mu_i$ and $\\text{E}[bar{d}] = bar{\\mu}$ where $bar{\\mu} = (1/k) \\Sigma \\mu_i$:\n3407: \n3408: $$\n3409: \\mathbb{E}[\\operatorname{Var}(d)] = \\mathbb{E}\\left[\\frac{1}{k}\\sum_{i=1}^k (d_i - \\bar{d})^2\\right]\n3410: $$\n3411: \n3412: We decompose each squared deviation using the standard technique. For each walker $i$, we write:\n3413: \n3414: $$\n3415: (d_i - \\bar{d})^2 = [(d_i - \\mu_i) + (\\mu_i - \\bar{d})]^2\n3416: $$\n3417: \n3418: Expanding and taking expectations term by term:\n3419: \n3420: $$\n3421: \\mathbb{E}[(d_i - \\bar{d})^2] = \\mathbb{E}[(d_i - \\mu_i)^2] + \\mathbb{E}[(\\mu_i - \\bar{d})^2] + 2\\mathbb{E}[(d_i - \\mu_i)(\\mu_i - \\bar{d})]\n3422: $$\n3423: \n3424: The **cross-term vanishes**: Since $\\mu_i$ is a constant (the expectation of $d_i$), we have:\n3425: \n3426: $$\n3427: \\mathbb{E}[(d_i - \\mu_i)(\\mu_i - \\bar{d})] = (\\mu_i - \\mathbb{E}[\\bar{d}]) \\mathbb{E}[d_i - \\mu_i] = (\\mu_i - \\bar{\\mu}) \\cdot 0 = 0\n3428: $$\n3429: \n3430: The **first term** is simply the variance of $d_i$:\n3431: \n3432: $$\n3433: \\mathbb{E}[(d_i - \\mu_i)^2] = \\operatorname{Var}(d_i)\n3434: $$\n3435: \n3436: The **second term** requires care because $\\mu_i$ is constant but $\\bar{d}$ is random. Using the standard variance decomposition for $(X - c)^2$ where $c$ is constant:\n3437: \n3438: $$\n3439: \\mathbb{E}[(\\mu_i - \\bar{d})^2] = (\\mu_i - \\mathbb{E}[\\bar{d}])^2 + \\operatorname{Var}(\\bar{d}) = (\\mu_i - \\bar{\\mu})^2 + \\operatorname{Var}(\\bar{d})\n3440: $$\n3441: \n3442: Combining these results:\n3443: \n3444: $$\n3445: \\mathbb{E}[(d_i - \\bar{d})^2] = \\operatorname{Var}(d_i) + (\\mu_i - \\bar{\\mu})^2 + \\operatorname{Var}(\\bar{d})\n3446: $$\n3447: \n3448: Summing over all $k$ walkers and dividing by $k$ gives the expected empirical variance:\n3449: \n3450: $$\n3451: \\mathbb{E}[\\operatorname{Var}(d)] = \\frac{1}{k}\\sum_{i=1}^k \\mathbb{E}[(d_i - \\bar{d})^2] = \\underbrace{\\frac{1}{k}\\sum_{i=1}^k \\operatorname{Var}(d_i)}_{\\text{within-walker variance}} + \\underbrace{\\frac{1}{k}\\sum_{i=1}^k (\\mu_i - \\bar{\\mu})^2}_{\\text{= Var}(E_d)} + \\underbrace{\\operatorname{Var}(\\bar{d})}_{\\text{sample mean variance}}\n3452: $$\n3453: \n3454: Since all three terms are non-negative, we immediately obtain the key inequality:\n3455: \n3456: $$\n3457: \\mathbb{E}[\\operatorname{Var}(d)] \\ge \\operatorname{Var}(E_d) = \\frac{1}{k}\\sum_{i=1}^k (\\mu_i - \\bar{\\mu})^2\n3458: $$\n3459: \n3460: This establishes the key inequality rigorously.\n3461: \n3462: **4. Final Assembly.**\n3463: \n3464: Combining the results from Steps 2 and 3:\n3465: \n3466: $$\n3467: \\mathbb{E}[\\operatorname{Var}(d)] \\ge \\operatorname{Var}(E_d) \\ge f_H f_L (\\kappa'_{\\text{gap}}(\\epsilon))^2\n3468: $$\n3469: \n3470: We define the final constant $\\kappa_meas(\\varepsilon) := f_H f_L (\\kappa'_{gap}(\\varepsilon))^{2}$. Since `f_H`, `f_L`, and $\\kappa'_gap(\\varepsilon)$ are all positive, N-uniform, $\\varepsilon$-dependent constants derived from the geometric analysis in Chapter 6, their product $\\kappa_meas(\\varepsilon)$ is also a positive, N-uniform, $\\varepsilon$-dependent constant.\n3471: \n3472: This completes the proof. We have rigorously shown that a large internal positional variance is sufficient to guarantee a non-zero expected variance in the raw distance measurements.",
      "metadata": {
        "label": "proof-thm-geometry-guarantees-variance"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3339: \n3340: :::\n3341: :::{prf:proof}\n3342: :label: proof-thm-geometry-guarantees-variance\n3343: \n3344: **Proof.**\n3345: \n3346: The proof is constructive and proceeds in three stages. First, we invoke the proven geometric consequences for a high-variance swarm, which guarantee a separation in the *expected* distance measurements between the high-error and low-error subpopulations. Second, we prove that this separation between subpopulation means necessitates a non-zero variance in the set of all individual expected distances. Finally, we use the Law of Total Variance to show that this provides a direct lower bound for the total expected measurement variance.\n3347: \n3348: **1. Invoking Proven Guarantees on Expected Distances in the `d_alg` Metric.**\n3349: \n3350: The premise of the theorem is that $Var_x \\geq R^{2}_var$. From the results established in Chapter 6, this premise has two direct consequences:\n3351: \n3352: *   **Geometric Structure (Corollary 6.4.4 & Lemma 6.5.1):** The swarm's alive set `A_k` is guaranteed to contain a **unified high-error set** `H_k` and a **low-error set** `L_k = A_k \\ H_k`. The fractional sizes of these sets, `f_H = |H_k|/k` and `f_L = |L_k|/k`, are bounded below by positive, N-uniform constants. Furthermore, these sets possess distinct geometric separation properties in the **algorithmic phase-space metric (`d_alg`)**, as quantified by the constants $D_H(\\varepsilon)$ and $R_L(\\varepsilon)$.\n3353: \n3354: *   **Algorithmic Perception (Lemma 5.1.3):** The `Sequential Stochastic Greedy Pairing Operator`, when applied to this guaranteed geometric structure in `d_alg`, produces a statistical separation in the expected raw distance measurements for these two populations. Let $\\mu_d(H_k) = \\text{E}[d_i | i \\in H_k]$ be the mean expected distance for a high-error walker and $\\mu_d(L_k) = \\text{E}[d_j | j \\in L_k]$ be the mean for a low-error walker.\n3355: \n3356:     From Lemma 5.1.3, we have the bounds $\\mu_d(H_k) \\geq D_H(\\varepsilon)$ and $\\mu_d(L_k) \\leq R_L(\\varepsilon) + C_tail(\\varepsilon)$, where $C_tail(\\varepsilon)$ is a small, exponentially decaying error term accounting for boundary effects. As the separation $D_H(\\varepsilon) > R_L(\\varepsilon)$ is a required condition for a well-posed system (guaranteed by the Unified Condition from Section 6.5.4), we can choose parameters such that $D_H(\\varepsilon) - R_L(\\varepsilon)$ is large enough to dominate $C_tail(\\varepsilon)$.\n3357: \n3358:     We therefore define the guaranteed positive gap:\n3359: \n3360: \n3361: $$\n3362: \\kappa'_{\\text{gap}}(\\epsilon) := D_H(\\epsilon) - R_L(\\epsilon) - C_{\\text{tail}}(\\epsilon) > 0\n3363: $$\n3364: \n3365:     This ensures:\n3366: \n3367: \n3368: $$\n3369: \\mu_d(H_k) - \\mu_d(L_k) \\ge \\kappa'_{\\text{gap}}(\\epsilon) > 0\n3370: $$\n3371: \n3372: **2. From Subpopulation Mean Gap to Variance of Expectations.**\n3373: \n3374: Let `E_d` be the set of individual expected distances for all `k` alive walkers: `E_d = {E[d\u2081], E[d\u2082], ..., E[d_k]}`. We now prove that the gap between the subpopulation means, established above, forces the variance of this entire set, `Var(E_d)`, to be non-zero.\n3375: \n3376: The variance of a set partitioned into two subsets (`H_k`, `L_k`) is bounded below by the squared difference of their means, weighted by their population fractions. This follows from the Law of Total Variance, which states that for any partition:\n3377: \n3378: $$\n3379: \\operatorname{Var}(X) = \\operatorname{Var}_{\\text{within}}(X) + \\operatorname{Var}_{\\text{between}}(X)\n3380: $$\n3381: \n3382: where the within-group variance `Var_within(X)` is always non-negative. Therefore, the total variance is bounded below by the between-group variance:\n3383: \n3384: $$\n3385: \\operatorname{Var}(E_d) \\ge \\operatorname{Var}_{\\text{between}}(E_d) = f_H f_L (\\mu_d(H_k) - \\mu_d(L_k))^2\n3386: $$\n3387: \n3388: Substituting the guaranteed bounds from Step 1, we get a uniform lower bound on the variance of the *expected* raw distances:\n3389: \n3390: $$\n3391: \\operatorname{Var}(E_d) \\ge f_H f_L (\\kappa'_{\\text{gap}}(\\epsilon))^2 > 0\n3392: $$\n3393: \n3394: **3. From Variance of Expectations to Expected Variance (The Key Inequality).**\n3395: \n3396: The final step is to prove the key inequality connecting the variance of the *expectations* to the expectation of the *variance*: $\\text{E}[\\text{Var}(d)] \\geq \\text{Var}(E_d)$.\n3397: \n3398: Let `d_i` denote the random distance measurement for walker `i`, and let $\\mu_i = \\text{E}[d_i]$ be its expectation. The empirical variance of the measurements is:\n3399: \n3400: $$\n3401: \\operatorname{Var}(d) = \\frac{1}{k}\\sum_{i=1}^k (d_i - \\bar{d})^2\n3402: $$\n3403: \n3404: where $bar{d} = (1/k) \\Sigma d_i$ is the sample mean.\n3405: \n3406: Taking expectations and using the fact that $\\text{E}[d_i] = \\mu_i$ and $\\text{E}[bar{d}] = bar{\\mu}$ where $bar{\\mu} = (1/k) \\Sigma \\mu_i$:\n3407: \n3408: $$\n3409: \\mathbb{E}[\\operatorname{Var}(d)] = \\mathbb{E}\\left[\\frac{1}{k}\\sum_{i=1}^k (d_i - \\bar{d})^2\\right]\n3410: $$\n3411: \n3412: We decompose each squared deviation using the standard technique. For each walker $i$, we write:\n3413: \n3414: $$\n3415: (d_i - \\bar{d})^2 = [(d_i - \\mu_i) + (\\mu_i - \\bar{d})]^2\n3416: $$\n3417: \n3418: Expanding and taking expectations term by term:\n3419: \n3420: $$\n3421: \\mathbb{E}[(d_i - \\bar{d})^2] = \\mathbb{E}[(d_i - \\mu_i)^2] + \\mathbb{E}[(\\mu_i - \\bar{d})^2] + 2\\mathbb{E}[(d_i - \\mu_i)(\\mu_i - \\bar{d})]\n3422: $$\n3423: \n3424: The **cross-term vanishes**: Since $\\mu_i$ is a constant (the expectation of $d_i$), we have:\n3425: \n3426: $$\n3427: \\mathbb{E}[(d_i - \\mu_i)(\\mu_i - \\bar{d})] = (\\mu_i - \\mathbb{E}[\\bar{d}]) \\mathbb{E}[d_i - \\mu_i] = (\\mu_i - \\bar{\\mu}) \\cdot 0 = 0\n3428: $$\n3429: \n3430: The **first term** is simply the variance of $d_i$:\n3431: \n3432: $$\n3433: \\mathbb{E}[(d_i - \\mu_i)^2] = \\operatorname{Var}(d_i)\n3434: $$\n3435: \n3436: The **second term** requires care because $\\mu_i$ is constant but $\\bar{d}$ is random. Using the standard variance decomposition for $(X - c)^2$ where $c$ is constant:\n3437: \n3438: $$\n3439: \\mathbb{E}[(\\mu_i - \\bar{d})^2] = (\\mu_i - \\mathbb{E}[\\bar{d}])^2 + \\operatorname{Var}(\\bar{d}) = (\\mu_i - \\bar{\\mu})^2 + \\operatorname{Var}(\\bar{d})\n3440: $$\n3441: \n3442: Combining these results:\n3443: \n3444: $$\n3445: \\mathbb{E}[(d_i - \\bar{d})^2] = \\operatorname{Var}(d_i) + (\\mu_i - \\bar{\\mu})^2 + \\operatorname{Var}(\\bar{d})\n3446: $$\n3447: \n3448: Summing over all $k$ walkers and dividing by $k$ gives the expected empirical variance:\n3449: \n3450: $$\n3451: \\mathbb{E}[\\operatorname{Var}(d)] = \\frac{1}{k}\\sum_{i=1}^k \\mathbb{E}[(d_i - \\bar{d})^2] = \\underbrace{\\frac{1}{k}\\sum_{i=1}^k \\operatorname{Var}(d_i)}_{\\text{within-walker variance}} + \\underbrace{\\frac{1}{k}\\sum_{i=1}^k (\\mu_i - \\bar{\\mu})^2}_{\\text{= Var}(E_d)} + \\underbrace{\\operatorname{Var}(\\bar{d})}_{\\text{sample mean variance}}\n3452: $$\n3453: \n3454: Since all three terms are non-negative, we immediately obtain the key inequality:\n3455: \n3456: $$\n3457: \\mathbb{E}[\\operatorname{Var}(d)] \\ge \\operatorname{Var}(E_d) = \\frac{1}{k}\\sum_{i=1}^k (\\mu_i - \\bar{\\mu})^2\n3458: $$\n3459: \n3460: This establishes the key inequality rigorously.\n3461: \n3462: **4. Final Assembly.**\n3463: \n3464: Combining the results from Steps 2 and 3:\n3465: \n3466: $$\n3467: \\mathbb{E}[\\operatorname{Var}(d)] \\ge \\operatorname{Var}(E_d) \\ge f_H f_L (\\kappa'_{\\text{gap}}(\\epsilon))^2\n3468: $$\n3469: \n3470: We define the final constant $\\kappa_meas(\\varepsilon) := f_H f_L (\\kappa'_{gap}(\\varepsilon))^{2}$. Since `f_H`, `f_L`, and $\\kappa'_gap(\\varepsilon)$ are all positive, N-uniform, $\\varepsilon$-dependent constants derived from the geometric analysis in Chapter 6, their product $\\kappa_meas(\\varepsilon)$ is also a positive, N-uniform, $\\varepsilon$-dependent constant.\n3471: \n3472: This completes the proof. We have rigorously shown that a large internal positional variance is sufficient to guarantee a non-zero expected variance in the raw distance measurements.\n3473: "
    },
    {
      "directive_type": "proposition",
      "label": "prop-satisfiability-of-snr-gamma",
      "title": "**(Satisfiability of the Signal-to-Noise Condition via Signal Gain)**",
      "start_line": 3481,
      "end_line": 3493,
      "header_lines": [
        3482
      ],
      "content_start": 3484,
      "content_end": 3492,
      "content": "3484: :label: prop-satisfiability-of-snr-gamma\n3485: \n3486: Let the rescaled diversity values be defined as $d'_i = g_A(\\gamma \u00b7 z_{d,i}) + \\eta$, where $\\gamma > 0$ is a user-defined **Signal Gain** parameter and `g_A` is any function satisfying the **Axiom of a Well-Behaved Rescale Function**.\n3487: \n3488: For any system in a high-error state (`Var(x) > R^{2}_var`) that generates a non-zero raw distance signal ($\\kappa_meas(d) > 0$), there exists a sufficiently large choice of $\\gamma$ that satisfies the **Signal-to-Noise Condition**:\n3489: \n3490: $$\n3491: \\kappa_{\\mathrm{var}}(d') > \\operatorname{Var}_{\\max}(d')\n3492: $$",
      "metadata": {
        "label": "prop-satisfiability-of-snr-gamma"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3481: This section provides the formal proof that this condition, which we call the **Signal-to-Noise Condition**, is not an unstated assumption but a satisfiable criterion that can be met by a valid choice of the algorithm's user-defined parameters. We prove this by introducing a **Signal Gain** parameter, $\\gamma$, which acts as a sensitivity knob for the algorithm. This proves that the system is fundamentally \"learnable\": the signal generated by geometric error can always be amplified sufficiently to overcome the worst-case statistical noise, ensuring that a true difference between the high-error and low-error populations is always detectable.\n3482: \n3483: :::{prf:proposition} **(Satisfiability of the Signal-to-Noise Condition via Signal Gain)**\n3484: :label: prop-satisfiability-of-snr-gamma\n3485: \n3486: Let the rescaled diversity values be defined as $d'_i = g_A(\\gamma \u00b7 z_{d,i}) + \\eta$, where $\\gamma > 0$ is a user-defined **Signal Gain** parameter and `g_A` is any function satisfying the **Axiom of a Well-Behaved Rescale Function**.\n3487: \n3488: For any system in a high-error state (`Var(x) > R^{2}_var`) that generates a non-zero raw distance signal ($\\kappa_meas(d) > 0$), there exists a sufficiently large choice of $\\gamma$ that satisfies the **Signal-to-Noise Condition**:\n3489: \n3490: $$\n3491: \\kappa_{\\mathrm{var}}(d') > \\operatorname{Var}_{\\max}(d')\n3492: $$\n3493: "
    },
    {
      "directive_type": "proof",
      "label": "proof-prop-satisfiability-of-snr-gamma",
      "title": null,
      "start_line": 3494,
      "end_line": 3568,
      "header_lines": [
        3495
      ],
      "content_start": 3497,
      "content_end": 3567,
      "content": "3497: :label: proof-prop-satisfiability-of-snr-gamma\n3498: \n3499: **Proof.**\n3500: \n3501: The proof strategy is to show that the guaranteed signal variance of the rescaled values, $\\kappa_var(d')$, scales with $\\gamma^{2}$ in the small-signal limit, while the maximum possible noise, `Var_max(d')`, remains a fixed constant independent of $\\gamma$. This algebraic advantage allows $\\gamma$ to be chosen to ensure the signal always dominates the noise.\n3502: \n3503: **1. The Noise Term (`Var_max(d')`): A Fixed, $\\gamma$-Independent Constant.**\n3504: \n3505: The **Axiom of a Well-Behaved Rescale Function** requires `g_A` to have a bounded range, which we denote `(g_{A,\\min}, g_{A,\\max})`. Consequently, the rescaled values $d'_i = g_A(\\gamma \u00b7 z_{d,i}) + \\eta$ are always contained within the fixed interval $(g_{A,\\min} + \\eta, g_{A,\\max} + \\eta)$.\n3506: \n3507: The maximum possible variance for any set of values on this interval is given by Popoviciu's inequality:\n3508: \n3509: $$\n3510: \\operatorname{Var}_{\\max}(d') := \\frac{1}{4}(\\max(d') - \\min(d'))^2 = \\frac{1}{4}(g_{A,\\max} - g_{A,\\min})^2\n3511: $$\n3512: \n3513: This value is a constant determined solely by the choice of the rescale function `g_A`; it does not depend on the Signal Gain $\\gamma$. For the **Canonical Logistic Rescale function**, `g_A(z) = 2/(1+e^{-z})`, the range is `(0, 2)`, yielding a fixed maximum noise of `Var_max(d') = 1`.\n3514: \n3515: Our goal is to prove that we can choose $\\gamma$ such that the guaranteed signal variance $\\kappa_var(d')$ is greater than this fixed constant.\n3516: \n3517: **2. The Signal Term ($\\kappa_var(d')$): Amplification by $\\gamma$.**\n3518: \n3519: The signal originates from the raw distance measurements `d`, propagates to the standardized scores `z_d`, and is then amplified.\n3520: \n3521: *   **Raw and Standardized Signal:** From [](#thm-geometry-guarantees-variance), a high-error state guarantees $\\text{Var}(d) \\geq \\kappa_meas(d) > 0$. The Z-scores $z_d = (d - \\mu_d) / \\sigma'_d$ have a variance $\\text{Var}(z_d) = \\text{Var}(d) / (\\sigma'_d)^{2}$. Since the patched standard deviation $\\sigma'_d$ is uniformly bounded above by $\\sigma'_max$ (Definition 6.6.2.1), the Z-score variance has a uniform lower bound:\n3522: \n3523: \n3524: $$\n3525: \\operatorname{Var}(z_d) \\ge \\frac{\\kappa_{\\mathrm{meas}}(d)}{(\\sigma'_{\\max})^2} =: \\kappa_{\\mathrm{var}}(z) > 0\n3526: $$\n3527: \n3528: *   **Signal Amplification:** The input to the rescale function is $u_i = \\gammaz_{d,i}$. The variance of this amplified signal is $\\text{Var}(u) = \\gamma^{2}\\text{Var}(z_d) \\geq \\gamma^{2}\\kappa_var(z)$.\n3529: \n3530: *   **Rescaled Signal ($\\kappa_var(d')$):** The rescaled values are $d' = g_A(u) + \\eta$. For any differentiable function, a first-order Taylor expansion around the mean $\\mu_u$ gives $g_A(u_i) \\approx g_A(\\mu_u) + g'_A(\\mu_u)(u_i - \\mu_u)$. The variance is then approximated by:\n3531: \n3532: \n3533: $$\n3534: \\operatorname{Var}(d') = \\operatorname{Var}(g_A(u)) \\approx (g'_A(\\mu_u))^2 \\operatorname{Var}(u)\n3535: $$\n3536: \n3537:     This approximation becomes exact in the limit of small variance relative to the curvature of `g_A`. A more rigorous treatment using the Mean Value Theorem shows that the variance of the output is bounded below by the variance of the input multiplied by the squared infimum of the derivative.\n3538: \n3539: \n3540: $$\n3541: \\operatorname{Var}(d') \\ge (\\inf_{c \\in Z_{\\mathrm{eff}}} g'_A(c))^2 \\operatorname{Var}(u)\n3542: $$\n3543: \n3544:     where `Z_eff` is the effective range of inputs. Let `g'_{\\min} > 0` be the uniform lower bound on the derivative (guaranteed to exist on any compact operational range by the axiom). The guaranteed variance of the rescaled values is thus bounded below by a term proportional to $\\gamma^{2}$:\n3545: \n3546: \n3547: $$\n3548: \\kappa_{\\mathrm{var}}(d') \\ge (g'_{\\min})^2 \\cdot \\gamma^2 \\kappa_{\\mathrm{var}}(z)\n3549: $$\n3550: \n3551: **3. Proving Satisfiability.**\n3552: \n3553: The Signal-to-Noise Condition is $\\kappa_var(d') > Var_max(d')$. Substituting our results from the steps above:\n3554: \n3555: $$\n3556: (g'_{\\min})^2 \\cdot \\gamma^2 \\kappa_{\\mathrm{var}}(z) > \\frac{1}{4}(g_{A,\\max} - g_{A,\\min})^2\n3557: $$\n3558: \n3559: Solving for the Signal Gain $\\gamma$:\n3560: \n3561: $$\n3562: \\gamma > \\frac{g_{A,\\max} - g_{A,\\min}}{2 \\cdot g'_{\\min} \\cdot \\sqrt{\\kappa_{\\mathrm{var}}(z)}}\n3563: $$\n3564: \n3565: Since $\\kappa_var(z)$ is a fixed positive constant for a given $\\varepsilon$, and `g_A`'s properties (`g_{A,max}`, `g_{A,min}`, `g'_{min}`) are fixed, the right-hand side is a fixed, positive real number. This proves that there always exists a sufficiently large choice of $\\gamma$ that satisfies the condition.\n3566: \n3567: **Conclusion:** The Signal-to-Noise Condition is not a restrictive assumption on the environment but is a design criterion that can always be satisfied by appropriately tuning the algorithm's sensitivity $\\gamma$. This holds for any valid rescale function, including the Canonical choice.",
      "metadata": {
        "label": "proof-prop-satisfiability-of-snr-gamma"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3494: where `Var_max(d')` is the maximum possible variance of the rescaled values, and $\\kappa_var(d')$ is the guaranteed lower bound on the variance of the rescaled values in the high-error state.\n3495: :::\n3496: :::{prf:proof}\n3497: :label: proof-prop-satisfiability-of-snr-gamma\n3498: \n3499: **Proof.**\n3500: \n3501: The proof strategy is to show that the guaranteed signal variance of the rescaled values, $\\kappa_var(d')$, scales with $\\gamma^{2}$ in the small-signal limit, while the maximum possible noise, `Var_max(d')`, remains a fixed constant independent of $\\gamma$. This algebraic advantage allows $\\gamma$ to be chosen to ensure the signal always dominates the noise.\n3502: \n3503: **1. The Noise Term (`Var_max(d')`): A Fixed, $\\gamma$-Independent Constant.**\n3504: \n3505: The **Axiom of a Well-Behaved Rescale Function** requires `g_A` to have a bounded range, which we denote `(g_{A,\\min}, g_{A,\\max})`. Consequently, the rescaled values $d'_i = g_A(\\gamma \u00b7 z_{d,i}) + \\eta$ are always contained within the fixed interval $(g_{A,\\min} + \\eta, g_{A,\\max} + \\eta)$.\n3506: \n3507: The maximum possible variance for any set of values on this interval is given by Popoviciu's inequality:\n3508: \n3509: $$\n3510: \\operatorname{Var}_{\\max}(d') := \\frac{1}{4}(\\max(d') - \\min(d'))^2 = \\frac{1}{4}(g_{A,\\max} - g_{A,\\min})^2\n3511: $$\n3512: \n3513: This value is a constant determined solely by the choice of the rescale function `g_A`; it does not depend on the Signal Gain $\\gamma$. For the **Canonical Logistic Rescale function**, `g_A(z) = 2/(1+e^{-z})`, the range is `(0, 2)`, yielding a fixed maximum noise of `Var_max(d') = 1`.\n3514: \n3515: Our goal is to prove that we can choose $\\gamma$ such that the guaranteed signal variance $\\kappa_var(d')$ is greater than this fixed constant.\n3516: \n3517: **2. The Signal Term ($\\kappa_var(d')$): Amplification by $\\gamma$.**\n3518: \n3519: The signal originates from the raw distance measurements `d`, propagates to the standardized scores `z_d`, and is then amplified.\n3520: \n3521: *   **Raw and Standardized Signal:** From [](#thm-geometry-guarantees-variance), a high-error state guarantees $\\text{Var}(d) \\geq \\kappa_meas(d) > 0$. The Z-scores $z_d = (d - \\mu_d) / \\sigma'_d$ have a variance $\\text{Var}(z_d) = \\text{Var}(d) / (\\sigma'_d)^{2}$. Since the patched standard deviation $\\sigma'_d$ is uniformly bounded above by $\\sigma'_max$ (Definition 6.6.2.1), the Z-score variance has a uniform lower bound:\n3522: \n3523: \n3524: $$\n3525: \\operatorname{Var}(z_d) \\ge \\frac{\\kappa_{\\mathrm{meas}}(d)}{(\\sigma'_{\\max})^2} =: \\kappa_{\\mathrm{var}}(z) > 0\n3526: $$\n3527: \n3528: *   **Signal Amplification:** The input to the rescale function is $u_i = \\gammaz_{d,i}$. The variance of this amplified signal is $\\text{Var}(u) = \\gamma^{2}\\text{Var}(z_d) \\geq \\gamma^{2}\\kappa_var(z)$.\n3529: \n3530: *   **Rescaled Signal ($\\kappa_var(d')$):** The rescaled values are $d' = g_A(u) + \\eta$. For any differentiable function, a first-order Taylor expansion around the mean $\\mu_u$ gives $g_A(u_i) \\approx g_A(\\mu_u) + g'_A(\\mu_u)(u_i - \\mu_u)$. The variance is then approximated by:\n3531: \n3532: \n3533: $$\n3534: \\operatorname{Var}(d') = \\operatorname{Var}(g_A(u)) \\approx (g'_A(\\mu_u))^2 \\operatorname{Var}(u)\n3535: $$\n3536: \n3537:     This approximation becomes exact in the limit of small variance relative to the curvature of `g_A`. A more rigorous treatment using the Mean Value Theorem shows that the variance of the output is bounded below by the variance of the input multiplied by the squared infimum of the derivative.\n3538: \n3539: \n3540: $$\n3541: \\operatorname{Var}(d') \\ge (\\inf_{c \\in Z_{\\mathrm{eff}}} g'_A(c))^2 \\operatorname{Var}(u)\n3542: $$\n3543: \n3544:     where `Z_eff` is the effective range of inputs. Let `g'_{\\min} > 0` be the uniform lower bound on the derivative (guaranteed to exist on any compact operational range by the axiom). The guaranteed variance of the rescaled values is thus bounded below by a term proportional to $\\gamma^{2}$:\n3545: \n3546: \n3547: $$\n3548: \\kappa_{\\mathrm{var}}(d') \\ge (g'_{\\min})^2 \\cdot \\gamma^2 \\kappa_{\\mathrm{var}}(z)\n3549: $$\n3550: \n3551: **3. Proving Satisfiability.**\n3552: \n3553: The Signal-to-Noise Condition is $\\kappa_var(d') > Var_max(d')$. Substituting our results from the steps above:\n3554: \n3555: $$\n3556: (g'_{\\min})^2 \\cdot \\gamma^2 \\kappa_{\\mathrm{var}}(z) > \\frac{1}{4}(g_{A,\\max} - g_{A,\\min})^2\n3557: $$\n3558: \n3559: Solving for the Signal Gain $\\gamma$:\n3560: \n3561: $$\n3562: \\gamma > \\frac{g_{A,\\max} - g_{A,\\min}}{2 \\cdot g'_{\\min} \\cdot \\sqrt{\\kappa_{\\mathrm{var}}(z)}}\n3563: $$\n3564: \n3565: Since $\\kappa_var(z)$ is a fixed positive constant for a given $\\varepsilon$, and `g_A`'s properties (`g_{A,max}`, `g_{A,min}`, `g'_{min}`) are fixed, the right-hand side is a fixed, positive real number. This proves that there always exists a sufficiently large choice of $\\gamma$ that satisfies the condition.\n3566: \n3567: **Conclusion:** The Signal-to-Noise Condition is not a restrictive assumption on the environment but is a design criterion that can always be satisfied by appropriately tuning the algorithm's sensitivity $\\gamma$. This holds for any valid rescale function, including the Canonical choice.\n3568: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-variance-to-gap",
      "title": "From Bounded Variance to a Guaranteed Gap",
      "start_line": 3593,
      "end_line": 3602,
      "header_lines": [
        3594
      ],
      "content_start": 3596,
      "content_end": 3601,
      "content": "3596: :label: lem-variance-to-gap\n3597: \n3598: Let $\\{v_i\\}_{i=1}^k$ be a set of $k \\ge 2$ real numbers. If the empirical variance of this set is bounded below by a strictly positive constant, $\\text{Var}(\\{v_i\\}) \\geq \\kappa > 0$, then there must exist at least one pair of indices $(i, j)$ such that the gap between their values is bounded below:\n3599: \n3600: $$\n3601: \\max_{i,j} |v_i - v_j| \\ge \\sqrt{2\\kappa}",
      "metadata": {
        "label": "lem-variance-to-gap"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3593: The first step in the signal integrity proof is to show that the statistical property of variance, now proven in [](#thm-geometry-guarantees-variance), has a direct, concrete consequence: it forces a measurable separation between the raw values of at least two walkers.\n3594: \n3595: :::{prf:lemma} From Bounded Variance to a Guaranteed Gap\n3596: :label: lem-variance-to-gap\n3597: \n3598: Let $\\{v_i\\}_{i=1}^k$ be a set of $k \\ge 2$ real numbers. If the empirical variance of this set is bounded below by a strictly positive constant, $\\text{Var}(\\{v_i\\}) \\geq \\kappa > 0$, then there must exist at least one pair of indices $(i, j)$ such that the gap between their values is bounded below:\n3599: \n3600: $$\n3601: \\max_{i,j} |v_i - v_j| \\ge \\sqrt{2\\kappa}\n3602: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-variance-to-gap",
      "title": null,
      "start_line": 3603,
      "end_line": 3642,
      "header_lines": [
        3604
      ],
      "content_start": 3606,
      "content_end": 3641,
      "content": "3606: :label: proof-lem-variance-to-gap\n3607: \n3608: **Proof.**\n3609: \n3610: The proof relies on a standard identity that relates the empirical variance of a set to the sum of its pairwise squared differences.\n3611: \n3612: **1. The Pairwise Variance Identity.**\n3613: The empirical variance, $\\text{Var}(\\{v_i\\}) = \\frac{1}{k}\\sum_i v_i^2 - (\\frac{1}{k}\\sum_i v_i)^2$, can be expressed as:\n3614: \n3615: $$\n3616: \\mathrm{Var}(\\{v_i\\}) = \\frac{1}{2k^2} \\sum_{i=1}^k \\sum_{j=1}^k (v_i - v_j)^2\n3617: $$\n3618: \n3619: This identity is established by expanding the squared term in the double summation.\n3620: \n3621: **2. Bounding the Variance by the Maximum Gap.**\n3622: Let $\\Delta_{\\text{max}} := \\max_{i,j} |v_i - v_j|$. By definition, every term in the summation is bounded above by this maximum: $(v_i - v_j)^2 \\le \\Delta_{\\max}^2$. The double summation contains $k^2$ such terms. We can therefore bound the sum:\n3623: \n3624: $$\n3625: \\sum_{i=1}^k \\sum_{j=1}^k (v_i - v_j)^2 \\le \\sum_{i=1}^k \\sum_{j=1}^k \\Delta_{\\max}^2 = k^2 \\Delta_{\\max}^2\n3626: $$\n3627: \n3628: Substituting this into the identity from Step 1 gives an upper bound on the variance in terms of the maximum gap:\n3629: \n3630: $$\n3631: \\mathrm{Var}(\\{v_i\\}) \\le \\frac{1}{2k^2} (k^2 \\Delta_{\\max}^2) = \\frac{1}{2} \\Delta_{\\max}^2\n3632: $$\n3633: \n3634: **3. Final Derivation.**\n3635: We are given the premise that $\\mathrm{Var}(\\{v_i\\}) \\geq \\kappa$. Combining this with the result from Step 2:\n3636: \n3637: $$\n3638: \\kappa \\le \\mathrm{Var}(\\{v_i\\}) \\le \\frac{1}{2} \\Delta_{\\max}^2\n3639: $$\n3640: \n3641: Rearranging the inequality $\\kappa \\le \\frac{1}{2} \\Delta_{\\max}^2$ gives $\\Delta_{\\max}^2 \\ge 2\\kappa$. Taking the square root of both sides yields the desired result.",
      "metadata": {
        "label": "proof-lem-variance-to-gap"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3603: \n3604: :::\n3605: :::{prf:proof}\n3606: :label: proof-lem-variance-to-gap\n3607: \n3608: **Proof.**\n3609: \n3610: The proof relies on a standard identity that relates the empirical variance of a set to the sum of its pairwise squared differences.\n3611: \n3612: **1. The Pairwise Variance Identity.**\n3613: The empirical variance, $\\text{Var}(\\{v_i\\}) = \\frac{1}{k}\\sum_i v_i^2 - (\\frac{1}{k}\\sum_i v_i)^2$, can be expressed as:\n3614: \n3615: $$\n3616: \\mathrm{Var}(\\{v_i\\}) = \\frac{1}{2k^2} \\sum_{i=1}^k \\sum_{j=1}^k (v_i - v_j)^2\n3617: $$\n3618: \n3619: This identity is established by expanding the squared term in the double summation.\n3620: \n3621: **2. Bounding the Variance by the Maximum Gap.**\n3622: Let $\\Delta_{\\text{max}} := \\max_{i,j} |v_i - v_j|$. By definition, every term in the summation is bounded above by this maximum: $(v_i - v_j)^2 \\le \\Delta_{\\max}^2$. The double summation contains $k^2$ such terms. We can therefore bound the sum:\n3623: \n3624: $$\n3625: \\sum_{i=1}^k \\sum_{j=1}^k (v_i - v_j)^2 \\le \\sum_{i=1}^k \\sum_{j=1}^k \\Delta_{\\max}^2 = k^2 \\Delta_{\\max}^2\n3626: $$\n3627: \n3628: Substituting this into the identity from Step 1 gives an upper bound on the variance in terms of the maximum gap:\n3629: \n3630: $$\n3631: \\mathrm{Var}(\\{v_i\\}) \\le \\frac{1}{2k^2} (k^2 \\Delta_{\\max}^2) = \\frac{1}{2} \\Delta_{\\max}^2\n3632: $$\n3633: \n3634: **3. Final Derivation.**\n3635: We are given the premise that $\\mathrm{Var}(\\{v_i\\}) \\geq \\kappa$. Combining this with the result from Step 2:\n3636: \n3637: $$\n3638: \\kappa \\le \\mathrm{Var}(\\{v_i\\}) \\le \\frac{1}{2} \\Delta_{\\max}^2\n3639: $$\n3640: \n3641: Rearranging the inequality $\\kappa \\le \\frac{1}{2} \\Delta_{\\max}^2$ gives $\\Delta_{\\max}^2 \\ge 2\\kappa$. Taking the square root of both sides yields the desired result.\n3642: "
    },
    {
      "directive_type": "definition",
      "label": "def-max-patched-std",
      "title": "Maximum Patched Standard Deviation",
      "start_line": 3656,
      "end_line": 3666,
      "header_lines": [
        3657
      ],
      "content_start": 3659,
      "content_end": 3665,
      "content": "3659: :label: def-max-patched-std\n3660: \n3661: Let $V_{\\max}$ be the uniform upper bound on a raw measurement's absolute value (either $V_{\\max}^{(R)}$ for rewards or $D_{\\text{valid}}$ for distances). The **maximum patched standard deviation**, $\\sigma'_{\\max}$, is the maximum value that the patched standard deviation function can attain over its entire possible input domain.\n3662: \n3663: $$\n3664: \\sigma'_{\\max} := \\sup_{0 \\le V \\le V_{\\max}^2} \\sigma'_{\\mathrm{patch}}(V)\n3665: $$",
      "metadata": {
        "label": "def-max-patched-std"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3656: The first component we must bound is the denominator of the standardization formula. The following definition establishes a uniform upper bound on the patched standard deviation.\n3657: \n3658: :::{prf:definition} Maximum Patched Standard Deviation\n3659: :label: def-max-patched-std\n3660: \n3661: Let $V_{\\max}$ be the uniform upper bound on a raw measurement's absolute value (either $V_{\\max}^{(R)}$ for rewards or $D_{\\text{valid}}$ for distances). The **maximum patched standard deviation**, $\\sigma'_{\\max}$, is the maximum value that the patched standard deviation function can attain over its entire possible input domain.\n3662: \n3663: $$\n3664: \\sigma'_{\\max} := \\sup_{0 \\le V \\le V_{\\max}^2} \\sigma'_{\\mathrm{patch}}(V)\n3665: $$\n3666: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-rescale-derivative-lower-bound",
      "title": "Positive Derivative Bound for the Rescale Function",
      "start_line": 3670,
      "end_line": 3680,
      "header_lines": [
        3671
      ],
      "content_start": 3673,
      "content_end": 3679,
      "content": "3673: :label: lem-rescale-derivative-lower-bound\n3674: \n3675: For the Canonical Logistic Rescale function, the first derivative $g'_A(z)$ is uniformly bounded below by a strictly positive constant for all z-scores in the operational range $Z_{\\text{supp}}$. That is, there exists a constant $g'_{\\min} > 0$ such that:\n3676: \n3677: $$\n3678: \\inf_{z \\in Z_{\\mathrm{supp}}} g'_A(z) = g'_{\\min} > 0\n3679: $$",
      "metadata": {
        "label": "lem-rescale-derivative-lower-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3670: Next, we must prove that the rescale function `g_A` is sufficiently sensitive to preserve a standardized gap. This requires showing that its derivative is uniformly bounded below by a positive constant over its entire operational domain.\n3671: \n3672: :::{prf:lemma} Positive Derivative Bound for the Rescale Function\n3673: :label: lem-rescale-derivative-lower-bound\n3674: \n3675: For the Canonical Logistic Rescale function, the first derivative $g'_A(z)$ is uniformly bounded below by a strictly positive constant for all z-scores in the operational range $Z_{\\text{supp}}$. That is, there exists a constant $g'_{\\min} > 0$ such that:\n3676: \n3677: $$\n3678: \\inf_{z \\in Z_{\\mathrm{supp}}} g'_A(z) = g'_{\\min} > 0\n3679: $$\n3680: "
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-rescale-derivative-lower-bound",
      "title": null,
      "start_line": 3681,
      "end_line": 3694,
      "header_lines": [
        3682
      ],
      "content_start": 3684,
      "content_end": 3693,
      "content": "3684: :label: proof-lem-rescale-derivative-lower-bound\n3685: \n3686: **Proof.**\n3687: 1.  **Compactness of the Domain:** Any standardized score `z\u1d62` must lie within the interval `Z_supp`. This interval is defined by the uniform constants `V_max` and $\\sigma'_min,patch$, making `Z_supp` a compact set that is independent of the swarm state.\n3688: \n3689: 2.  **Properties of the Derivative:** The Canonical Logistic Rescale function is $g_A(z) = 2 / (1 + e^{-z})$. Its derivative, $g'_A(z) = 2e^{-z} / (1+e^{-z})^2$, is continuous and strictly positive for all $z \\in \\mathbb{R}$.\n3690: \n3691: 3.  **Application of the Extreme Value Theorem:** By the Extreme Value Theorem, a continuous function ($g'_A(z)$) must attain its minimum value on a compact set ($Z_{\\text{supp}}$).\n3692: \n3693: 4.  **Conclusion:** Since `g'_A(z)` is strictly positive on its entire domain, its minimum value on the compact subset `Z_supp`, which we define as `g'_min`, must also be a strictly positive constant.",
      "metadata": {
        "label": "proof-lem-rescale-derivative-lower-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3681: where $Z_{\\text{supp}} := \\left[ -2V_{\\max}/\\sigma'_{\\min,\\text{patch}}, 2V_{\\max}/\\sigma'_{\\min,\\text{patch}} \\right]$ is the compact support of all possible standardized scores.\n3682: :::\n3683: :::{prf:proof}\n3684: :label: proof-lem-rescale-derivative-lower-bound\n3685: \n3686: **Proof.**\n3687: 1.  **Compactness of the Domain:** Any standardized score `z\u1d62` must lie within the interval `Z_supp`. This interval is defined by the uniform constants `V_max` and $\\sigma'_min,patch$, making `Z_supp` a compact set that is independent of the swarm state.\n3688: \n3689: 2.  **Properties of the Derivative:** The Canonical Logistic Rescale function is $g_A(z) = 2 / (1 + e^{-z})$. Its derivative, $g'_A(z) = 2e^{-z} / (1+e^{-z})^2$, is continuous and strictly positive for all $z \\in \\mathbb{R}$.\n3690: \n3691: 3.  **Application of the Extreme Value Theorem:** By the Extreme Value Theorem, a continuous function ($g'_A(z)$) must attain its minimum value on a compact set ($Z_{\\text{supp}}$).\n3692: \n3693: 4.  **Conclusion:** Since `g'_A(z)` is strictly positive on its entire domain, its minimum value on the compact subset `Z_supp`, which we define as `g'_min`, must also be a strictly positive constant.\n3694: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-raw-gap-to-rescaled-gap",
      "title": "From Raw Measurement Gap to Rescaled Value Gap",
      "start_line": 3700,
      "end_line": 3715,
      "header_lines": [
        3701
      ],
      "content_start": 3703,
      "content_end": 3714,
      "content": "3703: :label: lem-raw-gap-to-rescaled-gap\n3704: \n3705: Let the system parameters be fixed. There exists a function $\\kappa_rescaled(\\kappa_raw)$ such that for *any* swarm state `S` with $k \\geq 2$ alive walkers, if the raw measurement values contain a gap $|v\u2090 - v\u1d66| \\geq \\kappa_raw > 0$, then the corresponding rescaled values are guaranteed to have a gap:\n3706: \n3707: $$\n3708: |g_A(z_a) - g_A(z_b)| \\ge \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}}) > 0\n3709: $$\n3710: \n3711: The function $\\kappa_rescaled$ is independent of the swarm state `S` and its size `k`, and is defined as:\n3712: \n3713: $$\n3714: \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}}) := \\frac{g'_{\\min}}{\\sigma'_{\\max}} \\cdot \\kappa_{\\mathrm{raw}}",
      "metadata": {
        "label": "lem-raw-gap-to-rescaled-gap"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3700: With the uniform bounds on the pipeline's components now established, we can prove the main result of this section: a guaranteed raw measurement gap is reliably transformed into a guaranteed rescaled value gap.\n3701: \n3702: :::{prf:lemma} From Raw Measurement Gap to Rescaled Value Gap\n3703: :label: lem-raw-gap-to-rescaled-gap\n3704: \n3705: Let the system parameters be fixed. There exists a function $\\kappa_rescaled(\\kappa_raw)$ such that for *any* swarm state `S` with $k \\geq 2$ alive walkers, if the raw measurement values contain a gap $|v\u2090 - v\u1d66| \\geq \\kappa_raw > 0$, then the corresponding rescaled values are guaranteed to have a gap:\n3706: \n3707: $$\n3708: |g_A(z_a) - g_A(z_b)| \\ge \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}}) > 0\n3709: $$\n3710: \n3711: The function $\\kappa_rescaled$ is independent of the swarm state `S` and its size `k`, and is defined as:\n3712: \n3713: $$\n3714: \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}}) := \\frac{g'_{\\min}}{\\sigma'_{\\max}} \\cdot \\kappa_{\\mathrm{raw}}\n3715: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-raw-gap-to-rescaled-gap",
      "title": null,
      "start_line": 3716,
      "end_line": 3759,
      "header_lines": [
        3717
      ],
      "content_start": 3719,
      "content_end": 3758,
      "content": "3719: :label: proof-lem-raw-gap-to-rescaled-gap\n3720: \n3721: **Proof.**\n3722: \n3723: The proof follows the signal gap as it propagates through the two main steps of the pipeline.\n3724: \n3725: **Stage 1: From Raw Value Gap to a Uniform Lower Bound on the Z-Score Gap**\n3726: We seek a uniform lower bound for the gap between standardized scores, `|z\u2090 - z\u1d66|`.\n3727: \n3728: $$\n3729: |z_a - z_b| = \\left| \\frac{v_a - \\mu}{\\sigma'} - \\frac{v_b - \\mu}{\\sigma'} \\right| = \\frac{|v_a - v_b|}{\\sigma'}\n3730: $$\n3731: \n3732: We are given the premise that the numerator is bounded below by $\\kappa_raw$. The denominator $\\sigma'$ is the patched standard deviation of the full set of `k` raw values. By Definition {prf:ref}`def-max-patched-std`, $\\sigma'$ is uniformly bounded above by the state-independent constant $\\sigma'_max$. Combining these gives a uniform lower bound on the z-score gap:\n3733: \n3734: $$\n3735: |z_a - z_b| \\ge \\frac{\\kappa_{\\mathrm{raw}}}{\\sigma'_{\\max}} =: \\kappa_z > 0\n3736: $$\n3737: \n3738: **Stage 2: From Z-Score Gap to Rescaled Value Gap**\n3739: The rescale function `g_A(z)` is continuously differentiable. By the Mean Value Theorem, there exists a point `c` on the line segment between `z\u2090` and `z\u1d66` such that:\n3740: \n3741: $$\n3742: |g_A(z_a) - g_A(z_b)| = |g'_A(c)| \\cdot |z_a - z_b|\n3743: $$\n3744: \n3745: The points `z\u2090`, `z\u1d66`, and `c` are all within the compact operational range `Z_supp`. By Lemma {prf:ref}`lem-rescale-derivative-lower-bound`, the derivative at `c` is uniformly bounded below by the positive constant `g'_min`. Substituting the lower bounds for both terms on the right-hand side gives:\n3746: \n3747: $$\n3748: |g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\kappa_z\n3749: $$\n3750: \n3751: **Conclusion**\n3752: Substituting the definition of $\\kappa_z$ from Stage 1 yields the final result:\n3753: \n3754: $$\n3755: |g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\left(\\frac{\\kappa_{\\mathrm{raw}}}{\\sigma'_{\\max}}\\right) = \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}})\n3756: $$\n3757: \n3758: Since `g'_min` and $\\sigma'_max$ are positive, N-uniform constants, the function $\\kappa_rescaled(\\kappa_raw)$ provides a strictly positive, N-uniform lower bound for any $\\kappa_raw > 0$. This completes the proof that a raw measurement gap robustly propagates to a guaranteed rescaled value gap.",
      "metadata": {
        "label": "proof-lem-raw-gap-to-rescaled-gap"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3716: \n3717: :::\n3718: :::{prf:proof}\n3719: :label: proof-lem-raw-gap-to-rescaled-gap\n3720: \n3721: **Proof.**\n3722: \n3723: The proof follows the signal gap as it propagates through the two main steps of the pipeline.\n3724: \n3725: **Stage 1: From Raw Value Gap to a Uniform Lower Bound on the Z-Score Gap**\n3726: We seek a uniform lower bound for the gap between standardized scores, `|z\u2090 - z\u1d66|`.\n3727: \n3728: $$\n3729: |z_a - z_b| = \\left| \\frac{v_a - \\mu}{\\sigma'} - \\frac{v_b - \\mu}{\\sigma'} \\right| = \\frac{|v_a - v_b|}{\\sigma'}\n3730: $$\n3731: \n3732: We are given the premise that the numerator is bounded below by $\\kappa_raw$. The denominator $\\sigma'$ is the patched standard deviation of the full set of `k` raw values. By Definition {prf:ref}`def-max-patched-std`, $\\sigma'$ is uniformly bounded above by the state-independent constant $\\sigma'_max$. Combining these gives a uniform lower bound on the z-score gap:\n3733: \n3734: $$\n3735: |z_a - z_b| \\ge \\frac{\\kappa_{\\mathrm{raw}}}{\\sigma'_{\\max}} =: \\kappa_z > 0\n3736: $$\n3737: \n3738: **Stage 2: From Z-Score Gap to Rescaled Value Gap**\n3739: The rescale function `g_A(z)` is continuously differentiable. By the Mean Value Theorem, there exists a point `c` on the line segment between `z\u2090` and `z\u1d66` such that:\n3740: \n3741: $$\n3742: |g_A(z_a) - g_A(z_b)| = |g'_A(c)| \\cdot |z_a - z_b|\n3743: $$\n3744: \n3745: The points `z\u2090`, `z\u1d66`, and `c` are all within the compact operational range `Z_supp`. By Lemma {prf:ref}`lem-rescale-derivative-lower-bound`, the derivative at `c` is uniformly bounded below by the positive constant `g'_min`. Substituting the lower bounds for both terms on the right-hand side gives:\n3746: \n3747: $$\n3748: |g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\kappa_z\n3749: $$\n3750: \n3751: **Conclusion**\n3752: Substituting the definition of $\\kappa_z$ from Stage 1 yields the final result:\n3753: \n3754: $$\n3755: |g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\left(\\frac{\\kappa_{\\mathrm{raw}}}{\\sigma'_{\\max}}\\right) = \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}})\n3756: $$\n3757: \n3758: Since `g'_min` and $\\sigma'_max$ are positive, N-uniform constants, the function $\\kappa_rescaled(\\kappa_raw)$ provides a strictly positive, N-uniform lower bound for any $\\kappa_raw > 0$. This completes the proof that a raw measurement gap robustly propagates to a guaranteed rescaled value gap.\n3759: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-variance-to-mean-separation",
      "title": "**(From Total Variance to Mean Separation)**",
      "start_line": 3783,
      "end_line": 3802,
      "header_lines": [
        3784
      ],
      "content_start": 3786,
      "content_end": 3801,
      "content": "3786: :label: lem-variance-to-mean-separation\n3787: \n3788: Let $\\mathcal{V} = \\{v_i\\}_{i=1}^k$ be a set of $k \\ge 2$ real numbers, with each element $v_i$ contained in the compact interval $[V_{\\min}, V_{\\max}]$. Let $\\mathcal{V}$ be partitioned into two disjoint, non-empty subsets, $H$ and $L$, with corresponding means $\\mu_H$ and $\\mu_L$. Let their fractional population sizes, $f_H = |H|/k$ and $f_L = |L|/k$, be bounded below by a strictly positive constant $f_{\\min} \\in (0, 1/2]$, such that $f_H \\ge f_{\\min}$ and $f_L \\ge f_{\\min}$.\n3789: \n3790: If the empirical variance of the total set, $\\operatorname{Var}(\\mathcal{V})$, is bounded below by a strictly positive constant $\\kappa_{\\mathrm{var}} > 0$, then the squared difference between the subset means is bounded below by:\n3791: \n3792: $$\n3793: (\\mu_H - \\mu_L)^2 \\ge \\frac{1}{f_H f_L} \\left( \\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}} \\right)\n3794: $$\n3795: \n3796: where $\\operatorname{Var}_{\\mathrm{max}} := \\frac{1}{4}(V_{\\max} - V_{\\min})^2$ is the maximum possible variance for any set of values on the interval.\n3797: \n3798: Consequently, if the guaranteed variance $\\kappa_{\\mathrm{var}}$ is sufficiently large to satisfy the **Signal-to-Noise Condition**, $\\kappa_{\\mathrm{var}} > \\operatorname{Var}_{\\mathrm{max}}$, then the mean separation is guaranteed to be positive:\n3799: \n3800: $$\n3801: |\\mu_H - \\mu_L| \\ge \\frac{1}{\\sqrt{f_H f_L}} \\sqrt{\\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}}} > 0",
      "metadata": {
        "label": "lem-variance-to-mean-separation"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3783: The following lemma provides this fundamental link. It proves, from first principles, that a sufficiently large variance within a population partitioned into two substantial subsets necessitates a statistically significant separation between the means of those subsets.\n3784: \n3785: :::{prf:lemma} **(From Total Variance to Mean Separation)**\n3786: :label: lem-variance-to-mean-separation\n3787: \n3788: Let $\\mathcal{V} = \\{v_i\\}_{i=1}^k$ be a set of $k \\ge 2$ real numbers, with each element $v_i$ contained in the compact interval $[V_{\\min}, V_{\\max}]$. Let $\\mathcal{V}$ be partitioned into two disjoint, non-empty subsets, $H$ and $L$, with corresponding means $\\mu_H$ and $\\mu_L$. Let their fractional population sizes, $f_H = |H|/k$ and $f_L = |L|/k$, be bounded below by a strictly positive constant $f_{\\min} \\in (0, 1/2]$, such that $f_H \\ge f_{\\min}$ and $f_L \\ge f_{\\min}$.\n3789: \n3790: If the empirical variance of the total set, $\\operatorname{Var}(\\mathcal{V})$, is bounded below by a strictly positive constant $\\kappa_{\\mathrm{var}} > 0$, then the squared difference between the subset means is bounded below by:\n3791: \n3792: $$\n3793: (\\mu_H - \\mu_L)^2 \\ge \\frac{1}{f_H f_L} \\left( \\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}} \\right)\n3794: $$\n3795: \n3796: where $\\operatorname{Var}_{\\mathrm{max}} := \\frac{1}{4}(V_{\\max} - V_{\\min})^2$ is the maximum possible variance for any set of values on the interval.\n3797: \n3798: Consequently, if the guaranteed variance $\\kappa_{\\mathrm{var}}$ is sufficiently large to satisfy the **Signal-to-Noise Condition**, $\\kappa_{\\mathrm{var}} > \\operatorname{Var}_{\\mathrm{max}}$, then the mean separation is guaranteed to be positive:\n3799: \n3800: $$\n3801: |\\mu_H - \\mu_L| \\ge \\frac{1}{\\sqrt{f_H f_L}} \\sqrt{\\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}}} > 0\n3802: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-variance-to-mean-separation",
      "title": null,
      "start_line": 3803,
      "end_line": 3894,
      "header_lines": [
        3804
      ],
      "content_start": 3806,
      "content_end": 3893,
      "content": "3806: :label: proof-lem-variance-to-mean-separation\n3807: \n3808: **Proof.**\n3809: \n3810: The proof is based on the decomposition of the total variance provided by the Law of Total Variance. We will establish a precise identity relating the total variance to the difference in subset means, find a sharp upper bound on the confounding variance term, and combine these results to derive the desired lower bound.\n3811: \n3812: **Step 1: The Law of Total Variance.**\n3813: Let $\\mu_{\\mathcal{V}}$ be the mean of the entire set $\\mathcal{V}$. The total empirical variance, $\\operatorname{Var}(\\mathcal{V}) := \\frac{1}{k}\\sum_{i \\in \\mathcal{V}} (v_i - \\mu_{\\mathcal{V}})^2$, can be decomposed into two components: the between-group variance ($\\operatorname{Var}_B$) and the within-group variance ($\\operatorname{Var}_W$).\n3814: \n3815: $$\n3816: \\operatorname{Var}(\\mathcal{V}) = \\operatorname{Var}_B(\\mathcal{V}) + \\operatorname{Var}_W(\\mathcal{V})\n3817: $$\n3818: \n3819: The **within-group variance** is the weighted average of the variances of the subsets:\n3820: \n3821: $$\n3822: \\operatorname{Var}_W(\\mathcal{V}) := f_H \\operatorname{Var}(H) + f_L \\operatorname{Var}(L)\n3823: $$\n3824: \n3825: The **between-group variance** is the variance of the subset means around the total mean:\n3826: \n3827: $$\n3828: \\operatorname{Var}_B(\\mathcal{V}) := f_H(\\mu_H - \\mu_{\\mathcal{V}})^2 + f_L(\\mu_L - \\mu_{\\mathcal{V}})^2\n3829: $$\n3830: \n3831: **Step 2: Relating Between-Group Variance to the Mean Separation.**\n3832: We will now prove that the between-group variance is directly proportional to $(\\mu_H - \\mu_L)^2$. The total mean is the weighted average of the subset means: $\\mu_{\\mathcal{V}} = f_H \\mu_H + f_L \\mu_L$. Substituting this into the definition of $\\operatorname{Var}_B(\\mathcal{V})$:\n3833: \n3834: $$\n3835: \\begin{aligned}\n3836: \\mu_H - \\mu_{\\mathcal{V}} &= \\mu_H - (f_H \\mu_H + f_L \\mu_L) = (1-f_H)\\mu_H - f_L \\mu_L = f_L \\mu_H - f_L \\mu_L = f_L(\\mu_H - \\mu_L) \\\\\n3837: \\mu_L - \\mu_{\\mathcal{V}} &= \\mu_L - (f_H \\mu_H + f_L \\mu_L) = -f_H \\mu_H + (1-f_L)\\mu_L = -f_H \\mu_H + f_H \\mu_L = -f_H(\\mu_H - \\mu_L)\n3838: \\end{aligned}\n3839: $$\n3840: \n3841: Substituting these expressions back into the formula for $\\operatorname{Var}_B(\\mathcal{V})$ yields:\n3842: \n3843: $$\n3844: \\begin{aligned}\n3845: \\operatorname{Var}_B(\\mathcal{V}) &= f_H (f_L(\\mu_H - \\mu_L))^2 + f_L (-f_H(\\mu_H - \\mu_L))^2 \\\\\n3846: &= f_H f_L^2 (\\mu_H - \\mu_L)^2 + f_L f_H^2 (\\mu_H - \\mu_L)^2 \\\\\n3847: &= (f_H f_L^2 + f_L f_H^2)(\\mu_H - \\mu_L)^2 \\\\\n3848: &= f_H f_L (f_L + f_H)(\\mu_H - \\mu_L)^2\n3849: \\end{aligned}\n3850: $$\n3851: \n3852: Since $f_H + f_L = 1$, we arrive at the exact identity:\n3853: \n3854: $$\n3855: \\operatorname{Var}_B(\\mathcal{V}) = f_H f_L (\\mu_H - \\mu_L)^2\n3856: $$\n3857: \n3858: **Step 3: A Uniform Upper Bound on the Within-Group Variance.**\n3859: The within-group variance, $\\operatorname{Var}_W(\\mathcal{V}) = f_H \\operatorname{Var}(H) + f_L \\operatorname{Var}(L)$, represents the noise that can mask the signal from the mean separation. We seek a sharp, state-independent upper bound. For any set of numbers on a compact interval $[a, b]$, the maximum possible variance is given by Popoviciu's inequality:\n3860: \n3861: $$\n3862: \\operatorname{Var}(S) \\le \\frac{1}{4}(\\max(S) - \\min(S))^2\n3863: $$\n3864: \n3865: Since for any subset $S \\subseteq \\mathcal{V}$, its elements are contained in $[V_{\\min}, V_{\\max}]$, we have $\\operatorname{Var}(H) \\le \\frac{1}{4}(V_{\\max} - V_{\\min})^2$ and $\\operatorname{Var}(L) \\le \\frac{1}{4}(V_{\\max} - V_{\\min})^2$.\n3866: Let $\\operatorname{Var}_{\\mathrm{max}} := \\frac{1}{4}(V_{\\max} - V_{\\min})^2$. The within-group variance is therefore uniformly bounded above:\n3867: \n3868: $$\n3869: \\operatorname{Var}_W(\\mathcal{V}) \\le f_H \\operatorname{Var}_{\\mathrm{max}} + f_L \\operatorname{Var}_{\\mathrm{max}} = (f_H+f_L)\\operatorname{Var}_{\\mathrm{max}} = \\operatorname{Var}_{\\mathrm{max}}\n3870: $$\n3871: \n3872: This upper bound is sharp; it is attained if both subsets consist of values located only at the endpoints of the interval.\n3873: \n3874: **Step 4: Assembling the Final Inequality.**\n3875: We rearrange the Law of Total Variance from Step 1:\n3876: \n3877: $$\n3878: \\operatorname{Var}_B(\\mathcal{V}) = \\operatorname{Var}(\\mathcal{V}) - \\operatorname{Var}_W(\\mathcal{V})\n3879: $$\n3880: \n3881: We substitute our identity for $\\operatorname{Var}_B(\\mathcal{V})$ from Step 2. Then, we use our premise, $\\operatorname{Var}(\\mathcal{V}) \\ge \\kappa_{\\mathrm{var}}$, and our upper bound for the within-group variance from Step 3:\n3882: \n3883: $$\n3884: f_H f_L (\\mu_H - \\mu_L)^2 \\ge \\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}}\n3885: $$\n3886: \n3887: Since the fractional sizes $f_H$ and $f_L$ are strictly positive, dividing by their product preserves the inequality:\n3888: \n3889: $$\n3890: (\\mu_H - \\mu_L)^2 \\ge \\frac{1}{f_H f_L} \\left( \\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}} \\right)\n3891: $$\n3892: \n3893: This proves the main inequality of the lemma. The final conclusion follows directly. If $\\kappa_{\\mathrm{var}} > \\operatorname{Var}_{\\mathrm{max}}$, the right-hand side is strictly positive. Taking the square root gives the lower bound on $|\\mu_H - \\mu_L|$. The pre-factor $1/\\sqrt{f_H f_L}$ is well-defined and uniformly bounded above because the premises guarantee $f_H, f_L \\ge f_{\\min} > 0$. The entire lower bound is therefore a strictly positive constant.",
      "metadata": {
        "label": "proof-lem-variance-to-mean-separation"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3803: \n3804: :::\n3805: :::{prf:proof}\n3806: :label: proof-lem-variance-to-mean-separation\n3807: \n3808: **Proof.**\n3809: \n3810: The proof is based on the decomposition of the total variance provided by the Law of Total Variance. We will establish a precise identity relating the total variance to the difference in subset means, find a sharp upper bound on the confounding variance term, and combine these results to derive the desired lower bound.\n3811: \n3812: **Step 1: The Law of Total Variance.**\n3813: Let $\\mu_{\\mathcal{V}}$ be the mean of the entire set $\\mathcal{V}$. The total empirical variance, $\\operatorname{Var}(\\mathcal{V}) := \\frac{1}{k}\\sum_{i \\in \\mathcal{V}} (v_i - \\mu_{\\mathcal{V}})^2$, can be decomposed into two components: the between-group variance ($\\operatorname{Var}_B$) and the within-group variance ($\\operatorname{Var}_W$).\n3814: \n3815: $$\n3816: \\operatorname{Var}(\\mathcal{V}) = \\operatorname{Var}_B(\\mathcal{V}) + \\operatorname{Var}_W(\\mathcal{V})\n3817: $$\n3818: \n3819: The **within-group variance** is the weighted average of the variances of the subsets:\n3820: \n3821: $$\n3822: \\operatorname{Var}_W(\\mathcal{V}) := f_H \\operatorname{Var}(H) + f_L \\operatorname{Var}(L)\n3823: $$\n3824: \n3825: The **between-group variance** is the variance of the subset means around the total mean:\n3826: \n3827: $$\n3828: \\operatorname{Var}_B(\\mathcal{V}) := f_H(\\mu_H - \\mu_{\\mathcal{V}})^2 + f_L(\\mu_L - \\mu_{\\mathcal{V}})^2\n3829: $$\n3830: \n3831: **Step 2: Relating Between-Group Variance to the Mean Separation.**\n3832: We will now prove that the between-group variance is directly proportional to $(\\mu_H - \\mu_L)^2$. The total mean is the weighted average of the subset means: $\\mu_{\\mathcal{V}} = f_H \\mu_H + f_L \\mu_L$. Substituting this into the definition of $\\operatorname{Var}_B(\\mathcal{V})$:\n3833: \n3834: $$\n3835: \\begin{aligned}\n3836: \\mu_H - \\mu_{\\mathcal{V}} &= \\mu_H - (f_H \\mu_H + f_L \\mu_L) = (1-f_H)\\mu_H - f_L \\mu_L = f_L \\mu_H - f_L \\mu_L = f_L(\\mu_H - \\mu_L) \\\\\n3837: \\mu_L - \\mu_{\\mathcal{V}} &= \\mu_L - (f_H \\mu_H + f_L \\mu_L) = -f_H \\mu_H + (1-f_L)\\mu_L = -f_H \\mu_H + f_H \\mu_L = -f_H(\\mu_H - \\mu_L)\n3838: \\end{aligned}\n3839: $$\n3840: \n3841: Substituting these expressions back into the formula for $\\operatorname{Var}_B(\\mathcal{V})$ yields:\n3842: \n3843: $$\n3844: \\begin{aligned}\n3845: \\operatorname{Var}_B(\\mathcal{V}) &= f_H (f_L(\\mu_H - \\mu_L))^2 + f_L (-f_H(\\mu_H - \\mu_L))^2 \\\\\n3846: &= f_H f_L^2 (\\mu_H - \\mu_L)^2 + f_L f_H^2 (\\mu_H - \\mu_L)^2 \\\\\n3847: &= (f_H f_L^2 + f_L f_H^2)(\\mu_H - \\mu_L)^2 \\\\\n3848: &= f_H f_L (f_L + f_H)(\\mu_H - \\mu_L)^2\n3849: \\end{aligned}\n3850: $$\n3851: \n3852: Since $f_H + f_L = 1$, we arrive at the exact identity:\n3853: \n3854: $$\n3855: \\operatorname{Var}_B(\\mathcal{V}) = f_H f_L (\\mu_H - \\mu_L)^2\n3856: $$\n3857: \n3858: **Step 3: A Uniform Upper Bound on the Within-Group Variance.**\n3859: The within-group variance, $\\operatorname{Var}_W(\\mathcal{V}) = f_H \\operatorname{Var}(H) + f_L \\operatorname{Var}(L)$, represents the noise that can mask the signal from the mean separation. We seek a sharp, state-independent upper bound. For any set of numbers on a compact interval $[a, b]$, the maximum possible variance is given by Popoviciu's inequality:\n3860: \n3861: $$\n3862: \\operatorname{Var}(S) \\le \\frac{1}{4}(\\max(S) - \\min(S))^2\n3863: $$\n3864: \n3865: Since for any subset $S \\subseteq \\mathcal{V}$, its elements are contained in $[V_{\\min}, V_{\\max}]$, we have $\\operatorname{Var}(H) \\le \\frac{1}{4}(V_{\\max} - V_{\\min})^2$ and $\\operatorname{Var}(L) \\le \\frac{1}{4}(V_{\\max} - V_{\\min})^2$.\n3866: Let $\\operatorname{Var}_{\\mathrm{max}} := \\frac{1}{4}(V_{\\max} - V_{\\min})^2$. The within-group variance is therefore uniformly bounded above:\n3867: \n3868: $$\n3869: \\operatorname{Var}_W(\\mathcal{V}) \\le f_H \\operatorname{Var}_{\\mathrm{max}} + f_L \\operatorname{Var}_{\\mathrm{max}} = (f_H+f_L)\\operatorname{Var}_{\\mathrm{max}} = \\operatorname{Var}_{\\mathrm{max}}\n3870: $$\n3871: \n3872: This upper bound is sharp; it is attained if both subsets consist of values located only at the endpoints of the interval.\n3873: \n3874: **Step 4: Assembling the Final Inequality.**\n3875: We rearrange the Law of Total Variance from Step 1:\n3876: \n3877: $$\n3878: \\operatorname{Var}_B(\\mathcal{V}) = \\operatorname{Var}(\\mathcal{V}) - \\operatorname{Var}_W(\\mathcal{V})\n3879: $$\n3880: \n3881: We substitute our identity for $\\operatorname{Var}_B(\\mathcal{V})$ from Step 2. Then, we use our premise, $\\operatorname{Var}(\\mathcal{V}) \\ge \\kappa_{\\mathrm{var}}$, and our upper bound for the within-group variance from Step 3:\n3882: \n3883: $$\n3884: f_H f_L (\\mu_H - \\mu_L)^2 \\ge \\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}}\n3885: $$\n3886: \n3887: Since the fractional sizes $f_H$ and $f_L$ are strictly positive, dividing by their product preserves the inequality:\n3888: \n3889: $$\n3890: (\\mu_H - \\mu_L)^2 \\ge \\frac{1}{f_H f_L} \\left( \\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}} \\right)\n3891: $$\n3892: \n3893: This proves the main inequality of the lemma. The final conclusion follows directly. If $\\kappa_{\\mathrm{var}} > \\operatorname{Var}_{\\mathrm{max}}$, the right-hand side is strictly positive. Taking the square root gives the lower bound on $|\\mu_H - \\mu_L|$. The pre-factor $1/\\sqrt{f_H f_L}$ is well-defined and uniformly bounded above because the premises guarantee $f_H, f_L \\ge f_{\\min} > 0$. The entire lower bound is therefore a strictly positive constant.\n3894: "
    },
    {
      "directive_type": "theorem",
      "label": "thm-derivation-of-stability-condition",
      "title": "Derivation of the Stability Condition for Intelligent Adaptation",
      "start_line": 3915,
      "end_line": 3927,
      "header_lines": [
        3916
      ],
      "content_start": 3918,
      "content_end": 3926,
      "content": "3918: :label: thm-derivation-of-stability-condition\n3919: \n3920: Let the system satisfy the foundational axioms, including the **Axiom of Non-Deceptive Landscapes (EG-7)**. Let a swarm `k` have a sufficiently large internal positional variance, $\\mathrm{Var}_x(S_k) > R^2_{\\mathrm{var}}$.\n3921: \n3922: The algorithm's targeting mechanism is \"intelligent\" (i.e., the expected fitness of a high-error walker is systematically lower than that of a low-error walker) if and only if the system parameters ($\\alpha$, $\\beta$, $\\varepsilon$, etc.) satisfy the following **Stability Condition**:\n3923: \n3924: $$\n3925: \\beta \\ln\\left(1 + \\frac{\\kappa_{\\text{mean},d'}(\\epsilon)}{g_{A,max}+\\eta}\\right) > \\alpha \\ln\\left(1 + \\frac{\\kappa_{\\text{mean},r'}}{\\eta}\\right)\n3926: $$",
      "metadata": {
        "label": "thm-derivation-of-stability-condition"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3915: This property is not unconditional. We will rigorously derive a **Stability Condition** on the algorithm's dynamics parameters ($\\alpha$, $\\beta$, $\\varepsilon$) from first principles. The proof will be statistical in nature, analyzing the expected fitness of the entire \"high-error\" population versus the \"low-error\" population. We will demonstrate that satisfying this Stability Condition is the necessary and sufficient condition to guarantee that the high-error walkers are, on average, less fit. This derivation provides the formal justification for elevating this condition to the status of a foundational axiom for any well-posed Fragile Gas system.\n3916: \n3917: :::{prf:theorem} Derivation of the Stability Condition for Intelligent Adaptation\n3918: :label: thm-derivation-of-stability-condition\n3919: \n3920: Let the system satisfy the foundational axioms, including the **Axiom of Non-Deceptive Landscapes (EG-7)**. Let a swarm `k` have a sufficiently large internal positional variance, $\\mathrm{Var}_x(S_k) > R^2_{\\mathrm{var}}$.\n3921: \n3922: The algorithm's targeting mechanism is \"intelligent\" (i.e., the expected fitness of a high-error walker is systematically lower than that of a low-error walker) if and only if the system parameters ($\\alpha$, $\\beta$, $\\varepsilon$, etc.) satisfy the following **Stability Condition**:\n3923: \n3924: $$\n3925: \\beta \\ln\\left(1 + \\frac{\\kappa_{\\text{mean},d'}(\\epsilon)}{g_{A,max}+\\eta}\\right) > \\alpha \\ln\\left(1 + \\frac{\\kappa_{\\text{mean},r'}}{\\eta}\\right)\n3926: $$\n3927: "
    },
    {
      "directive_type": "proof",
      "label": "proof-thm-derivation-of-stability-condition",
      "title": null,
      "start_line": 3928,
      "end_line": 4014,
      "header_lines": [
        3929
      ],
      "content_start": 3931,
      "content_end": 4013,
      "content": "3931: :label: proof-thm-derivation-of-stability-condition\n3932: \n3933: **Proof.**\n3934: \n3935: The proof proceeds in four stages. First, we formalize the condition for intelligent targeting in terms of the expected log-fitness of the high-error and low-error populations. Second, we decompose this condition to isolate the trade-off between the diversity and reward signals. Third, we derive rigorous, uniform bounds for these signal gaps under worst-case adversarial conditions. Finally, we assemble these bounds to derive the necessary and sufficient inequality.\n3936: \n3937: **1. The Formal Condition for Intelligent Targeting**\n3938: \n3939: For the algorithm's targeting mechanism to be corrective, the high-error population `H_k` must, on average, be less fit than the low-error population `L_k = A_k \\setminus H_k`. Due to the multiplicative form of the fitness potential, $V_{\\text{fit}} = (d')^\\beta (r')^\\alpha$, the most robust way to analyze this condition is by comparing the expected logarithms of the fitness. The condition for intelligent targeting is therefore:\n3940: \n3941: $$\n3942: \\mathbb{E}[\\ln(V_{\\text{fit}}) \\mid i \\in H_k] < \\mathbb{E}[\\ln(V_{\\text{fit}}) \\mid i \\in L_k]\n3943: $$\n3944: \n3945: **2. Decomposing the Condition into a Signal Trade-off**\n3946: \n3947: Using the definition $ln(V_fit) = \\beta ln(d') + \\alpha ln(r')$ and the linearity of expectation, the condition from Step 1 becomes:\n3948: \n3949: $$\n3950: \\beta \\mathbb{E}[\\ln(d')|H_k] + \\alpha \\mathbb{E}[\\ln(r')|H_k] < \\beta \\mathbb{E}[\\ln(d')|L_k] + \\alpha \\mathbb{E}[\\ln(r')|L_k]\n3951: $$\n3952: \n3953: Rearranging the terms to separate the contribution from the diversity signal and the reward signal yields the core trade-off inequality that must be satisfied:\n3954: \n3955: $$\n3956: \\beta \\left( \\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\right) > \\alpha \\left( \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\right) \\quad (*)\n3957: $$\n3958: \n3959: This inequality states that the fitness advantage from the reliable diversity signal (LHS) must be strong enough to overcome the potential fitness advantage from a deceptive reward signal (RHS).\n3960: \n3961: **3. Deriving Uniform Bounds on the Signal Gaps**\n3962: \n3963: We now find uniform bounds for the two parenthesized terms in inequality `(*)`. This is the critical step where we correctly apply Lemma 7.3.1 to establish rigorous bounds. These bounds must hold for any swarm configuration, including the most adversarial ones.\n3964: \n3965: *   **LHS: The Minimum Guaranteed Diversity Signal.**\n3966: \n3967:     The term `E[ln(d')|H_k] - E[ln(d')|L_k]` represents the guaranteed advantage in the diversity signal for the high-error population. We establish this through the following causal chain:\n3968: \n3969:     1. **From Geometry to Raw Measurement Variance:** A high-error state guarantees a raw measurement variance $\\text{E}[\\text{Var}(d)] \\geq \\kappa_meas(\\varepsilon) > 0$ (from [](#thm-geometry-guarantees-variance)).\n3970: \n3971:     2. **From Raw Variance to Rescaled Variance:** This raw variance propagates through the pipeline, guaranteeing a variance in the rescaled values $\\text{Var}(d') \\geq \\kappa_var(d') > 0$. The constant $\\kappa_var(d')$ is defined in terms of $\\kappa_meas(\\varepsilon)$ and the pipeline parameters via the gap propagation lemmas from Section 7.3.\n3972: \n3973:     3. **Signal-to-Noise Condition:** The Signal-to-Noise Condition $\\kappa_var(d') > Var_max(d')$ is satisfied by the choice of the gain parameter $\\gamma$ (from Proposition 7.2.2).\n3974: \n3975:     4. **Applying [](#lem-variance-to-mean-separation):** We now apply [](#lem-variance-to-mean-separation) to the set of rescaled diversity values `d'`. Let:\n3976:         - `V = d'` (the total set of rescaled diversity values)\n3977:         - `H = H_k` and `L = L_k` (the partition)\n3978:         - The premise $\\text{Var}(V) \\geq \\kappa_var$ is met with $\\kappa_var = \\kappa_var(d')$\n3979:         - The premise $\\kappa_var > Var_max$ is met by the Signal-to-Noise Condition\n3980: \n3981:     5. **Result from [](#lem-variance-to-mean-separation):** This yields a guaranteed lower bound on the separation between the subset means:\n3982: \n3983: \n3984: $$\n3985: |\\mathbb{E}[d'|H_k] - \\mathbb{E}[d'|L_k]| \\ge \\frac{1}{\\sqrt{f_H f_L}} \\sqrt{\\kappa_{\\mathrm{var}}(d') - \\operatorname{Var}_{\\max}(d')}\n3986: $$\n3987: \n3988:     6. **Define the Mean Gap Constant:** We define this entire N-uniform lower bound as:\n3989: \n3990: \n3991: $$\n3992: \\kappa_{\\text{mean},d'}(\\epsilon) := \\frac{1}{\\sqrt{f_H f_L}} \\sqrt{\\kappa_{\\mathrm{var}}(d') - \\operatorname{Var}_{\\max}(d')} > 0\n3993: $$\n3994: \n3995:     7. **From Mean Separation to Logarithmic Separation:** The smallest possible logarithmic gap corresponding to this minimal mean separation occurs when the values are compressed at the top of their allowed range, $[\\eta, g_A,max + \\eta]$. This provides a uniform lower bound on the reliable signal:\n3996: \n3997: \n3998: $$\n3999: \\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{\\text{mean},d'}(\\epsilon)}{g_{A,max}+\\eta}\\right)\n4000: $$\n4001: \n4002: *   **RHS: The Maximum Adversarial Reward Signal.**\n4003: \n4004:     Symmetrically, we apply the same logic to find an upper bound on the term `E[ln(r')|L_k] - E[ln(r')|H_k]`, which represents the maximum potential advantage from a deceptive reward signal. A potential adversarial raw gap $\\kappa_r'$ leads, through the application of [](#lem-variance-to-mean-separation) to the reward channel, to a maximum possible rescaled mean gap of $\\kappa_{\\text{mean},r'}$. The largest possible logarithmic gap corresponding to this reward separation occurs when the values are compressed at the bottom of their range. This gives a uniform upper bound on the adversarial signal:\n4005: \n4006: \n4007: $$\n4008: \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{\\kappa_{\\text{mean},r'}}{\\eta}\\right)\n4009: $$\n4010: \n4011: **4. Assembling the Final Stability Condition**\n4012: \n4013: For the intelligent targeting inequality `(*)` to hold robustly for *any* high-variance swarm, the guaranteed *minimum* of the LHS must be strictly greater than the allowed *maximum* of the RHS. The assembly of the final condition is now rigorous because it compares provably non-vanishing bounds on the *means of the populations*, not on unrepresentative individual values. Substituting the bounds derived in Stage 3 gives the necessary and sufficient condition.",
      "metadata": {
        "label": "proof-thm-derivation-of-stability-condition"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "3928: where $\\kappa_mean,d'(\\varepsilon)$ and $\\kappa_mean,r'$ are the guaranteed N-uniform separations between the *mean* rescaled values of the high-error and low-error populations, derived from the system's guaranteed signal variance and landscape regularity, respectively.\n3929: :::\n3930: :::{prf:proof}\n3931: :label: proof-thm-derivation-of-stability-condition\n3932: \n3933: **Proof.**\n3934: \n3935: The proof proceeds in four stages. First, we formalize the condition for intelligent targeting in terms of the expected log-fitness of the high-error and low-error populations. Second, we decompose this condition to isolate the trade-off between the diversity and reward signals. Third, we derive rigorous, uniform bounds for these signal gaps under worst-case adversarial conditions. Finally, we assemble these bounds to derive the necessary and sufficient inequality.\n3936: \n3937: **1. The Formal Condition for Intelligent Targeting**\n3938: \n3939: For the algorithm's targeting mechanism to be corrective, the high-error population `H_k` must, on average, be less fit than the low-error population `L_k = A_k \\setminus H_k`. Due to the multiplicative form of the fitness potential, $V_{\\text{fit}} = (d')^\\beta (r')^\\alpha$, the most robust way to analyze this condition is by comparing the expected logarithms of the fitness. The condition for intelligent targeting is therefore:\n3940: \n3941: $$\n3942: \\mathbb{E}[\\ln(V_{\\text{fit}}) \\mid i \\in H_k] < \\mathbb{E}[\\ln(V_{\\text{fit}}) \\mid i \\in L_k]\n3943: $$\n3944: \n3945: **2. Decomposing the Condition into a Signal Trade-off**\n3946: \n3947: Using the definition $ln(V_fit) = \\beta ln(d') + \\alpha ln(r')$ and the linearity of expectation, the condition from Step 1 becomes:\n3948: \n3949: $$\n3950: \\beta \\mathbb{E}[\\ln(d')|H_k] + \\alpha \\mathbb{E}[\\ln(r')|H_k] < \\beta \\mathbb{E}[\\ln(d')|L_k] + \\alpha \\mathbb{E}[\\ln(r')|L_k]\n3951: $$\n3952: \n3953: Rearranging the terms to separate the contribution from the diversity signal and the reward signal yields the core trade-off inequality that must be satisfied:\n3954: \n3955: $$\n3956: \\beta \\left( \\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\right) > \\alpha \\left( \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\right) \\quad (*)\n3957: $$\n3958: \n3959: This inequality states that the fitness advantage from the reliable diversity signal (LHS) must be strong enough to overcome the potential fitness advantage from a deceptive reward signal (RHS).\n3960: \n3961: **3. Deriving Uniform Bounds on the Signal Gaps**\n3962: \n3963: We now find uniform bounds for the two parenthesized terms in inequality `(*)`. This is the critical step where we correctly apply Lemma 7.3.1 to establish rigorous bounds. These bounds must hold for any swarm configuration, including the most adversarial ones.\n3964: \n3965: *   **LHS: The Minimum Guaranteed Diversity Signal.**\n3966: \n3967:     The term `E[ln(d')|H_k] - E[ln(d')|L_k]` represents the guaranteed advantage in the diversity signal for the high-error population. We establish this through the following causal chain:\n3968: \n3969:     1. **From Geometry to Raw Measurement Variance:** A high-error state guarantees a raw measurement variance $\\text{E}[\\text{Var}(d)] \\geq \\kappa_meas(\\varepsilon) > 0$ (from [](#thm-geometry-guarantees-variance)).\n3970: \n3971:     2. **From Raw Variance to Rescaled Variance:** This raw variance propagates through the pipeline, guaranteeing a variance in the rescaled values $\\text{Var}(d') \\geq \\kappa_var(d') > 0$. The constant $\\kappa_var(d')$ is defined in terms of $\\kappa_meas(\\varepsilon)$ and the pipeline parameters via the gap propagation lemmas from Section 7.3.\n3972: \n3973:     3. **Signal-to-Noise Condition:** The Signal-to-Noise Condition $\\kappa_var(d') > Var_max(d')$ is satisfied by the choice of the gain parameter $\\gamma$ (from Proposition 7.2.2).\n3974: \n3975:     4. **Applying [](#lem-variance-to-mean-separation):** We now apply [](#lem-variance-to-mean-separation) to the set of rescaled diversity values `d'`. Let:\n3976:         - `V = d'` (the total set of rescaled diversity values)\n3977:         - `H = H_k` and `L = L_k` (the partition)\n3978:         - The premise $\\text{Var}(V) \\geq \\kappa_var$ is met with $\\kappa_var = \\kappa_var(d')$\n3979:         - The premise $\\kappa_var > Var_max$ is met by the Signal-to-Noise Condition\n3980: \n3981:     5. **Result from [](#lem-variance-to-mean-separation):** This yields a guaranteed lower bound on the separation between the subset means:\n3982: \n3983: \n3984: $$\n3985: |\\mathbb{E}[d'|H_k] - \\mathbb{E}[d'|L_k]| \\ge \\frac{1}{\\sqrt{f_H f_L}} \\sqrt{\\kappa_{\\mathrm{var}}(d') - \\operatorname{Var}_{\\max}(d')}\n3986: $$\n3987: \n3988:     6. **Define the Mean Gap Constant:** We define this entire N-uniform lower bound as:\n3989: \n3990: \n3991: $$\n3992: \\kappa_{\\text{mean},d'}(\\epsilon) := \\frac{1}{\\sqrt{f_H f_L}} \\sqrt{\\kappa_{\\mathrm{var}}(d') - \\operatorname{Var}_{\\max}(d')} > 0\n3993: $$\n3994: \n3995:     7. **From Mean Separation to Logarithmic Separation:** The smallest possible logarithmic gap corresponding to this minimal mean separation occurs when the values are compressed at the top of their allowed range, $[\\eta, g_A,max + \\eta]$. This provides a uniform lower bound on the reliable signal:\n3996: \n3997: \n3998: $$\n3999: \\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{\\text{mean},d'}(\\epsilon)}{g_{A,max}+\\eta}\\right)\n4000: $$\n4001: \n4002: *   **RHS: The Maximum Adversarial Reward Signal.**\n4003: \n4004:     Symmetrically, we apply the same logic to find an upper bound on the term `E[ln(r')|L_k] - E[ln(r')|H_k]`, which represents the maximum potential advantage from a deceptive reward signal. A potential adversarial raw gap $\\kappa_r'$ leads, through the application of [](#lem-variance-to-mean-separation) to the reward channel, to a maximum possible rescaled mean gap of $\\kappa_{\\text{mean},r'}$. The largest possible logarithmic gap corresponding to this reward separation occurs when the values are compressed at the bottom of their range. This gives a uniform upper bound on the adversarial signal:\n4005: \n4006: \n4007: $$\n4008: \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{\\kappa_{\\text{mean},r'}}{\\eta}\\right)\n4009: $$\n4010: \n4011: **4. Assembling the Final Stability Condition**\n4012: \n4013: For the intelligent targeting inequality `(*)` to hold robustly for *any* high-variance swarm, the guaranteed *minimum* of the LHS must be strictly greater than the allowed *maximum* of the RHS. The assembly of the final condition is now rigorous because it compares provably non-vanishing bounds on the *means of the populations*, not on unrepresentative individual values. Substituting the bounds derived in Stage 3 gives the necessary and sufficient condition.\n4014: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-log-gap-lower-bound",
      "title": "Lower Bound on Logarithmic Mean Gap",
      "start_line": 4032,
      "end_line": 4043,
      "header_lines": [
        4033
      ],
      "content_start": 4035,
      "content_end": 4042,
      "content": "4035: :label: lem-log-gap-lower-bound\n4036: \n4037: Let $X$ and $Y$ be two random variables whose values are contained in the compact interval $[V_{\\min}, V_{\\max}]$, where $V_{\\min} > 0$. Let their means, $\\mu_X = \\mathbb{E}[X]$ and $\\mu_Y = \\mathbb{E}[Y]$, satisfy $\\mu_X \\ge \\mu_Y + \\kappa$ for some constant $\\kappa > 0$.\n4038: \n4039: Then the difference of their expected logarithms is bounded below as follows:\n4040: \n4041: $$\n4042: \\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\ge \\ln\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right)",
      "metadata": {
        "label": "lem-log-gap-lower-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4032: This lemma establishes that a guaranteed separation in the means of two distributions implies a guaranteed, non-vanishing separation in their expected logarithms. The proof uses extremal distribution theory to find the worst-case scenario.\n4033: \n4034: :::{prf:lemma} Lower Bound on Logarithmic Mean Gap\n4035: :label: lem-log-gap-lower-bound\n4036: \n4037: Let $X$ and $Y$ be two random variables whose values are contained in the compact interval $[V_{\\min}, V_{\\max}]$, where $V_{\\min} > 0$. Let their means, $\\mu_X = \\mathbb{E}[X]$ and $\\mu_Y = \\mathbb{E}[Y]$, satisfy $\\mu_X \\ge \\mu_Y + \\kappa$ for some constant $\\kappa > 0$.\n4038: \n4039: Then the difference of their expected logarithms is bounded below as follows:\n4040: \n4041: $$\n4042: \\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\ge \\ln\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right)\n4043: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-log-gap-lower-bound",
      "title": null,
      "start_line": 4045,
      "end_line": 4154,
      "header_lines": [
        4046
      ],
      "content_start": 4048,
      "content_end": 4153,
      "content": "4048: :label: proof-lem-log-gap-lower-bound\n4049: \n4050: **Proof.**\n4051: \n4052: The proof uses the theory of extremal distributions for concave functions. We establish tight bounds on each term by identifying the distributions that minimize $\\mathbb{E}[\\ln(X)]$ and maximize $\\mathbb{E}[\\ln(Y)]$, then find the minimum of their difference over all valid mean pairs.\n4053: \n4054: **Step 1: Extremal Distributions for the Logarithm.**\n4055: \n4056: Since $f(t) = \\ln(t)$ is strictly concave for $t > 0$, the extremal distributions are well-known:\n4057: - For a **fixed mean** $\\mu$, the minimum of $\\mathbb{E}[\\ln(X)]$ is achieved by a **two-point distribution** with mass only at the endpoints $\\{V_{\\min}, V_{\\max}\\}$.\n4058: - For a **fixed mean** $\\mu$, the maximum of $\\mathbb{E}[\\ln(Y)]$ is achieved by a **deterministic distribution**: $Y = \\mu$ with probability 1. By Jensen's inequality, $\\mathbb{E}[\\ln(Y)] \\le \\ln(\\mu_Y)$, with equality when $Y$ is deterministic.\n4059: \n4060: **Step 2: Bounding the Difference Using Extremal Cases.**\n4061: \n4062: The difference $\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]$ is minimized in the worst-case scenario where:\n4063: - $\\mathbb{E}[\\ln(X)]$ is as small as possible for mean $\\mu_X$ \u2192 Use the extremal two-point distribution $X_{\\min}$\n4064: - $\\mathbb{E}[\\ln(Y)]$ is as large as possible for mean $\\mu_Y$ \u2192 Use the deterministic distribution $Y = \\mu_Y$\n4065: \n4066: Therefore, for any distributions $X$, $Y$ with means $\\mu_X$, $\\mu_Y$:\n4067: \n4068: $$\n4069: \\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\ge \\mathbb{E}[\\ln(X_{\\min})] - \\ln(\\mu_Y)\n4070: $$\n4071: \n4072: where $X_{\\min}$ is the two-point distribution with mean $\\mu_X$:\n4073: \n4074: $$\n4075: P(X_{\\min} = V_{\\min}) = \\frac{V_{\\max} - \\mu_X}{V_{\\max} - V_{\\min}}, \\quad P(X_{\\min} = V_{\\max}) = \\frac{\\mu_X - V_{\\min}}{V_{\\max} - V_{\\min}}\n4076: $$\n4077: \n4078: **Step 3: Reduction to a One-Dimensional Optimization Problem.**\n4079: \n4080: We now minimize $\\mathbb{E}[\\ln(X_{\\min})] - \\ln(\\mu_Y)$ over all valid pairs $(\\mu_X, \\mu_Y)$ satisfying:\n4081: - $\\mu_X \\ge \\mu_Y + \\kappa$\n4082: - $\\mu_X, \\mu_Y \\in [V_{\\min}, V_{\\max}]$\n4083: \n4084: First, observe that for any fixed $\\mu_Y$, the expected value $\\mathbb{E}[\\ln(X_{\\min})]$ is an increasing function of $\\mu_X$. Therefore, to minimize the difference, we should choose $\\mu_X$ as small as possible, which places us on the boundary: $\\mu_X = \\mu_Y + \\kappa$.\n4085: \n4086: The problem reduces to minimizing the one-dimensional function:\n4087: \n4088: $$\n4089: h(\\mu_Y) := \\mathbb{E}[\\ln(X_{\\min,\\mu_Y+\\kappa})] - \\ln(\\mu_Y)\n4090: $$\n4091: \n4092: for $\\mu_Y \\in [V_{\\min}, V_{\\max} - \\kappa]$.\n4093: \n4094: Now we prove that this function is **convex**. The expected log of the two-point extremal distribution is a linear function of its mean:\n4095: \n4096: $$\n4097: \\mathbb{E}[\\ln(X_{\\min,\\mu})] = \\ln(V_{\\max}) + \\frac{V_{\\max} - \\mu}{V_{\\max} - V_{\\min}}(\\ln(V_{\\min}) - \\ln(V_{\\max}))\n4098: $$\n4099: \n4100: This can be written as $C_0 + C_1 \\mu$ where $C_1 = (\\ln(V_{\\max}) - \\ln(V_{\\min}))/(V_{\\max} - V_{\\min}) > 0$. Substituting $\\mu = \\mu_Y + \\kappa$:\n4101: \n4102: $$\n4103: h(\\mu_Y) = [C_0 + C_1(\\mu_Y + \\kappa)] - \\ln(\\mu_Y)\n4104: $$\n4105: \n4106: The function $h(\\mu_Y)$ is the sum of a linear function (in $\\mu_Y$) and the function $-\\ln(\\mu_Y)$, which is strictly convex. Therefore, $h(\\mu_Y)$ is strictly convex.\n4107: \n4108: **A convex function on a closed interval attains its minimum at one of the endpoints.** We must check the values at $\\mu_Y = V_{\\min}$ and $\\mu_Y = V_{\\max} - \\kappa$.\n4109: \n4110: **Key insight:** The logarithm becomes flatter as its argument increases (decreasing derivative). For a fixed gap $\\kappa$ between means, the logarithmic gap is smaller when the means are at higher values. This suggests the minimum occurs at the right endpoint: $\\mu_Y = V_{\\max} - \\kappa$.\n4111: \n4112: The worst-case configuration is therefore:\n4113: - $\\mu_Y = V_{\\max} - \\kappa$ (right endpoint)\n4114: - $\\mu_X = V_{\\max}$ (forced by the boundary constraint)\n4115: \n4116: **Step 4: Computing the Lower Bound for the Worst Case.**\n4117: \n4118: At this worst-case configuration:\n4119: - For $X$ with mean $\\mu_X = V_{\\max}$, the two-point extremal distribution degenerates to a deterministic distribution: $X = V_{\\max}$ with probability 1. Thus:\n4120: \n4121: $$\n4122: \\mathbb{E}[\\ln(X)] = \\ln(V_{\\max})\n4123: $$\n4124: \n4125: - For $Y$, the extremal case (maximum expected log) is deterministic: $Y = \\mu_Y = V_{\\max} - \\kappa$. Thus:\n4126: \n4127: $$\n4128: \\mathbb{E}[\\ln(Y)] = \\ln(V_{\\max} - \\kappa)\n4129: $$\n4130: \n4131: The worst-case lower bound is:\n4132: \n4133: $$\n4134: \\ln(V_{\\max}) - \\ln(V_{\\max} - \\kappa) = \\ln\\left(\\frac{V_{\\max}}{V_{\\max} - \\kappa}\\right) = \\ln\\left(1 + \\frac{\\kappa}{V_{\\max} - \\kappa}\\right)\n4135: $$\n4136: \n4137: **Step 5: Simplification to the Stated Bound.**\n4138: \n4139: The tight bound from Step 4 is $\\ln(1 + \\kappa/(V_{\\max} - \\kappa))$. The lemma states the slightly looser but simpler bound $\\ln(1 + \\kappa/V_{\\max})$.\n4140: \n4141: To verify this is valid, note that for $\\kappa < V_{\\max}$:\n4142: \n4143: $$\n4144: \\frac{\\kappa}{V_{\\max}} < \\frac{\\kappa}{V_{\\max} - \\kappa}\n4145: $$\n4146: \n4147: Since $\\ln(1+t)$ is strictly increasing in $t$:\n4148: \n4149: $$\n4150: \\ln\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right) < \\ln\\left(1 + \\frac{\\kappa}{V_{\\max} - \\kappa}\\right)\n4151: $$\n4152: \n4153: Therefore, $\\ln(1 + \\kappa/V_{\\max})$ is a valid (conservative) lower bound that is simpler to use in subsequent analysis.",
      "metadata": {
        "label": "proof-lem-log-gap-lower-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4045: :::\n4046: \n4047: :::{prf:proof}\n4048: :label: proof-lem-log-gap-lower-bound\n4049: \n4050: **Proof.**\n4051: \n4052: The proof uses the theory of extremal distributions for concave functions. We establish tight bounds on each term by identifying the distributions that minimize $\\mathbb{E}[\\ln(X)]$ and maximize $\\mathbb{E}[\\ln(Y)]$, then find the minimum of their difference over all valid mean pairs.\n4053: \n4054: **Step 1: Extremal Distributions for the Logarithm.**\n4055: \n4056: Since $f(t) = \\ln(t)$ is strictly concave for $t > 0$, the extremal distributions are well-known:\n4057: - For a **fixed mean** $\\mu$, the minimum of $\\mathbb{E}[\\ln(X)]$ is achieved by a **two-point distribution** with mass only at the endpoints $\\{V_{\\min}, V_{\\max}\\}$.\n4058: - For a **fixed mean** $\\mu$, the maximum of $\\mathbb{E}[\\ln(Y)]$ is achieved by a **deterministic distribution**: $Y = \\mu$ with probability 1. By Jensen's inequality, $\\mathbb{E}[\\ln(Y)] \\le \\ln(\\mu_Y)$, with equality when $Y$ is deterministic.\n4059: \n4060: **Step 2: Bounding the Difference Using Extremal Cases.**\n4061: \n4062: The difference $\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]$ is minimized in the worst-case scenario where:\n4063: - $\\mathbb{E}[\\ln(X)]$ is as small as possible for mean $\\mu_X$ \u2192 Use the extremal two-point distribution $X_{\\min}$\n4064: - $\\mathbb{E}[\\ln(Y)]$ is as large as possible for mean $\\mu_Y$ \u2192 Use the deterministic distribution $Y = \\mu_Y$\n4065: \n4066: Therefore, for any distributions $X$, $Y$ with means $\\mu_X$, $\\mu_Y$:\n4067: \n4068: $$\n4069: \\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\ge \\mathbb{E}[\\ln(X_{\\min})] - \\ln(\\mu_Y)\n4070: $$\n4071: \n4072: where $X_{\\min}$ is the two-point distribution with mean $\\mu_X$:\n4073: \n4074: $$\n4075: P(X_{\\min} = V_{\\min}) = \\frac{V_{\\max} - \\mu_X}{V_{\\max} - V_{\\min}}, \\quad P(X_{\\min} = V_{\\max}) = \\frac{\\mu_X - V_{\\min}}{V_{\\max} - V_{\\min}}\n4076: $$\n4077: \n4078: **Step 3: Reduction to a One-Dimensional Optimization Problem.**\n4079: \n4080: We now minimize $\\mathbb{E}[\\ln(X_{\\min})] - \\ln(\\mu_Y)$ over all valid pairs $(\\mu_X, \\mu_Y)$ satisfying:\n4081: - $\\mu_X \\ge \\mu_Y + \\kappa$\n4082: - $\\mu_X, \\mu_Y \\in [V_{\\min}, V_{\\max}]$\n4083: \n4084: First, observe that for any fixed $\\mu_Y$, the expected value $\\mathbb{E}[\\ln(X_{\\min})]$ is an increasing function of $\\mu_X$. Therefore, to minimize the difference, we should choose $\\mu_X$ as small as possible, which places us on the boundary: $\\mu_X = \\mu_Y + \\kappa$.\n4085: \n4086: The problem reduces to minimizing the one-dimensional function:\n4087: \n4088: $$\n4089: h(\\mu_Y) := \\mathbb{E}[\\ln(X_{\\min,\\mu_Y+\\kappa})] - \\ln(\\mu_Y)\n4090: $$\n4091: \n4092: for $\\mu_Y \\in [V_{\\min}, V_{\\max} - \\kappa]$.\n4093: \n4094: Now we prove that this function is **convex**. The expected log of the two-point extremal distribution is a linear function of its mean:\n4095: \n4096: $$\n4097: \\mathbb{E}[\\ln(X_{\\min,\\mu})] = \\ln(V_{\\max}) + \\frac{V_{\\max} - \\mu}{V_{\\max} - V_{\\min}}(\\ln(V_{\\min}) - \\ln(V_{\\max}))\n4098: $$\n4099: \n4100: This can be written as $C_0 + C_1 \\mu$ where $C_1 = (\\ln(V_{\\max}) - \\ln(V_{\\min}))/(V_{\\max} - V_{\\min}) > 0$. Substituting $\\mu = \\mu_Y + \\kappa$:\n4101: \n4102: $$\n4103: h(\\mu_Y) = [C_0 + C_1(\\mu_Y + \\kappa)] - \\ln(\\mu_Y)\n4104: $$\n4105: \n4106: The function $h(\\mu_Y)$ is the sum of a linear function (in $\\mu_Y$) and the function $-\\ln(\\mu_Y)$, which is strictly convex. Therefore, $h(\\mu_Y)$ is strictly convex.\n4107: \n4108: **A convex function on a closed interval attains its minimum at one of the endpoints.** We must check the values at $\\mu_Y = V_{\\min}$ and $\\mu_Y = V_{\\max} - \\kappa$.\n4109: \n4110: **Key insight:** The logarithm becomes flatter as its argument increases (decreasing derivative). For a fixed gap $\\kappa$ between means, the logarithmic gap is smaller when the means are at higher values. This suggests the minimum occurs at the right endpoint: $\\mu_Y = V_{\\max} - \\kappa$.\n4111: \n4112: The worst-case configuration is therefore:\n4113: - $\\mu_Y = V_{\\max} - \\kappa$ (right endpoint)\n4114: - $\\mu_X = V_{\\max}$ (forced by the boundary constraint)\n4115: \n4116: **Step 4: Computing the Lower Bound for the Worst Case.**\n4117: \n4118: At this worst-case configuration:\n4119: - For $X$ with mean $\\mu_X = V_{\\max}$, the two-point extremal distribution degenerates to a deterministic distribution: $X = V_{\\max}$ with probability 1. Thus:\n4120: \n4121: $$\n4122: \\mathbb{E}[\\ln(X)] = \\ln(V_{\\max})\n4123: $$\n4124: \n4125: - For $Y$, the extremal case (maximum expected log) is deterministic: $Y = \\mu_Y = V_{\\max} - \\kappa$. Thus:\n4126: \n4127: $$\n4128: \\mathbb{E}[\\ln(Y)] = \\ln(V_{\\max} - \\kappa)\n4129: $$\n4130: \n4131: The worst-case lower bound is:\n4132: \n4133: $$\n4134: \\ln(V_{\\max}) - \\ln(V_{\\max} - \\kappa) = \\ln\\left(\\frac{V_{\\max}}{V_{\\max} - \\kappa}\\right) = \\ln\\left(1 + \\frac{\\kappa}{V_{\\max} - \\kappa}\\right)\n4135: $$\n4136: \n4137: **Step 5: Simplification to the Stated Bound.**\n4138: \n4139: The tight bound from Step 4 is $\\ln(1 + \\kappa/(V_{\\max} - \\kappa))$. The lemma states the slightly looser but simpler bound $\\ln(1 + \\kappa/V_{\\max})$.\n4140: \n4141: To verify this is valid, note that for $\\kappa < V_{\\max}$:\n4142: \n4143: $$\n4144: \\frac{\\kappa}{V_{\\max}} < \\frac{\\kappa}{V_{\\max} - \\kappa}\n4145: $$\n4146: \n4147: Since $\\ln(1+t)$ is strictly increasing in $t$:\n4148: \n4149: $$\n4150: \\ln\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right) < \\ln\\left(1 + \\frac{\\kappa}{V_{\\max} - \\kappa}\\right)\n4151: $$\n4152: \n4153: Therefore, $\\ln(1 + \\kappa/V_{\\max})$ is a valid (conservative) lower bound that is simpler to use in subsequent analysis.\n4154: "
    },
    {
      "directive_type": "remark",
      "label": "rem-log-gap-bound-tightness",
      "title": "On the Tightness of the Bound",
      "start_line": 4156,
      "end_line": 4167,
      "header_lines": [
        4157,
        4158
      ],
      "content_start": 4160,
      "content_end": 4166,
      "content": "4160: :class: note\n4161: \n4162: The stated bound $\\ln(1 + \\kappa/V_{\\max})$ is slightly conservative compared to the tight bound $\\ln(1 + \\kappa/(V_{\\max} - \\kappa))$ derived in Step 4. For most practical applications where $\\kappa \\ll V_{\\max}$, the difference is negligible:\n4163: \n4164: $$\n4165: \\frac{\\kappa}{V_{\\max} - \\kappa} \\approx \\frac{\\kappa}{V_{\\max}}\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right)\n4166: $$",
      "metadata": {
        "label": "rem-log-gap-bound-tightness",
        "class": "note"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4156: :::\n4157: \n4158: :::{prf:remark} On the Tightness of the Bound\n4159: :label: rem-log-gap-bound-tightness\n4160: :class: note\n4161: \n4162: The stated bound $\\ln(1 + \\kappa/V_{\\max})$ is slightly conservative compared to the tight bound $\\ln(1 + \\kappa/(V_{\\max} - \\kappa))$ derived in Step 4. For most practical applications where $\\kappa \\ll V_{\\max}$, the difference is negligible:\n4163: \n4164: $$\n4165: \\frac{\\kappa}{V_{\\max} - \\kappa} \\approx \\frac{\\kappa}{V_{\\max}}\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right)\n4166: $$\n4167: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-log-gap-upper-bound",
      "title": "Upper Bound on Logarithmic Mean Gap",
      "start_line": 4173,
      "end_line": 4184,
      "header_lines": [
        4174
      ],
      "content_start": 4176,
      "content_end": 4183,
      "content": "4176: :label: lem-log-gap-upper-bound\n4177: \n4178: Let $X$ and $Y$ be two random variables whose values are contained in the compact interval $[V_{\\min}, V_{\\max}]$, where $V_{\\min} > 0$. Let their means, $\\mu_X = \\mathbb{E}[X]$ and $\\mu_Y = \\mathbb{E}[Y]$, satisfy $|\\mu_X - \\mu_Y| \\le \\kappa$ for some constant $\\kappa > 0$.\n4179: \n4180: Then the absolute difference of their expected logarithms is bounded above as follows:\n4181: \n4182: $$\n4183: |\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]| \\le \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)",
      "metadata": {
        "label": "lem-log-gap-upper-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4173: This lemma provides a symmetric result, capping the maximum possible logarithmic gap. This is essential for bounding the influence of the potentially adversarial reward signal, ensuring it cannot overwhelm the corrective diversity signal.\n4174: \n4175: :::{prf:lemma} Upper Bound on Logarithmic Mean Gap\n4176: :label: lem-log-gap-upper-bound\n4177: \n4178: Let $X$ and $Y$ be two random variables whose values are contained in the compact interval $[V_{\\min}, V_{\\max}]$, where $V_{\\min} > 0$. Let their means, $\\mu_X = \\mathbb{E}[X]$ and $\\mu_Y = \\mathbb{E}[Y]$, satisfy $|\\mu_X - \\mu_Y| \\le \\kappa$ for some constant $\\kappa > 0$.\n4179: \n4180: Then the absolute difference of their expected logarithms is bounded above as follows:\n4181: \n4182: $$\n4183: |\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]| \\le \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)\n4184: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-log-gap-upper-bound",
      "title": null,
      "start_line": 4186,
      "end_line": 4274,
      "header_lines": [
        4187
      ],
      "content_start": 4189,
      "content_end": 4273,
      "content": "4189: :label: proof-lem-log-gap-upper-bound\n4190: \n4191: **Proof.**\n4192: \n4193: The proof uses extremal distribution theory to find the configuration that maximizes the logarithmic gap. By symmetry, it suffices to bound the one-sided difference $\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]$; the bound on the absolute value follows immediately.\n4194: \n4195: **Step 1: Extremal Distributions for the Logarithm.**\n4196: \n4197: For the concave function $f(t) = \\ln(t)$:\n4198: - To **maximize** $\\mathbb{E}[\\ln(X)]$ for a fixed mean $\\mu_X$: use a **deterministic distribution** $X = \\mu_X$. By Jensen's inequality, $\\mathbb{E}[\\ln(X)] \\le \\ln(\\mu_X)$, with equality achieved when $X$ is deterministic.\n4199: - To **minimize** $\\mathbb{E}[\\ln(Y)]$ for a fixed mean $\\mu_Y$: use a **two-point distribution** $Y_{\\min}$ with mass only at the endpoints $\\{V_{\\min}, V_{\\max}\\}$. This is the extremal distribution for concave functions.\n4200: \n4201: **Step 2: Bounding the Difference Using Extremal Cases.**\n4202: \n4203: The difference $\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]$ is maximized when:\n4204: - $\\mathbb{E}[\\ln(X)]$ is as large as possible for mean $\\mu_X$ \u2192 Use deterministic $X = \\mu_X$\n4205: - $\\mathbb{E}[\\ln(Y)]$ is as small as possible for mean $\\mu_Y$ \u2192 Use extremal two-point distribution $Y_{\\min}$\n4206: \n4207: Therefore, for any distributions $X$, $Y$ with means $\\mu_X$, $\\mu_Y$:\n4208: \n4209: $$\n4210: \\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\le \\ln(\\mu_X) - \\mathbb{E}[\\ln(Y_{\\min})]\n4211: $$\n4212: \n4213: where $Y_{\\min}$ has probability masses:\n4214: \n4215: $$\n4216: P(Y_{\\min} = V_{\\min}) = \\frac{V_{\\max} - \\mu_Y}{V_{\\max} - V_{\\min}}, \\quad P(Y_{\\min} = V_{\\max}) = \\frac{\\mu_Y - V_{\\min}}{V_{\\max} - V_{\\min}}\n4217: $$\n4218: \n4219: The expected logarithm of $Y_{\\min}$ is:\n4220: \n4221: $$\n4222: \\mathbb{E}[\\ln(Y_{\\min})] = \\frac{V_{\\max} - \\mu_Y}{V_{\\max} - V_{\\min}} \\ln(V_{\\min}) + \\frac{\\mu_Y - V_{\\min}}{V_{\\max} - V_{\\min}} \\ln(V_{\\max})\n4223: $$\n4224: \n4225: **Step 3: Finding the Worst-Case Mean Configuration.**\n4226: \n4227: We now maximize $\\ln(\\mu_X) - \\mathbb{E}[\\ln(Y_{\\min})]$ over all valid pairs $(\\mu_X, \\mu_Y)$ satisfying:\n4228: - $|\\mu_X - \\mu_Y| \\le \\kappa$\n4229: - $\\mu_X, \\mu_Y \\in [V_{\\min}, V_{\\max}]$\n4230: \n4231: **Key insight:** The logarithm function has the steepest slope (greatest curvature) near $V_{\\min}$. Therefore, the gap $\\ln(\\mu_X) - \\mathbb{E}[\\ln(Y_{\\min})]$ is maximized when both means are located at the bottom of the allowable range.\n4232: \n4233: Without loss of generality, assume $\\mu_X \\ge \\mu_Y$ (by symmetry). The constraint $|\\mu_X - \\mu_Y| \\le \\kappa$ allows $\\mu_X = \\mu_Y + \\kappa$.\n4234: \n4235: The worst-case configuration is:\n4236: - $\\mu_Y = V_{\\min}$ (minimum possible value)\n4237: - $\\mu_X = V_{\\min} + \\kappa$ (maximum separation at the bottom of the range)\n4238: \n4239: **Step 4: Computing the Upper Bound for the Worst Case.**\n4240: \n4241: With $\\mu_Y = V_{\\min}$, the extremal two-point distribution $Y_{\\min}$ degenerates to a **deterministic distribution** with all mass at $V_{\\min}$:\n4242: \n4243: $$\n4244: P(Y_{\\min} = V_{\\min}) = 1\n4245: $$\n4246: \n4247: Therefore:\n4248: \n4249: $$\n4250: \\mathbb{E}[\\ln(Y_{\\min})] = \\ln(V_{\\min})\n4251: $$\n4252: \n4253: For $X$ deterministic at $\\mu_X = V_{\\min} + \\kappa$:\n4254: \n4255: $$\n4256: \\ln(\\mu_X) = \\ln(V_{\\min} + \\kappa)\n4257: $$\n4258: \n4259: The worst-case upper bound is:\n4260: \n4261: $$\n4262: \\ln(V_{\\min} + \\kappa) - \\ln(V_{\\min}) = \\ln\\left(\\frac{V_{\\min} + \\kappa}{V_{\\min}}\\right) = \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)\n4263: $$\n4264: \n4265: **Step 5: Extension to the Absolute Value.**\n4266: \n4267: By symmetry (swapping the roles of $X$ and $Y$), the bound also holds for $\\mathbb{E}[\\ln(Y)] - \\mathbb{E}[\\ln(X)]$. Therefore:\n4268: \n4269: $$\n4270: |\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]| \\le \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)\n4271: $$\n4272: \n4273: This completes the proof.",
      "metadata": {
        "label": "proof-lem-log-gap-upper-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4186: :::\n4187: \n4188: :::{prf:proof}\n4189: :label: proof-lem-log-gap-upper-bound\n4190: \n4191: **Proof.**\n4192: \n4193: The proof uses extremal distribution theory to find the configuration that maximizes the logarithmic gap. By symmetry, it suffices to bound the one-sided difference $\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]$; the bound on the absolute value follows immediately.\n4194: \n4195: **Step 1: Extremal Distributions for the Logarithm.**\n4196: \n4197: For the concave function $f(t) = \\ln(t)$:\n4198: - To **maximize** $\\mathbb{E}[\\ln(X)]$ for a fixed mean $\\mu_X$: use a **deterministic distribution** $X = \\mu_X$. By Jensen's inequality, $\\mathbb{E}[\\ln(X)] \\le \\ln(\\mu_X)$, with equality achieved when $X$ is deterministic.\n4199: - To **minimize** $\\mathbb{E}[\\ln(Y)]$ for a fixed mean $\\mu_Y$: use a **two-point distribution** $Y_{\\min}$ with mass only at the endpoints $\\{V_{\\min}, V_{\\max}\\}$. This is the extremal distribution for concave functions.\n4200: \n4201: **Step 2: Bounding the Difference Using Extremal Cases.**\n4202: \n4203: The difference $\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]$ is maximized when:\n4204: - $\\mathbb{E}[\\ln(X)]$ is as large as possible for mean $\\mu_X$ \u2192 Use deterministic $X = \\mu_X$\n4205: - $\\mathbb{E}[\\ln(Y)]$ is as small as possible for mean $\\mu_Y$ \u2192 Use extremal two-point distribution $Y_{\\min}$\n4206: \n4207: Therefore, for any distributions $X$, $Y$ with means $\\mu_X$, $\\mu_Y$:\n4208: \n4209: $$\n4210: \\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\le \\ln(\\mu_X) - \\mathbb{E}[\\ln(Y_{\\min})]\n4211: $$\n4212: \n4213: where $Y_{\\min}$ has probability masses:\n4214: \n4215: $$\n4216: P(Y_{\\min} = V_{\\min}) = \\frac{V_{\\max} - \\mu_Y}{V_{\\max} - V_{\\min}}, \\quad P(Y_{\\min} = V_{\\max}) = \\frac{\\mu_Y - V_{\\min}}{V_{\\max} - V_{\\min}}\n4217: $$\n4218: \n4219: The expected logarithm of $Y_{\\min}$ is:\n4220: \n4221: $$\n4222: \\mathbb{E}[\\ln(Y_{\\min})] = \\frac{V_{\\max} - \\mu_Y}{V_{\\max} - V_{\\min}} \\ln(V_{\\min}) + \\frac{\\mu_Y - V_{\\min}}{V_{\\max} - V_{\\min}} \\ln(V_{\\max})\n4223: $$\n4224: \n4225: **Step 3: Finding the Worst-Case Mean Configuration.**\n4226: \n4227: We now maximize $\\ln(\\mu_X) - \\mathbb{E}[\\ln(Y_{\\min})]$ over all valid pairs $(\\mu_X, \\mu_Y)$ satisfying:\n4228: - $|\\mu_X - \\mu_Y| \\le \\kappa$\n4229: - $\\mu_X, \\mu_Y \\in [V_{\\min}, V_{\\max}]$\n4230: \n4231: **Key insight:** The logarithm function has the steepest slope (greatest curvature) near $V_{\\min}$. Therefore, the gap $\\ln(\\mu_X) - \\mathbb{E}[\\ln(Y_{\\min})]$ is maximized when both means are located at the bottom of the allowable range.\n4232: \n4233: Without loss of generality, assume $\\mu_X \\ge \\mu_Y$ (by symmetry). The constraint $|\\mu_X - \\mu_Y| \\le \\kappa$ allows $\\mu_X = \\mu_Y + \\kappa$.\n4234: \n4235: The worst-case configuration is:\n4236: - $\\mu_Y = V_{\\min}$ (minimum possible value)\n4237: - $\\mu_X = V_{\\min} + \\kappa$ (maximum separation at the bottom of the range)\n4238: \n4239: **Step 4: Computing the Upper Bound for the Worst Case.**\n4240: \n4241: With $\\mu_Y = V_{\\min}$, the extremal two-point distribution $Y_{\\min}$ degenerates to a **deterministic distribution** with all mass at $V_{\\min}$:\n4242: \n4243: $$\n4244: P(Y_{\\min} = V_{\\min}) = 1\n4245: $$\n4246: \n4247: Therefore:\n4248: \n4249: $$\n4250: \\mathbb{E}[\\ln(Y_{\\min})] = \\ln(V_{\\min})\n4251: $$\n4252: \n4253: For $X$ deterministic at $\\mu_X = V_{\\min} + \\kappa$:\n4254: \n4255: $$\n4256: \\ln(\\mu_X) = \\ln(V_{\\min} + \\kappa)\n4257: $$\n4258: \n4259: The worst-case upper bound is:\n4260: \n4261: $$\n4262: \\ln(V_{\\min} + \\kappa) - \\ln(V_{\\min}) = \\ln\\left(\\frac{V_{\\min} + \\kappa}{V_{\\min}}\\right) = \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)\n4263: $$\n4264: \n4265: **Step 5: Extension to the Absolute Value.**\n4266: \n4267: By symmetry (swapping the roles of $X$ and $Y$), the bound also holds for $\\mathbb{E}[\\ln(Y)] - \\mathbb{E}[\\ln(X)]$. Therefore:\n4268: \n4269: $$\n4270: |\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]| \\le \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)\n4271: $$\n4272: \n4273: This completes the proof.\n4274: "
    },
    {
      "directive_type": "remark",
      "label": "rem-log-gap-bound-tight-at-vmin",
      "title": "Why the Bound is Tight at $V_{\\min}$",
      "start_line": 4276,
      "end_line": 4286,
      "header_lines": [
        4277,
        4278
      ],
      "content_start": 4280,
      "content_end": 4285,
      "content": "4280: :class: note\n4281: \n4282: The upper bound $\\ln(1 + \\kappa/V_{\\min})$ is achieved in the worst-case scenario where:\n4283: 1. The means are separated by the maximum allowed gap $\\kappa$\n4284: 2. Both distributions are positioned at the bottom of the range near $V_{\\min}$, where the logarithm has the steepest slope\n4285: 3. One distribution is deterministic (maximizing its expected log) while the other is maximally dispersed (minimizing its expected log)",
      "metadata": {
        "label": "rem-log-gap-bound-tight-at-vmin",
        "class": "note"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4276: :::\n4277: \n4278: :::{prf:remark} Why the Bound is Tight at $V_{\\min}$\n4279: :label: rem-log-gap-bound-tight-at-vmin\n4280: :class: note\n4281: \n4282: The upper bound $\\ln(1 + \\kappa/V_{\\min})$ is achieved in the worst-case scenario where:\n4283: 1. The means are separated by the maximum allowed gap $\\kappa$\n4284: 2. Both distributions are positioned at the bottom of the range near $V_{\\min}$, where the logarithm has the steepest slope\n4285: 3. One distribution is deterministic (maximizing its expected log) while the other is maximally dispersed (minimizing its expected log)\n4286: "
    },
    {
      "directive_type": "proposition",
      "label": "prop-corrective-signal-bound",
      "title": "**(Lower Bound on the Corrective Diversity Signal)**",
      "start_line": 4296,
      "end_line": 4308,
      "header_lines": [
        4297
      ],
      "content_start": 4299,
      "content_end": 4307,
      "content": "4299: :label: prop-corrective-signal-bound\n4300: \n4301: Let a swarm state be in the high-error regime, such that the variance of its rescaled diversity values, `d'`, is bounded below, $\\operatorname{Var}(d') \\ge \\kappa_{d', \\text{var}} > 0$. Let the system parameters be chosen such that the Signal-to-Noise Condition of [](#lem-variance-to-mean-separation) is satisfied, i.e., $\\kappa_{d', \\text{var}} > \\operatorname{Var}_{\\max}(d')$.\n4302: \n4303: Then the expected logarithmic gap in the diversity signal between the high-error population $H_k$ and the low-error population $L_k$ is bounded below by a strictly positive, N-uniform constant:\n4304: \n4305: $$\n4306: \\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right) > 0\n4307: $$",
      "metadata": {
        "label": "prop-corrective-signal-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4296: This proposition forges the complete link from a macroscopic state of high geometric error to a guaranteed, non-vanishing corrective signal in the logarithmic space of the fitness potential.\n4297: \n4298: :::{prf:proposition} **(Lower Bound on the Corrective Diversity Signal)**\n4299: :label: prop-corrective-signal-bound\n4300: \n4301: Let a swarm state be in the high-error regime, such that the variance of its rescaled diversity values, `d'`, is bounded below, $\\operatorname{Var}(d') \\ge \\kappa_{d', \\text{var}} > 0$. Let the system parameters be chosen such that the Signal-to-Noise Condition of [](#lem-variance-to-mean-separation) is satisfied, i.e., $\\kappa_{d', \\text{var}} > \\operatorname{Var}_{\\max}(d')$.\n4302: \n4303: Then the expected logarithmic gap in the diversity signal between the high-error population $H_k$ and the low-error population $L_k$ is bounded below by a strictly positive, N-uniform constant:\n4304: \n4305: $$\n4306: \\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right) > 0\n4307: $$\n4308: "
    },
    {
      "directive_type": "proof",
      "label": "proof-prop-corrective-signal-bound",
      "title": null,
      "start_line": 4309,
      "end_line": 4336,
      "header_lines": [
        4310
      ],
      "content_start": 4312,
      "content_end": 4335,
      "content": "4312: :label: proof-prop-corrective-signal-bound\n4313: \n4314: **Proof.**\n4315: \n4316: The proof proceeds in two steps. First, we translate the guaranteed variance into a guaranteed separation between the means of the high-error and low-error populations. Second, we translate this mean separation into a guaranteed separation in the expected logarithms.\n4317: \n4318: **1. From Variance to Mean Separation:**\n4319: The premises state that $\\operatorname{Var}(d') \\ge \\kappa_{d', \\text{var}}$ and that the Signal-to-Noise Condition is satisfied. The population fractions $f_H$ and $f_L$ are N-uniform and bounded below by a constant $f_{\\min} > 0$. We apply [](#lem-variance-to-mean-separation) directly. This yields a guaranteed separation between the means of the rescaled diversity values:\n4320: \n4321: $$\n4322: |\\mu_{d'}(H_k) - \\mu_{d'}(L_k)| \\ge \\kappa_{d', \\text{mean}} > 0\n4323: $$\n4324: \n4325: The direction of this inequality is also guaranteed. The geometric analysis in Chapter 6 (Lemma 6.5.1) established that high-error walkers are systematically more isolated, which implies their expected raw distance-to-companion is larger: $\\mathbb{E}[d|H_k] > \\mathbb{E}[d|L_k]$. Since the standardization and rescaling operators (specifically the monotonic rescale function $g_A$) preserve the ordering of the means, this inequality propagates through the entire pipeline. This guarantees that the mean of the *rescaled* diversity values is also larger for the high-error set, $\\mu_{d'}(H_k) > \\mu_{d'}(L_k)$. We can therefore remove the absolute value and state the inequality directionally.\n4326: \n4327: **2. From Mean Separation to Logarithmic Mean Separation:**\n4328: We now have a guaranteed mean separation, $\\mu_{d'}(H_k) \\ge \\mu_{d'}(L_k) + \\kappa_{d', \\text{mean}}$. The rescaled values $d'$ are contained in the compact interval $[\\eta, g_{A,\\max}+\\eta]$. We apply [](#lem-log-gap-lower-bound) with $X$ representing the distribution of $d'$ in $H_k$, $Y$ representing the distribution in $L_k$, $\\kappa = \\kappa_{d', \\text{mean}}$, and $V_{\\max} = g_{A,\\max}+\\eta$.\n4329: The lemma gives the stated result directly:\n4330: \n4331: $$\n4332: \\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right)\n4333: $$\n4334: \n4335: Since $\\kappa_{d', \\text{mean}} > 0$, the argument of the logarithm is strictly greater than 1, ensuring the lower bound is strictly positive.",
      "metadata": {
        "label": "proof-prop-corrective-signal-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4309: where $\\kappa_{d', \\text{mean}} := \\frac{1}{\\sqrt{f_H f_L}}\\sqrt{\\kappa_{d', \\text{var}} - \\operatorname{Var}_{\\max}(d')}$.\n4310: :::\n4311: :::{prf:proof}\n4312: :label: proof-prop-corrective-signal-bound\n4313: \n4314: **Proof.**\n4315: \n4316: The proof proceeds in two steps. First, we translate the guaranteed variance into a guaranteed separation between the means of the high-error and low-error populations. Second, we translate this mean separation into a guaranteed separation in the expected logarithms.\n4317: \n4318: **1. From Variance to Mean Separation:**\n4319: The premises state that $\\operatorname{Var}(d') \\ge \\kappa_{d', \\text{var}}$ and that the Signal-to-Noise Condition is satisfied. The population fractions $f_H$ and $f_L$ are N-uniform and bounded below by a constant $f_{\\min} > 0$. We apply [](#lem-variance-to-mean-separation) directly. This yields a guaranteed separation between the means of the rescaled diversity values:\n4320: \n4321: $$\n4322: |\\mu_{d'}(H_k) - \\mu_{d'}(L_k)| \\ge \\kappa_{d', \\text{mean}} > 0\n4323: $$\n4324: \n4325: The direction of this inequality is also guaranteed. The geometric analysis in Chapter 6 (Lemma 6.5.1) established that high-error walkers are systematically more isolated, which implies their expected raw distance-to-companion is larger: $\\mathbb{E}[d|H_k] > \\mathbb{E}[d|L_k]$. Since the standardization and rescaling operators (specifically the monotonic rescale function $g_A$) preserve the ordering of the means, this inequality propagates through the entire pipeline. This guarantees that the mean of the *rescaled* diversity values is also larger for the high-error set, $\\mu_{d'}(H_k) > \\mu_{d'}(L_k)$. We can therefore remove the absolute value and state the inequality directionally.\n4326: \n4327: **2. From Mean Separation to Logarithmic Mean Separation:**\n4328: We now have a guaranteed mean separation, $\\mu_{d'}(H_k) \\ge \\mu_{d'}(L_k) + \\kappa_{d', \\text{mean}}$. The rescaled values $d'$ are contained in the compact interval $[\\eta, g_{A,\\max}+\\eta]$. We apply [](#lem-log-gap-lower-bound) with $X$ representing the distribution of $d'$ in $H_k$, $Y$ representing the distribution in $L_k$, $\\kappa = \\kappa_{d', \\text{mean}}$, and $V_{\\max} = g_{A,\\max}+\\eta$.\n4329: The lemma gives the stated result directly:\n4330: \n4331: $$\n4332: \\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right)\n4333: $$\n4334: \n4335: Since $\\kappa_{d', \\text{mean}} > 0$, the argument of the logarithm is strictly greater than 1, ensuring the lower bound is strictly positive.\n4336: "
    },
    {
      "directive_type": "proposition",
      "label": "prop-adversarial-signal-bound-naive",
      "title": "**(Worst-Case Upper Bound on the Adversarial Reward Signal)**",
      "start_line": 4342,
      "end_line": 4351,
      "header_lines": [
        4343
      ],
      "content_start": 4345,
      "content_end": 4350,
      "content": "4345: :label: prop-adversarial-signal-bound-naive\n4346: \n4347: For any swarm state, the maximum possible expected logarithmic gap in the rescaled reward signal, $r'$, between the low-error and high-error populations is uniformly bounded above by a constant derived only from the rescale function's range:\n4348: \n4349: $$\n4350: \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)",
      "metadata": {
        "label": "prop-adversarial-signal-bound-naive"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4342: Before deriving the final stability condition, it is instructive to first establish a \"naive\" upper bound on the adversarial reward signal. This bound considers the absolute worst-case scenario allowed by the range of the rescale function, without yet invoking the axioms that constrain the reward landscape's structure. This will serve as a baseline to demonstrate the critical importance of those axioms.\n4343: \n4344: :::{prf:proposition} **(Worst-Case Upper Bound on the Adversarial Reward Signal)**\n4345: :label: prop-adversarial-signal-bound-naive\n4346: \n4347: For any swarm state, the maximum possible expected logarithmic gap in the rescaled reward signal, $r'$, between the low-error and high-error populations is uniformly bounded above by a constant derived only from the rescale function's range:\n4348: \n4349: $$\n4350: \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)\n4351: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-prop-adversarial-signal-bound-naive",
      "title": null,
      "start_line": 4352,
      "end_line": 4379,
      "header_lines": [
        4353
      ],
      "content_start": 4355,
      "content_end": 4378,
      "content": "4355: :label: proof-prop-adversarial-signal-bound-naive\n4356: \n4357: **Proof.**\n4358: \n4359: The proof finds the maximum possible separation by considering the most extreme allowable configuration of mean rewards, unconstrained by any landscape regularity.\n4360: \n4361: **1. Bounding the Maximum Possible Mean Separation:**\n4362: The rescaled reward values $r'$ are contained in the interval $[\\eta, g_{A,\\max}+\\eta]$. The mean reward for any subpopulation, e.g., $\\mu_{r'}(L_k)$, must also lie within this interval. The absolute difference between the means of any two subpopulations is therefore bounded by the total width of this interval:\n4363: \n4364: $$\n4365: |\\mu_{r'}(L_k) - \\mu_{r'}(H_k)| \\le (g_{A,\\max}+\\eta) - \\eta = g_{A,\\max}\n4366: $$\n4367: \n4368: We define the maximum possible mean separation as $\\kappa_{r', \\text{mean, max}} := g_{A,\\max}$. This represents the most adversarial scenario, where the low-error set $L_k$ achieves the maximum possible mean reward ($g_{A,\\max} + \\eta$) and the high-error set $H_k$ achieves the minimum possible mean reward ($\\eta$), maximizing the gap between them.\n4369: \n4370: **2. From Mean Separation to Logarithmic Mean Separation:**\n4371: We now seek an upper bound for the expression $\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]$. We apply [](#lem-log-gap-upper-bound). Let $X$ represent the distribution of $r'$ in $L_k$ and $Y$ represent the distribution in $H_k$. We use the maximum possible mean separation $\\kappa = \\kappa_{r', \\text{mean, max}}$ and note that the minimum value for any $r'$ is $V_{\\min} = \\eta$.\n4372: The lemma gives the stated result directly:\n4373: \n4374: $$\n4375: |\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]| \\le \\ln\\left(1 + \\frac{\\kappa_{r', \\text{mean, max}}}{\\eta}\\right) = \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)\n4376: $$\n4377: \n4378: This provides a uniform upper bound on the magnitude of the adversarial signal under the weakest possible assumptions.",
      "metadata": {
        "label": "proof-prop-adversarial-signal-bound-naive"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4352: \n4353: :::\n4354: :::{prf:proof}\n4355: :label: proof-prop-adversarial-signal-bound-naive\n4356: \n4357: **Proof.**\n4358: \n4359: The proof finds the maximum possible separation by considering the most extreme allowable configuration of mean rewards, unconstrained by any landscape regularity.\n4360: \n4361: **1. Bounding the Maximum Possible Mean Separation:**\n4362: The rescaled reward values $r'$ are contained in the interval $[\\eta, g_{A,\\max}+\\eta]$. The mean reward for any subpopulation, e.g., $\\mu_{r'}(L_k)$, must also lie within this interval. The absolute difference between the means of any two subpopulations is therefore bounded by the total width of this interval:\n4363: \n4364: $$\n4365: |\\mu_{r'}(L_k) - \\mu_{r'}(H_k)| \\le (g_{A,\\max}+\\eta) - \\eta = g_{A,\\max}\n4366: $$\n4367: \n4368: We define the maximum possible mean separation as $\\kappa_{r', \\text{mean, max}} := g_{A,\\max}$. This represents the most adversarial scenario, where the low-error set $L_k$ achieves the maximum possible mean reward ($g_{A,\\max} + \\eta$) and the high-error set $H_k$ achieves the minimum possible mean reward ($\\eta$), maximizing the gap between them.\n4369: \n4370: **2. From Mean Separation to Logarithmic Mean Separation:**\n4371: We now seek an upper bound for the expression $\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]$. We apply [](#lem-log-gap-upper-bound). Let $X$ represent the distribution of $r'$ in $L_k$ and $Y$ represent the distribution in $H_k$. We use the maximum possible mean separation $\\kappa = \\kappa_{r', \\text{mean, max}}$ and note that the minimum value for any $r'$ is $V_{\\min} = \\eta$.\n4372: The lemma gives the stated result directly:\n4373: \n4374: $$\n4375: |\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]| \\le \\ln\\left(1 + \\frac{\\kappa_{r', \\text{mean, max}}}{\\eta}\\right) = \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)\n4376: $$\n4377: \n4378: This provides a uniform upper bound on the magnitude of the adversarial signal under the weakest possible assumptions.\n4379: "
    },
    {
      "directive_type": "proposition",
      "label": "prop-raw-reward-mean-gap-bound",
      "title": "**(Lipschitz Bound on the Raw Reward Mean Gap)**",
      "start_line": 4405,
      "end_line": 4416,
      "header_lines": [
        4406
      ],
      "content_start": 4408,
      "content_end": 4415,
      "content": "4408: :label: prop-raw-reward-mean-gap-bound\n4409: \n4410: Let the reward function's positional component, $R_{\\text{pos}}(x)$, be Lipschitz continuous on the valid domain $\\mathcal{X}_{\\text{valid}}$ with constant $L_{R}$, as per the **Axiom of Reward Regularity**. Let the diameter of $\\mathcal{X}_{\\text{valid}}$ be $D_{\\text{valid}}$.\n4411: \n4412: For any swarm, the absolute difference between the mean raw rewards of the high-error population $H_k$ and the low-error population $L_k$ is uniformly bounded:\n4413: \n4414: $$\n4415: |\\mu_R(L_k) - \\mu_R(H_k)| \\le L_{R} \\cdot D_{\\mathrm{valid}} =: \\kappa_{\\mathrm{raw},r,\\text{adv}}",
      "metadata": {
        "label": "prop-raw-reward-mean-gap-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4405: The following propositions build a chain of reasoning from the Lipschitz continuity of the raw reward function to a final, tight bound on the expected logarithmic gap.\n4406: \n4407: :::{prf:proposition} **(Lipschitz Bound on the Raw Reward Mean Gap)**\n4408: :label: prop-raw-reward-mean-gap-bound\n4409: \n4410: Let the reward function's positional component, $R_{\\text{pos}}(x)$, be Lipschitz continuous on the valid domain $\\mathcal{X}_{\\text{valid}}$ with constant $L_{R}$, as per the **Axiom of Reward Regularity**. Let the diameter of $\\mathcal{X}_{\\text{valid}}$ be $D_{\\text{valid}}$.\n4411: \n4412: For any swarm, the absolute difference between the mean raw rewards of the high-error population $H_k$ and the low-error population $L_k$ is uniformly bounded:\n4413: \n4414: $$\n4415: |\\mu_R(L_k) - \\mu_R(H_k)| \\le L_{R} \\cdot D_{\\mathrm{valid}} =: \\kappa_{\\mathrm{raw},r,\\text{adv}}\n4416: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-prop-raw-reward-mean-gap-bound",
      "title": null,
      "start_line": 4417,
      "end_line": 4438,
      "header_lines": [
        4418
      ],
      "content_start": 4420,
      "content_end": 4437,
      "content": "4420: :label: proof-prop-raw-reward-mean-gap-bound\n4421: \n4422: **Proof.**\n4423: The mean reward difference is $|\\frac{1}{|L_k|}\\sum_{l \\in L_k} R(x_l) - \\frac{1}{|H_k|}\\sum_{h \\in H_k} R(x_h)|$. This can be rewritten as the average difference over all pairs: $\\frac{1}{|L_k||H_k|} |\\sum_{l,h} (R(x_l) - R(x_h))|$.\n4424: \n4425: By the triangle inequality and the Lipschitz property:\n4426: \n4427: $$\n4428: |\\sum_{l,h} (R(x_l) - R(x_h))| \\le \\sum_{l,h} |R(x_l) - R(x_h)| \\le \\sum_{l,h} L_{R} \\cdot d(x_l, x_h)\n4429: $$\n4430: \n4431: The distance between any two points $x_l, x_h$ in the valid domain is bounded by its diameter, $D_{\\mathrm{valid}}$. There are $|L_k||H_k|$ pairs in the sum.\n4432: \n4433: $$\n4434: \\le \\sum_{l,h} L_{R} \\cdot D_{\\mathrm{valid}} = |L_k||H_k| \\cdot L_{R} \\cdot D_{\\mathrm{valid}}\n4435: $$\n4436: \n4437: Dividing by $|L_k||H_k|$ gives the final bound. This maximum possible raw reward gap, $\\kappa_{\\mathrm{raw},r,\\text{adv}}$, represents the tightest axiom-based constraint on how deceptive the landscape can be.",
      "metadata": {
        "label": "proof-prop-raw-reward-mean-gap-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4417: \n4418: :::\n4419: :::{prf:proof}\n4420: :label: proof-prop-raw-reward-mean-gap-bound\n4421: \n4422: **Proof.**\n4423: The mean reward difference is $|\\frac{1}{|L_k|}\\sum_{l \\in L_k} R(x_l) - \\frac{1}{|H_k|}\\sum_{h \\in H_k} R(x_h)|$. This can be rewritten as the average difference over all pairs: $\\frac{1}{|L_k||H_k|} |\\sum_{l,h} (R(x_l) - R(x_h))|$.\n4424: \n4425: By the triangle inequality and the Lipschitz property:\n4426: \n4427: $$\n4428: |\\sum_{l,h} (R(x_l) - R(x_h))| \\le \\sum_{l,h} |R(x_l) - R(x_h)| \\le \\sum_{l,h} L_{R} \\cdot d(x_l, x_h)\n4429: $$\n4430: \n4431: The distance between any two points $x_l, x_h$ in the valid domain is bounded by its diameter, $D_{\\mathrm{valid}}$. There are $|L_k||H_k|$ pairs in the sum.\n4432: \n4433: $$\n4434: \\le \\sum_{l,h} L_{R} \\cdot D_{\\mathrm{valid}} = |L_k||H_k| \\cdot L_{R} \\cdot D_{\\mathrm{valid}}\n4435: $$\n4436: \n4437: Dividing by $|L_k||H_k|$ gives the final bound. This maximum possible raw reward gap, $\\kappa_{\\mathrm{raw},r,\\text{adv}}$, represents the tightest axiom-based constraint on how deceptive the landscape can be.\n4438: "
    },
    {
      "directive_type": "proposition",
      "label": "prop-log-reward-gap-axiom-bound",
      "title": "**(Axiom-Based Bound on the Logarithmic Reward Gap)**",
      "start_line": 4442,
      "end_line": 4452,
      "header_lines": [
        4443
      ],
      "content_start": 4445,
      "content_end": 4451,
      "content": "4445: :label: prop-log-reward-gap-axiom-bound\n4446: \n4447: Under the **Axiom of Reward Regularity**, the expected logarithmic gap in the rescaled reward signal is bounded by:\n4448: \n4449: $$\n4450: \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R \\cdot D_{\\mathrm{valid}})}{\\eta}\\right)\n4451: $$",
      "metadata": {
        "label": "prop-log-reward-gap-axiom-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4442: This raw reward gap now propagates through the measurement pipeline.\n4443: \n4444: :::{prf:proposition} **(Axiom-Based Bound on the Logarithmic Reward Gap)**\n4445: :label: prop-log-reward-gap-axiom-bound\n4446: \n4447: Under the **Axiom of Reward Regularity**, the expected logarithmic gap in the rescaled reward signal is bounded by:\n4448: \n4449: $$\n4450: \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R \\cdot D_{\\mathrm{valid}})}{\\eta}\\right)\n4451: $$\n4452: "
    },
    {
      "directive_type": "proof",
      "label": "proof-prop-log-reward-gap-axiom-bound",
      "title": null,
      "start_line": 4453,
      "end_line": 4506,
      "header_lines": [
        4454
      ],
      "content_start": 4456,
      "content_end": 4505,
      "content": "4456: :label: proof-prop-log-reward-gap-axiom-bound\n4457: \n4458: **Proof.**\n4459: \n4460: The proof proceeds in three direct steps. First, we establish a uniform upper bound on the maximum possible *microscopic* gap between any two rescaled reward values, using the Lipschitz axiom. Second, we argue that the gap between the *means* of any two subpopulations cannot exceed this maximum microscopic gap. Finally, we apply the upper-bound lemma for logarithmic gaps to this bounded mean separation.\n4461: \n4462: **1. Bounding the Maximum Microscopic Rescaled Gap.**\n4463: Let $r'_a$ and $r'_b$ be the rescaled reward values for any two walkers $a$ and $b$. We seek an upper bound for $|r'_a - r'_b|$.\n4464: \n4465: $$\n4466: |r'_a - r'_b| = |g_A(z_a) - g_A(z_b)|\n4467: $$\n4468: \n4469: Since the rescale function $g_A$ is Lipschitz with constant $L_g$ (its maximum derivative), we have:\n4470: \n4471: $$\n4472: |r'_a - r'_b| \\le L_g |z_a - z_b| = L_g \\left| \\frac{R_a - \\mu_R}{\\sigma'_R} - \\frac{R_b - \\mu_R}{\\sigma'_R} \\right| = \\frac{L_g}{\\sigma'_R} |R_a - R_b|\n4473: $$\n4474: \n4475: The patched standard deviation $\\sigma'_R$ is uniformly bounded below by $\\sigma'_{\\min,\\text{patch}} > 0$. The raw reward gap $|R_a - R_b|$ is bounded by the Lipschitz property: $|R_a - R_b| \\le L_R D_{\\text{valid}}$. Combining these gives a uniform upper bound on the microscopic rescaled gap:\n4476: \n4477: $$\n4478: |r'_a - r'_b| \\le \\frac{L_g}{\\sigma'_{\\min,\\text{patch}}} (L_R D_{\\text{valid}}) = \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})\n4479: $$\n4480: \n4481: This is precisely the result of applying the signal propagation function $\\kappa_{\\mathrm{rescaled}}$ to the maximum possible raw reward gap.\n4482: \n4483: **2. Bounding the Macroscopic Mean Separation.**\n4484: The absolute difference between the mean rescaled rewards of the low-error and high-error sets, $|\\mu_{r'}(L_k) - \\mu_{r'}(H_k)|$, is a weighted average of the differences between all cross-set pairs. As such, it cannot be larger than the maximum possible difference between any single pair. Therefore:\n4485: \n4486: $$\n4487: |\\mu_{r'}(L_k) - \\mu_{r'}(H_k)| \\le \\max_{a,b} |r'_a - r'_b| \\le \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})\n4488: $$\n4489: \n4490: We define this upper bound on the mean separation as $\\kappa_{r',\\text{mean,adv}} := \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})$.\n4491: \n4492: **3. From Mean Separation to Logarithmic Mean Separation.**\n4493: We now have a valid upper bound on the mean separation, which is the required premise for [](#lem-log-gap-upper-bound). We apply this lemma with:\n4494: *   $X$ representing the distribution of $r'$ in $L_k$.\n4495: *   $Y$ representing the distribution of $r'$ in $H_k$.\n4496: *   $\\kappa = \\kappa_{r',\\text{mean,adv}}$.\n4497: *   $V_{\\min} = \\eta$ (the minimum value for any rescaled value $r'$).\n4498: \n4499: The lemma directly yields the stated result:\n4500: \n4501: $$\n4502: |\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]| \\le \\ln\\left(1 + \\frac{\\kappa_{r',\\text{mean,adv}}}{\\eta}\\right) = \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})}{\\eta}\\right)\n4503: $$\n4504: \n4505: Since we are interested in the one-sided difference, this bound also holds for $\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]$.",
      "metadata": {
        "label": "proof-prop-log-reward-gap-axiom-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4453: where $\\kappa_{\\mathrm{rescaled}}(\\cdot)$ is the signal propagation function.\n4454: :::\n4455: :::{prf:proof}\n4456: :label: proof-prop-log-reward-gap-axiom-bound\n4457: \n4458: **Proof.**\n4459: \n4460: The proof proceeds in three direct steps. First, we establish a uniform upper bound on the maximum possible *microscopic* gap between any two rescaled reward values, using the Lipschitz axiom. Second, we argue that the gap between the *means* of any two subpopulations cannot exceed this maximum microscopic gap. Finally, we apply the upper-bound lemma for logarithmic gaps to this bounded mean separation.\n4461: \n4462: **1. Bounding the Maximum Microscopic Rescaled Gap.**\n4463: Let $r'_a$ and $r'_b$ be the rescaled reward values for any two walkers $a$ and $b$. We seek an upper bound for $|r'_a - r'_b|$.\n4464: \n4465: $$\n4466: |r'_a - r'_b| = |g_A(z_a) - g_A(z_b)|\n4467: $$\n4468: \n4469: Since the rescale function $g_A$ is Lipschitz with constant $L_g$ (its maximum derivative), we have:\n4470: \n4471: $$\n4472: |r'_a - r'_b| \\le L_g |z_a - z_b| = L_g \\left| \\frac{R_a - \\mu_R}{\\sigma'_R} - \\frac{R_b - \\mu_R}{\\sigma'_R} \\right| = \\frac{L_g}{\\sigma'_R} |R_a - R_b|\n4473: $$\n4474: \n4475: The patched standard deviation $\\sigma'_R$ is uniformly bounded below by $\\sigma'_{\\min,\\text{patch}} > 0$. The raw reward gap $|R_a - R_b|$ is bounded by the Lipschitz property: $|R_a - R_b| \\le L_R D_{\\text{valid}}$. Combining these gives a uniform upper bound on the microscopic rescaled gap:\n4476: \n4477: $$\n4478: |r'_a - r'_b| \\le \\frac{L_g}{\\sigma'_{\\min,\\text{patch}}} (L_R D_{\\text{valid}}) = \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})\n4479: $$\n4480: \n4481: This is precisely the result of applying the signal propagation function $\\kappa_{\\mathrm{rescaled}}$ to the maximum possible raw reward gap.\n4482: \n4483: **2. Bounding the Macroscopic Mean Separation.**\n4484: The absolute difference between the mean rescaled rewards of the low-error and high-error sets, $|\\mu_{r'}(L_k) - \\mu_{r'}(H_k)|$, is a weighted average of the differences between all cross-set pairs. As such, it cannot be larger than the maximum possible difference between any single pair. Therefore:\n4485: \n4486: $$\n4487: |\\mu_{r'}(L_k) - \\mu_{r'}(H_k)| \\le \\max_{a,b} |r'_a - r'_b| \\le \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})\n4488: $$\n4489: \n4490: We define this upper bound on the mean separation as $\\kappa_{r',\\text{mean,adv}} := \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})$.\n4491: \n4492: **3. From Mean Separation to Logarithmic Mean Separation.**\n4493: We now have a valid upper bound on the mean separation, which is the required premise for [](#lem-log-gap-upper-bound). We apply this lemma with:\n4494: *   $X$ representing the distribution of $r'$ in $L_k$.\n4495: *   $Y$ representing the distribution of $r'$ in $H_k$.\n4496: *   $\\kappa = \\kappa_{r',\\text{mean,adv}}$.\n4497: *   $V_{\\min} = \\eta$ (the minimum value for any rescaled value $r'$).\n4498: \n4499: The lemma directly yields the stated result:\n4500: \n4501: $$\n4502: |\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]| \\le \\ln\\left(1 + \\frac{\\kappa_{r',\\text{mean,adv}}}{\\eta}\\right) = \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})}{\\eta}\\right)\n4503: $$\n4504: \n4505: Since we are interested in the one-sided difference, this bound also holds for $\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]$.\n4506: "
    },
    {
      "directive_type": "theorem",
      "label": "thm-stability-condition-final-corrected",
      "title": "**(The Corrected Stability Condition for Intelligent Adaptation)**",
      "start_line": 4512,
      "end_line": 4520,
      "header_lines": [
        4513
      ],
      "content_start": 4515,
      "content_end": 4519,
      "content": "4515: :label: thm-stability-condition-final-corrected\n4516: \n4517: Let a swarm `k$ be in a high-error state. The algorithm's targeting mechanism is intelligent (i.e., $\\mathbb{E}[\\ln(V_{\\text{fit}})|H_k] < \\mathbb{E}[\\ln(V_{\\text{fit}})|L_k]$) if and only if the system parameters satisfy the following **Corrected Stability Condition**:\n4518: $$\n4519: \\beta \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right) > \\alpha \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_{R} \\cdot D_{\\mathrm{valid}})}{\\eta}\\right)",
      "metadata": {
        "label": "thm-stability-condition-final-corrected"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4512: With a rigorous, axiom-based bound on the adversarial reward signal now established, we can state the corrected and final stability condition.\n4513: \n4514: :::{prf:theorem} **(The Corrected Stability Condition for Intelligent Adaptation)**\n4515: :label: thm-stability-condition-final-corrected\n4516: \n4517: Let a swarm `k$ be in a high-error state. The algorithm's targeting mechanism is intelligent (i.e., $\\mathbb{E}[\\ln(V_{\\text{fit}})|H_k] < \\mathbb{E}[\\ln(V_{\\text{fit}})|L_k]$) if and only if the system parameters satisfy the following **Corrected Stability Condition**:\n4518: $$\n4519: \\beta \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right) > \\alpha \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_{R} \\cdot D_{\\mathrm{valid}})}{\\eta}\\right)\n4520: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-thm-stability-condition-final-corrected",
      "title": null,
      "start_line": 4521,
      "end_line": 4534,
      "header_lines": [
        4522
      ],
      "content_start": 4524,
      "content_end": 4533,
      "content": "4524: :label: proof-thm-stability-condition-final-corrected\n4525: \n4526: **Proof.**\n4527: \n4528: The proof is a direct assembly of the bounds derived in the preceding propositions. The condition for intelligence, as established in the core trade-off inequality of the main stability proof, is that the guaranteed *minimum* of the corrective signal must be strictly greater than the allowed *maximum* of the adversarial signal.\n4529: \n4530: *   **LHS (Corrective Signal):** The lower bound is given by **Proposition 7.5.2.1**.\n4531: *   **RHS (Adversarial Signal):** The upper bound is now given by **Proposition 7.5.2.3** (the axiom-based bound).\n4532: \n4533: Substituting the lower bound for the corrective signal (from Proposition 7.5.2.1) and the upper bound for the adversarial signal (from Proposition 7.5.2.3) into the inequality $\\beta \\times (\\text{Corrective Gap}) > \\alpha \\times (\\text{Adversarial Gap})$ yields the final, corrected stability condition.",
      "metadata": {
        "label": "proof-thm-stability-condition-final-corrected"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4521: where $\\kappa_{d', \\text{mean}}$ is the guaranteed N-uniform separation between the mean rescaled diversity values of the high-error and low-error populations, as derived in **Proposition 7.5.2.1**.\n4522: :::\n4523: :::{prf:proof}\n4524: :label: proof-thm-stability-condition-final-corrected\n4525: \n4526: **Proof.**\n4527: \n4528: The proof is a direct assembly of the bounds derived in the preceding propositions. The condition for intelligence, as established in the core trade-off inequality of the main stability proof, is that the guaranteed *minimum* of the corrective signal must be strictly greater than the allowed *maximum* of the adversarial signal.\n4529: \n4530: *   **LHS (Corrective Signal):** The lower bound is given by **Proposition 7.5.2.1**.\n4531: *   **RHS (Adversarial Signal):** The upper bound is now given by **Proposition 7.5.2.3** (the axiom-based bound).\n4532: \n4533: Substituting the lower bound for the corrective signal (from Proposition 7.5.2.1) and the upper bound for the adversarial signal (from Proposition 7.5.2.3) into the inequality $\\beta \\times (\\text{Corrective Gap}) > \\alpha \\times (\\text{Adversarial Gap})$ yields the final, corrected stability condition.\n4534: "
    },
    {
      "directive_type": "definition",
      "label": "def-unfit-set",
      "title": "The Unfit Set",
      "start_line": 4559,
      "end_line": 4568,
      "header_lines": [
        4560
      ],
      "content_start": 4562,
      "content_end": 4567,
      "content": "4562: :label: def-unfit-set\n4563: \n4564: For a given swarm `k` with alive set $\\mathcal{A}_k$ and a calculated fitness potential vector $(V_{k,i})_{i \\in \\mathcal{A}_k}$, the **unfit set**, $U_k$, is the subset of alive walkers whose fitness potential is less than or equal to the swarm's mean fitness potential, $\\mu_{V,k} = \\frac{1}{k}\\sum_{j \\in \\mathcal{A}_k} V_{k,j}$.\n4565: \n4566: $$\n4567: U_k := \\{i \\in \\mathcal{A}_k \\mid V_{k,i} \\le \\mu_{V,k}\\}",
      "metadata": {
        "label": "def-unfit-set"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4559: The first pillar of the overlap proof is to show that the fitness signal generated by a high-error swarm is a collective phenomenon. A large `V_struct` implies a large internal variance, which is guaranteed to produce a non-vanishing raw measurement variance ([](#thm-geometry-guarantees-variance)). This raw signal robustly propagates through the measurement pipeline to create a guaranteed microscopic gap in the final fitness potentials, ensuring a non-trivial overall fitness range, $\\kappa_V,gap(\\varepsilon) > 0$. The following lemma proves that such a gap cannot be explained by a single unusually fit or unfit walker; it necessitates a statistically significant imbalance in the fitness distribution of the entire population.\n4560: \n4561: :::{prf:definition} The Unfit Set\n4562: :label: def-unfit-set\n4563: \n4564: For a given swarm `k` with alive set $\\mathcal{A}_k$ and a calculated fitness potential vector $(V_{k,i})_{i \\in \\mathcal{A}_k}$, the **unfit set**, $U_k$, is the subset of alive walkers whose fitness potential is less than or equal to the swarm's mean fitness potential, $\\mu_{V,k} = \\frac{1}{k}\\sum_{j \\in \\mathcal{A}_k} V_{k,j}$.\n4565: \n4566: $$\n4567: U_k := \\{i \\in \\mathcal{A}_k \\mid V_{k,i} \\le \\mu_{V,k}\\}\n4568: $$"
    },
    {
      "directive_type": "lemma",
      "label": "lem-unfit-fraction-lower-bound",
      "title": "N-Uniform Lower Bound on the Unfit Fraction",
      "start_line": 4570,
      "end_line": 4582,
      "header_lines": [
        4571
      ],
      "content_start": 4573,
      "content_end": 4581,
      "content": "4573: :label: lem-unfit-fraction-lower-bound\n4574: \n4575: Let a swarm `k` with `k >= 2` alive walkers have a fitness potential range that is bounded below by a positive, $\\varepsilon$-dependent constant: $V_{\\max,k} - V_{\\min,k} \\ge \\kappa_{V,\\text{gap}}(\\epsilon) > 0$.\n4576: \n4577: The fraction of alive walkers in the unfit set $U_k$ is bounded below by a positive, N-uniform, $\\varepsilon$-dependent constant $f_U(\\epsilon) > 0$:\n4578: \n4579: $$\n4580: \\frac{|U_k|}{k} \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} - V_{\\text{pot,min}})} =: f_U(\\epsilon)\n4581: $$",
      "metadata": {
        "label": "lem-unfit-fraction-lower-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4570: :::\n4571: \n4572: :::{prf:lemma} N-Uniform Lower Bound on the Unfit Fraction\n4573: :label: lem-unfit-fraction-lower-bound\n4574: \n4575: Let a swarm `k` with `k >= 2` alive walkers have a fitness potential range that is bounded below by a positive, $\\varepsilon$-dependent constant: $V_{\\max,k} - V_{\\min,k} \\ge \\kappa_{V,\\text{gap}}(\\epsilon) > 0$.\n4576: \n4577: The fraction of alive walkers in the unfit set $U_k$ is bounded below by a positive, N-uniform, $\\varepsilon$-dependent constant $f_U(\\epsilon) > 0$:\n4578: \n4579: $$\n4580: \\frac{|U_k|}{k} \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} - V_{\\text{pot,min}})} =: f_U(\\epsilon)\n4581: $$\n4582: "
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-unfit-fraction-lower-bound",
      "title": null,
      "start_line": 4583,
      "end_line": 4626,
      "header_lines": [
        4584
      ],
      "content_start": 4586,
      "content_end": 4625,
      "content": "4586: :label: proof-lem-unfit-fraction-lower-bound\n4587: \n4588: **Proof.**\n4589: \n4590: The proof establishes the bound by analyzing the balance of deviations from the mean fitness, a fundamental statistical property.\n4591: \n4592: 1.  **The Principle of Balanced Deviations:** By the definition of the mean $\\mu_{V,k}$, the sum of all deviations from the mean is zero. Let $F_k$ be the \"fit\" set (the complement of $U_k$). Partitioning the sum over these two sets shows that the total positive deviation equals the magnitude of the total negative deviation:\n4593: \n4594: \n4595: $$\n4596: \\sum_{i \\in F_k} (V_{k,i} - \\mu_{V,k}) = \\sum_{j \\in U_k} (\\mu_{V,k} - V_{k,j})\n4597: $$\n4598: \n4599: 2.  **Bounding the Total Deviation:** The total range of fitness values, $V_{\\max,k} - V_{\\min,k}$, can be partitioned at the mean: $V_{\\max,k} - V_{\\min,k} = (V_{\\max,k} - \\mu_{V,k}) + (\\mu_{V,k} - V_{\\min,k})$. Since both terms on the right are non-negative, at least one of them must be greater than or equal to half of the total range. Using the premise, we have:\n4600: \n4601: \n4602: $$\n4603: \\max\\left( (V_{\\max,k} - \\mu_{V,k}), (\\mu_{V,k} - V_{\\min,k}) \\right) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}\n4604: $$\n4605: \n4606: 3.  **Case Analysis:**\n4607:     *   **Case A:** If $(V_{\\max,k} - \\mu_{V,k}) \\ge \\kappa_{V,\\text{gap}}(\\epsilon)/2$. The sum of positive deviations, $\\sum_{i \\in F_k} (V_{k,i} - \\mu_{V,k})$, must be at least this large. By the balance of deviations, the sum of negative deviations must also satisfy this bound. We can then bound this sum by its size, $|U_k|$, multiplied by the maximum possible value for any single term, which is bounded by the total potential range $V_{\\text{pot,max}} - V_{\\text{pot,min}}$:\n4608: \n4609: \n4610: $$\n4611: |U_k| \\cdot (V_{\\text{pot,max}} - V_{\\text{pot,min}}) \\ge \\sum_{j \\in U_k} (\\mu_{V,k} - V_{k,j}) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}\n4612: $$\n4613: \n4614:         This directly yields the desired lower bound on $|U_k|$.\n4615: \n4616:     *   **Case B:** If $(\\mu_{V,k} - V_{\\min,k}) \\ge \\kappa_{V,\\text{gap}}(\\epsilon)/2$. By a symmetric argument, the sum of negative deviations is at least this large. This implies the sum of positive deviations is also this large. Bounding the sum of positive deviations:\n4617: \n4618: \n4619: $$\n4620: |F_k| \\cdot (V_{\\text{pot,max}} - V_{\\text{pot,min}}) \\ge \\sum_{i \\in F_k} (V_{i,k} - \\mu_{V,k}) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}\n4621: $$\n4622: \n4623:         This gives a lower bound on the size of the *fit* set: $|F_k|/k \\ge \\kappa_{V,\\text{gap}} / (2(V_{\\text{pot,max}} - V_{\\text{pot,min}}))$. Since $|U_k| + |F_k| = k$, this implies an *upper bound* on the size of the unfit set. However, a simpler argument is to note that if the unfit set were vanishingly small, the mean would be pulled towards $V_{\\max,k}$, making $(\\mu_{V,k} - V_{\\min,k})$ large and contradicting the mean's location. The bound from Case A thus represents the worst-case scenario for the size of the unfit set, providing a valid global lower bound.\n4624: \n4625: 4.  **Conclusion:** Rearranging the inequality from Step 3 and dividing by `k` gives the final result for the fraction of unfit walkers. The lower bound, $f_U(\\epsilon)$, is a strictly positive, N-uniform, and $\\varepsilon$-dependent constant, as it is constructed from other N-uniform constants.",
      "metadata": {
        "label": "proof-lem-unfit-fraction-lower-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4583: where $V_{\\text{pot,max}}$ and $V_{\\text{pot,min}}$ are the N-uniform bounds on the fitness potential from [](#lem-potential-bounds).\n4584: :::\n4585: :::{prf:proof}\n4586: :label: proof-lem-unfit-fraction-lower-bound\n4587: \n4588: **Proof.**\n4589: \n4590: The proof establishes the bound by analyzing the balance of deviations from the mean fitness, a fundamental statistical property.\n4591: \n4592: 1.  **The Principle of Balanced Deviations:** By the definition of the mean $\\mu_{V,k}$, the sum of all deviations from the mean is zero. Let $F_k$ be the \"fit\" set (the complement of $U_k$). Partitioning the sum over these two sets shows that the total positive deviation equals the magnitude of the total negative deviation:\n4593: \n4594: \n4595: $$\n4596: \\sum_{i \\in F_k} (V_{k,i} - \\mu_{V,k}) = \\sum_{j \\in U_k} (\\mu_{V,k} - V_{k,j})\n4597: $$\n4598: \n4599: 2.  **Bounding the Total Deviation:** The total range of fitness values, $V_{\\max,k} - V_{\\min,k}$, can be partitioned at the mean: $V_{\\max,k} - V_{\\min,k} = (V_{\\max,k} - \\mu_{V,k}) + (\\mu_{V,k} - V_{\\min,k})$. Since both terms on the right are non-negative, at least one of them must be greater than or equal to half of the total range. Using the premise, we have:\n4600: \n4601: \n4602: $$\n4603: \\max\\left( (V_{\\max,k} - \\mu_{V,k}), (\\mu_{V,k} - V_{\\min,k}) \\right) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}\n4604: $$\n4605: \n4606: 3.  **Case Analysis:**\n4607:     *   **Case A:** If $(V_{\\max,k} - \\mu_{V,k}) \\ge \\kappa_{V,\\text{gap}}(\\epsilon)/2$. The sum of positive deviations, $\\sum_{i \\in F_k} (V_{k,i} - \\mu_{V,k})$, must be at least this large. By the balance of deviations, the sum of negative deviations must also satisfy this bound. We can then bound this sum by its size, $|U_k|$, multiplied by the maximum possible value for any single term, which is bounded by the total potential range $V_{\\text{pot,max}} - V_{\\text{pot,min}}$:\n4608: \n4609: \n4610: $$\n4611: |U_k| \\cdot (V_{\\text{pot,max}} - V_{\\text{pot,min}}) \\ge \\sum_{j \\in U_k} (\\mu_{V,k} - V_{k,j}) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}\n4612: $$\n4613: \n4614:         This directly yields the desired lower bound on $|U_k|$.\n4615: \n4616:     *   **Case B:** If $(\\mu_{V,k} - V_{\\min,k}) \\ge \\kappa_{V,\\text{gap}}(\\epsilon)/2$. By a symmetric argument, the sum of negative deviations is at least this large. This implies the sum of positive deviations is also this large. Bounding the sum of positive deviations:\n4617: \n4618: \n4619: $$\n4620: |F_k| \\cdot (V_{\\text{pot,max}} - V_{\\text{pot,min}}) \\ge \\sum_{i \\in F_k} (V_{i,k} - \\mu_{V,k}) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}\n4621: $$\n4622: \n4623:         This gives a lower bound on the size of the *fit* set: $|F_k|/k \\ge \\kappa_{V,\\text{gap}} / (2(V_{\\text{pot,max}} - V_{\\text{pot,min}}))$. Since $|U_k| + |F_k| = k$, this implies an *upper bound* on the size of the unfit set. However, a simpler argument is to note that if the unfit set were vanishingly small, the mean would be pulled towards $V_{\\max,k}$, making $(\\mu_{V,k} - V_{\\min,k})$ large and contradicting the mean's location. The bound from Case A thus represents the worst-case scenario for the size of the unfit set, providing a valid global lower bound.\n4624: \n4625: 4.  **Conclusion:** Rearranging the inequality from Step 3 and dividing by `k` gives the final result for the fraction of unfit walkers. The lower bound, $f_U(\\epsilon)$, is a strictly positive, N-uniform, and $\\varepsilon$-dependent constant, as it is constructed from other N-uniform constants.\n4626: "
    },
    {
      "directive_type": "theorem",
      "label": "thm-unfit-high-error-overlap-fraction",
      "title": "N-Uniform Lower Bound on the Unfit-High-Error Overlap Fraction",
      "start_line": 4634,
      "end_line": 4646,
      "header_lines": [
        4635
      ],
      "content_start": 4637,
      "content_end": 4645,
      "content": "4637: :label: thm-unfit-high-error-overlap-fraction\n4638: \n4639: Let a swarm state satisfy $V_{\\mathrm{struct}} > R^2_{\\mathrm{spread}}$. Let $U_k$ be the unfit set for swarm `k` and let $H_k(\\epsilon)$ be its corresponding unified high-error set.\n4640: \n4641: If the **Stability Condition** ([](#thm-stability-condition-final-corrected)) holds for the chosen system parameters, then the fraction of alive walkers in the intersection set $I_{UH} = U_k \\cap H_k(\\epsilon)$ is bounded below by a positive, N-uniform constant:\n4642: \n4643: $$\n4644: \\frac{|I_{UH}|}{k} \\ge f_{UH}(\\epsilon) > 0\n4645: $$",
      "metadata": {
        "label": "thm-unfit-high-error-overlap-fraction"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4634: This theorem proves that the set of walkers targeted by the algorithm for corrective cloning has a substantial and non-vanishing overlap with the set of walkers that are the primary source of the system's geometric error. This is the ultimate proof of the Keystone Principle's corrective feedback loop.\n4635: \n4636: :::{prf:theorem} N-Uniform Lower Bound on the Unfit-High-Error Overlap Fraction\n4637: :label: thm-unfit-high-error-overlap-fraction\n4638: \n4639: Let a swarm state satisfy $V_{\\mathrm{struct}} > R^2_{\\mathrm{spread}}$. Let $U_k$ be the unfit set for swarm `k` and let $H_k(\\epsilon)$ be its corresponding unified high-error set.\n4640: \n4641: If the **Stability Condition** ([](#thm-stability-condition-final-corrected)) holds for the chosen system parameters, then the fraction of alive walkers in the intersection set $I_{UH} = U_k \\cap H_k(\\epsilon)$ is bounded below by a positive, N-uniform constant:\n4642: \n4643: $$\n4644: \\frac{|I_{UH}|}{k} \\ge f_{UH}(\\epsilon) > 0\n4645: $$\n4646: "
    },
    {
      "directive_type": "proof",
      "label": "proof-thm-unfit-high-error-overlap-fraction",
      "title": null,
      "start_line": 4647,
      "end_line": 4690,
      "header_lines": [
        4648
      ],
      "content_start": 4650,
      "content_end": 4689,
      "content": "4650: :label: proof-thm-unfit-high-error-overlap-fraction\n4651: \n4652: **Proof (by contradiction).**\n4653: \n4654: The proof follows directly from the consequences of the **Stability Condition** ([](#thm-stability-condition-final-corrected)). This condition guarantees that the high-error population is systematically less fit, a statistical property that makes a vanishing overlap with the unfit set impossible.\n4655: \n4656: **1. Setup for Contradiction.**\n4657: Assume the premises hold: the swarm `k` has a large structural error, and the **Stability Condition** is satisfied. Now, assume for the sake of contradiction that the overlap between the unfit and high-error sets is vanishingly small. Formally, this means the fraction of their intersection approaches zero:\n4658: \n4659: $$\n4660: f_{UH} = \\frac{|U_k \\cap H_k|}{k} \\approx 0\n4661: $$\n4662: \n4663: **2. Consequence 1: High-Error Walkers Must Be \"Fit\".**\n4664: If the overlap is nearly zero, then the high-error set `H_k` must consist almost entirely of walkers that are *not* in the unfit set. This means they must belong to the complementary \"fit\" set, $F_k = \\mathcal{A}_k \\setminus U_k$. Formally, $H_k \\approx H_k \\cap F_k$.\n4665: \n4666: By the definition of the fit set, any walker $j \\in F_k$ has a fitness greater than the swarm's mean fitness, $V_{k,j} > \\mu_{V,k}$. Therefore, the expected fitness of the high-error set, which is composed almost entirely of fit walkers, must also be greater than the mean:\n4667: \n4668: $$\n4669: \\mathbb{E}[V_{\\text{fit}} \\mid i \\in H_k] > \\mu_{V,k} \\quad (*)\n4670: $$\n4671: \n4672: **3. Consequence 2: The Axiom's Guarantee.**\n4673: The **Stability Condition** is precisely the condition required to ensure that the algorithm's targeting is intelligent. As proven in [](#thm-stability-condition-final-corrected), satisfying this condition guarantees that the expected fitness of the high-error population is *strictly less than* the expected fitness of the low-error population:\n4674: \n4675: $$\n4676: \\mathbb{E}[V_{\\text{fit}} \\mid i \\in H_k] < \\mathbb{E}[V_{\\text{fit}} \\mid i \\in L_k]\n4677: $$\n4678: \n4679: The mean fitness of the entire swarm, $\\mu_{V,k}$, is the weighted average of the means of these two disjoint populations: $\\mu_{V,k} = f_H \\mathbb{E}[V_{\\text{fit}}|H_k] + f_L \\mathbb{E}[V_{\\text{fit}}|L_k]$. A weighted average must lie strictly between its two components **as long as both components have non-zero weight**. From Chapter 6, we are guaranteed that both the high-error ($H_k$) and low-error ($L_k$) sets are non-empty and constitute non-vanishing fractions of the population, so $f_H > 0$ and $f_L > 0$. Therefore, it must be that:\n4680: \n4681: $$\n4682: \\mathbb{E}[V_{\\text{fit}} \\mid i \\in H_k] < \\mu_{V,k} \\quad (**)\n4683: $$\n4684: \n4685: **4. The Contradiction.**\n4686: The conclusion from Step 2, $\\text{E}[V_fit | H_k] > \\mu_V$, directly contradicts the conclusion from Step 3, $\\text{E}[V_fit | H_k] < \\mu_V$. Both conclusions follow from our premises, so the initial assumption of a vanishing overlap must be false.\n4687: \n4688: **5. Conclusion.**\n4689: The assumption of a vanishing overlap leads to a contradiction. Therefore, the overlap fraction must be bounded below by a strictly positive constant. A more detailed analysis of the underlying distributions shows that this lower bound, $f_UH(\\varepsilon)$, is an N-uniform constant that is a monotonic function of the fitness gap between the high-error and low-error populations guaranteed by the axiom. This completes the proof.",
      "metadata": {
        "label": "proof-thm-unfit-high-error-overlap-fraction"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "raw_directive": "4647: where `k` is the number of alive walkers in swarm `k`.\n4648: :::\n4649: :::{prf:proof}\n4650: :label: proof-thm-unfit-high-error-overlap-fraction\n4651: \n4652: **Proof (by contradiction).**\n4653: \n4654: The proof follows directly from the consequences of the **Stability Condition** ([](#thm-stability-condition-final-corrected)). This condition guarantees that the high-error population is systematically less fit, a statistical property that makes a vanishing overlap with the unfit set impossible.\n4655: \n4656: **1. Setup for Contradiction.**\n4657: Assume the premises hold: the swarm `k` has a large structural error, and the **Stability Condition** is satisfied. Now, assume for the sake of contradiction that the overlap between the unfit and high-error sets is vanishingly small. Formally, this means the fraction of their intersection approaches zero:\n4658: \n4659: $$\n4660: f_{UH} = \\frac{|U_k \\cap H_k|}{k} \\approx 0\n4661: $$\n4662: \n4663: **2. Consequence 1: High-Error Walkers Must Be \"Fit\".**\n4664: If the overlap is nearly zero, then the high-error set `H_k` must consist almost entirely of walkers that are *not* in the unfit set. This means they must belong to the complementary \"fit\" set, $F_k = \\mathcal{A}_k \\setminus U_k$. Formally, $H_k \\approx H_k \\cap F_k$.\n4665: \n4666: By the definition of the fit set, any walker $j \\in F_k$ has a fitness greater than the swarm's mean fitness, $V_{k,j} > \\mu_{V,k}$. Therefore, the expected fitness of the high-error set, which is composed almost entirely of fit walkers, must also be greater than the mean:\n4667: \n4668: $$\n4669: \\mathbb{E}[V_{\\text{fit}} \\mid i \\in H_k] > \\mu_{V,k} \\quad (*)\n4670: $$\n4671: \n4672: **3. Consequence 2: The Axiom's Guarantee.**\n4673: The **Stability Condition** is precisely the condition required to ensure that the algorithm's targeting is intelligent. As proven in [](#thm-stability-condition-final-corrected), satisfying this condition guarantees that the expected fitness of the high-error population is *strictly less than* the expected fitness of the low-error population:\n4674: \n4675: $$\n4676: \\mathbb{E}[V_{\\text{fit}} \\mid i \\in H_k] < \\mathbb{E}[V_{\\text{fit}} \\mid i \\in L_k]\n4677: $$\n4678: \n4679: The mean fitness of the entire swarm, $\\mu_{V,k}$, is the weighted average of the means of these two disjoint populations: $\\mu_{V,k} = f_H \\mathbb{E}[V_{\\text{fit}}|H_k] + f_L \\mathbb{E}[V_{\\text{fit}}|L_k]$. A weighted average must lie strictly between its two components **as long as both components have non-zero weight**. From Chapter 6, we are guaranteed that both the high-error ($H_k$) and low-error ($L_k$) sets are non-empty and constitute non-vanishing fractions of the population, so $f_H > 0$ and $f_L > 0$. Therefore, it must be that:\n4680: \n4681: $$\n4682: \\mathbb{E}[V_{\\text{fit}} \\mid i \\in H_k] < \\mu_{V,k} \\quad (**)\n4683: $$\n4684: \n4685: **4. The Contradiction.**\n4686: The conclusion from Step 2, $\\text{E}[V_fit | H_k] > \\mu_V$, directly contradicts the conclusion from Step 3, $\\text{E}[V_fit | H_k] < \\mu_V$. Both conclusions follow from our premises, so the initial assumption of a vanishing overlap must be false.\n4687: \n4688: **5. Conclusion.**\n4689: The assumption of a vanishing overlap leads to a contradiction. Therefore, the overlap fraction must be bounded below by a strictly positive constant. A more detailed analysis of the underlying distributions shows that this lower bound, $f_UH(\\varepsilon)$, is an N-uniform constant that is a monotonic function of the fitness gap between the high-error and low-error populations guaranteed by the axiom. This completes the proof.\n4690: "
    }
  ],
  "validation": {
    "ok": true,
    "errors": []
  }
}