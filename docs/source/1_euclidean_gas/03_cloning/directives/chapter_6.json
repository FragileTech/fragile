{
  "chapter_index": 6,
  "section_id": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
  "directive_count": 15,
  "hints": [
    {
      "directive_type": "lemma",
      "label": "lem-V_Varx-implies-variance",
      "title": "Large $V_{\\text{Var},x}$ Implies Large Single-Swarm Positional Variance",
      "start_line": 2322,
      "end_line": 2337,
      "header_lines": [
        2323
      ],
      "content_start": 2325,
      "content_end": 2336,
      "content": "2325: :label: lem-V_Varx-implies-variance\n2326: \n2327: Let $V_{Var,x}(S_1, S_2)$ be the total intra-swarm positional variance component of the Lyapunov function as defined in [](#def-full-synergistic-lyapunov-function):\n2328: \n2329: $$\n2330: V_{Var,x}(S_1, S_2) = \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2 + \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2\n2331: $$\n2332: \n2333: If this component is large, such that $V_{Var,x} > R_{total\\_var,x}^2$ for some threshold $R_{total\\_var,x}^2 > 0$, then at least one swarm $k \\in \\{1, 2\\}$ must have a large sum of squared deviations:\n2334: \n2335: $$\n2336: \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2 > \\frac{R_{total\\_var,x}^2}{2}",
      "metadata": {
        "label": "lem-V_Varx-implies-variance"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [],
      "raw_directive": "2322: The first step in the Keystone analysis is to connect the relevant component of the synergistic Lyapunov function to the state of a single swarm. As this document aims to prove the contractive nature of cloning on **positional error**, the Keystone mechanism is triggered specifically by the **positional variance component, $V_{\\text{Var},x}$**. The following lemma provides the simple but necessary guarantee that if this $V_{\\text{Var},x}$ term is large, then at least one of the two swarms must have a large internal positional variance.\n2323: \n2324: :::{prf:lemma} Large $V_{\\text{Var},x}$ Implies Large Single-Swarm Positional Variance\n2325: :label: lem-V_Varx-implies-variance\n2326: \n2327: Let $V_{Var,x}(S_1, S_2)$ be the total intra-swarm positional variance component of the Lyapunov function as defined in [](#def-full-synergistic-lyapunov-function):\n2328: \n2329: $$\n2330: V_{Var,x}(S_1, S_2) = \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2 + \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2\n2331: $$\n2332: \n2333: If this component is large, such that $V_{Var,x} > R_{total\\_var,x}^2$ for some threshold $R_{total\\_var,x}^2 > 0$, then at least one swarm $k \\in \\{1, 2\\}$ must have a large sum of squared deviations:\n2334: \n2335: $$\n2336: \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2 > \\frac{R_{total\\_var,x}^2}{2}\n2337: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-V_Varx-implies-variance",
      "title": null,
      "start_line": 2339,
      "end_line": 2361,
      "header_lines": [
        2340
      ],
      "content_start": 2341,
      "content_end": 2360,
      "content": "2341: :::{prf:proof}\n2342: :label: proof-lem-V_Varx-implies-variance\n2343: **Proof.**\n2344: \n2345: The proof is by contradiction. Assume the premise holds: $V_{Var,x} > R_{total\\_var,x}^2$. Assume for contradiction that the conclusion is false. This would mean that for *both* swarms (`k=1` and `k=2`):\n2346: \n2347: $$\n2348: \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2 \\le \\frac{R_{total\\_var,x}^2}{2}, \\quad \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2 \\le \\frac{R_{total\\_var,x}^2}{2}\n2349: $$\n2350: \n2351: Now, we bound the total intra-swarm positional error $V_{Var,x}$ under this assumption:\n2352: \n2353: $$\n2354: \\begin{aligned}\n2355: V_{Var,x} &= \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2 + \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2 \\\\\n2356: &\\le \\frac{R_{total\\_var,x}^2}{2} + \\frac{R_{total\\_var,x}^2}{2} = R_{total\\_var,x}^2\n2357: \\end{aligned}\n2358: $$\n2359: \n2360: The result $V_{Var,x} \\le R_{total\\_var,x}^2$ directly contradicts our premise. Therefore, the assumption must be false, and the conclusion must be true.",
      "metadata": {
        "label": "proof-lem-V_Varx-implies-variance"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [],
      "raw_directive": "2339: :::\n2340: \n2341: :::{prf:proof}\n2342: :label: proof-lem-V_Varx-implies-variance\n2343: **Proof.**\n2344: \n2345: The proof is by contradiction. Assume the premise holds: $V_{Var,x} > R_{total\\_var,x}^2$. Assume for contradiction that the conclusion is false. This would mean that for *both* swarms (`k=1` and `k=2`):\n2346: \n2347: $$\n2348: \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2 \\le \\frac{R_{total\\_var,x}^2}{2}, \\quad \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2 \\le \\frac{R_{total\\_var,x}^2}{2}\n2349: $$\n2350: \n2351: Now, we bound the total intra-swarm positional error $V_{Var,x}$ under this assumption:\n2352: \n2353: $$\n2354: \\begin{aligned}\n2355: V_{Var,x} &= \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2 + \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2 \\\\\n2356: &\\le \\frac{R_{total\\_var,x}^2}{2} + \\frac{R_{total\\_var,x}^2}{2} = R_{total\\_var,x}^2\n2357: \\end{aligned}\n2358: $$\n2359: \n2360: The result $V_{Var,x} \\le R_{total\\_var,x}^2$ directly contradicts our premise. Therefore, the assumption must be false, and the conclusion must be true.\n2361: "
    },
    {
      "directive_type": "definition",
      "label": "def-unified-high-low-error-sets",
      "title": "The Unified High-Error and Low-Error Sets",
      "start_line": 2371,
      "end_line": 2414,
      "header_lines": [
        2372
      ],
      "content_start": 2374,
      "content_end": 2413,
      "content": "2374: :label: def-unified-high-low-error-sets\n2375: \n2376: For a given swarm `k` with alive set $\\mathcal{A}_k$ ($k \\ge 2$), we define a partition into a unified high-error set $H_k(\\epsilon)$ and a unified low-error set $L_k(\\epsilon)$ using a **clustering-based approach** that applies uniformly across all interaction regimes. This unified approach captures both global outlier structure and local phase-space clustering through a single consistent mechanism.\n2377: \n2378: **Phase-Space Clustering Construction:**\n2379: \n2380: The construction proceeds in four steps:\n2381: \n2382: 1.  **Clustering:** Partition the alive set $\\mathcal{A}_k$ into disjoint clusters $\\{G_1, \\ldots, G_M\\}$ using complete-linkage hierarchical clustering with a maximum cluster diameter $D_{\\text{diam}}(\\epsilon) := c_d \\cdot \\epsilon$ (where $c_d > 0$ is a fixed constant, typically $c_d = 2$). Each cluster $G_m$ satisfies:\n2383: \n2384: $$\n2385: \\text{diam}(G_m) := \\max_{i,j \\in G_m} d_{\\text{alg}}(i, j) \\le D_{\\text{diam}}(\\epsilon)\n2386: $$\n2387: \n2388: where $d_{\\text{alg}}(i, j)^2 := \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2$ is the algorithmic phase-space distance.\n2389: \n2390: 2.  **Statistical Validity Constraint:** To ensure that cluster-level statistics are meaningful, we impose a minimum cluster size requirement. Let $k_{\\min} := \\max(5, \\lceil 0.05k \\rceil)$ be the minimum statistically valid cluster size. All clusters with $|G_m| < k_{\\min}$ are marked as **invalid** and their walkers are automatically included in the high-error set (as they represent statistically unreliable outlier configurations).\n2391: \n2392: 3.  **Outlier Cluster Identification:** For each valid cluster $G_m$ (with $|G_m| \\ge k_{\\min}$), compute its center of mass in phase space: $(\\mu_{x,m}, \\mu_{v,m})$. Compute the between-cluster hypocoercive variance contribution:\n2393: \n2394: $$\n2395: \\text{Contrib}(G_m) := |G_m| \\left(\\|\\mu_{x,m} - \\mu_x\\|^2 + \\lambda_v \\|\\mu_{v,m} - \\mu_v\\|^2\\right)\n2396: $$\n2397: \n2398: where $(\\mu_x, \\mu_v)$ is the global center of mass. Sort valid clusters by $\\text{Contrib}(G_m)$ in descending order, and let $O_M \\subseteq \\{1, \\ldots, M\\}$ be the smallest set of cluster indices (among valid clusters) whose cumulative contribution meets or exceeds a fraction $(1-\\varepsilon_O)$ of the total contribution from valid clusters (where $\\varepsilon_O \\in (0, 1)$ is a fixed structural parameter, typically $\\varepsilon_O = 0.1$):\n2399: \n2400: $$\n2401: \\sum_{m \\in O_M} \\text{Contrib}(G_m) \\ge (1-\\varepsilon_O) \\sum_{\\substack{m=1 \\\\ |G_m| \\ge k_{\\min}}}^M \\text{Contrib}(G_m)\n2402: $$\n2403: \n2404: 4.  **Unified High-Error Set Construction:** The unified high-error set is the union of all walkers in outlier clusters plus all walkers in invalid clusters:\n2405: \n2406: $$\n2407: H_k(\\epsilon) := \\left(\\bigcup_{m \\in O_M} G_m\\right) \\cup \\left(\\bigcup_{\\substack{m: |G_m| < k_{\\min}}} G_m\\right)\n2408: $$\n2409: \n2410: The **Unified Low-Error Set** is the complement:\n2411: \n2412: $$\n2413: L_k(\\epsilon) := \\mathcal{A}_k \\setminus H_k(\\epsilon)",
      "metadata": {
        "label": "def-unified-high-low-error-sets"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [],
      "raw_directive": "2371: Therefore, this section formally defines these sets based on the swarm's full **phase-space configuration**. We will partition the swarm based on two distinct phase-space measures: global kinematic dispersion and local phase-space density. These definitions create the crucial link between the state of the swarm and the signals measured by the algorithm. The remainder of the Keystone analysis will then be dedicated to proving that high *positional* variance is a sufficient condition to force a non-trivial number of walkers into these *phase-space* defined error sets.\n2372: \n2373: :::{prf:definition} The Unified High-Error and Low-Error Sets\n2374: :label: def-unified-high-low-error-sets\n2375: \n2376: For a given swarm `k` with alive set $\\mathcal{A}_k$ ($k \\ge 2$), we define a partition into a unified high-error set $H_k(\\epsilon)$ and a unified low-error set $L_k(\\epsilon)$ using a **clustering-based approach** that applies uniformly across all interaction regimes. This unified approach captures both global outlier structure and local phase-space clustering through a single consistent mechanism.\n2377: \n2378: **Phase-Space Clustering Construction:**\n2379: \n2380: The construction proceeds in four steps:\n2381: \n2382: 1.  **Clustering:** Partition the alive set $\\mathcal{A}_k$ into disjoint clusters $\\{G_1, \\ldots, G_M\\}$ using complete-linkage hierarchical clustering with a maximum cluster diameter $D_{\\text{diam}}(\\epsilon) := c_d \\cdot \\epsilon$ (where $c_d > 0$ is a fixed constant, typically $c_d = 2$). Each cluster $G_m$ satisfies:\n2383: \n2384: $$\n2385: \\text{diam}(G_m) := \\max_{i,j \\in G_m} d_{\\text{alg}}(i, j) \\le D_{\\text{diam}}(\\epsilon)\n2386: $$\n2387: \n2388: where $d_{\\text{alg}}(i, j)^2 := \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2$ is the algorithmic phase-space distance.\n2389: \n2390: 2.  **Statistical Validity Constraint:** To ensure that cluster-level statistics are meaningful, we impose a minimum cluster size requirement. Let $k_{\\min} := \\max(5, \\lceil 0.05k \\rceil)$ be the minimum statistically valid cluster size. All clusters with $|G_m| < k_{\\min}$ are marked as **invalid** and their walkers are automatically included in the high-error set (as they represent statistically unreliable outlier configurations).\n2391: \n2392: 3.  **Outlier Cluster Identification:** For each valid cluster $G_m$ (with $|G_m| \\ge k_{\\min}$), compute its center of mass in phase space: $(\\mu_{x,m}, \\mu_{v,m})$. Compute the between-cluster hypocoercive variance contribution:\n2393: \n2394: $$\n2395: \\text{Contrib}(G_m) := |G_m| \\left(\\|\\mu_{x,m} - \\mu_x\\|^2 + \\lambda_v \\|\\mu_{v,m} - \\mu_v\\|^2\\right)\n2396: $$\n2397: \n2398: where $(\\mu_x, \\mu_v)$ is the global center of mass. Sort valid clusters by $\\text{Contrib}(G_m)$ in descending order, and let $O_M \\subseteq \\{1, \\ldots, M\\}$ be the smallest set of cluster indices (among valid clusters) whose cumulative contribution meets or exceeds a fraction $(1-\\varepsilon_O)$ of the total contribution from valid clusters (where $\\varepsilon_O \\in (0, 1)$ is a fixed structural parameter, typically $\\varepsilon_O = 0.1$):\n2399: \n2400: $$\n2401: \\sum_{m \\in O_M} \\text{Contrib}(G_m) \\ge (1-\\varepsilon_O) \\sum_{\\substack{m=1 \\\\ |G_m| \\ge k_{\\min}}}^M \\text{Contrib}(G_m)\n2402: $$\n2403: \n2404: 4.  **Unified High-Error Set Construction:** The unified high-error set is the union of all walkers in outlier clusters plus all walkers in invalid clusters:\n2405: \n2406: $$\n2407: H_k(\\epsilon) := \\left(\\bigcup_{m \\in O_M} G_m\\right) \\cup \\left(\\bigcup_{\\substack{m: |G_m| < k_{\\min}}} G_m\\right)\n2408: $$\n2409: \n2410: The **Unified Low-Error Set** is the complement:\n2411: \n2412: $$\n2413: L_k(\\epsilon) := \\mathcal{A}_k \\setminus H_k(\\epsilon)\n2414: $$"
    },
    {
      "directive_type": "lemma",
      "label": "lem-phase-space-packing",
      "title": "The Phase-Space Packing Lemma",
      "start_line": 2430,
      "end_line": 2448,
      "header_lines": [
        2431
      ],
      "content_start": 2433,
      "content_end": 2447,
      "content": "2433: :label: lem-phase-space-packing\n2434: \n2435: For a swarm `k` consisting of $k \\geq 2$ walkers with phase-space states $\\{(x_i, v_i)\\}_{i=1}^k$ within a compact domain, define the **total hypocoercive variance** of the swarm as:\n2436: \n2437: $$\n2438: \\mathrm{Var}_h(S_k) := \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k)\n2439: $$\n2440: \n2441: For any chosen proximity threshold $d_{\\text{close}} > 0$, let $N_{\\text{close}}$ be the number of unique pairs $(i, j)$ with $i<j$ and $d_{\\text{alg}}(i, j) < d_{\\text{close}}$, where $d_{\\text{alg}}(i, j)^2 := \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2$ is the algorithmic phase-space distance.\n2442: \n2443: The fraction of such \"close pairs in phase space\", $f_{\\text{close}} = N_{\\text{close}} / \\binom{k}{2}$, is bounded above by a function of the swarm's hypocoercive variance. Specifically, assuming $\\lambda_v \\le \\lambda_{\\text{alg}}$ and defining the phase-space diameter $D_{\\text{valid}}^2 := D_x^2 + \\lambda_{\\text{alg}} D_v^2$ where $D_x$ and $D_v$ are the spatial and velocity domain diameters, there exists a continuous, monotonically decreasing function such that:\n2444: \n2445: $$\n2446: f_{\\text{close}} \\le g(\\mathrm{Var}_h(S_k)) := \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}\n2447: $$",
      "metadata": {
        "label": "lem-phase-space-packing"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [],
      "raw_directive": "2430: Before analyzing the specific regimes of the $\\varepsilon$-dichotomy, we establish a precise, quantitative relationship between a swarm's global dispersion in phase space, as measured by its **total hypocoercive variance**, and its local phase-space clustering structure. This lemma generalizes the classical packing argument to phase space, proving that a swarm cannot be simultaneously spread out in the hypocoercive norm while being highly clustered under the algorithmic distance metric `d_alg`. This result is the foundational geometric constraint upon which both regimes of our analysis will depend.\n2431: \n2432: :::{prf:lemma} The Phase-Space Packing Lemma\n2433: :label: lem-phase-space-packing\n2434: \n2435: For a swarm `k` consisting of $k \\geq 2$ walkers with phase-space states $\\{(x_i, v_i)\\}_{i=1}^k$ within a compact domain, define the **total hypocoercive variance** of the swarm as:\n2436: \n2437: $$\n2438: \\mathrm{Var}_h(S_k) := \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k)\n2439: $$\n2440: \n2441: For any chosen proximity threshold $d_{\\text{close}} > 0$, let $N_{\\text{close}}$ be the number of unique pairs $(i, j)$ with $i<j$ and $d_{\\text{alg}}(i, j) < d_{\\text{close}}$, where $d_{\\text{alg}}(i, j)^2 := \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2$ is the algorithmic phase-space distance.\n2442: \n2443: The fraction of such \"close pairs in phase space\", $f_{\\text{close}} = N_{\\text{close}} / \\binom{k}{2}$, is bounded above by a function of the swarm's hypocoercive variance. Specifically, assuming $\\lambda_v \\le \\lambda_{\\text{alg}}$ and defining the phase-space diameter $D_{\\text{valid}}^2 := D_x^2 + \\lambda_{\\text{alg}} D_v^2$ where $D_x$ and $D_v$ are the spatial and velocity domain diameters, there exists a continuous, monotonically decreasing function such that:\n2444: \n2445: $$\n2446: f_{\\text{close}} \\le g(\\mathrm{Var}_h(S_k)) := \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}\n2447: $$\n2448: "
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-phase-space-packing",
      "title": null,
      "start_line": 2450,
      "end_line": 2565,
      "header_lines": [
        2451
      ],
      "content_start": 2452,
      "content_end": 2564,
      "content": "2452: :::{prf:proof}\n2453: :label: proof-lem-phase-space-packing\n2454: **Proof.**\n2455: \n2456: The proof generalizes the classical packing argument to phase space and proceeds in four parts. First, we establish fundamental identities relating the hypocoercive variance to sums of pairwise squared distances in both position and velocity. Second, we partition pairs by their algorithmic distance and bound the hypocoercive variance. Third, we carefully account for the potentially different velocity weighting factors $\\lambda_v$ (in the variance) and $\\lambda_{\\text{alg}}$ (in the distance). Finally, we invert the relationship to derive the desired upper bound on the fraction of close pairs.\n2457: \n2458: **Part 1: Pairwise Identities for Hypocoercive Variance**\n2459: \n2460: We begin by establishing pairwise representations for both positional and velocity variances. For the positional variance, the standard identity states:\n2461: \n2462: $$\n2463: 2k^2 \\mathrm{Var}_x(S_k) = \\sum_{i=1}^k \\sum_{j=1}^k \\|x_i - x_j\\|^2\n2464: $$\n2465: \n2466: This can be verified by expanding the right-hand side:\n2467: \n2468: $$\n2469: \\begin{aligned}\n2470: \\sum_{i,j} \\|x_i - x_j\\|^2 &= \\sum_{i,j} (\\|x_i\\|^2 - 2\\langle x_i, x_j \\rangle + \\|x_j\\|^2) \\\\\n2471: &= 2k \\sum_i \\|x_i\\|^2 - 2\\langle k\\mu_x, k\\mu_x \\rangle \\\\\n2472: &= 2k \\sum_i \\|x_i\\|^2 - 2k^2 \\|\\mu_x\\|^2 \\\\\n2473: &= 2k(k \\cdot \\mathrm{Var}_x + k\\|\\mu_x\\|^2) - 2k^2\\|\\mu_x\\|^2 = 2k^2 \\mathrm{Var}_x\n2474: \\end{aligned}\n2475: $$\n2476: \n2477: An identical derivation applies to the velocity variance:\n2478: \n2479: $$\n2480: 2k^2 \\mathrm{Var}_v(S_k) = \\sum_{i=1}^k \\sum_{j=1}^k \\|v_i - v_j\\|^2\n2481: $$\n2482: \n2483: Multiplying the velocity identity by $\\lambda_v$ and adding the two identities yields:\n2484: \n2485: $$\n2486: 2k^2 \\mathrm{Var}_h(S_k) = 2k^2 (\\mathrm{Var}_x + \\lambda_v \\mathrm{Var}_v) = \\sum_{i,j} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right)\n2487: $$\n2488: \n2489: Since the sum over all ordered pairs $(i,j)$ is twice the sum over unique pairs where $i<j$, we obtain:\n2490: \n2491: $$\n2492: \\mathrm{Var}_h(S_k) = \\frac{1}{k^2} \\sum_{i<j} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right)\n2493: $$\n2494: \n2495: **Part 2: Partitioning by Algorithmic Distance**\n2496: \n2497: We now partition the set of unique pairs into two subsets based on the algorithmic distance $d_{\\text{alg}}(i,j)^2 = \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2$:\n2498: - $P_{\\text{close}}$: the set of $N_{\\text{close}}$ pairs with $d_{\\text{alg}}(i,j) < d_{\\text{close}}$\n2499: - $P_{\\text{far}}$: the set of $N_{\\text{far}}$ pairs with $d_{\\text{alg}}(i,j) \\ge d_{\\text{close}}$\n2500: \n2501: The hypocoercive variance can be written as:\n2502: \n2503: $$\n2504: \\mathrm{Var}_h(S_k) = \\frac{1}{k^2} \\left( \\sum_{(i,j) \\in P_{\\text{close}}} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right) + \\sum_{(i,j) \\in P_{\\text{far}}} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right) \\right)\n2505: $$\n2506: \n2507: **Part 3: Bounding the Variance Terms**\n2508: \n2509: For pairs in $P_{\\text{close}}$, we have $d_{\\text{alg}}(i,j)^2 = \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2 < d_{\\text{close}}^2$. Under our assumption that $\\lambda_v \\le \\lambda_{\\text{alg}}$, we can bound:\n2510: \n2511: $$\n2512: \\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2 \\le \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2 = d_{\\text{alg}}(i,j)^2 < d_{\\text{close}}^2\n2513: $$\n2514: \n2515: For pairs in $P_{\\text{far}}$, each component is bounded by the corresponding domain diameter:\n2516: \n2517: $$\n2518: \\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2 \\le D_x^2 + \\lambda_v D_v^2 \\le D_x^2 + \\lambda_{\\text{alg}} D_v^2 = D_{\\text{valid}}^2\n2519: $$\n2520: \n2521: where we again used $\\lambda_v \\le \\lambda_{\\text{alg}}$. Therefore:\n2522: \n2523: $$\n2524: \\mathrm{Var}_h(S_k) \\le \\frac{1}{k^2} \\left( N_{\\text{close}} \\cdot d_{\\text{close}}^2 + N_{\\text{far}} \\cdot D_{\\text{valid}}^2 \\right)\n2525: $$\n2526: \n2527: Let $f_{\\text{close}} = N_{\\text{close}} / \\binom{k}{2}$ be the fraction of close pairs. Substituting $N_{\\text{close}} = f_{\\text{close}} \\binom{k}{2}$ and $N_{\\text{far}} = (1 - f_{\\text{close}}) \\binom{k}{2}$:\n2528: \n2529: $$\n2530: \\mathrm{Var}_h(S_k) \\le \\frac{\\binom{k}{2}}{k^2} \\left( f_{\\text{close}} d_{\\text{close}}^2 + (1-f_{\\text{close}})D_{\\text{valid}}^2 \\right) = \\frac{k-1}{2k} \\left( f_{\\text{close}} (d_{\\text{close}}^2 - D_{\\text{valid}}^2) + D_{\\text{valid}}^2 \\right)\n2531: $$\n2532: \n2533: **Part 4: Deriving the Upper Bound on the Fraction of Close Pairs**\n2534: \n2535: To obtain a simpler bound, we use $(k-1)/(2k) < 1/2$ for $k \\ge 2$:\n2536: \n2537: $$\n2538: \\mathrm{Var}_h(S_k) < \\frac{1}{2} \\left( f_{\\text{close}} (d_{\\text{close}}^2 - D_{\\text{valid}}^2) + D_{\\text{valid}}^2 \\right)\n2539: $$\n2540: \n2541: Solving for $f_{\\text{close}}$:\n2542: \n2543: $$\n2544: \\begin{aligned}\n2545: 2\\mathrm{Var}_h(S_k) &< f_{\\text{close}} (d_{\\text{close}}^2 - D_{\\text{valid}}^2) + D_{\\text{valid}}^2 \\\\\n2546: 2\\mathrm{Var}_h(S_k) - D_{\\text{valid}}^2 &< f_{\\text{close}}(d_{\\text{close}}^2 - D_{\\text{valid}}^2)\n2547: \\end{aligned}\n2548: $$\n2549: \n2550: Since $d_{\\text{close}} < D_{\\text{valid}}$, the term $(d_{\\text{close}}^2 - D_{\\text{valid}}^2)$ is strictly negative. Dividing by it reverses the inequality:\n2551: \n2552: $$\n2553: f_{\\text{close}} < \\frac{2\\mathrm{Var}_h(S_k) - D_{\\text{valid}}^2}{d_{\\text{close}}^2 - D_{\\text{valid}}^2} = \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}\n2554: $$\n2555: \n2556: This establishes $f_{\\text{close}} \\le g(\\mathrm{Var}_h(S_k))$ where $g(V) := (D_{\\text{valid}}^2 - 2V) / (D_{\\text{valid}}^2 - d_{\\text{close}}^2)$. As an affine function of $V$ with negative coefficient, $g(V)$ is continuous and strictly decreasing.\n2557: \n2558: Finally, we verify that $g(\\mathrm{Var}_h) < 1$ when $\\mathrm{Var}_h > d_{\\text{close}}^2 / 2$:\n2559: \n2560: $$\n2561: \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h}{D_{\\text{valid}}^2 - d_{\\text{close}}^2} < 1 \\implies D_{\\text{valid}}^2 - 2\\mathrm{Var}_h < D_{\\text{valid}}^2 - d_{\\text{close}}^2 \\implies \\mathrm{Var}_h > \\frac{d_{\\text{close}}^2}{2}\n2562: $$\n2563: \n2564: This completes the proof.",
      "metadata": {
        "label": "proof-lem-phase-space-packing"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [],
      "raw_directive": "2450: :::\n2451: \n2452: :::{prf:proof}\n2453: :label: proof-lem-phase-space-packing\n2454: **Proof.**\n2455: \n2456: The proof generalizes the classical packing argument to phase space and proceeds in four parts. First, we establish fundamental identities relating the hypocoercive variance to sums of pairwise squared distances in both position and velocity. Second, we partition pairs by their algorithmic distance and bound the hypocoercive variance. Third, we carefully account for the potentially different velocity weighting factors $\\lambda_v$ (in the variance) and $\\lambda_{\\text{alg}}$ (in the distance). Finally, we invert the relationship to derive the desired upper bound on the fraction of close pairs.\n2457: \n2458: **Part 1: Pairwise Identities for Hypocoercive Variance**\n2459: \n2460: We begin by establishing pairwise representations for both positional and velocity variances. For the positional variance, the standard identity states:\n2461: \n2462: $$\n2463: 2k^2 \\mathrm{Var}_x(S_k) = \\sum_{i=1}^k \\sum_{j=1}^k \\|x_i - x_j\\|^2\n2464: $$\n2465: \n2466: This can be verified by expanding the right-hand side:\n2467: \n2468: $$\n2469: \\begin{aligned}\n2470: \\sum_{i,j} \\|x_i - x_j\\|^2 &= \\sum_{i,j} (\\|x_i\\|^2 - 2\\langle x_i, x_j \\rangle + \\|x_j\\|^2) \\\\\n2471: &= 2k \\sum_i \\|x_i\\|^2 - 2\\langle k\\mu_x, k\\mu_x \\rangle \\\\\n2472: &= 2k \\sum_i \\|x_i\\|^2 - 2k^2 \\|\\mu_x\\|^2 \\\\\n2473: &= 2k(k \\cdot \\mathrm{Var}_x + k\\|\\mu_x\\|^2) - 2k^2\\|\\mu_x\\|^2 = 2k^2 \\mathrm{Var}_x\n2474: \\end{aligned}\n2475: $$\n2476: \n2477: An identical derivation applies to the velocity variance:\n2478: \n2479: $$\n2480: 2k^2 \\mathrm{Var}_v(S_k) = \\sum_{i=1}^k \\sum_{j=1}^k \\|v_i - v_j\\|^2\n2481: $$\n2482: \n2483: Multiplying the velocity identity by $\\lambda_v$ and adding the two identities yields:\n2484: \n2485: $$\n2486: 2k^2 \\mathrm{Var}_h(S_k) = 2k^2 (\\mathrm{Var}_x + \\lambda_v \\mathrm{Var}_v) = \\sum_{i,j} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right)\n2487: $$\n2488: \n2489: Since the sum over all ordered pairs $(i,j)$ is twice the sum over unique pairs where $i<j$, we obtain:\n2490: \n2491: $$\n2492: \\mathrm{Var}_h(S_k) = \\frac{1}{k^2} \\sum_{i<j} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right)\n2493: $$\n2494: \n2495: **Part 2: Partitioning by Algorithmic Distance**\n2496: \n2497: We now partition the set of unique pairs into two subsets based on the algorithmic distance $d_{\\text{alg}}(i,j)^2 = \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2$:\n2498: - $P_{\\text{close}}$: the set of $N_{\\text{close}}$ pairs with $d_{\\text{alg}}(i,j) < d_{\\text{close}}$\n2499: - $P_{\\text{far}}$: the set of $N_{\\text{far}}$ pairs with $d_{\\text{alg}}(i,j) \\ge d_{\\text{close}}$\n2500: \n2501: The hypocoercive variance can be written as:\n2502: \n2503: $$\n2504: \\mathrm{Var}_h(S_k) = \\frac{1}{k^2} \\left( \\sum_{(i,j) \\in P_{\\text{close}}} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right) + \\sum_{(i,j) \\in P_{\\text{far}}} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right) \\right)\n2505: $$\n2506: \n2507: **Part 3: Bounding the Variance Terms**\n2508: \n2509: For pairs in $P_{\\text{close}}$, we have $d_{\\text{alg}}(i,j)^2 = \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2 < d_{\\text{close}}^2$. Under our assumption that $\\lambda_v \\le \\lambda_{\\text{alg}}$, we can bound:\n2510: \n2511: $$\n2512: \\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2 \\le \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2 = d_{\\text{alg}}(i,j)^2 < d_{\\text{close}}^2\n2513: $$\n2514: \n2515: For pairs in $P_{\\text{far}}$, each component is bounded by the corresponding domain diameter:\n2516: \n2517: $$\n2518: \\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2 \\le D_x^2 + \\lambda_v D_v^2 \\le D_x^2 + \\lambda_{\\text{alg}} D_v^2 = D_{\\text{valid}}^2\n2519: $$\n2520: \n2521: where we again used $\\lambda_v \\le \\lambda_{\\text{alg}}$. Therefore:\n2522: \n2523: $$\n2524: \\mathrm{Var}_h(S_k) \\le \\frac{1}{k^2} \\left( N_{\\text{close}} \\cdot d_{\\text{close}}^2 + N_{\\text{far}} \\cdot D_{\\text{valid}}^2 \\right)\n2525: $$\n2526: \n2527: Let $f_{\\text{close}} = N_{\\text{close}} / \\binom{k}{2}$ be the fraction of close pairs. Substituting $N_{\\text{close}} = f_{\\text{close}} \\binom{k}{2}$ and $N_{\\text{far}} = (1 - f_{\\text{close}}) \\binom{k}{2}$:\n2528: \n2529: $$\n2530: \\mathrm{Var}_h(S_k) \\le \\frac{\\binom{k}{2}}{k^2} \\left( f_{\\text{close}} d_{\\text{close}}^2 + (1-f_{\\text{close}})D_{\\text{valid}}^2 \\right) = \\frac{k-1}{2k} \\left( f_{\\text{close}} (d_{\\text{close}}^2 - D_{\\text{valid}}^2) + D_{\\text{valid}}^2 \\right)\n2531: $$\n2532: \n2533: **Part 4: Deriving the Upper Bound on the Fraction of Close Pairs**\n2534: \n2535: To obtain a simpler bound, we use $(k-1)/(2k) < 1/2$ for $k \\ge 2$:\n2536: \n2537: $$\n2538: \\mathrm{Var}_h(S_k) < \\frac{1}{2} \\left( f_{\\text{close}} (d_{\\text{close}}^2 - D_{\\text{valid}}^2) + D_{\\text{valid}}^2 \\right)\n2539: $$\n2540: \n2541: Solving for $f_{\\text{close}}$:\n2542: \n2543: $$\n2544: \\begin{aligned}\n2545: 2\\mathrm{Var}_h(S_k) &< f_{\\text{close}} (d_{\\text{close}}^2 - D_{\\text{valid}}^2) + D_{\\text{valid}}^2 \\\\\n2546: 2\\mathrm{Var}_h(S_k) - D_{\\text{valid}}^2 &< f_{\\text{close}}(d_{\\text{close}}^2 - D_{\\text{valid}}^2)\n2547: \\end{aligned}\n2548: $$\n2549: \n2550: Since $d_{\\text{close}} < D_{\\text{valid}}$, the term $(d_{\\text{close}}^2 - D_{\\text{valid}}^2)$ is strictly negative. Dividing by it reverses the inequality:\n2551: \n2552: $$\n2553: f_{\\text{close}} < \\frac{2\\mathrm{Var}_h(S_k) - D_{\\text{valid}}^2}{d_{\\text{close}}^2 - D_{\\text{valid}}^2} = \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}\n2554: $$\n2555: \n2556: This establishes $f_{\\text{close}} \\le g(\\mathrm{Var}_h(S_k))$ where $g(V) := (D_{\\text{valid}}^2 - 2V) / (D_{\\text{valid}}^2 - d_{\\text{close}}^2)$. As an affine function of $V$ with negative coefficient, $g(V)$ is continuous and strictly decreasing.\n2557: \n2558: Finally, we verify that $g(\\mathrm{Var}_h) < 1$ when $\\mathrm{Var}_h > d_{\\text{close}}^2 / 2$:\n2559: \n2560: $$\n2561: \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h}{D_{\\text{valid}}^2 - d_{\\text{close}}^2} < 1 \\implies D_{\\text{valid}}^2 - 2\\mathrm{Var}_h < D_{\\text{valid}}^2 - d_{\\text{close}}^2 \\implies \\mathrm{Var}_h > \\frac{d_{\\text{close}}^2}{2}\n2562: $$\n2563: \n2564: This completes the proof.\n2565: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-var-x-implies-var-h",
      "title": "Positional Variance as a Lower Bound for Hypocoercive Variance",
      "start_line": 2575,
      "end_line": 2585,
      "header_lines": [
        2576
      ],
      "content_start": 2578,
      "content_end": 2584,
      "content": "2578: :label: lem-var-x-implies-var-h\n2579: \n2580: For any swarm `k`, its total hypocoercive variance is bounded below by its positional variance:\n2581: \n2582: $$\n2583: \\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k)\n2584: $$",
      "metadata": {
        "label": "lem-var-x-implies-var-h"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [],
      "raw_directive": "2575: To connect this analysis back to the positional variance component of the Lyapunov function, we first establish a simple but crucial relationship.\n2576: \n2577: :::{prf:lemma} Positional Variance as a Lower Bound for Hypocoercive Variance\n2578: :label: lem-var-x-implies-var-h\n2579: \n2580: For any swarm `k`, its total hypocoercive variance is bounded below by its positional variance:\n2581: \n2582: $$\n2583: \\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k)\n2584: $$\n2585: "
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-var-x-implies-var-h",
      "title": null,
      "start_line": 2587,
      "end_line": 2606,
      "header_lines": [
        2588
      ],
      "content_start": 2589,
      "content_end": 2605,
      "content": "2589: :::{prf:proof}\n2590: :label: proof-lem-var-x-implies-var-h\n2591: **Proof.**\n2592: \n2593: By definition, the hypocoercive variance is:\n2594: \n2595: $$\n2596: \\mathrm{Var}_h(S_k) := \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k)\n2597: $$\n2598: \n2599: Since $\\lambda_v > 0$ is a positive hypocoercive parameter and $\\mathrm{Var}_v(S_k) \\ge 0$ (variance is non-negative), we immediately have:\n2600: \n2601: $$\n2602: \\mathrm{Var}_h(S_k) = \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k) \\ge \\mathrm{Var}_x(S_k)\n2603: $$\n2604: \n2605: The second claim follows directly: if $\\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$, then by the above inequality, $\\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$.",
      "metadata": {
        "label": "proof-lem-var-x-implies-var-h"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [],
      "raw_directive": "2587: :::\n2588: \n2589: :::{prf:proof}\n2590: :label: proof-lem-var-x-implies-var-h\n2591: **Proof.**\n2592: \n2593: By definition, the hypocoercive variance is:\n2594: \n2595: $$\n2596: \\mathrm{Var}_h(S_k) := \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k)\n2597: $$\n2598: \n2599: Since $\\lambda_v > 0$ is a positive hypocoercive parameter and $\\mathrm{Var}_v(S_k) \\ge 0$ (variance is non-negative), we immediately have:\n2600: \n2601: $$\n2602: \\mathrm{Var}_h(S_k) = \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k) \\ge \\mathrm{Var}_x(S_k)\n2603: $$\n2604: \n2605: The second claim follows directly: if $\\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$, then by the above inequality, $\\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$.\n2606: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-outlier-fraction-lower-bound",
      "title": "N-Uniform Lower Bound on the Outlier Fraction",
      "start_line": 2610,
      "end_line": 2622,
      "header_lines": [
        2611
      ],
      "content_start": 2613,
      "content_end": 2621,
      "content": "2613: :label: lem-outlier-fraction-lower-bound\n2614: \n2615: Let $O_k$ be the **global kinematic outlier set** for a swarm `k` with `k >= 2` alive walkers, as defined in Section 6.3, with structural parameter $\\varepsilon_O \\in (0, 1)$.\n2616: \n2617: If the swarm's internal hypocoercive variance is large, such that $\\mathrm{Var}_h(S_k) > R^2_h$ for some threshold $R^2_h > 0$, then the fraction of *alive* walkers in the outlier set is bounded below by a positive constant that is independent of `N`. Specifically:\n2618: \n2619: $$\n2620: \\frac{|O_k|}{k} \\ge \\frac{(1-\\varepsilon_O) R^2_h}{D_h^2} =: f_O > 0\n2621: $$",
      "metadata": {
        "label": "lem-outlier-fraction-lower-bound"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [],
      "raw_directive": "2610: This lemma establishes the crucial bridge: the analysis in Section 6.2 guaranteed a large positional variance `Var_x`, and this lemma proves that such a condition is sufficient to guarantee a large hypocoercive variance `Var_h`, which is the relevant measure for the phase-space outlier set `O_k`. We can now proceed with the main result.\n2611: \n2612: :::{prf:lemma} N-Uniform Lower Bound on the Outlier Fraction\n2613: :label: lem-outlier-fraction-lower-bound\n2614: \n2615: Let $O_k$ be the **global kinematic outlier set** for a swarm `k` with `k >= 2` alive walkers, as defined in Section 6.3, with structural parameter $\\varepsilon_O \\in (0, 1)$.\n2616: \n2617: If the swarm's internal hypocoercive variance is large, such that $\\mathrm{Var}_h(S_k) > R^2_h$ for some threshold $R^2_h > 0$, then the fraction of *alive* walkers in the outlier set is bounded below by a positive constant that is independent of `N`. Specifically:\n2618: \n2619: $$\n2620: \\frac{|O_k|}{k} \\ge \\frac{(1-\\varepsilon_O) R^2_h}{D_h^2} =: f_O > 0\n2621: $$\n2622: "
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-outlier-fraction-lower-bound",
      "title": null,
      "start_line": 2623,
      "end_line": 2691,
      "header_lines": [
        2624
      ],
      "content_start": 2626,
      "content_end": 2690,
      "content": "2626: :label: proof-lem-outlier-fraction-lower-bound\n2627: \n2628: **Proof.**\n2629: \n2630: The proof establishes the lower bound by relating the total hypocoercive variance of the swarm to the maximum possible contribution of any single walker in phase space, which is a fixed geometric property of the environment.\n2631: \n2632: **1. Recall Definitions and Outlier Set Property:**\n2633: *   The sum of squared hypocoercive norms of the centered phase-space vectors for the `k` alive walkers is:\n2634: \n2635: \n2636: $$\n2637: T_k = \\sum_{j \\in \\mathcal{A}_k} \\left(\\|\\delta_{x,k,j}\\|^2 + \\lambda_v \\|\\delta_{v,k,j}\\|^2\\right) = k \\cdot \\mathrm{Var}_h(S_k)\n2638: $$\n2639: \n2640: *   By the definition of the global kinematic outlier set $O_k$ (Section 6.3), the sum of squared hypocoercive norms over this subset is bounded below by a fixed fraction of the total sum:\n2641: \n2642: \n2643: $$\n2644: \\sum_{i \\in O_k} \\left(\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2\\right) \\ge (1-\\varepsilon_O) T_k = (1-\\varepsilon_O) k \\cdot \\mathrm{Var}_h(S_k)\n2645: $$\n2646: \n2647: **2. Establish a Uniform Upper Bound on Single-Walker Contribution:**\n2648: *   For any single alive walker `i`, its centered phase-space state is $(\\delta_{x,k,i}, \\delta_{v,k,i}) = (x_{k,i} - \\mu_{x,k}, v_{k,i} - \\mu_{v,k})$.\n2649: *   The walker's position $x_{k,i}$ must lie within the valid domain $\\mathcal{X}_{\\text{valid}}$. If $\\mathcal{X}_{\\text{valid}}$ is convex (a standard assumption), the center of mass $\\mu_{x,k}$ must also lie within $\\mathcal{X}_{\\text{valid}}$. Therefore, $\\|\\delta_{x,k,i}\\| \\le D_x$, where $D_x$ is the positional domain diameter.\n2650: *   Similarly, the velocity $v_{k,i}$ is bounded by the velocity domain diameter: $\\|\\delta_{v,k,i}\\| \\le D_v$.\n2651: *   Therefore, the squared hypocoercive norm of any centered phase-space vector is uniformly bounded:\n2652: \n2653: \n2654: $$\n2655: \\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2 \\le D_x^2 + \\lambda_v D_v^2 = D_h^2\n2656: $$\n2657: \n2658:     This bound is a geometric property of the environment and is independent of the number of walkers `N` or `k`.\n2659: \n2660: **3. Bound the Sum over the Outlier Set:**\n2661: *   The sum of squared hypocoercive norms over the outlier set can also be bounded above by multiplying the number of walkers in the set, $|O_k|$, by the maximum possible value of any single term:\n2662: \n2663: \n2664: $$\n2665: \\sum_{i \\in O_k} \\left(\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2\\right) \\le |O_k| \\cdot \\sup_{j \\in O_k} \\left(\\|\\delta_{x,k,j}\\|^2 + \\lambda_v \\|\\delta_{v,k,j}\\|^2\\right) \\le |O_k| \\cdot D_h^2\n2666: $$\n2667: \n2668: **4. Combine Bounds and Finalize:**\n2669: *   We now have both a lower and an upper bound for the same quantity. Combining them yields:\n2670: \n2671: \n2672: $$\n2673: (1-\\varepsilon_O) k \\cdot \\mathrm{Var}_h(S_k) \\le \\sum_{i \\in O_k} \\left(\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2\\right) \\le |O_k| \\cdot D_h^2\n2674: $$\n2675: \n2676: *   We are given the premise that the hypocoercive variance is large: $\\mathrm{Var}_h(S_k) > R^2_h$. Substituting this into the left-hand side gives:\n2677: \n2678: \n2679: $$\n2680: (1-\\varepsilon_O) k \\cdot R^2_h < |O_k| \\cdot D_h^2\n2681: $$\n2682: \n2683: *   Rearranging to find a bound on the fraction of outliers relative to the number of *alive* walkers `k`, we get:\n2684: \n2685: \n2686: $$\n2687: \\frac{|O_k|}{k} > \\frac{(1-\\varepsilon_O) R^2_h}{D_h^2}\n2688: $$\n2689: \n2690: *   The resulting lower bound, $f_O := (1-\\varepsilon_O) R^2_h / D_h^2$, is a positive constant constructed entirely from `N`-independent parameters. This completes the proof that a large hypocoercive variance guarantees a non-vanishing fraction of global phase-space outliers among the alive population.",
      "metadata": {
        "label": "proof-lem-outlier-fraction-lower-bound"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [],
      "raw_directive": "2623: where $D_h^2 := D_x^2 + \\lambda_v D_v^2$ is the squared **hypocoercive diameter** of the valid domain, with $D_x := \\sup_{x_1, x_2 \\in \\mathcal{X}_{\\text{valid}}} \\|x_1 - x_2\\|$ being the positional domain diameter and $D_v$ being the velocity domain diameter.\n2624: :::\n2625: :::{prf:proof}\n2626: :label: proof-lem-outlier-fraction-lower-bound\n2627: \n2628: **Proof.**\n2629: \n2630: The proof establishes the lower bound by relating the total hypocoercive variance of the swarm to the maximum possible contribution of any single walker in phase space, which is a fixed geometric property of the environment.\n2631: \n2632: **1. Recall Definitions and Outlier Set Property:**\n2633: *   The sum of squared hypocoercive norms of the centered phase-space vectors for the `k` alive walkers is:\n2634: \n2635: \n2636: $$\n2637: T_k = \\sum_{j \\in \\mathcal{A}_k} \\left(\\|\\delta_{x,k,j}\\|^2 + \\lambda_v \\|\\delta_{v,k,j}\\|^2\\right) = k \\cdot \\mathrm{Var}_h(S_k)\n2638: $$\n2639: \n2640: *   By the definition of the global kinematic outlier set $O_k$ (Section 6.3), the sum of squared hypocoercive norms over this subset is bounded below by a fixed fraction of the total sum:\n2641: \n2642: \n2643: $$\n2644: \\sum_{i \\in O_k} \\left(\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2\\right) \\ge (1-\\varepsilon_O) T_k = (1-\\varepsilon_O) k \\cdot \\mathrm{Var}_h(S_k)\n2645: $$\n2646: \n2647: **2. Establish a Uniform Upper Bound on Single-Walker Contribution:**\n2648: *   For any single alive walker `i`, its centered phase-space state is $(\\delta_{x,k,i}, \\delta_{v,k,i}) = (x_{k,i} - \\mu_{x,k}, v_{k,i} - \\mu_{v,k})$.\n2649: *   The walker's position $x_{k,i}$ must lie within the valid domain $\\mathcal{X}_{\\text{valid}}$. If $\\mathcal{X}_{\\text{valid}}$ is convex (a standard assumption), the center of mass $\\mu_{x,k}$ must also lie within $\\mathcal{X}_{\\text{valid}}$. Therefore, $\\|\\delta_{x,k,i}\\| \\le D_x$, where $D_x$ is the positional domain diameter.\n2650: *   Similarly, the velocity $v_{k,i}$ is bounded by the velocity domain diameter: $\\|\\delta_{v,k,i}\\| \\le D_v$.\n2651: *   Therefore, the squared hypocoercive norm of any centered phase-space vector is uniformly bounded:\n2652: \n2653: \n2654: $$\n2655: \\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2 \\le D_x^2 + \\lambda_v D_v^2 = D_h^2\n2656: $$\n2657: \n2658:     This bound is a geometric property of the environment and is independent of the number of walkers `N` or `k`.\n2659: \n2660: **3. Bound the Sum over the Outlier Set:**\n2661: *   The sum of squared hypocoercive norms over the outlier set can also be bounded above by multiplying the number of walkers in the set, $|O_k|$, by the maximum possible value of any single term:\n2662: \n2663: \n2664: $$\n2665: \\sum_{i \\in O_k} \\left(\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2\\right) \\le |O_k| \\cdot \\sup_{j \\in O_k} \\left(\\|\\delta_{x,k,j}\\|^2 + \\lambda_v \\|\\delta_{v,k,j}\\|^2\\right) \\le |O_k| \\cdot D_h^2\n2666: $$\n2667: \n2668: **4. Combine Bounds and Finalize:**\n2669: *   We now have both a lower and an upper bound for the same quantity. Combining them yields:\n2670: \n2671: \n2672: $$\n2673: (1-\\varepsilon_O) k \\cdot \\mathrm{Var}_h(S_k) \\le \\sum_{i \\in O_k} \\left(\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2\\right) \\le |O_k| \\cdot D_h^2\n2674: $$\n2675: \n2676: *   We are given the premise that the hypocoercive variance is large: $\\mathrm{Var}_h(S_k) > R^2_h$. Substituting this into the left-hand side gives:\n2677: \n2678: \n2679: $$\n2680: (1-\\varepsilon_O) k \\cdot R^2_h < |O_k| \\cdot D_h^2\n2681: $$\n2682: \n2683: *   Rearranging to find a bound on the fraction of outliers relative to the number of *alive* walkers `k`, we get:\n2684: \n2685: \n2686: $$\n2687: \\frac{|O_k|}{k} > \\frac{(1-\\varepsilon_O) R^2_h}{D_h^2}\n2688: $$\n2689: \n2690: *   The resulting lower bound, $f_O := (1-\\varepsilon_O) R^2_h / D_h^2$, is a positive constant constructed entirely from `N`-independent parameters. This completes the proof that a large hypocoercive variance guarantees a non-vanishing fraction of global phase-space outliers among the alive population.\n2691: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-outlier-cluster-fraction-lower-bound",
      "title": "N-Uniform Lower Bound on the Outlier-Cluster Fraction",
      "start_line": 2716,
      "end_line": 2729,
      "header_lines": [
        2717
      ],
      "content_start": 2719,
      "content_end": 2728,
      "content": "2719: :label: lem-outlier-cluster-fraction-lower-bound\n2720: \n2721: Let the high-error set $H_k(\\varepsilon)$ be defined via the phase-space clustering-based approach (as $C_k(\\varepsilon)$ in {prf:ref}`def-unified-high-low-error-sets`) for the local-interaction regime, with maximum cluster diameter $D_diam(\\varepsilon) = c_d \u00b7 \\varepsilon$ where $c_d > 0$ is a fixed constant.\n2722: \n2723: For any choice of $c_d$ and variance threshold $R^2_{\\text{var}}$ satisfying $c_d \u00b7 \\epsilon < 2\\sqrt{R^2_{\\text{var}}}$, there exists a positive constant $f_H(\\epsilon) > 0$, independent of `N` and `k`, such that:\n2724: \n2725: If the swarm's internal positional variance is large, $\\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$, then the fraction of *alive* walkers in the high-error set is bounded below:\n2726: \n2727: $$\n2728: \\frac{|H_k(\\epsilon)|}{k} \\ge f_H(\\epsilon) > 0",
      "metadata": {
        "label": "lem-outlier-cluster-fraction-lower-bound"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [
        "def-unified-high-low-error-sets"
      ],
      "raw_directive": "2716: With the clustering-based definition from Section 6.3 established, we can now prove the main result for this regime.\n2717: \n2718: :::{prf:lemma} N-Uniform Lower Bound on the Outlier-Cluster Fraction\n2719: :label: lem-outlier-cluster-fraction-lower-bound\n2720: \n2721: Let the high-error set $H_k(\\varepsilon)$ be defined via the phase-space clustering-based approach (as $C_k(\\varepsilon)$ in {prf:ref}`def-unified-high-low-error-sets`) for the local-interaction regime, with maximum cluster diameter $D_diam(\\varepsilon) = c_d \u00b7 \\varepsilon$ where $c_d > 0$ is a fixed constant.\n2722: \n2723: For any choice of $c_d$ and variance threshold $R^2_{\\text{var}}$ satisfying $c_d \u00b7 \\epsilon < 2\\sqrt{R^2_{\\text{var}}}$, there exists a positive constant $f_H(\\epsilon) > 0$, independent of `N` and `k`, such that:\n2724: \n2725: If the swarm's internal positional variance is large, $\\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$, then the fraction of *alive* walkers in the high-error set is bounded below:\n2726: \n2727: $$\n2728: \\frac{|H_k(\\epsilon)|}{k} \\ge f_H(\\epsilon) > 0\n2729: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-outlier-cluster-fraction-lower-bound",
      "title": null,
      "start_line": 2730,
      "end_line": 2798,
      "header_lines": [
        2731
      ],
      "content_start": 2733,
      "content_end": 2797,
      "content": "2733: :label: proof-lem-outlier-cluster-fraction-lower-bound\n2734: \n2735: **Proof.**\n2736: \n2737: The proof is constructive. We use the Law of Total Variance to show that a large global variance forces a large variance *between* the cluster centers. We then apply the same logic used in the mean-field regime ({prf:ref}`lem-outlier-fraction-lower-bound`) to this set of cluster centers to prove that a non-vanishing fraction of the population must reside in these outlier clusters.\n2738: \n2739: **1. Decomposing the Total Variance.**\n2740: The Law of Total Variance provides an exact identity for the swarm's variance based on the cluster partition `{G_1, ..., G_M}`. Let $\\mu$ be the global center of mass of the `k` alive walkers, $\\mu_m$ be the center of mass of cluster `G_m`, and `|G_m|` be the number of walkers in it. The total sum of squared deviations can be decomposed as:\n2741: \n2742: $$\n2743: k \\cdot \\mathrm{Var}_k(x) = \\sum_{m=1}^M \\sum_{i \\in G_m} \\|x_i - \\mu\\|^2 = \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) + \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n2744: $$\n2745: \n2746: The first term is the \"within-cluster\" sum of squares, and the second is the size-weighted \"between-cluster\" sum of squares.\n2747: \n2748: **2. A Uniform Upper Bound on the Within-Cluster Variance.**\n2749: By the definition of our clustering algorithm, the diameter of any cluster `G_m` is at most $D_diam(\\varepsilon)$. The maximum possible internal variance for any set of points with a given diameter is achieved when the points are at the extremes of an interval, which gives $\\text{Var}(G_m) \\leq (D_diam(\\varepsilon)/2)^{2}$. This provides a uniform, N-independent upper bound for the within-cluster variance of any cluster.\n2750: The total within-cluster sum of squares is therefore bounded:\n2751: \n2752: $$\n2753: \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) \\le \\sum_{m=1}^M |G_m| \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2 = k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2\n2754: $$\n2755: \n2756: **3. A Uniform Lower Bound on the Between-Cluster Variance.**\n2757: We can now find a lower bound for the between-cluster sum of squares. Rearranging the identity from Step 1 and using our premise `Var_k(x) > R^{2}_var`:\n2758: \n2759: $$\n2760: \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 = k \\cdot \\mathrm{Var}_k(x) - \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) > k \\cdot R^2_{\\mathrm{var}} - k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2\n2761: $$\n2762: \n2763: Let's define a new positive, N-uniform constant $R^{2}_means := R^{2}_var - (D_diam(\\varepsilon)/2)^{2}$. The premise of this lemma requires that we choose $D_diam(\\varepsilon)$ small enough to ensure `R^{2}_means > 0`. With this, we have a guaranteed lower bound on the size-weighted variance of the cluster means:\n2764: \n2765: $$\n2766: \\frac{1}{k}\\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 > R^2_{\\mathrm{means}} > 0\n2767: $$\n2768: \n2769: **4. Applying the Outlier Argument to the Cluster Centers.**\n2770: We have now reduced the problem to one that is formally identical to the mean-field case. We have a set of `M` \"meta-particles\" (the cluster centers $\\mu_m$) with associated weights (`|G_m|`) whose size-weighted variance is guaranteed to be large.\n2771: \n2772: By the definition of the high-error set $H_k(\\varepsilon)$, it is the union of all walkers in the \"outlier clusters\" `O_M`. These are the clusters whose weighted contribution to the between-cluster variance sums to at least $(1-\\varepsilon_O)$ of the total.\n2773: \n2774: $$\n2775: \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 > (1-\\varepsilon_O) k \\cdot R^2_{\\mathrm{means}}\n2776: $$\n2777: \n2778: At the same time, we can find an upper bound for this sum. The maximum squared distance of any cluster mean from the global mean is bounded by `D_valid^{2}`.\n2779: \n2780: $$\n2781: \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\le \\sum_{m \\in O_M} |G_m|D_{\\mathrm{valid}}^2 = D_{\\mathrm{valid}}^2 \\sum_{m \\in O_M} |G_m|\n2782: $$\n2783: \n2784: The term $\\Sigma_{m\\inO_M} |G_m|$ is, by definition, the total number of walkers in the high-error set, $|H_k(\\varepsilon)|$. Combining the inequalities:\n2785: \n2786: $$\n2787: (1-\\varepsilon_O) k \\cdot R^2_{\\mathrm{means}} < |H_k(\\epsilon)| \\cdot D_{\\mathrm{valid}}^2\n2788: $$\n2789: \n2790: **5. Conclusion.**\n2791: Rearranging the final inequality gives the desired N-uniform lower bound on the high-error fraction:\n2792: \n2793: $$\n2794: \\frac{|H_k(\\epsilon)|}{k} > \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{D_{\\mathrm{valid}}^2} = \\frac{(1-\\varepsilon_O) \\left(R^2_{\\mathrm{var}} - (D_{\\mathrm{diam}}(\\epsilon)/2)^2\\right)}{D_{\\mathrm{valid}}^2}\n2795: $$\n2796: \n2797: We define the right-hand side as our N-uniform constant $f_H(\\varepsilon)$. It is strictly positive by our choice of $D_diam(\\varepsilon)$, and it is constructed entirely from N-independent system parameters ($\\varepsilon_O$, `R^{2}_var`, `D_diam`, `D_valid`). This completes the N-uniform proof.",
      "metadata": {
        "label": "proof-lem-outlier-cluster-fraction-lower-bound"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [
        "lem-outlier-fraction-lower-bound"
      ],
      "raw_directive": "2730: \n2731: :::\n2732: :::{prf:proof}\n2733: :label: proof-lem-outlier-cluster-fraction-lower-bound\n2734: \n2735: **Proof.**\n2736: \n2737: The proof is constructive. We use the Law of Total Variance to show that a large global variance forces a large variance *between* the cluster centers. We then apply the same logic used in the mean-field regime ({prf:ref}`lem-outlier-fraction-lower-bound`) to this set of cluster centers to prove that a non-vanishing fraction of the population must reside in these outlier clusters.\n2738: \n2739: **1. Decomposing the Total Variance.**\n2740: The Law of Total Variance provides an exact identity for the swarm's variance based on the cluster partition `{G_1, ..., G_M}`. Let $\\mu$ be the global center of mass of the `k` alive walkers, $\\mu_m$ be the center of mass of cluster `G_m`, and `|G_m|` be the number of walkers in it. The total sum of squared deviations can be decomposed as:\n2741: \n2742: $$\n2743: k \\cdot \\mathrm{Var}_k(x) = \\sum_{m=1}^M \\sum_{i \\in G_m} \\|x_i - \\mu\\|^2 = \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) + \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n2744: $$\n2745: \n2746: The first term is the \"within-cluster\" sum of squares, and the second is the size-weighted \"between-cluster\" sum of squares.\n2747: \n2748: **2. A Uniform Upper Bound on the Within-Cluster Variance.**\n2749: By the definition of our clustering algorithm, the diameter of any cluster `G_m` is at most $D_diam(\\varepsilon)$. The maximum possible internal variance for any set of points with a given diameter is achieved when the points are at the extremes of an interval, which gives $\\text{Var}(G_m) \\leq (D_diam(\\varepsilon)/2)^{2}$. This provides a uniform, N-independent upper bound for the within-cluster variance of any cluster.\n2750: The total within-cluster sum of squares is therefore bounded:\n2751: \n2752: $$\n2753: \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) \\le \\sum_{m=1}^M |G_m| \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2 = k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2\n2754: $$\n2755: \n2756: **3. A Uniform Lower Bound on the Between-Cluster Variance.**\n2757: We can now find a lower bound for the between-cluster sum of squares. Rearranging the identity from Step 1 and using our premise `Var_k(x) > R^{2}_var`:\n2758: \n2759: $$\n2760: \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 = k \\cdot \\mathrm{Var}_k(x) - \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) > k \\cdot R^2_{\\mathrm{var}} - k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2\n2761: $$\n2762: \n2763: Let's define a new positive, N-uniform constant $R^{2}_means := R^{2}_var - (D_diam(\\varepsilon)/2)^{2}$. The premise of this lemma requires that we choose $D_diam(\\varepsilon)$ small enough to ensure `R^{2}_means > 0`. With this, we have a guaranteed lower bound on the size-weighted variance of the cluster means:\n2764: \n2765: $$\n2766: \\frac{1}{k}\\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 > R^2_{\\mathrm{means}} > 0\n2767: $$\n2768: \n2769: **4. Applying the Outlier Argument to the Cluster Centers.**\n2770: We have now reduced the problem to one that is formally identical to the mean-field case. We have a set of `M` \"meta-particles\" (the cluster centers $\\mu_m$) with associated weights (`|G_m|`) whose size-weighted variance is guaranteed to be large.\n2771: \n2772: By the definition of the high-error set $H_k(\\varepsilon)$, it is the union of all walkers in the \"outlier clusters\" `O_M`. These are the clusters whose weighted contribution to the between-cluster variance sums to at least $(1-\\varepsilon_O)$ of the total.\n2773: \n2774: $$\n2775: \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 > (1-\\varepsilon_O) k \\cdot R^2_{\\mathrm{means}}\n2776: $$\n2777: \n2778: At the same time, we can find an upper bound for this sum. The maximum squared distance of any cluster mean from the global mean is bounded by `D_valid^{2}`.\n2779: \n2780: $$\n2781: \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\le \\sum_{m \\in O_M} |G_m|D_{\\mathrm{valid}}^2 = D_{\\mathrm{valid}}^2 \\sum_{m \\in O_M} |G_m|\n2782: $$\n2783: \n2784: The term $\\Sigma_{m\\inO_M} |G_m|$ is, by definition, the total number of walkers in the high-error set, $|H_k(\\varepsilon)|$. Combining the inequalities:\n2785: \n2786: $$\n2787: (1-\\varepsilon_O) k \\cdot R^2_{\\mathrm{means}} < |H_k(\\epsilon)| \\cdot D_{\\mathrm{valid}}^2\n2788: $$\n2789: \n2790: **5. Conclusion.**\n2791: Rearranging the final inequality gives the desired N-uniform lower bound on the high-error fraction:\n2792: \n2793: $$\n2794: \\frac{|H_k(\\epsilon)|}{k} > \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{D_{\\mathrm{valid}}^2} = \\frac{(1-\\varepsilon_O) \\left(R^2_{\\mathrm{var}} - (D_{\\mathrm{diam}}(\\epsilon)/2)^2\\right)}{D_{\\mathrm{valid}}^2}\n2795: $$\n2796: \n2797: We define the right-hand side as our N-uniform constant $f_H(\\varepsilon)$. It is strictly positive by our choice of $D_diam(\\varepsilon)$, and it is constructed entirely from N-independent system parameters ($\\varepsilon_O$, `R^{2}_var`, `D_diam`, `D_valid`). This completes the N-uniform proof.\n2798: "
    },
    {
      "directive_type": "corollary",
      "label": "cor-vvarx-to-high-error-fraction",
      "title": "A Large Intra-Swarm Positional Variance Guarantees a Non-Vanishing High-Error Fraction",
      "start_line": 2806,
      "end_line": 2817,
      "header_lines": [
        2807
      ],
      "content_start": 2809,
      "content_end": 2816,
      "content": "2809: :label: cor-vvarx-to-high-error-fraction\n2810: \n2811: For any fixed interaction range $\\varepsilon > 0$, there exists a positional variance threshold $R^2_{\\text{total\\_var},x} > 0$ and a corresponding N-uniform constant $f_H(\\epsilon) > 0$ such that:\n2812: \n2813: If the total intra-swarm positional variance is large, $V_{\\text{Var},x} > R^2_{\\text{total\\_var},x}$, then the fraction of *alive* walkers in the unified high-error set of at least one of the swarms, $k \\in {1, 2}$, is bounded below:\n2814: \n2815: $$\n2816: \\frac{|H_k(\\epsilon)|}{k} \\ge f_H(\\epsilon) > 0",
      "metadata": {
        "label": "cor-vvarx-to-high-error-fraction"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [],
      "raw_directive": "2806: This corollary provides the final, synthesized result of our geometric analysis. It proves that a large **total intra-swarm positional variance ($V_{\\text{Var},x}$)** is sufficient to guarantee that a non-vanishing fraction of at least one of the swarms belongs to this high-error set, providing the clean, unified input required for the subsequent analysis. This directly aligns with the Keystone Principle's central thesis: the cloning operator contracts the positional variance component of the Lyapunov function.\n2807: \n2808: :::{prf:corollary} A Large Intra-Swarm Positional Variance Guarantees a Non-Vanishing High-Error Fraction\n2809: :label: cor-vvarx-to-high-error-fraction\n2810: \n2811: For any fixed interaction range $\\varepsilon > 0$, there exists a positional variance threshold $R^2_{\\text{total\\_var},x} > 0$ and a corresponding N-uniform constant $f_H(\\epsilon) > 0$ such that:\n2812: \n2813: If the total intra-swarm positional variance is large, $V_{\\text{Var},x} > R^2_{\\text{total\\_var},x}$, then the fraction of *alive* walkers in the unified high-error set of at least one of the swarms, $k \\in {1, 2}$, is bounded below:\n2814: \n2815: $$\n2816: \\frac{|H_k(\\epsilon)|}{k} \\ge f_H(\\epsilon) > 0\n2817: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-cor-vvarx-to-high-error-fraction",
      "title": null,
      "start_line": 2818,
      "end_line": 2863,
      "header_lines": [
        2819
      ],
      "content_start": 2821,
      "content_end": 2862,
      "content": "2821: :label: proof-cor-vvarx-to-high-error-fraction\n2822: \n2823: **Proof.**\n2824: \n2825: This corollary is a direct synthesis of the lemmas established in this chapter.\n2826: \n2827: **1. From Total Positional Variance to Single-Swarm Positional Variance:**\n2828: By **{prf:ref}`lem-V_Varx-implies-variance`** (labeled $lem-V_{\\text{Var}}x-implies-variance$), if the total intra-swarm positional variance is large, $V_{\\text{Var},x} > R^2_{\\text{total\\_var},x}$, then at least one of the two swarms, say swarm `k`, must have a large internal positional variance:\n2829: \n2830: $$\n2831: \\mathrm{Var}_x(S_k) > \\frac{R^2_{\\text{total\\_var},x}}{2}\n2832: $$\n2833: \n2834: We define the threshold $R^2_{\\text{var}} := R^2_{\\text{total\\_var},x} / 2$.\n2835: \n2836: **2. From Positional Variance to Hypocoercive Variance:**\n2837: Since the hypocoercive variance satisfies $\\mathrm{Var}_h(S_k) = \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k) \\ge \\mathrm{Var}_x(S_k)$ (as established in the bridging lemma of Section 6.4.2), the condition $\\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$ is sufficient to guarantee that the total hypocoercive variance is also large:\n2838: \n2839: $$\n2840: \\mathrm{Var}_h(S_k) > R^2_{\\text{var}}\n2841: $$\n2842: \n2843: This satisfies the necessary premise for the lemmas governing both regimes of the $\\varepsilon$-dichotomy.\n2844: \n2845: **3. From Hypocoercive Variance to a High-Error Fraction:**\n2846: With the condition $\\mathrm{Var}_h(S_k) > R^2_{\\text{var}}$ met, we can now invoke the results of the $\\varepsilon$-dichotomy analysis:\n2847: \n2848: *   **If the swarm is in the large-$\\varepsilon$ regime** (where $\\varepsilon > D_swarm$): By {prf:ref}`def-unified-high-low-error-sets`, $H_k(\\epsilon) = O_k$ in this regime. **{prf:ref}`lem-outlier-fraction-lower-bound`** guarantees that the fraction of walkers in the global kinematic outlier set is bounded below by a positive, N-uniform constant: $|H_k(\\epsilon)|/k \\ge f_O > 0$.\n2849: \n2850: *   **If the swarm is in the small-$\\varepsilon$ regime** (where $\\varepsilon \\leq D_swarm$): By {prf:ref}`def-unified-high-low-error-sets`, $H_k(\\epsilon) = C_k(\\epsilon)$ (the clustering-based outlier set) in this regime. **{prf:ref}`lem-outlier-cluster-fraction-lower-bound`** guarantees that the fraction of walkers in the outlier clusters is bounded below by a positive, N-uniform constant: $|H_k(\\epsilon)|/k \\ge f_{H,\\text{cluster}}(\\epsilon) > 0$.\n2851: \n2852: **4. Define the Unified Lower Bound:**\n2853: We can define a single, unified lower bound $f_H(\\epsilon)$ that is valid for all regimes by taking the minimum of the bounds from the two cases:\n2854: \n2855: $$\n2856: f_H(\\epsilon) := \\min(f_O, f_{H,\\text{cluster}}(\\epsilon))\n2857: $$\n2858: \n2859: Since both $f_O$ and $f_{H,\\text{cluster}}(\\epsilon)$ are strictly positive, N-uniform constants, their minimum $f_H(\\epsilon)$ is also a strictly positive, N-uniform constant.\n2860: \n2861: **5. Conclusion:**\n2862: We have rigorously shown that for any $\\varepsilon > 0$, if the total intra-swarm positional variance $V_{\\text{Var},x}$ is sufficiently large, then at least one swarm `k` is guaranteed to have a large hypocoercive variance, which in turn guarantees that the fraction of alive walkers in its unified high-error set $H_k(\\epsilon)$ is bounded below by the positive, N-uniform constant $f_H(\\epsilon)$. This establishes the direct causal link from the Lyapunov function's positional variance component to the guaranteed existence of a substantial high-error population.",
      "metadata": {
        "label": "proof-cor-vvarx-to-high-error-fraction"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [
        "lem-V_Varx-implies-variance",
        "def-unified-high-low-error-sets",
        "lem-outlier-fraction-lower-bound",
        "lem-outlier-cluster-fraction-lower-bound"
      ],
      "raw_directive": "2818: \n2819: :::\n2820: :::{prf:proof}\n2821: :label: proof-cor-vvarx-to-high-error-fraction\n2822: \n2823: **Proof.**\n2824: \n2825: This corollary is a direct synthesis of the lemmas established in this chapter.\n2826: \n2827: **1. From Total Positional Variance to Single-Swarm Positional Variance:**\n2828: By **{prf:ref}`lem-V_Varx-implies-variance`** (labeled $lem-V_{\\text{Var}}x-implies-variance$), if the total intra-swarm positional variance is large, $V_{\\text{Var},x} > R^2_{\\text{total\\_var},x}$, then at least one of the two swarms, say swarm `k`, must have a large internal positional variance:\n2829: \n2830: $$\n2831: \\mathrm{Var}_x(S_k) > \\frac{R^2_{\\text{total\\_var},x}}{2}\n2832: $$\n2833: \n2834: We define the threshold $R^2_{\\text{var}} := R^2_{\\text{total\\_var},x} / 2$.\n2835: \n2836: **2. From Positional Variance to Hypocoercive Variance:**\n2837: Since the hypocoercive variance satisfies $\\mathrm{Var}_h(S_k) = \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k) \\ge \\mathrm{Var}_x(S_k)$ (as established in the bridging lemma of Section 6.4.2), the condition $\\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$ is sufficient to guarantee that the total hypocoercive variance is also large:\n2838: \n2839: $$\n2840: \\mathrm{Var}_h(S_k) > R^2_{\\text{var}}\n2841: $$\n2842: \n2843: This satisfies the necessary premise for the lemmas governing both regimes of the $\\varepsilon$-dichotomy.\n2844: \n2845: **3. From Hypocoercive Variance to a High-Error Fraction:**\n2846: With the condition $\\mathrm{Var}_h(S_k) > R^2_{\\text{var}}$ met, we can now invoke the results of the $\\varepsilon$-dichotomy analysis:\n2847: \n2848: *   **If the swarm is in the large-$\\varepsilon$ regime** (where $\\varepsilon > D_swarm$): By {prf:ref}`def-unified-high-low-error-sets`, $H_k(\\epsilon) = O_k$ in this regime. **{prf:ref}`lem-outlier-fraction-lower-bound`** guarantees that the fraction of walkers in the global kinematic outlier set is bounded below by a positive, N-uniform constant: $|H_k(\\epsilon)|/k \\ge f_O > 0$.\n2849: \n2850: *   **If the swarm is in the small-$\\varepsilon$ regime** (where $\\varepsilon \\leq D_swarm$): By {prf:ref}`def-unified-high-low-error-sets`, $H_k(\\epsilon) = C_k(\\epsilon)$ (the clustering-based outlier set) in this regime. **{prf:ref}`lem-outlier-cluster-fraction-lower-bound`** guarantees that the fraction of walkers in the outlier clusters is bounded below by a positive, N-uniform constant: $|H_k(\\epsilon)|/k \\ge f_{H,\\text{cluster}}(\\epsilon) > 0$.\n2851: \n2852: **4. Define the Unified Lower Bound:**\n2853: We can define a single, unified lower bound $f_H(\\epsilon)$ that is valid for all regimes by taking the minimum of the bounds from the two cases:\n2854: \n2855: $$\n2856: f_H(\\epsilon) := \\min(f_O, f_{H,\\text{cluster}}(\\epsilon))\n2857: $$\n2858: \n2859: Since both $f_O$ and $f_{H,\\text{cluster}}(\\epsilon)$ are strictly positive, N-uniform constants, their minimum $f_H(\\epsilon)$ is also a strictly positive, N-uniform constant.\n2860: \n2861: **5. Conclusion:**\n2862: We have rigorously shown that for any $\\varepsilon > 0$, if the total intra-swarm positional variance $V_{\\text{Var},x}$ is sufficiently large, then at least one swarm `k` is guaranteed to have a large hypocoercive variance, which in turn guarantees that the fraction of alive walkers in its unified high-error set $H_k(\\epsilon)$ is bounded below by the positive, N-uniform constant $f_H(\\epsilon)$. This establishes the direct causal link from the Lyapunov function's positional variance component to the guaranteed existence of a substantial high-error population.\n2863: "
    },
    {
      "directive_type": "lemma",
      "label": "lem-geometric-separation-of-partition",
      "title": "Geometric Separation of the Partition",
      "start_line": 2890,
      "end_line": 2912,
      "header_lines": [
        2891
      ],
      "content_start": 2893,
      "content_end": 2911,
      "content": "2893: :label: lem-geometric-separation-of-partition\n2894: \n2895: Let $H_k(\\epsilon)$ and $L_k(\\epsilon)$ be the unified high-error and low-error sets for swarm $k$ as defined in {prf:ref}`def-unified-high-low-error-sets`. Assume the swarm's internal positional variance is large: $\\mathrm{Var}(x) > R^2_{\\mathrm{var}}$.\n2896: \n2897: Then there exist N-uniform, $\\epsilon$-dependent constants $D_H(\\epsilon) > R_L(\\epsilon) > 0$ and a fractional constant $f_c > 0$ such that:\n2898: \n2899: **Part 1 (Separation Between Sets):** For any walker $i \\in H_k(\\epsilon)$ from a high-error cluster and any walker $j \\in L_k(\\epsilon)$ from a low-error cluster, their algorithmic distance is bounded below:\n2900: \n2901: $$\n2902: d_{\\text{alg}}(i, j) \\ge D_H(\\epsilon)\n2903: $$\n2904: \n2905: **Part 2 (Clustering of Low-Error Walkers):** For any walker $j \\in L_k(\\epsilon)$, there exists a non-empty subset of companion walkers $C_j \\subset L_k(\\epsilon) \\setminus \\{j\\}$ of minimum size $|C_j| \\ge f_c k$ such that all members of this cluster are within a small algorithmic radius:\n2906: \n2907: $$\n2908: d_{\\text{alg}}(j, \\ell) \\le R_L(\\epsilon) \\quad \\text{for all } \\ell \\in C_j\n2909: $$\n2910: \n2911: The separation property $D_H(\\epsilon) > R_L(\\epsilon)$ ensures that the geometric signatures of the two sets are fundamentally distinct and non-overlapping **in the algorithmic phase space**.",
      "metadata": {
        "label": "lem-geometric-separation-of-partition"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [
        "def-unified-high-low-error-sets"
      ],
      "raw_directive": "2890: #### 6.5.1. Main Lemma: Statement of Geometric Separation\n2891: \n2892: :::{prf:lemma} Geometric Separation of the Partition\n2893: :label: lem-geometric-separation-of-partition\n2894: \n2895: Let $H_k(\\epsilon)$ and $L_k(\\epsilon)$ be the unified high-error and low-error sets for swarm $k$ as defined in {prf:ref}`def-unified-high-low-error-sets`. Assume the swarm's internal positional variance is large: $\\mathrm{Var}(x) > R^2_{\\mathrm{var}}$.\n2896: \n2897: Then there exist N-uniform, $\\epsilon$-dependent constants $D_H(\\epsilon) > R_L(\\epsilon) > 0$ and a fractional constant $f_c > 0$ such that:\n2898: \n2899: **Part 1 (Separation Between Sets):** For any walker $i \\in H_k(\\epsilon)$ from a high-error cluster and any walker $j \\in L_k(\\epsilon)$ from a low-error cluster, their algorithmic distance is bounded below:\n2900: \n2901: $$\n2902: d_{\\text{alg}}(i, j) \\ge D_H(\\epsilon)\n2903: $$\n2904: \n2905: **Part 2 (Clustering of Low-Error Walkers):** For any walker $j \\in L_k(\\epsilon)$, there exists a non-empty subset of companion walkers $C_j \\subset L_k(\\epsilon) \\setminus \\{j\\}$ of minimum size $|C_j| \\ge f_c k$ such that all members of this cluster are within a small algorithmic radius:\n2906: \n2907: $$\n2908: d_{\\text{alg}}(j, \\ell) \\le R_L(\\epsilon) \\quad \\text{for all } \\ell \\in C_j\n2909: $$\n2910: \n2911: The separation property $D_H(\\epsilon) > R_L(\\epsilon)$ ensures that the geometric signatures of the two sets are fundamentally distinct and non-overlapping **in the algorithmic phase space**.\n2912: "
    },
    {
      "directive_type": "proof",
      "label": "proof-geometric-separation-all-regimes",
      "title": "Proof of Geometric Separation (All Regimes)",
      "start_line": 2934,
      "end_line": 3158,
      "header_lines": [
        2935,
        3131
      ],
      "content_start": 2937,
      "content_end": 3157,
      "content": "2937: :label: proof-geometric-separation-all-regimes\n2938: \n2939: **Objective:** Using the unified clustering-based definition from Section 6.3, we will prove that high-error clusters are geometrically isolated from low-error clusters in the algorithmic phase-space metric $d_{\\text{alg}}$, starting from the premise $\\mathrm{Var}_x(S_k) > R^2_{\\mathrm{var}}$. This proof applies uniformly across all interaction regimes.\n2940: \n2941: **Proof Strategy: Clustering-Based Separation**\n2942: \n2943: The unified definition partitions walkers into clusters $\\{G_1, \\ldots, G_M\\}$ with maximum diameter $D_{\\text{diam}}(\\epsilon) = c_d \\cdot \\epsilon$ in the algorithmic phase-space metric. High-error clusters are those whose centers contribute significantly to the between-cluster hypocoercive variance. We will prove:\n2944: \n2945: 1. **Within-cluster cohesion**: Walkers within any cluster (especially low-error clusters) remain close in phase space by construction ($d_{\\text{alg}} \\le D_{\\text{diam}}(\\epsilon)$)\n2946: 2. **Between-cluster separation**: High-error cluster centers are far from low-error cluster centers in phase space\n2947: 3. **Geometric separation**: These properties combine to ensure $D_H(\\epsilon) > R_L(\\epsilon)$\n2948: \n2949: The proof uses the reverse triangle inequality with explicit verification that the resulting bounds are positive and meaningful, ensuring rigorous separation between high-error and low-error populations.\n2950: \n2951: **Step 1: Establish Clustering Properties**\n2952: \n2953: By {prf:ref}`def-unified-high-low-error-sets`, the alive set $\\mathcal{A}_k$ is partitioned into clusters $\\{G_1, \\ldots, G_M\\}$ where each cluster satisfies:\n2954: \n2955: $$\n2956: \\text{diam}(G_m) := \\max_{i,j \\in G_m} d_{\\text{alg}}(i, j) \\le D_{\\text{diam}}(\\epsilon) = c_d \\cdot \\epsilon\n2957: $$\n2958: \n2959: This immediately gives us the **low-error clustering radius**. For any walker $j \\in L_k(\\epsilon)$ belonging to a valid low-error cluster $G_\\ell$ (with $|G_\\ell| \\ge k_{\\min}$), all other walkers in that cluster satisfy:\n2960: \n2961: $$\n2962: d_{\\text{alg}}(j, m) \\le D_{\\text{diam}}(\\epsilon) \\quad \\text{for all } m \\in G_\\ell\n2963: $$\n2964: \n2965: We define:\n2966: \n2967: $$\n2968: R_L(\\epsilon) := D_{\\text{diam}}(\\epsilon) = c_d \\cdot \\epsilon\n2969: $$\n2970: \n2971: **Step 2: Bridge to Hypocoercive Variance**\n2972: \n2973: As established in Section 6.4.2, the premise $\\mathrm{Var}_x(S_k) > R^2_{\\mathrm{var}}$ guarantees:\n2974: \n2975: $$\n2976: \\mathrm{Var}_h(S_k) = \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k) \\ge \\mathrm{Var}_x(S_k) > R^2_{\\mathrm{var}}\n2977: $$\n2978: \n2979: **Step 3: Decompose Variance via Law of Total Variance**\n2980: \n2981: The hypocoercive variance can be decomposed into within-cluster and between-cluster components. For the positional component:\n2982: \n2983: $$\n2984: k \\cdot \\mathrm{Var}_x(S_k) = \\sum_{m=1}^M \\sum_{i \\in G_m} \\|x_i - \\mu_x\\|^2 = \\underbrace{\\sum_{m=1}^M |G_m| \\mathrm{Var}_x(G_m)}_{\\text{within-cluster}} + \\underbrace{\\sum_{m=1}^M |G_m| \\|\\mu_{x,m} - \\mu_x\\|^2}_{\\text{between-cluster}}\n2985: $$\n2986: \n2987: where $\\mu_{x,m}$ is the positional center of mass of cluster $G_m$.\n2988: \n2989: **Step 4: Bound Within-Cluster Variance**\n2990: \n2991: Since each cluster has algorithmic diameter at most $D_{\\text{diam}}(\\epsilon)$, the positional diameter is bounded:\n2992: \n2993: $$\n2994: \\max_{i,j \\in G_m} \\|x_i - x_j\\| \\le \\max_{i,j \\in G_m} d_{\\text{alg}}(i,j) \\le D_{\\text{diam}}(\\epsilon)\n2995: $$\n2996: \n2997: Therefore, the maximum internal positional variance of any cluster satisfies:\n2998: \n2999: $$\n3000: \\mathrm{Var}_x(G_m) \\le \\left(\\frac{D_{\\text{diam}}(\\epsilon)}{2}\\right)^2\n3001: $$\n3002: \n3003: The total within-cluster sum of squares is bounded:\n3004: \n3005: $$\n3006: \\sum_{m=1}^M |G_m| \\mathrm{Var}_x(G_m) \\le k \\left(\\frac{D_{\\text{diam}}(\\epsilon)}{2}\\right)^2\n3007: $$\n3008: \n3009: **Step 5: Lower Bound on Between-Cluster Variance**\n3010: \n3011: Rearranging the variance decomposition and using $\\mathrm{Var}_x(S_k) > R^2_{\\mathrm{var}}$:\n3012: \n3013: $$\n3014: \\sum_{m=1}^M |G_m| \\|\\mu_{x,m} - \\mu_x\\|^2 = k \\cdot \\mathrm{Var}_x(S_k) - \\sum_{m=1}^M |G_m| \\mathrm{Var}_x(G_m) > k \\cdot R^2_{\\mathrm{var}} - k \\left(\\frac{D_{\\text{diam}}(\\epsilon)}{2}\\right)^2\n3015: $$\n3016: \n3017: Define the **minimum cluster mean separation threshold**:\n3018: \n3019: $$\n3020: R^2_{\\mathrm{means}} := R^2_{\\mathrm{var}} - \\left(\\frac{D_{\\text{diam}}(\\epsilon)}{2}\\right)^2\n3021: $$\n3022: \n3023: For this to be positive, we require the **admissibility condition**:\n3024: \n3025: $$\n3026: D_{\\text{diam}}(\\epsilon) = c_d \\cdot \\epsilon < 2\\sqrt{R^2_{\\mathrm{var}}}\n3027: $$\n3028: \n3029: Under this condition:\n3030: \n3031: $$\n3032: \\frac{1}{k} \\sum_{m=1}^M |G_m| \\|\\mu_{x,m} - \\mu_x\\|^2 > R^2_{\\mathrm{means}} > 0\n3033: $$\n3034: \n3035: **Step 6: Apply Outlier Analysis to Cluster Centers**\n3036: \n3037: By {prf:ref}`def-unified-high-low-error-sets`, valid outlier clusters (with $|G_m| \\ge k_{\\min}$) satisfy:\n3038: \n3039: $$\n3040: \\sum_{m \\in O_M} |G_m| \\|\\mu_{x,m} - \\mu_x\\|^2 \\ge (1-\\varepsilon_O) \\sum_{\\substack{m: |G_m| \\ge k_{\\min}}} |G_m| \\|\\mu_{x,m} - \\mu_x\\|^2\n3041: $$\n3042: \n3043: Let $H_k(\\epsilon) = \\bigcup_{m \\in O_M} G_m$ be the union of valid outlier clusters, and let $L_k(\\epsilon)$ be the union of valid low-error clusters.\n3044: \n3045: For any high-error cluster $G_h \\in O_M$ and any low-error cluster $G_\\ell \\notin O_M$ (with both having $|G_h|, |G_\\ell| \\ge k_{\\min}$), we derive a lower bound on the positional separation of their centers.\n3046: \n3047: **Step 7: Derive Minimum Cluster Mean Separation**\n3048: \n3049: Using the averaging argument from the outlier analysis: if the minimum positional distance from any outlier cluster center to the global center is $r_h$, then:\n3050: \n3051: $$\n3052: \\sum_{m \\in O_M} |G_m| \\|\\mu_{x,m} - \\mu_x\\|^2 \\ge |H_k(\\epsilon)| \\cdot r_h^2\n3053: $$\n3054: \n3055: Combined with Step 6 and using $|H_k(\\epsilon)| \\le k$:\n3056: \n3057: $$\n3058: r_h^2 \\ge (1-\\varepsilon_O) R^2_{\\mathrm{means}}\n3059: $$\n3060: \n3061: Therefore:\n3062: \n3063: $$\n3064: \\|\\mu_{x,h} - \\mu_x\\| \\ge \\sqrt{(1-\\varepsilon_O) R^2_{\\mathrm{means}}} \\quad \\text{for all } G_h \\in O_M\n3065: $$\n3066: \n3067: Similarly, for low-error clusters:\n3068: \n3069: $$\n3070: \\|\\mu_{x,\\ell} - \\mu_x\\| \\le \\sqrt{\\frac{\\varepsilon_O R^2_{\\mathrm{means}} k}{|L_k(\\epsilon)|}}\n3071: $$\n3072: \n3073: **Step 8: Prove Separation Between High-Error and Low-Error Sets**\n3074: \n3075: We now establish that walkers from high-error clusters are separated from walkers in low-error clusters. For any walker $i \\in H_k(\\epsilon)$ (in outlier cluster $G_h$), we consider two cases:\n3076: \n3077: **Case 1 (Within High-Error Set):** If $j \\in H_k(\\epsilon)$ and belongs to the same cluster $j \\in G_h$, then by the cluster diameter bound:\n3078: \n3079: $$\n3080: d_{\\text{alg}}(i,j) \\le D_{\\text{diam}}(\\epsilon) = R_L(\\epsilon)\n3081: $$\n3082: \n3083: This case shows that walkers within the same high-error cluster are **not** isolated from each other. This is a critical observation: we do not claim universal isolation for high-error walkers.\n3084: \n3085: **Case 2 (Between Different Sets):** If $j \\in L_k(\\epsilon)$ (low-error cluster $G_\\ell$), we use positional separation of cluster centers. By the reverse triangle inequality in position space:\n3086: \n3087: $$\n3088: \\|x_i - x_j\\| \\ge \\|\\mu_{x,h} - \\mu_{x,j'}\\| - \\|x_i - \\mu_{x,h}\\| - \\|x_j - \\mu_{x,j'}\\|\n3089: $$\n3090: \n3091: where $G_{j'}$ is the cluster containing $j$. This application of the reverse triangle inequality is valid when the separation between cluster centers dominates the within-cluster radii, which we now verify.\n3092: \n3093: Using our established bounds:\n3094: - $\\|\\mu_{x,h} - \\mu_{x,j'}\\| \\ge \\|\\mu_{x,h} - \\mu_x\\| - \\|\\mu_{x,j'} - \\mu_x\\|$ (reverse triangle inequality)\n3095: - $\\|x_i - \\mu_{x,h}\\| \\le D_{\\text{diam}}(\\epsilon)/2$ (radius bound within cluster)\n3096: - $\\|x_j - \\mu_{x,j'}\\| \\le D_{\\text{diam}}(\\epsilon)/2$ (radius bound within cluster)\n3097: \n3098: **Verification of Positivity:** For the bound to be meaningful, we must verify that:\n3099: \n3100: $$\n3101: \\|\\mu_{x,h} - \\mu_{x,j'}\\| > \\|x_i - \\mu_{x,h}\\| + \\|x_j - \\mu_{x,j'}\\|\n3102: $$\n3103: \n3104: From Steps 6-7, we have:\n3105: - $\\|\\mu_{x,h} - \\mu_{x,j'}\\| \\geq \\|\\mu_{x,h} - \\mu_x\\| - \\|\\mu_{x,j'} - \\mu_x\\| \\geq \\sqrt{(1-\\varepsilon_O) R^2_{\\mathrm{means}}} - \\sqrt{\\frac{\\varepsilon_O R^2_{\\mathrm{means}} k}{|L_k(\\epsilon)|}}$\n3106: - $\\|x_i - \\mu_{x,h}\\| + \\|x_j - \\mu_{x,j'}\\| \\leq D_{\\mathrm{diam}}(\\epsilon)$\n3107: \n3108: Therefore, positivity requires:\n3109: \n3110: $$\n3111: \\sqrt{(1-\\varepsilon_O) R^2_{\\mathrm{means}}} - \\sqrt{\\frac{\\varepsilon_O R^2_{\\mathrm{means}} k}{|L_k(\\epsilon)|}} > D_{\\mathrm{diam}}(\\epsilon)\n3112: $$\n3113: \n3114: This condition will be guaranteed by the admissibility constraints derived in Step 9 below. Proceeding under this guarantee, we obtain:\n3115: \n3116: $$\n3117: \\|x_i - x_j\\| \\ge \\sqrt{(1-\\varepsilon_O) R^2_{\\mathrm{means}}} - \\sqrt{\\frac{\\varepsilon_O R^2_{\\mathrm{means}} k}{|L_k(\\epsilon)|}} - D_{\\text{diam}}(\\epsilon)\n3118: $$\n3119: \n3120: Since $d_{\\text{alg}}(i,j) \\ge \\|x_i - x_j\\|$, we define the **high-error isolation distance**:\n3121: \n3122: $$\n3123: D_H(\\epsilon) := \\sqrt{(1-\\varepsilon_O) R^2_{\\mathrm{means}}} - \\sqrt{\\frac{\\varepsilon_O R^2_{\\mathrm{means}} k}{k(1-f_H(\\epsilon))}} - D_{\\text{diam}}(\\epsilon)\n3124: $$\n3125: \n3126: where $f_H(\\epsilon)$ is the N-uniform lower bound on the high-error fraction from Section 6.4. Simplifying:\n3127: \n3128: $$\n3129: D_H(\\epsilon) := \\sqrt{(1-\\varepsilon_O) R^2_{\\mathrm{means}}} - \\sqrt{\\frac{\\varepsilon_O R^2_{\\mathrm{means}}}{1-f_H(\\epsilon)}} - c_d \\cdot \\epsilon\n3130: $$\n3131: \n3132: :::{admonition} Mathematical Rigour Note\n3133: :class: note\n3134: \n3135: The application of the reverse triangle inequality in Step 8 deserves careful examination. For three points $a, b, c$ in a metric space, the reverse triangle inequality states:\n3136: \n3137: $$\n3138: \\|a - c\\| \\geq \\|a - b\\| - \\|b - c\\|\n3139: $$\n3140: \n3141: In our application with $a = x_i$, $b = \\mu_{x,h}$, and $c = x_j$, this becomes:\n3142: \n3143: $$\n3144: \\|x_i - x_j\\| \\geq \\|x_i - \\mu_{x,h}\\| - \\|\\mu_{x,h} - x_j\\|\n3145: $$\n3146: \n3147: However, to obtain a useful **lower bound**, we need the term $\\|\\mu_{x,h} - x_j\\|$ to be expressible in terms of quantities we can control. Using the triangle inequality $\\|\\mu_{x,h} - x_j\\| \\leq \\|\\mu_{x,h} - \\mu_{x,j'}\\| + \\|\\mu_{x,j'} - x_j\\|$, we substitute to get:\n3148: \n3149: $$\n3150: \\|x_i - x_j\\| \\geq \\|x_i - \\mu_{x,h}\\| - (\\|\\mu_{x,h} - \\mu_{x,j'}\\| + \\|\\mu_{x,j'} - x_j\\|)\n3151: $$\n3152: \n3153: Rearranging yields the form used in the proof:\n3154: \n3155: $$\n3156: \\|x_i - x_j\\| \\geq \\|\\mu_{x,h} - \\mu_{x,j'}\\| - \\|x_i - \\mu_{x,h}\\| - \\|x_j - \\mu_{x,j'}\\|\n3157: $$",
      "metadata": {
        "label": "proof-geometric-separation-all-regimes",
        "class": "note"
      },
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "references": [
        "def-unified-high-low-error-sets"
      ],
      "raw_directive": "2934: #### 6.5.2. Unified Proof via Clustering-Based Geometric Separation\n2935: \n2936: :::{prf:proof} Proof of Geometric Separation (All Regimes)\n2937: :label: proof-geometric-separation-all-regimes\n2938: \n2939: **Objective:** Using the unified clustering-based definition from Section 6.3, we will prove that high-error clusters are geometrically isolated from low-error clusters in the algorithmic phase-space metric $d_{\\text{alg}}$, starting from the premise $\\mathrm{Var}_x(S_k) > R^2_{\\mathrm{var}}$. This proof applies uniformly across all interaction regimes.\n2940: \n2941: **Proof Strategy: Clustering-Based Separation**\n2942: \n2943: The unified definition partitions walkers into clusters $\\{G_1, \\ldots, G_M\\}$ with maximum diameter $D_{\\text{diam}}(\\epsilon) = c_d \\cdot \\epsilon$ in the algorithmic phase-space metric. High-error clusters are those whose centers contribute significantly to the between-cluster hypocoercive variance. We will prove:\n2944: \n2945: 1. **Within-cluster cohesion**: Walkers within any cluster (especially low-error clusters) remain close in phase space by construction ($d_{\\text{alg}} \\le D_{\\text{diam}}(\\epsilon)$)\n2946: 2. **Between-cluster separation**: High-error cluster centers are far from low-error cluster centers in phase space\n2947: 3. **Geometric separation**: These properties combine to ensure $D_H(\\epsilon) > R_L(\\epsilon)$\n2948: \n2949: The proof uses the reverse triangle inequality with explicit verification that the resulting bounds are positive and meaningful, ensuring rigorous separation between high-error and low-error populations.\n2950: \n2951: **Step 1: Establish Clustering Properties**\n2952: \n2953: By {prf:ref}`def-unified-high-low-error-sets`, the alive set $\\mathcal{A}_k$ is partitioned into clusters $\\{G_1, \\ldots, G_M\\}$ where each cluster satisfies:\n2954: \n2955: $$\n2956: \\text{diam}(G_m) := \\max_{i,j \\in G_m} d_{\\text{alg}}(i, j) \\le D_{\\text{diam}}(\\epsilon) = c_d \\cdot \\epsilon\n2957: $$\n2958: \n2959: This immediately gives us the **low-error clustering radius**. For any walker $j \\in L_k(\\epsilon)$ belonging to a valid low-error cluster $G_\\ell$ (with $|G_\\ell| \\ge k_{\\min}$), all other walkers in that cluster satisfy:\n2960: \n2961: $$\n2962: d_{\\text{alg}}(j, m) \\le D_{\\text{diam}}(\\epsilon) \\quad \\text{for all } m \\in G_\\ell\n2963: $$\n2964: \n2965: We define:\n2966: \n2967: $$\n2968: R_L(\\epsilon) := D_{\\text{diam}}(\\epsilon) = c_d \\cdot \\epsilon\n2969: $$\n2970: \n2971: **Step 2: Bridge to Hypocoercive Variance**\n2972: \n2973: As established in Section 6.4.2, the premise $\\mathrm{Var}_x(S_k) > R^2_{\\mathrm{var}}$ guarantees:\n2974: \n2975: $$\n2976: \\mathrm{Var}_h(S_k) = \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k) \\ge \\mathrm{Var}_x(S_k) > R^2_{\\mathrm{var}}\n2977: $$\n2978: \n2979: **Step 3: Decompose Variance via Law of Total Variance**\n2980: \n2981: The hypocoercive variance can be decomposed into within-cluster and between-cluster components. For the positional component:\n2982: \n2983: $$\n2984: k \\cdot \\mathrm{Var}_x(S_k) = \\sum_{m=1}^M \\sum_{i \\in G_m} \\|x_i - \\mu_x\\|^2 = \\underbrace{\\sum_{m=1}^M |G_m| \\mathrm{Var}_x(G_m)}_{\\text{within-cluster}} + \\underbrace{\\sum_{m=1}^M |G_m| \\|\\mu_{x,m} - \\mu_x\\|^2}_{\\text{between-cluster}}\n2985: $$\n2986: \n2987: where $\\mu_{x,m}$ is the positional center of mass of cluster $G_m$.\n2988: \n2989: **Step 4: Bound Within-Cluster Variance**\n2990: \n2991: Since each cluster has algorithmic diameter at most $D_{\\text{diam}}(\\epsilon)$, the positional diameter is bounded:\n2992: \n2993: $$\n2994: \\max_{i,j \\in G_m} \\|x_i - x_j\\| \\le \\max_{i,j \\in G_m} d_{\\text{alg}}(i,j) \\le D_{\\text{diam}}(\\epsilon)\n2995: $$\n2996: \n2997: Therefore, the maximum internal positional variance of any cluster satisfies:\n2998: \n2999: $$\n3000: \\mathrm{Var}_x(G_m) \\le \\left(\\frac{D_{\\text{diam}}(\\epsilon)}{2}\\right)^2\n3001: $$\n3002: \n3003: The total within-cluster sum of squares is bounded:\n3004: \n3005: $$\n3006: \\sum_{m=1}^M |G_m| \\mathrm{Var}_x(G_m) \\le k \\left(\\frac{D_{\\text{diam}}(\\epsilon)}{2}\\right)^2\n3007: $$\n3008: \n3009: **Step 5: Lower Bound on Between-Cluster Variance**\n3010: \n3011: Rearranging the variance decomposition and using $\\mathrm{Var}_x(S_k) > R^2_{\\mathrm{var}}$:\n3012: \n3013: $$\n3014: \\sum_{m=1}^M |G_m| \\|\\mu_{x,m} - \\mu_x\\|^2 = k \\cdot \\mathrm{Var}_x(S_k) - \\sum_{m=1}^M |G_m| \\mathrm{Var}_x(G_m) > k \\cdot R^2_{\\mathrm{var}} - k \\left(\\frac{D_{\\text{diam}}(\\epsilon)}{2}\\right)^2\n3015: $$\n3016: \n3017: Define the **minimum cluster mean separation threshold**:\n3018: \n3019: $$\n3020: R^2_{\\mathrm{means}} := R^2_{\\mathrm{var}} - \\left(\\frac{D_{\\text{diam}}(\\epsilon)}{2}\\right)^2\n3021: $$\n3022: \n3023: For this to be positive, we require the **admissibility condition**:\n3024: \n3025: $$\n3026: D_{\\text{diam}}(\\epsilon) = c_d \\cdot \\epsilon < 2\\sqrt{R^2_{\\mathrm{var}}}\n3027: $$\n3028: \n3029: Under this condition:\n3030: \n3031: $$\n3032: \\frac{1}{k} \\sum_{m=1}^M |G_m| \\|\\mu_{x,m} - \\mu_x\\|^2 > R^2_{\\mathrm{means}} > 0\n3033: $$\n3034: \n3035: **Step 6: Apply Outlier Analysis to Cluster Centers**\n3036: \n3037: By {prf:ref}`def-unified-high-low-error-sets`, valid outlier clusters (with $|G_m| \\ge k_{\\min}$) satisfy:\n3038: \n3039: $$\n3040: \\sum_{m \\in O_M} |G_m| \\|\\mu_{x,m} - \\mu_x\\|^2 \\ge (1-\\varepsilon_O) \\sum_{\\substack{m: |G_m| \\ge k_{\\min}}} |G_m| \\|\\mu_{x,m} - \\mu_x\\|^2\n3041: $$\n3042: \n3043: Let $H_k(\\epsilon) = \\bigcup_{m \\in O_M} G_m$ be the union of valid outlier clusters, and let $L_k(\\epsilon)$ be the union of valid low-error clusters.\n3044: \n3045: For any high-error cluster $G_h \\in O_M$ and any low-error cluster $G_\\ell \\notin O_M$ (with both having $|G_h|, |G_\\ell| \\ge k_{\\min}$), we derive a lower bound on the positional separation of their centers.\n3046: \n3047: **Step 7: Derive Minimum Cluster Mean Separation**\n3048: \n3049: Using the averaging argument from the outlier analysis: if the minimum positional distance from any outlier cluster center to the global center is $r_h$, then:\n3050: \n3051: $$\n3052: \\sum_{m \\in O_M} |G_m| \\|\\mu_{x,m} - \\mu_x\\|^2 \\ge |H_k(\\epsilon)| \\cdot r_h^2\n3053: $$\n3054: \n3055: Combined with Step 6 and using $|H_k(\\epsilon)| \\le k$:\n3056: \n3057: $$\n3058: r_h^2 \\ge (1-\\varepsilon_O) R^2_{\\mathrm{means}}\n3059: $$\n3060: \n3061: Therefore:\n3062: \n3063: $$\n3064: \\|\\mu_{x,h} - \\mu_x\\| \\ge \\sqrt{(1-\\varepsilon_O) R^2_{\\mathrm{means}}} \\quad \\text{for all } G_h \\in O_M\n3065: $$\n3066: \n3067: Similarly, for low-error clusters:\n3068: \n3069: $$\n3070: \\|\\mu_{x,\\ell} - \\mu_x\\| \\le \\sqrt{\\frac{\\varepsilon_O R^2_{\\mathrm{means}} k}{|L_k(\\epsilon)|}}\n3071: $$\n3072: \n3073: **Step 8: Prove Separation Between High-Error and Low-Error Sets**\n3074: \n3075: We now establish that walkers from high-error clusters are separated from walkers in low-error clusters. For any walker $i \\in H_k(\\epsilon)$ (in outlier cluster $G_h$), we consider two cases:\n3076: \n3077: **Case 1 (Within High-Error Set):** If $j \\in H_k(\\epsilon)$ and belongs to the same cluster $j \\in G_h$, then by the cluster diameter bound:\n3078: \n3079: $$\n3080: d_{\\text{alg}}(i,j) \\le D_{\\text{diam}}(\\epsilon) = R_L(\\epsilon)\n3081: $$\n3082: \n3083: This case shows that walkers within the same high-error cluster are **not** isolated from each other. This is a critical observation: we do not claim universal isolation for high-error walkers.\n3084: \n3085: **Case 2 (Between Different Sets):** If $j \\in L_k(\\epsilon)$ (low-error cluster $G_\\ell$), we use positional separation of cluster centers. By the reverse triangle inequality in position space:\n3086: \n3087: $$\n3088: \\|x_i - x_j\\| \\ge \\|\\mu_{x,h} - \\mu_{x,j'}\\| - \\|x_i - \\mu_{x,h}\\| - \\|x_j - \\mu_{x,j'}\\|\n3089: $$\n3090: \n3091: where $G_{j'}$ is the cluster containing $j$. This application of the reverse triangle inequality is valid when the separation between cluster centers dominates the within-cluster radii, which we now verify.\n3092: \n3093: Using our established bounds:\n3094: - $\\|\\mu_{x,h} - \\mu_{x,j'}\\| \\ge \\|\\mu_{x,h} - \\mu_x\\| - \\|\\mu_{x,j'} - \\mu_x\\|$ (reverse triangle inequality)\n3095: - $\\|x_i - \\mu_{x,h}\\| \\le D_{\\text{diam}}(\\epsilon)/2$ (radius bound within cluster)\n3096: - $\\|x_j - \\mu_{x,j'}\\| \\le D_{\\text{diam}}(\\epsilon)/2$ (radius bound within cluster)\n3097: \n3098: **Verification of Positivity:** For the bound to be meaningful, we must verify that:\n3099: \n3100: $$\n3101: \\|\\mu_{x,h} - \\mu_{x,j'}\\| > \\|x_i - \\mu_{x,h}\\| + \\|x_j - \\mu_{x,j'}\\|\n3102: $$\n3103: \n3104: From Steps 6-7, we have:\n3105: - $\\|\\mu_{x,h} - \\mu_{x,j'}\\| \\geq \\|\\mu_{x,h} - \\mu_x\\| - \\|\\mu_{x,j'} - \\mu_x\\| \\geq \\sqrt{(1-\\varepsilon_O) R^2_{\\mathrm{means}}} - \\sqrt{\\frac{\\varepsilon_O R^2_{\\mathrm{means}} k}{|L_k(\\epsilon)|}}$\n3106: - $\\|x_i - \\mu_{x,h}\\| + \\|x_j - \\mu_{x,j'}\\| \\leq D_{\\mathrm{diam}}(\\epsilon)$\n3107: \n3108: Therefore, positivity requires:\n3109: \n3110: $$\n3111: \\sqrt{(1-\\varepsilon_O) R^2_{\\mathrm{means}}} - \\sqrt{\\frac{\\varepsilon_O R^2_{\\mathrm{means}} k}{|L_k(\\epsilon)|}} > D_{\\mathrm{diam}}(\\epsilon)\n3112: $$\n3113: \n3114: This condition will be guaranteed by the admissibility constraints derived in Step 9 below. Proceeding under this guarantee, we obtain:\n3115: \n3116: $$\n3117: \\|x_i - x_j\\| \\ge \\sqrt{(1-\\varepsilon_O) R^2_{\\mathrm{means}}} - \\sqrt{\\frac{\\varepsilon_O R^2_{\\mathrm{means}} k}{|L_k(\\epsilon)|}} - D_{\\text{diam}}(\\epsilon)\n3118: $$\n3119: \n3120: Since $d_{\\text{alg}}(i,j) \\ge \\|x_i - x_j\\|$, we define the **high-error isolation distance**:\n3121: \n3122: $$\n3123: D_H(\\epsilon) := \\sqrt{(1-\\varepsilon_O) R^2_{\\mathrm{means}}} - \\sqrt{\\frac{\\varepsilon_O R^2_{\\mathrm{means}} k}{k(1-f_H(\\epsilon))}} - D_{\\text{diam}}(\\epsilon)\n3124: $$\n3125: \n3126: where $f_H(\\epsilon)$ is the N-uniform lower bound on the high-error fraction from Section 6.4. Simplifying:\n3127: \n3128: $$\n3129: D_H(\\epsilon) := \\sqrt{(1-\\varepsilon_O) R^2_{\\mathrm{means}}} - \\sqrt{\\frac{\\varepsilon_O R^2_{\\mathrm{means}}}{1-f_H(\\epsilon)}} - c_d \\cdot \\epsilon\n3130: $$\n3131: \n3132: :::{admonition} Mathematical Rigour Note\n3133: :class: note\n3134: \n3135: The application of the reverse triangle inequality in Step 8 deserves careful examination. For three points $a, b, c$ in a metric space, the reverse triangle inequality states:\n3136: \n3137: $$\n3138: \\|a - c\\| \\geq \\|a - b\\| - \\|b - c\\|\n3139: $$\n3140: \n3141: In our application with $a = x_i$, $b = \\mu_{x,h}$, and $c = x_j$, this becomes:\n3142: \n3143: $$\n3144: \\|x_i - x_j\\| \\geq \\|x_i - \\mu_{x,h}\\| - \\|\\mu_{x,h} - x_j\\|\n3145: $$\n3146: \n3147: However, to obtain a useful **lower bound**, we need the term $\\|\\mu_{x,h} - x_j\\|$ to be expressible in terms of quantities we can control. Using the triangle inequality $\\|\\mu_{x,h} - x_j\\| \\leq \\|\\mu_{x,h} - \\mu_{x,j'}\\| + \\|\\mu_{x,j'} - x_j\\|$, we substitute to get:\n3148: \n3149: $$\n3150: \\|x_i - x_j\\| \\geq \\|x_i - \\mu_{x,h}\\| - (\\|\\mu_{x,h} - \\mu_{x,j'}\\| + \\|\\mu_{x,j'} - x_j\\|)\n3151: $$\n3152: \n3153: Rearranging yields the form used in the proof:\n3154: \n3155: $$\n3156: \\|x_i - x_j\\| \\geq \\|\\mu_{x,h} - \\mu_{x,j'}\\| - \\|x_i - \\mu_{x,h}\\| - \\|x_j - \\mu_{x,j'}\\|\n3157: $$\n3158: "
    }
  ],
  "validation": {
    "ok": true,
    "errors": []
  }
}