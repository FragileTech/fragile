[
  {
    "label": "lem-wasserstein-decomposition",
    "title": "Decomposition of the Hypocoercive Wasserstein Distance",
    "type": "lemma",
    "nl_statement": "The squared hypocoercive Wasserstein distance between two empirical measures \u03bc\u2081 and \u03bc\u2082 decomposes exactly into the sum of location and structural error components.",
    "equations": [
      {
        "label": null,
        "latex": "W_h^2(\\mu_1, \\mu_2) = V_{\\text{loc}} + V_{\\text{struct}}"
      }
    ],
    "hypotheses": [],
    "conclusion": {
      "text": "W_h^2(\u03bc\u2081, \u03bc\u2082) = V_loc + V_struct",
      "latex": "W_h^2(\\mu_1, \\mu_2) = V_{\\text{loc}} + V_{\\text{struct}}"
    },
    "variables": [
      {
        "symbol": "\\mu_1",
        "name": "\u03bc\u2081",
        "description": "Empirical measure of the first swarm",
        "constraints": [
          "Probability measure"
        ],
        "tags": [
          "measure",
          "swarm"
        ]
      },
      {
        "symbol": "\\mu_2",
        "name": "\u03bc\u2082",
        "description": "Empirical measure of the second swarm",
        "constraints": [
          "Probability measure"
        ],
        "tags": [
          "measure",
          "swarm"
        ]
      },
      {
        "symbol": "W_h",
        "name": "W_h",
        "description": "Hypocoercive Wasserstein distance",
        "constraints": [],
        "tags": [
          "distance",
          "hypocoercive"
        ]
      },
      {
        "symbol": "V_{\\text{loc}}",
        "name": "V_loc",
        "description": "Location error component",
        "constraints": [
          "Non-negative"
        ],
        "tags": [
          "error",
          "location"
        ]
      },
      {
        "symbol": "V_{\\text{struct}}",
        "name": "V_struct",
        "description": "Structural error component",
        "constraints": [
          "Non-negative"
        ],
        "tags": [
          "error",
          "structural"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "\u03bc\u2081 and \u03bc\u2082 are probability measures on a space where the hypocoercive Wasserstein distance is defined",
        "confidence": 1.0
      },
      {
        "text": "V_loc and V_struct are the distances between centers of mass and centered shapes, respectively",
        "confidence": 0.9
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-wasserstein-decomposition",
      "title": null,
      "type": "proof",
      "proves": "lem-wasserstein-decomposition",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":::{prf:proof}\n:label: proof-lem-wasserstein-decomposition\n**Proof.**\n\nThis fundamental decomposition theorem for Wasserstein distances with quadratic costs is a consequence of the gluing lemma in optimal transport and the geometry of barycenters. We provide a complete proof adapted to the hypocoercive cost structure.\n\n**Step 1: Setting up notation and the cost function.**\n\nLet $\\mathcal{Z} = \\mathbb{R}^d \\times \\mathbb{R}^d$ denote the phase space (positions and velocities). For two swarms, let $\\mu_1$ and $\\mu_2$ be their empirical measures over alive walkers:\n\n$$\n\\mu_k = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\delta_{z_{k,i}}, \\quad z_{k,i} = (x_{k,i}, v_{k,i})\n$$\n\nThe hypocoercive cost function is:\n\n$$\nc(z_1, z_2) = \\|x_1 - x_2\\|^2 + \\lambda_v \\|v_1 - v_2\\|^2 + b\\langle x_1 - x_2, v_1 - v_2 \\rangle\n$$\n\nThis is a **quadratic form** in $(z_1, z_2)$, which we write as $c(z_1, z_2) = q(z_1 - z_2)$ where $q$ is the quadratic form $q(\\Delta z) = \\|\\Delta x\\|^2 + \\lambda_v \\|\\Delta v\\|^2 + b\\langle \\Delta x, \\Delta v \\rangle$.\n\n**Step 2: Barycentric projections and centered measures.**\n\nDefine the barycenters:\n\n$$\n\\bar{z}_k = \\int z \\, d\\mu_k(z) = (\\mu_{x,k}, \\mu_{v,k})\n$$\n\nFor empirical measures over alive walkers, this is simply:\n\n$$\n\\bar{z}_k = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} z_{k,i} = (\\mu_{x,k}, \\mu_{v,k})\n$$\n\nDefine the **centered measures** $\\tilde{\\mu}_k$ by shifting each measure to have zero barycenter:\n\n$$\n\\tilde{\\mu}_k = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\delta_{\\delta_{z,k,i}}, \\quad \\delta_{z,k,i} = z_{k,i} - \\bar{z}_k = (\\delta_{x,k,i}, \\delta_{v,k,i})\n$$\n\nBy construction, $\\int \\delta_z \\, d\\tilde{\\mu}_k(\\delta_z) = 0$ for both $k = 1, 2$.\n\n**Step 3: Decomposition via optimal couplings.**\n\nLet $\\gamma^* \\in \\Gamma(\\mu_1, \\mu_2)$ be an optimal coupling achieving $W_h^2(\\mu_1, \\mu_2)$. We will show that $\\gamma^*$ induces a natural coupling structure that decomposes the cost.\n\nFor any coupling $\\gamma \\in \\Gamma(\\mu_1, \\mu_2)$, the total transport cost is:\n\n$$\n\\int_{\\mathcal{Z} \\times \\mathcal{Z}} c(z_1, z_2) \\, d\\gamma(z_1, z_2) = \\int_{\\mathcal{Z} \\times \\mathcal{Z}} q(z_1 - z_2) \\, d\\gamma(z_1, z_2)\n$$\n\nSince $q$ is a quadratic form, we can decompose $z_1 - z_2$ as:\n\n$$\nz_1 - z_2 = (z_1 - \\bar{z}_1) - (z_2 - \\bar{z}_2) + (\\bar{z}_1 - \\bar{z}_2) = \\delta_{z_1} - \\delta_{z_2} + \\Delta\\bar{z}\n$$\n\nwhere $\\Delta\\bar{z} = \\bar{z}_1 - \\bar{z}_2 = (\\Delta\\mu_x, \\Delta\\mu_v)$ is the barycenter difference and $\\delta_{z_i} = z_i - \\bar{z}_i$ are centered coordinates.\n\n**Step 4: Expanding the quadratic form.**\n\nExpanding $q(z_1 - z_2)$ using the decomposition:\n\n$$\n\\begin{aligned}\nq(z_1 - z_2) &= q(\\delta_{z_1} - \\delta_{z_2} + \\Delta\\bar{z}) \\\\\n&= q(\\delta_{z_1} - \\delta_{z_2}) + q(\\Delta\\bar{z}) + 2\\langle \\delta_{z_1} - \\delta_{z_2}, \\Delta\\bar{z} \\rangle_q\n\\end{aligned}\n$$\n\nwhere $\\langle \\cdot, \\cdot \\rangle_q$ denotes the inner product associated with the quadratic form $q$ (i.e., the bilinear form such that $q(\\Delta z) = \\langle \\Delta z, \\Delta z \\rangle_q$).\n\nIntegrating over the coupling $\\gamma$:\n\n$$\n\\begin{aligned}\n\\int c(z_1, z_2) \\, d\\gamma &= \\int q(\\delta_{z_1} - \\delta_{z_2}) \\, d\\gamma + q(\\Delta\\bar{z}) + 2\\int \\langle \\delta_{z_1} - \\delta_{z_2}, \\Delta\\bar{z} \\rangle_q \\, d\\gamma\n\\end{aligned}\n$$\n\n**Step 5: The cross-term vanishes.**\n\nThe key observation is that the cross-term vanishes:\n\n$$\n\\int \\langle \\delta_{z_1} - \\delta_{z_2}, \\Delta\\bar{z} \\rangle_q \\, d\\gamma = \\left\\langle \\int \\delta_{z_1} \\, d\\gamma(z_1, z_2), \\Delta\\bar{z} \\right\\rangle_q - \\left\\langle \\int \\delta_{z_2} \\, d\\gamma(z_1, z_2), \\Delta\\bar{z} \\right\\rangle_q\n$$\n\nFor any coupling $\\gamma \\in \\Gamma(\\mu_1, \\mu_2)$, the marginals satisfy $\\gamma(\\cdot \\times \\mathcal{Z}) = \\mu_1$ and $\\gamma(\\mathcal{Z} \\times \\cdot) = \\mu_2$. Therefore:\n\n$$\n\\int \\delta_{z_1} \\, d\\gamma(z_1, z_2) = \\int (z_1 - \\bar{z}_1) \\, d\\gamma(z_1, z_2) = \\int z_1 \\, d\\mu_1(z_1) - \\bar{z}_1 = \\bar{z}_1 - \\bar{z}_1 = 0\n$$\n\nSimilarly, $\\int \\delta_{z_2} \\, d\\gamma(z_1, z_2) = 0$. Thus the cross-term is zero.\n\n**Step 6: Identifying the decomposition terms.**\n\nWith the cross-term eliminated:\n\n$$\n\\int c(z_1, z_2) \\, d\\gamma = \\int q(\\delta_{z_1} - \\delta_{z_2}) \\, d\\gamma + q(\\Delta\\bar{z})\n$$\n\nThe second term is the barycenter cost:\n\n$$\nq(\\Delta\\bar{z}) = \\|\\Delta\\mu_x\\|^2 + \\lambda_v \\|\\Delta\\mu_v\\|^2 + b\\langle \\Delta\\mu_x, \\Delta\\mu_v \\rangle = V_{\\text{loc}}\n$$\n\nThe first term involves the centered coordinates. Note that $\\gamma$ induces a coupling $\\tilde{\\gamma} \\in \\Gamma(\\tilde{\\mu}_1, \\tilde{\\mu}_2)$ between the centered measures via the map $(z_1, z_2) \\mapsto (\\delta_{z_1}, \\delta_{z_2})$. Thus:\n\n$$\n\\int q(\\delta_{z_1} - \\delta_{z_2}) \\, d\\gamma(z_1, z_2) = \\int q(\\delta_{z_1}' - \\delta_{z_2}') \\, d\\tilde{\\gamma}(\\delta_{z_1}', \\delta_{z_2}')\n$$\n\n**Step 7: Taking the infimum.**\n\nTaking the infimum over all couplings $\\gamma \\in \\Gamma(\\mu_1, \\mu_2)$:\n\n$$\nW_h^2(\\mu_1, \\mu_2) = \\inf_{\\gamma \\in \\Gamma(\\mu_1, \\mu_2)} \\int c(z_1, z_2) \\, d\\gamma = V_{\\text{loc}} + \\inf_{\\tilde{\\gamma} \\in \\Gamma(\\tilde{\\mu}_1, \\tilde{\\mu}_2)} \\int c(\\delta_{z_1}, \\delta_{z_2}) \\, d\\tilde{\\gamma}\n$$\n\nThe infimum over centered couplings is precisely $W_h^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) = V_{\\text{struct}}$.\n\n**Conclusion:**\n\n$$\nW_h^2(\\mu_1, \\mu_2) = V_{\\text{loc}} + V_{\\text{struct}}\n$$\n\nThis decomposition is exact and holds for any pair of measures with finite second moments and any quadratic cost function.",
      "raw_directive": "476: \n477: :::\n478: :::{prf:proof}\n479: :label: proof-lem-wasserstein-decomposition\n480: **Proof.**\n481: \n482: This fundamental decomposition theorem for Wasserstein distances with quadratic costs is a consequence of the gluing lemma in optimal transport and the geometry of barycenters. We provide a complete proof adapted to the hypocoercive cost structure.\n483: \n484: **Step 1: Setting up notation and the cost function.**\n485: \n486: Let $\\mathcal{Z} = \\mathbb{R}^d \\times \\mathbb{R}^d$ denote the phase space (positions and velocities). For two swarms, let $\\mu_1$ and $\\mu_2$ be their empirical measures over alive walkers:\n487: \n488: $$\n489: \\mu_k = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\delta_{z_{k,i}}, \\quad z_{k,i} = (x_{k,i}, v_{k,i})\n490: $$\n491: \n492: The hypocoercive cost function is:\n493: \n494: $$\n495: c(z_1, z_2) = \\|x_1 - x_2\\|^2 + \\lambda_v \\|v_1 - v_2\\|^2 + b\\langle x_1 - x_2, v_1 - v_2 \\rangle\n496: $$\n497: \n498: This is a **quadratic form** in $(z_1, z_2)$, which we write as $c(z_1, z_2) = q(z_1 - z_2)$ where $q$ is the quadratic form $q(\\Delta z) = \\|\\Delta x\\|^2 + \\lambda_v \\|\\Delta v\\|^2 + b\\langle \\Delta x, \\Delta v \\rangle$.\n499: \n500: **Step 2: Barycentric projections and centered measures.**\n501: \n502: Define the barycenters:\n503: \n504: $$\n505: \\bar{z}_k = \\int z \\, d\\mu_k(z) = (\\mu_{x,k}, \\mu_{v,k})\n506: $$\n507: \n508: For empirical measures over alive walkers, this is simply:\n509: \n510: $$\n511: \\bar{z}_k = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} z_{k,i} = (\\mu_{x,k}, \\mu_{v,k})\n512: $$\n513: \n514: Define the **centered measures** $\\tilde{\\mu}_k$ by shifting each measure to have zero barycenter:\n515: \n516: $$\n517: \\tilde{\\mu}_k = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\delta_{\\delta_{z,k,i}}, \\quad \\delta_{z,k,i} = z_{k,i} - \\bar{z}_k = (\\delta_{x,k,i}, \\delta_{v,k,i})\n518: $$\n519: \n520: By construction, $\\int \\delta_z \\, d\\tilde{\\mu}_k(\\delta_z) = 0$ for both $k = 1, 2$.\n521: \n522: **Step 3: Decomposition via optimal couplings.**\n523: \n524: Let $\\gamma^* \\in \\Gamma(\\mu_1, \\mu_2)$ be an optimal coupling achieving $W_h^2(\\mu_1, \\mu_2)$. We will show that $\\gamma^*$ induces a natural coupling structure that decomposes the cost.\n525: \n526: For any coupling $\\gamma \\in \\Gamma(\\mu_1, \\mu_2)$, the total transport cost is:\n527: \n528: $$\n529: \\int_{\\mathcal{Z} \\times \\mathcal{Z}} c(z_1, z_2) \\, d\\gamma(z_1, z_2) = \\int_{\\mathcal{Z} \\times \\mathcal{Z}} q(z_1 - z_2) \\, d\\gamma(z_1, z_2)\n530: $$\n531: \n532: Since $q$ is a quadratic form, we can decompose $z_1 - z_2$ as:\n533: \n534: $$\n535: z_1 - z_2 = (z_1 - \\bar{z}_1) - (z_2 - \\bar{z}_2) + (\\bar{z}_1 - \\bar{z}_2) = \\delta_{z_1} - \\delta_{z_2} + \\Delta\\bar{z}\n536: $$\n537: \n538: where $\\Delta\\bar{z} = \\bar{z}_1 - \\bar{z}_2 = (\\Delta\\mu_x, \\Delta\\mu_v)$ is the barycenter difference and $\\delta_{z_i} = z_i - \\bar{z}_i$ are centered coordinates.\n539: \n540: **Step 4: Expanding the quadratic form.**\n541: \n542: Expanding $q(z_1 - z_2)$ using the decomposition:\n543: \n544: $$\n545: \\begin{aligned}\n546: q(z_1 - z_2) &= q(\\delta_{z_1} - \\delta_{z_2} + \\Delta\\bar{z}) \\\\\n547: &= q(\\delta_{z_1} - \\delta_{z_2}) + q(\\Delta\\bar{z}) + 2\\langle \\delta_{z_1} - \\delta_{z_2}, \\Delta\\bar{z} \\rangle_q\n548: \\end{aligned}\n549: $$\n550: \n551: where $\\langle \\cdot, \\cdot \\rangle_q$ denotes the inner product associated with the quadratic form $q$ (i.e., the bilinear form such that $q(\\Delta z) = \\langle \\Delta z, \\Delta z \\rangle_q$).\n552: \n553: Integrating over the coupling $\\gamma$:\n554: \n555: $$\n556: \\begin{aligned}\n557: \\int c(z_1, z_2) \\, d\\gamma &= \\int q(\\delta_{z_1} - \\delta_{z_2}) \\, d\\gamma + q(\\Delta\\bar{z}) + 2\\int \\langle \\delta_{z_1} - \\delta_{z_2}, \\Delta\\bar{z} \\rangle_q \\, d\\gamma\n558: \\end{aligned}\n559: $$\n560: \n561: **Step 5: The cross-term vanishes.**\n562: \n563: The key observation is that the cross-term vanishes:\n564: \n565: $$\n566: \\int \\langle \\delta_{z_1} - \\delta_{z_2}, \\Delta\\bar{z} \\rangle_q \\, d\\gamma = \\left\\langle \\int \\delta_{z_1} \\, d\\gamma(z_1, z_2), \\Delta\\bar{z} \\right\\rangle_q - \\left\\langle \\int \\delta_{z_2} \\, d\\gamma(z_1, z_2), \\Delta\\bar{z} \\right\\rangle_q\n567: $$\n568: \n569: For any coupling $\\gamma \\in \\Gamma(\\mu_1, \\mu_2)$, the marginals satisfy $\\gamma(\\cdot \\times \\mathcal{Z}) = \\mu_1$ and $\\gamma(\\mathcal{Z} \\times \\cdot) = \\mu_2$. Therefore:\n570: \n571: $$\n572: \\int \\delta_{z_1} \\, d\\gamma(z_1, z_2) = \\int (z_1 - \\bar{z}_1) \\, d\\gamma(z_1, z_2) = \\int z_1 \\, d\\mu_1(z_1) - \\bar{z}_1 = \\bar{z}_1 - \\bar{z}_1 = 0\n573: $$\n574: \n575: Similarly, $\\int \\delta_{z_2} \\, d\\gamma(z_1, z_2) = 0$. Thus the cross-term is zero.\n576: \n577: **Step 6: Identifying the decomposition terms.**\n578: \n579: With the cross-term eliminated:\n580: \n581: $$\n582: \\int c(z_1, z_2) \\, d\\gamma = \\int q(\\delta_{z_1} - \\delta_{z_2}) \\, d\\gamma + q(\\Delta\\bar{z})\n583: $$\n584: \n585: The second term is the barycenter cost:\n586: \n587: $$\n588: q(\\Delta\\bar{z}) = \\|\\Delta\\mu_x\\|^2 + \\lambda_v \\|\\Delta\\mu_v\\|^2 + b\\langle \\Delta\\mu_x, \\Delta\\mu_v \\rangle = V_{\\text{loc}}\n589: $$\n590: \n591: The first term involves the centered coordinates. Note that $\\gamma$ induces a coupling $\\tilde{\\gamma} \\in \\Gamma(\\tilde{\\mu}_1, \\tilde{\\mu}_2)$ between the centered measures via the map $(z_1, z_2) \\mapsto (\\delta_{z_1}, \\delta_{z_2})$. Thus:\n592: \n593: $$\n594: \\int q(\\delta_{z_1} - \\delta_{z_2}) \\, d\\gamma(z_1, z_2) = \\int q(\\delta_{z_1}' - \\delta_{z_2}') \\, d\\tilde{\\gamma}(\\delta_{z_1}', \\delta_{z_2}')\n595: $$\n596: \n597: **Step 7: Taking the infimum.**\n598: \n599: Taking the infimum over all couplings $\\gamma \\in \\Gamma(\\mu_1, \\mu_2)$:\n600: \n601: $$\n602: W_h^2(\\mu_1, \\mu_2) = \\inf_{\\gamma \\in \\Gamma(\\mu_1, \\mu_2)} \\int c(z_1, z_2) \\, d\\gamma = V_{\\text{loc}} + \\inf_{\\tilde{\\gamma} \\in \\Gamma(\\tilde{\\mu}_1, \\tilde{\\mu}_2)} \\int c(\\delta_{z_1}, \\delta_{z_2}) \\, d\\tilde{\\gamma}\n603: $$\n604: \n605: The infimum over centered couplings is precisely $W_h^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) = V_{\\text{struct}}$.\n606: \n607: **Conclusion:**\n608: \n609: $$\n610: W_h^2(\\mu_1, \\mu_2) = V_{\\text{loc}} + V_{\\text{struct}}\n611: $$\n612: \n613: This decomposition is exact and holds for any pair of measures with finite second moments and any quadratic cost function.\n614: ",
      "strategy_summary": "The proof decomposes the hypocoercive Wasserstein distance by expressing the cost as a quadratic form, centering the measures around their barycenters, expanding the form to separate barycenter and centered components, and showing the cross-term integrates to zero over any coupling, yielding the sum of local and structural terms.",
      "conclusion": {
        "text": "W_h^2(\u03bc_1, \u03bc_2) = V_loc + V_struct",
        "latex": "W_h^2(\\mu_1, \\mu_2) = V_{\\text{loc}} + V_{\\text{struct}}"
      },
      "assumptions": [
        {
          "text": "The measures \u03bc_1 and \u03bc_2 have finite second moments",
          "latex": null
        },
        {
          "text": "The cost function is quadratic in the phase space differences",
          "latex": null
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "setup",
          "text": "Introduce notation for phase space, empirical measures over alive walkers, and the hypocoercive cost function as a quadratic form q(\u0394z).",
          "latex": "c(z_1, z_2) = q(z_1 - z_2) = \\|\\Delta x\\|^2 + \\lambda_v \\|\\Delta v\\|^2 + b \\langle \\Delta x, \\Delta v \\rangle",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 2.0,
          "kind": "definition",
          "text": "Define barycenters \\bar{z}_k and centered measures \\tilde{\u03bc}_k by subtracting the barycenter from each point.",
          "latex": "\\tilde{\\mu}_k = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\delta_{z_{k,i}} \\quad \\delta_{z_{k,i}} = z_{k,i} - \\bar{z}_k",
          "references": [],
          "derived_statement": "Barycenters and centered measures have zero mean."
        },
        {
          "order": 3.0,
          "kind": "decomposition",
          "text": "Decompose z_1 - z_2 = \\delta_{z_1} - \\delta_{z_2} + \\Delta \\bar{z} for the difference in the quadratic form.",
          "latex": "z_1 - z_2 = \\delta_{z_1} - \\delta_{z_2} + \\Delta \\bar{z}",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 4.0,
          "kind": "expansion",
          "text": "Expand q(z_1 - z_2) = q(\\delta_{z_1} - \\delta_{z_2}) + q(\\Delta \\bar{z}) + 2 \\langle \\delta_{z_1} - \\delta_{z_2}, \\Delta \\bar{z} \\rangle_q and integrate over the coupling \u03b3.",
          "latex": "\\int q(z_1 - z_2) d\\gamma = \\int q(\\delta_{z_1} - \\delta_{z_2}) d\\gamma + q(\\Delta \\bar{z}) + 2 \\int \\langle \\delta_{z_1} - \\delta_{z_2}, \\Delta \\bar{z} \\rangle_q d\\gamma",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 5.0,
          "kind": "vanishing",
          "text": "Show the cross-term integrates to zero because the marginal integrals of the centered coordinates are zero.",
          "latex": "\\int \\delta_{z_1} d\\gamma = 0, \\quad \\int \\delta_{z_2} d\\gamma = 0",
          "references": [],
          "derived_statement": "Cross-term vanishes for any coupling."
        },
        {
          "order": 6.0,
          "kind": "identification",
          "text": "Identify q(\\Delta \\bar{z}) = V_loc and the remaining integral as the cost for the induced coupling on centered measures, equal to V_struct.",
          "latex": "\\int q(\\delta_{z_1} - \\delta_{z_2}) d\\gamma = \\int q(\\delta_{z_1}' - \\delta_{z_2}') d\\tilde{\\gamma}",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 7.0,
          "kind": "infimum",
          "text": "Take infimum over couplings to get W_h^2(\u03bc_1, \u03bc_2) = V_loc + inf over centered couplings = V_loc + V_struct.",
          "latex": "W_h^2(\\mu_1, \\mu_2) = V_{\\text{loc}} + \\inf_{\\tilde{\\gamma}} \\int c(\\delta_{z_1}, \\delta_{z_2}) d\\tilde{\\gamma} = V_{\\text{loc}} + V_{\\text{struct}}",
          "references": [],
          "derived_statement": null
        }
      ],
      "key_equations": [
        {
          "label": "eq-cost",
          "latex": "c(z_1, z_2) = \\|x_1 - x_2\\|^2 + \\lambda_v \\|v_1 - v_2\\|^2 + b \\langle x_1 - x_2, v_1 - v_2 \\rangle",
          "role": "Defines the hypocoercive cost function."
        },
        {
          "label": "eq-barycenter",
          "latex": "\\bar{z}_k = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} z_{k,i}",
          "role": "Barycenter computation for empirical measures."
        },
        {
          "label": "eq-centered",
          "latex": "\\delta_{z_{k,i}} = z_{k,i} - \\bar{z}_k",
          "role": "Centering the points to form zero-mean measures."
        },
        {
          "label": "eq-decomp-diff",
          "latex": "z_1 - z_2 = \\delta_{z_1} - \\delta_{z_2} + \\Delta \\bar{z}",
          "role": "Decomposition of the difference vector."
        },
        {
          "label": "eq-q-expansion",
          "latex": "q(z_1 - z_2) = q(\\delta_{z_1} - \\delta_{z_2}) + q(\\Delta \\bar{z}) + 2 \\langle \\delta_{z_1} - \\delta_{z_2}, \\Delta \\bar{z} \\rangle_q",
          "role": "Expansion of the quadratic form."
        },
        {
          "label": "eq-cross-zero",
          "latex": "\\int \\langle \\delta_{z_1} - \\delta_{z_2}, \\Delta \\bar{z} \\rangle_q d\\gamma = 0",
          "role": "Vanishing of the cross-term."
        },
        {
          "label": "eq-final-decomp",
          "latex": "W_h^2(\\mu_1, \\mu_2) = V_{\\text{loc}} + V_{\\text{struct}}",
          "role": "The main decomposition result."
        }
      ],
      "references": [],
      "math_tools": [
        {
          "toolName": "Optimal Transport",
          "field": "Optimal Transport",
          "description": "Mathematical framework for comparing probability measures via minimal cost transport plans.",
          "roleInProof": "Defines the Wasserstein distance and optimal couplings used to decompose the total cost.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Wasserstein Distance",
            "Coupling"
          ]
        },
        {
          "toolName": "Quadratic Forms",
          "field": "Linear Algebra",
          "description": "Functions of the form q(x) = \u27e6x, Ax\u27e6 where A is symmetric positive semi-definite.",
          "roleInProof": "Expresses the hypocoercive cost and enables expansion and integration of the transport cost.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Inner Product",
            "Bilinear Forms"
          ]
        },
        {
          "toolName": "Barycenters",
          "field": "Probability Theory",
          "description": "The mean or center of mass of a probability measure, generalizing the expectation.",
          "roleInProof": "Used to shift measures to centered versions, separating global shifts from local variations in the decomposition.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Empirical Measures",
            "Centering"
          ]
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "introductory",
          "text": "This fundamental decomposition theorem for Wasserstein distances with quadratic costs is a consequence of the gluing lemma in optimal transport and the geometry of barycenters. We provide a complete proof adapted to the hypocoercive cost structure."
        },
        {
          "type": "concluding",
          "text": "This decomposition is exact and holds for any pair of measures with finite second moments and any quadratic cost function."
        }
      ],
      "gaps": [],
      "tags": [
        "wasserstein",
        "decomposition",
        "optimal-transport",
        "barycenters",
        "quadratic-form",
        "hypocoercive-cost",
        "coupling"
      ],
      "document_id": "03_cloning",
      "section": "## 3. The Augmented Hypocoercive Lyapunov Function",
      "span": {
        "start_line": 476,
        "end_line": 614,
        "content_start": 478,
        "content_end": 613,
        "header_lines": [
          477
        ]
      },
      "metadata": {
        "label": "proof-lem-wasserstein-decomposition"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 3,
        "chapter_file": "chapter_3.json",
        "section_id": "## 3. The Augmented Hypocoercive Lyapunov Function"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "optimal-transport",
      "wasserstein-distance",
      "decomposition",
      "hypocoercive",
      "location-error",
      "structural-error",
      "swarms"
    ],
    "content_markdown": ":label: lem-wasserstein-decomposition\n\nThe total inter-swarm error, as measured by the squared hypocoercive Wasserstein distance $W_h^2(\\mu_1, \\mu_2)$ between the two swarms' full empirical measures $\\mu_1$ and $\\mu_2$, decomposes exactly into the sum of the location and structural error components:\n\n$$\nW_h^2(\\mu_1, \\mu_2) = V_{\\text{loc}} + V_{\\text{struct}}",
    "raw_directive": "466: A key result from optimal transport theory allows us to relate these components. The total distance between two distributions can be precisely decomposed into the distance between their centers of mass and the distance between their centered shapes.\n467: \n468: :::{prf:lemma} Decomposition of the Hypocoercive Wasserstein Distance\n469: :label: lem-wasserstein-decomposition\n470: \n471: The total inter-swarm error, as measured by the squared hypocoercive Wasserstein distance $W_h^2(\\mu_1, \\mu_2)$ between the two swarms' full empirical measures $\\mu_1$ and $\\mu_2$, decomposes exactly into the sum of the location and structural error components:\n472: \n473: $$\n474: W_h^2(\\mu_1, \\mu_2) = V_{\\text{loc}} + V_{\\text{struct}}\n475: $$",
    "document_id": "03_cloning",
    "section": "## 3. The Augmented Hypocoercive Lyapunov Function",
    "span": {
      "start_line": 466,
      "end_line": 475,
      "content_start": 469,
      "content_end": 474,
      "header_lines": [
        467
      ]
    },
    "references": [],
    "metadata": {
      "label": "lem-wasserstein-decomposition"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 3,
      "chapter_file": "chapter_3.json",
      "section_id": "## 3. The Augmented Hypocoercive Lyapunov Function"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-sx-implies-variance",
    "title": "Structural Positional Error and Internal Variance",
    "type": "lemma",
    "nl_statement": "The positional component of the structural error between the alive-walker distributions of two swarms is at most twice the sum of their physical internal positional variances of alive walkers.",
    "equations": [
      {
        "label": null,
        "latex": "k_1 := |\\mathcal{A}(S_1)| \\quad k_2 := |\\mathcal{A}(S_2)|"
      },
      {
        "label": null,
        "latex": "V_{\\text{x,struct}} \\text{ as the positional component of the structural error between the two swarms' alive-walker distributions}"
      },
      {
        "label": null,
        "latex": "\\text{Var}_k(x) := \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2 \\text{ as the physical internal positional variance of the alive walkers in swarm } k"
      },
      {
        "label": null,
        "latex": "V_{\\text{x,struct}} \\le 2(\\text{Var}_1(x) + \\text{Var}_2(x))"
      }
    ],
    "hypotheses": [
      {
        "text": "Let $k_1 := |\\mathcal{A}(S_1)|$ and $k_2 := |\\mathcal{A}(S_2)|$ denote the numbers of alive walkers in each swarm.",
        "latex": "k_1 := |\\mathcal{A}(S_1)|, \\quad k_2 := |\\mathcal{A}(S_2)|"
      },
      {
        "text": "$V_{\\text{x,struct}}$ is the positional component of the structural error between the two swarms' alive-walker distributions.",
        "latex": "V_{\\text{x,struct}} \\text{ as the positional component of the structural error between the two swarms' alive-walker distributions}"
      },
      {
        "text": "$\\text{Var}_k(x) := \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2$ is the physical internal positional variance of the alive walkers in swarm $k$ (distinct from the $N$-normalized Lyapunov variance component $V_{\\text{Var},x}$).",
        "latex": "\\text{Var}_k(x) := \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2"
      }
    ],
    "conclusion": {
      "text": "$V_{\\text{x,struct}} \\le 2(\\text{Var}_1(x) + \\text{Var}_2(x))$",
      "latex": "V_{\\text{x,struct}} \\le 2(\\text{Var}_1(x) + \\text{Var}_2(x))"
    },
    "variables": [
      {
        "symbol": "k_1",
        "name": "number of alive walkers in swarm 1",
        "description": "Size of the alive set in swarm 1",
        "constraints": [
          "positive integer"
        ],
        "tags": [
          "swarm 1",
          "alive walkers"
        ]
      },
      {
        "symbol": "k_2",
        "name": "number of alive walkers in swarm 2",
        "description": "Size of the alive set in swarm 2",
        "constraints": [
          "positive integer"
        ],
        "tags": [
          "swarm 2",
          "alive walkers"
        ]
      },
      {
        "symbol": "V_{\\text{x,struct}}",
        "name": "positional structural error",
        "description": "Positional component of the structural error between alive-walker distributions",
        "constraints": [
          "non-negative"
        ],
        "tags": [
          "error",
          "positional"
        ]
      },
      {
        "symbol": "\\text{Var}_k(x)",
        "name": "internal positional variance for swarm k",
        "description": "k_alive-normalized variance of positions of alive walkers in swarm k",
        "constraints": [
          "non-negative"
        ],
        "tags": [
          "variance",
          "internal",
          "positional"
        ]
      },
      {
        "symbol": "\\text{Var}_1(x)",
        "name": "internal positional variance for swarm 1",
        "description": "Variance for alive walkers in swarm 1",
        "constraints": [
          "non-negative"
        ],
        "tags": [
          "swarm 1",
          "variance"
        ]
      },
      {
        "symbol": "\\text{Var}_2(x)",
        "name": "internal positional variance for swarm 2",
        "description": "Variance for alive walkers in swarm 2",
        "constraints": [
          "non-negative"
        ],
        "tags": [
          "swarm 2",
          "variance"
        ]
      },
      {
        "symbol": "\\delta_{x,k,i}",
        "name": "position deviation for walker i in swarm k",
        "description": "Position vector of alive walker i relative to center or reference",
        "constraints": [],
        "tags": [
          "position",
          "deviation"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Swarms $S_1$ and $S_2$ are defined with alive walker sets $\\mathcal{A}(S_k)$ non-empty.",
        "confidence": 0.9
      },
      {
        "text": "The norm $\\|| \\cdot \\||$ is the Euclidean norm on positions.",
        "confidence": 0.95
      },
      {
        "text": "The structural error is defined such that its positional component measures mismatch in alive-walker distributions.",
        "confidence": 1.0
      },
      {
        "text": "Variances are computed with respect to a common reference frame for positions.",
        "confidence": 0.8
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-sx-implies-variance",
      "title": null,
      "type": "proof",
      "proves": "lem-sx-implies-variance",
      "proof_type": "construction",
      "proof_status": "complete",
      "content_markdown": ":::{prf:proof}\n:label: proof-lem-sx-implies-variance\n**Proof.**\n\nThe proof is in two parts. First, we rigorously establish the primary inequality by analyzing the optimal transport structure and using a carefully constructed sub-optimal coupling. Second, we demonstrate the consequence using a proof by contradiction.\n\n**Part 1: Rigorous Proof of the Main Inequality**\n\nLet $\\tilde{\\mu}_1$ and $\\tilde{\\mu}_2$ denote the centered empirical measures of the alive walkers in swarms $S_1$ and $S_2$:\n\n$$\n\\tilde{\\mu}_k = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\delta_{\\delta_{x,k,i}}\n$$\n\nwhere $\\delta_{x,k,i} = x_{k,i} - \\mu_{x,k}$ are the centered position vectors and $\\mu_{x,k} = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} x_{k,i}$ is the positional barycenter.\n\nThe structural positional error is defined as the squared Wasserstein distance:\n\n$$\nV_{\\text{x,struct}} := W_2^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) = \\inf_{\\gamma \\in \\Gamma(\\tilde{\\mu}_1, \\tilde{\\mu}_2)} \\int \\|\\delta_{x,1} - \\delta_{x,2}\\|^2 \\, d\\gamma(\\delta_{x,1}, \\delta_{x,2})\n$$\n\nwhere $\\Gamma(\\tilde{\\mu}_1, \\tilde{\\mu}_2)$ is the set of couplings (joint probability measures with marginals $\\tilde{\\mu}_1$ and $\\tilde{\\mu}_2$).\n\n**Step 1.1: Construction of a sub-optimal coupling.**\n\nWe construct a specific coupling $\\gamma_{\\text{id}}$ to obtain an upper bound. Let $m := \\min(k_1, k_2)$ where $k_1 = |\\mathcal{A}(S_1)|$ and $k_2 = |\\mathcal{A}(S_2)|$.\n\nWithout loss of generality, relabel the walkers in each swarm by their indices $1, 2, \\ldots, k_1$ and $1, 2, \\ldots, k_2$. Define the **identity-plus-remainder coupling** $\\gamma_{\\text{id}}$ as follows:\n\n- For $i \\leq m$: couple walker $i$ in swarm 1 with walker $i$ in swarm 2 with mass $1/\\max(k_1, k_2)$.\n- For the excess walkers in the larger swarm: couple each with an arbitrary uniform distribution over the other swarm.\n\nThe precise construction depends on the relative sizes, but the key property is that this coupling costs at most the sum of:\n1. The average squared centered norm in swarm 1: $\\frac{1}{k_1} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2$\n2. The average squared centered norm in swarm 2: $\\frac{1}{k_2} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2$\n\n**Step 1.2: Bounding the cost of the identity coupling (equal sizes).**\n\nFirst consider the case $k_1 = k_2 = k$. The identity coupling matches walker $i$ to walker $i$. Its cost is:\n\n$$\n\\int \\|\\delta_{x,1} - \\delta_{x,2}\\|^2 \\, d\\gamma_{\\text{id}} = \\frac{1}{k} \\sum_{i=1}^k \\|\\delta_{x,1,i} - \\delta_{x,2,i}\\|^2\n$$\n\nUsing the elementary inequality $\\|a - b\\|^2 \\leq 2\\|a\\|^2 + 2\\|b\\|^2$ for any $a, b \\in \\mathbb{R}^d$ (which follows from $\\|a-b\\|^2 = \\|a\\|^2 - 2\\langle a, b \\rangle + \\|b\\|^2 \\leq \\|a\\|^2 + \\|b\\|^2 + |\\langle a, b \\rangle|^2 \\leq \\|a\\|^2 + \\|b\\|^2 + \\|a\\|^2 + \\|b\\|^2$ by Cauchy-Schwarz and the polarization identity):\n\n$$\n\\|\\delta_{x,1,i} - \\delta_{x,2,i}\\|^2 \\leq 2\\|\\delta_{x,1,i}\\|^2 + 2\\|\\delta_{x,2,i}\\|^2\n$$\n\nSumming over all $i$ and dividing by $k$:\n\n$$\n\\begin{aligned}\n\\frac{1}{k} \\sum_{i=1}^k \\|\\delta_{x,1,i} - \\delta_{x,2,i}\\|^2 &\\leq \\frac{2}{k} \\sum_{i=1}^k \\|\\delta_{x,1,i}\\|^2 + \\frac{2}{k} \\sum_{i=1}^k \\|\\delta_{x,2,i}\\|^2 \\\\\n&= 2\\text{Var}_1(x) + 2\\text{Var}_2(x)\n\\end{aligned}\n$$\n\n**Step 1.3: Extension to unequal sizes.**\n\nFor unequal sizes $k_1 \\neq k_2$, a more careful analysis is required. Consider a coupling that matches $\\min(k_1, k_2)$ pairs and distributes the excess mass. By the triangle inequality for Wasserstein distances and properties of Dirac measures, one can show that the cost is still bounded by $2(\\text{Var}_1(x) + \\text{Var}_2(x))$.\n\nSpecifically, for any centered measure $\\tilde{\\mu}$, we have $W_2^2(\\tilde{\\mu}, \\delta_0) = \\int \\|\\delta_x\\|^2 \\, d\\tilde{\\mu}(\\delta_x) = \\text{Var}(x)$ where $\\delta_0$ is the Dirac measure at the origin. Using the triangle inequality:\n\n$$\nW_2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) \\leq W_2(\\tilde{\\mu}_1, \\delta_0) + W_2(\\delta_0, \\tilde{\\mu}_2) = \\sqrt{\\text{Var}_1(x)} + \\sqrt{\\text{Var}_2(x)}\n$$\n\nSquaring both sides and using $(a + b)^2 \\leq 2a^2 + 2b^2$:\n\n$$\nW_2^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) \\leq \\left(\\sqrt{\\text{Var}_1(x)} + \\sqrt{\\text{Var}_2(x)}\\right)^2 \\leq 2\\text{Var}_1(x) + 2\\text{Var}_2(x)\n$$\n\n**Step 1.4: Conclusion of Part 1.**\n\nSince the Wasserstein distance is the infimum over all couplings and we've constructed a coupling with cost at most $2(\\text{Var}_1(x) + \\text{Var}_2(x))$:\n\n$$\nV_{\\text{x,struct}} = W_2^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) \\leq 2(\\text{Var}_1(x) + \\text{Var}_2(x))\n$$\n\nThis establishes the main inequality rigorously.\n\n**Part 2: Proof of the Consequence**\n\nWe prove the implication $V_{\\text{x,struct}} > R^2_{\\text{spread}} \\implies \\exists k \\in \\{1,2\\} : \\text{Var}_k(x) > R^2_{\\text{spread}}/4$ by contrapositive.\n\n**Contrapositive statement:** If $\\text{Var}_1(x) \\leq R^2_{\\text{spread}}/4$ and $\\text{Var}_2(x) \\leq R^2_{\\text{spread}}/4$, then $V_{\\text{x,struct}} \\leq R^2_{\\text{spread}}$.\n\n**Proof of contrapositive:** Assume $\\text{Var}_1(x) \\leq R^2_{\\text{spread}}/4$ and $\\text{Var}_2(x) \\leq R^2_{\\text{spread}}/4$. By the inequality established in Part 1:\n\n$$\nV_{\\text{x,struct}} \\leq 2(\\text{Var}_1(x) + \\text{Var}_2(x)) \\leq 2\\left(\\frac{R^2_{\\text{spread}}}{4} + \\frac{R^2_{\\text{spread}}}{4}\\right) = 2 \\cdot \\frac{R^2_{\\text{spread}}}{2} = R^2_{\\text{spread}}\n$$\n\nThis proves the contrapositive statement. By logical equivalence, the original implication is proven: if $V_{\\text{x,struct}} > R^2_{\\text{spread}}$, then at least one swarm must satisfy $\\text{Var}_k(x) > R^2_{\\text{spread}}/4$.",
      "raw_directive": "638: Consequently, if $V_{\\text{x,struct}} > R^2_{\\text{spread}}$ for some threshold $R_{\\text{spread}}$, then at least one swarm $k$ must have an internal variance $\\text{Var}_k(x) > R^2_{\\text{spread}} / 4$.\n639: :::\n640: :::{prf:proof}\n641: :label: proof-lem-sx-implies-variance\n642: **Proof.**\n643: \n644: The proof is in two parts. First, we rigorously establish the primary inequality by analyzing the optimal transport structure and using a carefully constructed sub-optimal coupling. Second, we demonstrate the consequence using a proof by contradiction.\n645: \n646: **Part 1: Rigorous Proof of the Main Inequality**\n647: \n648: Let $\\tilde{\\mu}_1$ and $\\tilde{\\mu}_2$ denote the centered empirical measures of the alive walkers in swarms $S_1$ and $S_2$:\n649: \n650: $$\n651: \\tilde{\\mu}_k = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\delta_{\\delta_{x,k,i}}\n652: $$\n653: \n654: where $\\delta_{x,k,i} = x_{k,i} - \\mu_{x,k}$ are the centered position vectors and $\\mu_{x,k} = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} x_{k,i}$ is the positional barycenter.\n655: \n656: The structural positional error is defined as the squared Wasserstein distance:\n657: \n658: $$\n659: V_{\\text{x,struct}} := W_2^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) = \\inf_{\\gamma \\in \\Gamma(\\tilde{\\mu}_1, \\tilde{\\mu}_2)} \\int \\|\\delta_{x,1} - \\delta_{x,2}\\|^2 \\, d\\gamma(\\delta_{x,1}, \\delta_{x,2})\n660: $$\n661: \n662: where $\\Gamma(\\tilde{\\mu}_1, \\tilde{\\mu}_2)$ is the set of couplings (joint probability measures with marginals $\\tilde{\\mu}_1$ and $\\tilde{\\mu}_2$).\n663: \n664: **Step 1.1: Construction of a sub-optimal coupling.**\n665: \n666: We construct a specific coupling $\\gamma_{\\text{id}}$ to obtain an upper bound. Let $m := \\min(k_1, k_2)$ where $k_1 = |\\mathcal{A}(S_1)|$ and $k_2 = |\\mathcal{A}(S_2)|$.\n667: \n668: Without loss of generality, relabel the walkers in each swarm by their indices $1, 2, \\ldots, k_1$ and $1, 2, \\ldots, k_2$. Define the **identity-plus-remainder coupling** $\\gamma_{\\text{id}}$ as follows:\n669: \n670: - For $i \\leq m$: couple walker $i$ in swarm 1 with walker $i$ in swarm 2 with mass $1/\\max(k_1, k_2)$.\n671: - For the excess walkers in the larger swarm: couple each with an arbitrary uniform distribution over the other swarm.\n672: \n673: The precise construction depends on the relative sizes, but the key property is that this coupling costs at most the sum of:\n674: 1. The average squared centered norm in swarm 1: $\\frac{1}{k_1} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2$\n675: 2. The average squared centered norm in swarm 2: $\\frac{1}{k_2} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2$\n676: \n677: **Step 1.2: Bounding the cost of the identity coupling (equal sizes).**\n678: \n679: First consider the case $k_1 = k_2 = k$. The identity coupling matches walker $i$ to walker $i$. Its cost is:\n680: \n681: $$\n682: \\int \\|\\delta_{x,1} - \\delta_{x,2}\\|^2 \\, d\\gamma_{\\text{id}} = \\frac{1}{k} \\sum_{i=1}^k \\|\\delta_{x,1,i} - \\delta_{x,2,i}\\|^2\n683: $$\n684: \n685: Using the elementary inequality $\\|a - b\\|^2 \\leq 2\\|a\\|^2 + 2\\|b\\|^2$ for any $a, b \\in \\mathbb{R}^d$ (which follows from $\\|a-b\\|^2 = \\|a\\|^2 - 2\\langle a, b \\rangle + \\|b\\|^2 \\leq \\|a\\|^2 + \\|b\\|^2 + |\\langle a, b \\rangle|^2 \\leq \\|a\\|^2 + \\|b\\|^2 + \\|a\\|^2 + \\|b\\|^2$ by Cauchy-Schwarz and the polarization identity):\n686: \n687: $$\n688: \\|\\delta_{x,1,i} - \\delta_{x,2,i}\\|^2 \\leq 2\\|\\delta_{x,1,i}\\|^2 + 2\\|\\delta_{x,2,i}\\|^2\n689: $$\n690: \n691: Summing over all $i$ and dividing by $k$:\n692: \n693: $$\n694: \\begin{aligned}\n695: \\frac{1}{k} \\sum_{i=1}^k \\|\\delta_{x,1,i} - \\delta_{x,2,i}\\|^2 &\\leq \\frac{2}{k} \\sum_{i=1}^k \\|\\delta_{x,1,i}\\|^2 + \\frac{2}{k} \\sum_{i=1}^k \\|\\delta_{x,2,i}\\|^2 \\\\\n696: &= 2\\text{Var}_1(x) + 2\\text{Var}_2(x)\n697: \\end{aligned}\n698: $$\n699: \n700: **Step 1.3: Extension to unequal sizes.**\n701: \n702: For unequal sizes $k_1 \\neq k_2$, a more careful analysis is required. Consider a coupling that matches $\\min(k_1, k_2)$ pairs and distributes the excess mass. By the triangle inequality for Wasserstein distances and properties of Dirac measures, one can show that the cost is still bounded by $2(\\text{Var}_1(x) + \\text{Var}_2(x))$.\n703: \n704: Specifically, for any centered measure $\\tilde{\\mu}$, we have $W_2^2(\\tilde{\\mu}, \\delta_0) = \\int \\|\\delta_x\\|^2 \\, d\\tilde{\\mu}(\\delta_x) = \\text{Var}(x)$ where $\\delta_0$ is the Dirac measure at the origin. Using the triangle inequality:\n705: \n706: $$\n707: W_2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) \\leq W_2(\\tilde{\\mu}_1, \\delta_0) + W_2(\\delta_0, \\tilde{\\mu}_2) = \\sqrt{\\text{Var}_1(x)} + \\sqrt{\\text{Var}_2(x)}\n708: $$\n709: \n710: Squaring both sides and using $(a + b)^2 \\leq 2a^2 + 2b^2$:\n711: \n712: $$\n713: W_2^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) \\leq \\left(\\sqrt{\\text{Var}_1(x)} + \\sqrt{\\text{Var}_2(x)}\\right)^2 \\leq 2\\text{Var}_1(x) + 2\\text{Var}_2(x)\n714: $$\n715: \n716: **Step 1.4: Conclusion of Part 1.**\n717: \n718: Since the Wasserstein distance is the infimum over all couplings and we've constructed a coupling with cost at most $2(\\text{Var}_1(x) + \\text{Var}_2(x))$:\n719: \n720: $$\n721: V_{\\text{x,struct}} = W_2^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) \\leq 2(\\text{Var}_1(x) + \\text{Var}_2(x))\n722: $$\n723: \n724: This establishes the main inequality rigorously.\n725: \n726: **Part 2: Proof of the Consequence**\n727: \n728: We prove the implication $V_{\\text{x,struct}} > R^2_{\\text{spread}} \\implies \\exists k \\in \\{1,2\\} : \\text{Var}_k(x) > R^2_{\\text{spread}}/4$ by contrapositive.\n729: \n730: **Contrapositive statement:** If $\\text{Var}_1(x) \\leq R^2_{\\text{spread}}/4$ and $\\text{Var}_2(x) \\leq R^2_{\\text{spread}}/4$, then $V_{\\text{x,struct}} \\leq R^2_{\\text{spread}}$.\n731: \n732: **Proof of contrapositive:** Assume $\\text{Var}_1(x) \\leq R^2_{\\text{spread}}/4$ and $\\text{Var}_2(x) \\leq R^2_{\\text{spread}}/4$. By the inequality established in Part 1:\n733: \n734: $$\n735: V_{\\text{x,struct}} \\leq 2(\\text{Var}_1(x) + \\text{Var}_2(x)) \\leq 2\\left(\\frac{R^2_{\\text{spread}}}{4} + \\frac{R^2_{\\text{spread}}}{4}\\right) = 2 \\cdot \\frac{R^2_{\\text{spread}}}{2} = R^2_{\\text{spread}}\n736: $$\n737: \n738: This proves the contrapositive statement. By logical equivalence, the original implication is proven: if $V_{\\text{x,struct}} > R^2_{\\text{spread}}$, then at least one swarm must satisfy $\\text{Var}_k(x) > R^2_{\\text{spread}}/4$.\n739: ",
      "strategy_summary": "The proof first constructs a sub-optimal coupling to bound the squared Wasserstein distance V_x,struct by 2(Var_1(x) + Var_2(x)) using the triangle inequality and basic norm inequalities. It then uses the contrapositive to show that if both variances are at most R_spread\u00b2/4, then V_x,struct \u2264 R_spread\u00b2, implying the desired consequence.",
      "conclusion": {
        "text": "if $V_{\\text{x,struct}} > R^2_{\\text{spread}}$, then at least one swarm $k$ must have $\\text{Var}_k(x) > R^2_{\\text{spread}} / 4$.",
        "latex": "if $V_{\\text{x,struct}} > R^2_{\\text{spread}}$, then at least one swarm $k$ must have $\\text{Var}_k(x) > R^2_{\\text{spread}} / 4$."
      },
      "assumptions": [
        {
          "text": "The measures $\\tilde{\\mu}_1$ and $\\tilde{\\mu}_2$ are centered empirical measures of alive walkers in swarms $S_1$ and $S_2$.",
          "latex": "$\\tilde{\\mu}_k = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\delta_{\\delta_{x,k,i}}$ where $\\delta_{x,k,i} = x_{k,i} - \\mu_{x,k}$ and $\\mu_{x,k}$ is the barycenter."
        },
        {
          "text": "The swarms have positive numbers of alive walkers $k_1, k_2 > 0$.",
          "latex": "$k_1 = |\\mathcal{A}(S_1)|$, $k_2 = |\\mathcal{A}(S_2)| > 0$."
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "definition",
          "text": "Define centered empirical measures $\\tilde{\\mu}_1$ and $\\tilde{\\mu}_2$.",
          "latex": "$\\tilde{\\mu}_k = \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\delta_{\\delta_{x,k,i}}$",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 2.0,
          "kind": "definition",
          "text": "Define $V_{\\text{x,struct}} := W_2^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2)$ as the squared Wasserstein-2 distance.",
          "latex": "$V_{\\text{x,struct}} := W_2^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) = \\inf_{\\gamma \\in \\Gamma(\\tilde{\\mu}_1, \\tilde{\\mu}_2)} \\int \\|\\delta_{x,1} - \\delta_{x,2}\\|^2 \\, d\\gamma(\\delta_{x,1}, \\delta_{x,2})$",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 3.0,
          "kind": "construction",
          "text": "Construct sub-optimal identity-plus-remainder coupling $\\gamma_{\\text{id}}$ for upper bounding the cost.",
          "latex": null,
          "references": [],
          "derived_statement": "Cost of $\\gamma_{\\text{id}}$ at most sum of average squared centered norms."
        },
        {
          "order": 4.0,
          "kind": "case",
          "text": "Case of equal sizes $k_1 = k_2 = k$: Use identity coupling and norm inequality.",
          "latex": "$\\frac{1}{k} \\sum_{i=1}^k \\|\\delta_{x,1,i} - \\delta_{x,2,i}\\|^2 \\leq 2\\text{Var}_1(x) + 2\\text{Var}_2(x)$",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 5.0,
          "kind": "case",
          "text": "Case of unequal sizes: Use triangle inequality $W_2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) \\leq \\sqrt{\\text{Var}_1(x)} + \\sqrt{\\text{Var}_2(x)}$, then square and bound.",
          "latex": "$W_2^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) \\leq 2\\text{Var}_1(x) + 2\\text{Var}_2(x)$",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 6.0,
          "kind": "conclusion",
          "text": "Thus, $V_{\\text{x,struct}} \\leq 2(\\text{Var}_1(x) + \\text{Var}_2(x))$ for any sizes.",
          "latex": "$V_{\\text{x,struct}} = W_2^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) \\leq 2(\\text{Var}_1(x) + \\text{Var}_2(x))$",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 7.0,
          "kind": "contrapositive",
          "text": "Assume $\\text{Var}_1(x) \\leq R^2_{\\text{spread}}/4$ and $\\text{Var}_2(x) \\leq R^2_{\\text{spread}}/4$.",
          "latex": null,
          "references": [],
          "derived_statement": null
        },
        {
          "order": 8.0,
          "kind": "derivation",
          "text": "Then $V_{\\text{x,struct}} \\leq 2(R^2_{\\text{spread}}/4 + R^2_{\\text{spread}}/4) = R^2_{\\text{spread}}$.",
          "latex": "$V_{\\text{x,struct}} \\leq 2(\\text{Var}_1(x) + \\text{Var}_2(x)) \\leq R^2_{\\text{spread}}$",
          "references": [],
          "derived_statement": "Contrapositive holds, proving the implication."
        }
      ],
      "key_equations": [
        {
          "label": "eq-var-struct-def",
          "latex": "$V_{\\text{x,struct}} := W_2^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) = \\inf_{\\gamma \\in \\Gamma(\\tilde{\\mu}_1, \\tilde{\\mu}_2)} \\int \\|\\delta_{x,1} - \\delta_{x,2}\\|^2 \\, d\\gamma(\\delta_{x,1}, \\delta_{x,2})$",
          "role": "Definition of structural variance"
        },
        {
          "label": "eq-norm-ineq",
          "latex": "$\\|\\delta_{x,1,i} - \\delta_{x,2,i}\\|^2 \\leq 2\\|\\delta_{x,1,i}\\|^2 + 2\\|\\delta_{x,2,i}\\|^2$",
          "role": "Key inequality for bounding coupling cost"
        },
        {
          "label": "eq-w2-bound",
          "latex": "$W_2^2(\\tilde{\\mu}_1, \\tilde{\\mu}_2) \\leq 2\\text{Var}_1(x) + 2\\text{Var}_2(x)$",
          "role": "Main upper bound on Wasserstein distance"
        },
        {
          "label": "eq-contrap",
          "latex": "$V_{\\text{x,struct}} \\leq 2(\\text{Var}_1(x) + \\text{Var}_2(x)) \\leq R^2_{\\text{spread}}$",
          "role": "Contrapositive derivation"
        }
      ],
      "references": [],
      "math_tools": [
        {
          "toolName": "Wasserstein-2 distance",
          "field": "Optimal Transport",
          "description": "The 2-Wasserstein distance between two probability measures is the square root of the infimum over couplings of the expected squared Euclidean distance between points.",
          "roleInProof": "Defines the structural variance V_x,struct and is bounded using triangle inequality to relate it to internal variances.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Coupling"
          ]
        },
        {
          "toolName": "Optimal Coupling",
          "field": "Probability Theory",
          "description": "A coupling is a joint distribution with given marginals; the optimal one minimizes the transport cost for Wasserstein distance.",
          "roleInProof": "A sub-optimal identity-plus-remainder coupling is constructed to upper-bound the Wasserstein distance cost.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Wasserstein-2 distance"
          ]
        },
        {
          "toolName": "Triangle Inequality",
          "field": "Metric Spaces",
          "description": "For any metric space, d(x,z) \u2264 d(x,y) + d(y,z); extends to Wasserstein distances.",
          "roleInProof": "Applied to bound W_2(\u03bc1, \u03bc2) \u2264 W_2(\u03bc1, \u03b4_0) + W_2(\u03b4_0, \u03bc2), where \u03b4_0 is the Dirac at the origin.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Wasserstein-2 distance"
          ]
        },
        {
          "toolName": "Norm Inequality",
          "field": "Linear Algebra",
          "description": "The inequality ||a - b||\u00b2 \u2264 2||a||\u00b2 + 2||b||\u00b2 holds for Euclidean norms.",
          "roleInProof": "Used to bound the cost of the identity coupling by twice the sum of variances.",
          "levelOfAbstraction": "Technique",
          "relatedTools": []
        }
      ],
      "cases": [
        {
          "name": "Equal swarm sizes",
          "condition": "$k_1 = k_2 = k$",
          "summary": "Identity coupling cost bounded using norm inequality."
        },
        {
          "name": "Unequal swarm sizes",
          "condition": "$k_1 \\neq k_2$",
          "summary": "Triangle inequality on Wasserstein distances to bound the cost."
        }
      ],
      "remarks": [],
      "gaps": [],
      "tags": [
        "wasserstein-distance",
        "variance",
        "coupling",
        "contrapositive",
        "optimal-transport",
        "triangle-inequality"
      ],
      "document_id": "03_cloning",
      "section": "## 3. The Augmented Hypocoercive Lyapunov Function",
      "span": {
        "start_line": 638,
        "end_line": 739,
        "content_start": 640,
        "content_end": 738,
        "header_lines": [
          639
        ]
      },
      "metadata": {
        "label": "proof-lem-sx-implies-variance"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 3,
        "chapter_file": "chapter_3.json",
        "section_id": "## 3. The Augmented Hypocoercive Lyapunov Function"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "structural error",
      "positional variance",
      "swarms",
      "alive walkers",
      "inequality",
      "causal chain"
    ],
    "content_markdown": ":label: lem-sx-implies-variance\n\nLet $k_1 := |\\mathcal{A}(S_1)|$ and $k_2 := |\\mathcal{A}(S_2)|$ denote the numbers of alive walkers in each swarm. Define:\n\n- $V_{\\text{x,struct}}$ as the positional component of the structural error between the two swarms' **alive-walker distributions**\n- $\\text{Var}_k(x) := \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2$ as the **physical internal positional variance** of the **alive walkers** in swarm $k$ (note: this is $k_{\\text{alive}}$-normalized, representing the actual spread of alive walkers, distinct from the Lyapunov variance component $V_{Var,x}$ which is $N$-normalized)\n\nThen:\n\n$$\nV_{\\text{x,struct}} \\le 2(\\text{Var}_1(x) + \\text{Var}_2(x))\n$$",
    "raw_directive": "622: The first step in our causal chain is to connect the state of the coupled system to the internal configuration of the individual swarms. A large mismatch in the geometric shapes of the two swarms, as measured by the positional component of the structural error ($V_{x,\\text{struct}}$), implies that at least one of the swarms must be internally spread out, i.e., have a large positional variance. This lemma makes that connection rigorous.\n623: \n624: :::{prf:lemma} Structural Positional Error and Internal Variance\n625: :label: lem-sx-implies-variance\n626: \n627: Let $k_1 := |\\mathcal{A}(S_1)|$ and $k_2 := |\\mathcal{A}(S_2)|$ denote the numbers of alive walkers in each swarm. Define:\n628: \n629: - $V_{\\text{x,struct}}$ as the positional component of the structural error between the two swarms' **alive-walker distributions**\n630: - $\\text{Var}_k(x) := \\frac{1}{k_{\\text{alive}}} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2$ as the **physical internal positional variance** of the **alive walkers** in swarm $k$ (note: this is $k_{\\text{alive}}$-normalized, representing the actual spread of alive walkers, distinct from the Lyapunov variance component $V_{Var,x}$ which is $N$-normalized)\n631: \n632: Then:\n633: \n634: $$\n635: V_{\\text{x,struct}} \\le 2(\\text{Var}_1(x) + \\text{Var}_2(x))\n636: $$\n637: ",
    "document_id": "03_cloning",
    "section": "## 3. The Augmented Hypocoercive Lyapunov Function",
    "span": {
      "start_line": 622,
      "end_line": 637,
      "content_start": 625,
      "content_end": 636,
      "header_lines": [
        623
      ]
    },
    "references": [],
    "metadata": {
      "label": "lem-sx-implies-variance"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 3,
      "chapter_file": "chapter_3.json",
      "section_id": "## 3. The Augmented Hypocoercive Lyapunov Function"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-greedy-preserves-signal",
    "title": "Greedy Pairing Guarantees Signal Separation",
    "type": "lemma",
    "nl_statement": "In a swarm state with high internal variance where alive walkers are partitioned into high-error and low-error sets, the sequential stochastic greedy pairing operator guarantees statistical separation in expected raw phase-space distances: large for high-error walkers and small for low-error walkers, with epsilon-dependent bounds.",
    "equations": [
      {
        "label": null,
        "latex": "\\mathbb{E}[d_i \\mid S_t, i \\in H_k] \\ge D_H(\\epsilon)"
      },
      {
        "label": null,
        "latex": "\\mathbb{E}[d_j \\mid S_t, j \\in L_k] \\le R_L(\\epsilon) + D_{\\mathrm{valid}} \\cdot c_k \\exp\\left(-\\frac{D_H(\\epsilon)^2 - R_L(\\epsilon)^2}{2\\epsilon^2}\\right)"
      }
    ],
    "hypotheses": [
      {
        "text": "A swarm is in a state with high internal variance.",
        "latex": null
      },
      {
        "text": "Its alive walkers $A_k$ are partitioned into a high-error set $H_k$ and a low-error set $L_k$ as per def-geometric-partition.",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "The Sequential Stochastic Greedy Pairing Operator guarantees statistical separation in expected raw phase-space distance measurements: (1) For high-error walkers $i \\in H_k$, $\\mathbb{E}[d_i \\mid S_t, i \\in H_k] \\ge D_H(\\epsilon)$; (2) For low-error walkers $j \\in L_k$, $\\mathbb{E}[d_j \\mid S_t, j \\in L_k] \\le R_L(\\epsilon) + D_{\\mathrm{valid}} \\cdot c_k \\exp\\left(-\\frac{D_H(\\epsilon)^2 - R_L(\\epsilon)^2}{2\\epsilon^2}\\right)$, with N-uniform, $\\varepsilon$-dependent bounds.",
      "latex": null
    },
    "variables": [
      {
        "symbol": "A_k",
        "name": "alive walkers",
        "description": "Set of alive walkers in the swarm at iteration k.",
        "constraints": [],
        "tags": [
          "swarm",
          "walkers"
        ]
      },
      {
        "symbol": "H_k",
        "name": "high-error set",
        "description": "Partition of alive walkers with high error.",
        "constraints": [
          "high-error"
        ],
        "tags": [
          "partition",
          "error"
        ]
      },
      {
        "symbol": "L_k",
        "name": "low-error set",
        "description": "Partition of alive walkers with low error.",
        "constraints": [
          "low-error"
        ],
        "tags": [
          "partition",
          "error"
        ]
      },
      {
        "symbol": "S_t",
        "name": "state",
        "description": "Swarm state at time t.",
        "constraints": [
          "high internal variance"
        ],
        "tags": [
          "swarm",
          "state"
        ]
      },
      {
        "symbol": "\\epsilon",
        "name": "epsilon",
        "description": "Noise or error parameter.",
        "constraints": [],
        "tags": [
          "noise",
          "bound"
        ]
      },
      {
        "symbol": "D_H(\\epsilon)",
        "name": "high distance bound",
        "description": "Lower bound on expected distance for high-error walkers, dependent on epsilon.",
        "constraints": [
          "N-uniform"
        ],
        "tags": [
          "bound",
          "distance"
        ]
      },
      {
        "symbol": "R_L(\\epsilon)",
        "name": "low radius bound",
        "description": "Upper bound related to low-error set radius, dependent on epsilon.",
        "constraints": [
          "N-uniform"
        ],
        "tags": [
          "bound",
          "radius"
        ]
      },
      {
        "symbol": "D_{\\mathrm{valid}}",
        "name": "valid distance",
        "description": "Nominal valid pairing distance.",
        "constraints": [],
        "tags": [
          "distance",
          "valid"
        ]
      },
      {
        "symbol": "c_k",
        "name": "constant",
        "description": "Iteration-dependent constant for the exponential term.",
        "constraints": [],
        "tags": [
          "constant"
        ]
      },
      {
        "symbol": "d_i",
        "name": "raw distance i",
        "description": "Raw phase-space distance for walker i.",
        "constraints": [],
        "tags": [
          "distance",
          "measurement"
        ]
      },
      {
        "symbol": "d_j",
        "name": "raw distance j",
        "description": "Raw phase-space distance for walker j.",
        "constraints": [],
        "tags": [
          "distance",
          "measurement"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The partition into H_k and L_k is well-defined based on geometric criteria from def-geometric-partition.",
        "confidence": 0.9
      },
      {
        "text": "The swarm state has sufficiently high internal variance to enable separation.",
        "confidence": 0.8
      },
      {
        "text": "Distances are measured in phase-space with Gaussian noise of variance epsilon^2.",
        "confidence": 0.7
      }
    ],
    "local_refs": [
      "def-geometric-partition",
      "def-greedy-pairing-algorithm"
    ],
    "proof": {
      "label": "proof-lem-greedy-preserves-signal",
      "title": null,
      "type": "proof",
      "proves": "lem-greedy-preserves-signal",
      "proof_type": "probabilistic",
      "proof_status": "complete",
      "content_markdown": ":::{prf:proof}\n:label: proof-lem-greedy-preserves-signal\n**Proof.**\n\nThe proof establishes rigorous probabilistic bounds on the expected distance measurements by carefully analyzing the sequential pairing process. We show that the geometric partition imposed by high variance creates an unavoidable statistical signature in the measurements, regardless of the pairing order.\n\n**Framework: Conditional Expectations and the Sequential Process.**\n\nThe Sequential Stochastic Greedy Pairing algorithm builds the matching iteratively. At any stage of the algorithm, let $P_t$ denote the set of already-paired walkers and $U_t = \\mathcal{A}_k \\setminus P_t$ denote the set of unpaired walkers remaining. For a walker $i$ selected at stage $t$, the probability of pairing with walker $u \\in U_t \\setminus \\{i\\}$ is:\n\n$$\n\\mathbb{P}(c_i = u \\mid U_t, i) = \\frac{\\exp\\left(-\\frac{d_{\\text{alg}}(i,u)^2}{2\\epsilon_d^2}\\right)}{\\sum_{l \\in U_t \\setminus \\{i\\}} \\exp\\left(-\\frac{d_{\\text{alg}}(i,l)^2}{2\\epsilon_d^2}\\right)} =: \\frac{w_{iu}}{Z_i(U_t)}\n$$\n\nwhere $Z_i(U_t) = \\sum_{l \\in U_t \\setminus \\{i\\}} w_{il}$ is the partition function normalizing the softmax distribution.\n\nThe conditional expected distance for walker $i$ given the remaining set $U_t$ is:\n\n$$\n\\mathbb{E}[d_i \\mid U_t, i \\in U_t] = \\sum_{u \\in U_t \\setminus \\{i\\}} d_{\\text{alg}}(i, u) \\cdot \\mathbb{P}(c_i = u \\mid U_t, i)\n$$\n\nThe unconditional expected distance $\\mathbb{E}[d_i \\mid \\mathcal{S}_t, i]$ is obtained by averaging over all possible pairing histories that lead to $i$ being paired.\n\n**Key Insight:** The geometric properties of $H_k$ and $L_k$ (specifically, the separation constants $D_H(\\epsilon)$ and $R_L(\\epsilon)$) provide **uniform bounds** on these conditional expectations that are **independent of the pairing history** $P_t$. This history-independence is the crucial property that allows us to bound the full expectation.\n\n**Part 1: Rigorous Lower Bound for High-Error Walkers.**\n\n**Claim:** For any high-error walker $i \\in H_k$, $\\mathbb{E}[d_i \\mid \\mathcal{S}_t, i \\in H_k] \\geq D_H(\\epsilon) \\cdot \\mathbb{P}(\\text{pair with } L_k) + R_L(\\epsilon) \\cdot \\mathbb{P}(\\text{pair within } H_k)$.\n\nSince the low-error set $L_k$ contains a non-vanishing fraction of walkers ($|L_k| / k \\geq f_L > 0$), and pairing is done uniformly over unpaired walkers, the expected distance is bounded below.\n\n**Step 1.1: Geometric property from corrected {prf:ref}`lem-geometric-separation-of-partition`.**\n\nBy {prf:ref}`lem-geometric-separation-of-partition` (corrected), for a high-error walker $i \\in H_k$:\n- For any low-error walker $u \\in L_k$: $d_{\\text{alg}}(i, u) \\geq D_H(\\epsilon)$\n- For any high-error walker in the same cluster: $d_{\\text{alg}}(i, u) \\leq R_L(\\epsilon)$\n\nThis is a **deterministic geometric property** of the state $\\mathcal{S}_t$.\n\n**Step 1.2: Population-weighted bound on conditional expectations.**\n\nFor any stage $t$ in the pairing process where $i \\in U_t$, partition the unpaired walkers:\n- $U_L := U_t \\cap L_k$ (low-error walkers, far from $i$)\n- $U_H := U_t \\cap H_k \\setminus \\{i\\}$ (other high-error walkers, may be close)\n\nThe conditional expectation decomposes as:\n\n$$\n\\begin{aligned}\n\\mathbb{E}[d_i \\mid U_t, i \\in U_t] &= \\sum_{u \\in U_L} d_{\\text{alg}}(i, u) \\cdot \\mathbb{P}(c_i = u \\mid U_t, i) \\\\\n&\\quad + \\sum_{u \\in U_H} d_{\\text{alg}}(i, u) \\cdot \\mathbb{P}(c_i = u \\mid U_t, i)\n\\end{aligned}\n$$\n\nUsing the geometric bounds:\n\n$$\n\\begin{aligned}\n\\mathbb{E}[d_i \\mid U_t, i \\in U_t] &\\geq \\sum_{u \\in U_L} D_H(\\epsilon) \\cdot \\mathbb{P}(c_i = u \\mid U_t, i) \\\\\n&= D_H(\\epsilon) \\cdot \\mathbb{P}(c_i \\in U_L \\mid U_t, i)\n\\end{aligned}\n$$\n\nSince $|U_L| \\geq |L_k| - k/2 \\geq k \\cdot f_L - k/2 = k(f_L - 1/2) > 0$ for $f_L > 1/2$, the probability of pairing with a low-error walker is bounded below. This gives us a worst-case lower bound by considering the minimum over all possible unpaired sets $U_t$.\n\n**Step 1.3: History-independence and unconditional bound.**\n\nSince the bound $\\mathbb{E}[d_i \\mid U_t, i \\in U_t] \\geq D_H(\\epsilon)$ holds for **every possible set** $U_t$ containing $i$, it holds regardless of the specific pairing history. Taking the expectation over all possible pairing orders:\n\n$$\n\\mathbb{E}[d_i \\mid \\mathcal{S}_t, i \\in H_k] = \\mathbb{E}_{U_t}\\left[\\mathbb{E}[d_i \\mid U_t, i \\in U_t]\\right] \\geq \\mathbb{E}_{U_t}[D_H(\\epsilon)] = D_H(\\epsilon)\n$$\n\nThis establishes the first claim. The bound is **N-uniform** because $D_H(\\epsilon)$ is an N-uniform geometric constant (proven in Chapter 6).\n\n**Part 2: Rigorous Upper Bound for Low-Error Walkers.**\n\n**Claim:** For any low-error walker $j \\in L_k$, $\\mathbb{E}[d_j \\mid \\mathcal{S}_t, j \\in L_k] \\leq R_L(\\epsilon) + (D_{\\text{valid}} - R_L(\\epsilon)) \\cdot c_k \\exp\\left(-\\frac{[D_H(\\epsilon) - R_L(\\epsilon)]^2}{2\\epsilon_d^2}\\right)$ where $c_k$ is N-uniform.\n\n**Step 2.1: Geometric property of low-error walkers.**\n\nBy {prf:ref}`def-geometric-partition`, a walker $j \\in L_k$ has a **local cluster** $C_j \\subset L_k$ with the following properties:\n- $|C_j| \\geq f_c k$ for an N-uniform constant $f_c > 0$\n- For all $l \\in C_j$: $d_{\\text{alg}}(j, l) \\leq R_L(\\epsilon)$\n- For all $m \\notin C_j$: $d_{\\text{alg}}(j, m) \\geq D_H(\\epsilon)$\n\n(Note: The last property follows from the fact that walkers outside the cluster must be either in $H_k$ or in other clusters, both of which are separated by at least $D_H(\\epsilon)$ by the geometric partition structure.)\n\n**Step 2.2: Worst-case cluster depletion bound.**\n\nAt any stage $t$ of the pairing process, at most $\\lfloor k/2 \\rfloor$ pairs have been formed, removing at most $k$ walkers from consideration. In the worst case, all removed walkers could have been from $C_j$. Therefore:\n\n$$\n|U_t \\cap C_j| \\geq |C_j| - k \\geq f_c k - k = k(f_c - 1)\n$$\n\nFor the axiom $f_c > 1/2$ to be meaningful, we typically have $f_c \\geq 2/3$, giving $|U_t \\cap C_j| \\geq k/3 > 0$ (strictly positive cluster survivors).\n\n**Step 2.3: Partition of available companions.**\n\nFor $j$ being paired at stage $t$ with remaining set $U_t$, partition:\n- $U_{\\text{in}} := U_t \\cap C_j$ (nearby cluster members)\n- $U_{\\text{out}} := U_t \\setminus C_j$ (distant walkers)\n\nWe have $|U_{\\text{in}}| \\geq k(f_c - 1) > 0$ and $|U_{\\text{out}}| \\leq k$.\n\n**Step 2.4: Bounding the normalization constant.**\n\nThe partition function for $j$ satisfies:\n\n$$\n\\begin{aligned}\nZ_j(U_t) &= \\sum_{l \\in U_t \\setminus \\{j\\}} \\exp\\left(-\\frac{d_{\\text{alg}}(j,l)^2}{2\\epsilon_d^2}\\right) \\\\\n&= \\sum_{l \\in U_{\\text{in}}} \\exp\\left(-\\frac{d_{\\text{alg}}(j,l)^2}{2\\epsilon_d^2}\\right) + \\sum_{m \\in U_{\\text{out}}} \\exp\\left(-\\frac{d_{\\text{alg}}(j,m)^2}{2\\epsilon_d^2}\\right) \\\\\n&\\geq \\sum_{l \\in U_{\\text{in}}} \\exp\\left(-\\frac{R_L(\\epsilon)^2}{2\\epsilon_d^2}\\right) \\\\\n&= |U_{\\text{in}}| \\exp\\left(-\\frac{R_L(\\epsilon)^2}{2\\epsilon_d^2}\\right) \\\\\n&\\geq k(f_c - 1) \\exp\\left(-\\frac{R_L(\\epsilon)^2}{2\\epsilon_d^2}\\right)\n\\end{aligned}\n$$\n\nusing $d_{\\text{alg}}(j,l) \\leq R_L(\\epsilon)$ for $l \\in U_{\\text{in}}$.\n\n**Step 2.5: Bounding the tail probability.**\n\nThe probability of $j$ being paired with a distant walker is:\n\n$$\n\\begin{aligned}\n\\mathbb{P}(c_j \\in U_{\\text{out}} \\mid U_t, j) &= \\frac{\\sum_{m \\in U_{\\text{out}}} \\exp\\left(-\\frac{d_{\\text{alg}}(j,m)^2}{2\\epsilon_d^2}\\right)}{Z_j(U_t)} \\\\\n&\\leq \\frac{|U_{\\text{out}}| \\exp\\left(-\\frac{D_H(\\epsilon)^2}{2\\epsilon_d^2}\\right)}{|U_{\\text{in}}| \\exp\\left(-\\frac{R_L(\\epsilon)^2}{2\\epsilon_d^2}\\right)} \\\\\n&\\leq \\frac{k}{k(f_c - 1)} \\exp\\left(-\\frac{D_H(\\epsilon)^2 - R_L(\\epsilon)^2}{2\\epsilon_d^2}\\right) \\\\\n&= \\frac{1}{f_c - 1} \\exp\\left(-\\frac{[D_H(\\epsilon) + R_L(\\epsilon)][D_H(\\epsilon) - R_L(\\epsilon)]}{2\\epsilon_d^2}\\right)\n\\end{aligned}\n$$\n\nDefine $c_k := 1/(f_c - 1)$, which is N-uniform. Using $D_H(\\epsilon) + R_L(\\epsilon) \\geq D_H(\\epsilon) - R_L(\\epsilon)$ (since both are positive):\n\n$$\n\\mathbb{P}(c_j \\in U_{\\text{out}} \\mid U_t, j) \\leq c_k \\exp\\left(-\\frac{[D_H(\\epsilon) - R_L(\\epsilon)]^2}{2\\epsilon_d^2}\\right) =: p_{\\text{tail}}\n$$\n\n**Step 2.6: Bounding the conditional expected distance.**\n\n$$\n\\begin{aligned}\n\\mathbb{E}[d_j \\mid U_t, j] &= \\sum_{l \\in U_{\\text{in}}} d_{\\text{alg}}(j,l) \\mathbb{P}(c_j = l \\mid U_t, j) + \\sum_{m \\in U_{\\text{out}}} d_{\\text{alg}}(j,m) \\mathbb{P}(c_j = m \\mid U_t, j) \\\\\n&\\leq R_L(\\epsilon) \\sum_{l \\in U_{\\text{in}}} \\mathbb{P}(c_j = l \\mid U_t, j) + D_{\\text{valid}} \\sum_{m \\in U_{\\text{out}}} \\mathbb{P}(c_j = m \\mid U_t, j) \\\\\n&= R_L(\\epsilon) \\cdot [1 - p_{\\text{tail}}] + D_{\\text{valid}} \\cdot p_{\\text{tail}} \\\\\n&= R_L(\\epsilon) + [D_{\\text{valid}} - R_L(\\epsilon)] p_{\\text{tail}}\n\\end{aligned}\n$$\n\n**Step 2.7: History-independence and unconditional bound.**\n\nThe bound on $\\mathbb{E}[d_j \\mid U_t, j]$ holds for every possible set $U_t$ containing $j$, with the same constants. Therefore:\n\n$$\n\\mathbb{E}[d_j \\mid \\mathcal{S}_t, j \\in L_k] \\leq R_L(\\epsilon) + [D_{\\text{valid}} - R_L(\\epsilon)] \\cdot c_k \\exp\\left(-\\frac{[D_H(\\epsilon) - R_L(\\epsilon)]^2}{2\\epsilon_d^2}\\right)\n$$\n\n**Conclusion:**\n\nBoth bounds are **N-uniform** because:\n- $D_H(\\epsilon), R_L(\\epsilon)$ are N-uniform geometric constants (from Chapter 6)\n- $f_c$ is an N-uniform population fraction (from Chapter 6)\n- $c_k = 1/(f_c - 1)$ is therefore N-uniform\n- $D_{\\text{valid}}$ is a fixed environmental parameter\n- $\\epsilon_d$ is a fixed algorithmic parameter\n\nThis completes the proof that the greedy pairing algorithm reliably detects the geometric partition structure.",
      "raw_directive": "1562: 3.  After completing Chapter 6, **return to this section** to verify the details of the proof, which will then be fully self-contained based on the established geometric results.\n1563: :::\n1564: :::{prf:proof}\n1565: :label: proof-lem-greedy-preserves-signal\n1566: **Proof.**\n1567: \n1568: The proof establishes rigorous probabilistic bounds on the expected distance measurements by carefully analyzing the sequential pairing process. We show that the geometric partition imposed by high variance creates an unavoidable statistical signature in the measurements, regardless of the pairing order.\n1569: \n1570: **Framework: Conditional Expectations and the Sequential Process.**\n1571: \n1572: The Sequential Stochastic Greedy Pairing algorithm builds the matching iteratively. At any stage of the algorithm, let $P_t$ denote the set of already-paired walkers and $U_t = \\mathcal{A}_k \\setminus P_t$ denote the set of unpaired walkers remaining. For a walker $i$ selected at stage $t$, the probability of pairing with walker $u \\in U_t \\setminus \\{i\\}$ is:\n1573: \n1574: $$\n1575: \\mathbb{P}(c_i = u \\mid U_t, i) = \\frac{\\exp\\left(-\\frac{d_{\\text{alg}}(i,u)^2}{2\\epsilon_d^2}\\right)}{\\sum_{l \\in U_t \\setminus \\{i\\}} \\exp\\left(-\\frac{d_{\\text{alg}}(i,l)^2}{2\\epsilon_d^2}\\right)} =: \\frac{w_{iu}}{Z_i(U_t)}\n1576: $$\n1577: \n1578: where $Z_i(U_t) = \\sum_{l \\in U_t \\setminus \\{i\\}} w_{il}$ is the partition function normalizing the softmax distribution.\n1579: \n1580: The conditional expected distance for walker $i$ given the remaining set $U_t$ is:\n1581: \n1582: $$\n1583: \\mathbb{E}[d_i \\mid U_t, i \\in U_t] = \\sum_{u \\in U_t \\setminus \\{i\\}} d_{\\text{alg}}(i, u) \\cdot \\mathbb{P}(c_i = u \\mid U_t, i)\n1584: $$\n1585: \n1586: The unconditional expected distance $\\mathbb{E}[d_i \\mid \\mathcal{S}_t, i]$ is obtained by averaging over all possible pairing histories that lead to $i$ being paired.\n1587: \n1588: **Key Insight:** The geometric properties of $H_k$ and $L_k$ (specifically, the separation constants $D_H(\\epsilon)$ and $R_L(\\epsilon)$) provide **uniform bounds** on these conditional expectations that are **independent of the pairing history** $P_t$. This history-independence is the crucial property that allows us to bound the full expectation.\n1589: \n1590: **Part 1: Rigorous Lower Bound for High-Error Walkers.**\n1591: \n1592: **Claim:** For any high-error walker $i \\in H_k$, $\\mathbb{E}[d_i \\mid \\mathcal{S}_t, i \\in H_k] \\geq D_H(\\epsilon) \\cdot \\mathbb{P}(\\text{pair with } L_k) + R_L(\\epsilon) \\cdot \\mathbb{P}(\\text{pair within } H_k)$.\n1593: \n1594: Since the low-error set $L_k$ contains a non-vanishing fraction of walkers ($|L_k| / k \\geq f_L > 0$), and pairing is done uniformly over unpaired walkers, the expected distance is bounded below.\n1595: \n1596: **Step 1.1: Geometric property from corrected {prf:ref}`lem-geometric-separation-of-partition`.**\n1597: \n1598: By {prf:ref}`lem-geometric-separation-of-partition` (corrected), for a high-error walker $i \\in H_k$:\n1599: - For any low-error walker $u \\in L_k$: $d_{\\text{alg}}(i, u) \\geq D_H(\\epsilon)$\n1600: - For any high-error walker in the same cluster: $d_{\\text{alg}}(i, u) \\leq R_L(\\epsilon)$\n1601: \n1602: This is a **deterministic geometric property** of the state $\\mathcal{S}_t$.\n1603: \n1604: **Step 1.2: Population-weighted bound on conditional expectations.**\n1605: \n1606: For any stage $t$ in the pairing process where $i \\in U_t$, partition the unpaired walkers:\n1607: - $U_L := U_t \\cap L_k$ (low-error walkers, far from $i$)\n1608: - $U_H := U_t \\cap H_k \\setminus \\{i\\}$ (other high-error walkers, may be close)\n1609: \n1610: The conditional expectation decomposes as:\n1611: \n1612: $$\n1613: \\begin{aligned}\n1614: \\mathbb{E}[d_i \\mid U_t, i \\in U_t] &= \\sum_{u \\in U_L} d_{\\text{alg}}(i, u) \\cdot \\mathbb{P}(c_i = u \\mid U_t, i) \\\\\n1615: &\\quad + \\sum_{u \\in U_H} d_{\\text{alg}}(i, u) \\cdot \\mathbb{P}(c_i = u \\mid U_t, i)\n1616: \\end{aligned}\n1617: $$\n1618: \n1619: Using the geometric bounds:\n1620: \n1621: $$\n1622: \\begin{aligned}\n1623: \\mathbb{E}[d_i \\mid U_t, i \\in U_t] &\\geq \\sum_{u \\in U_L} D_H(\\epsilon) \\cdot \\mathbb{P}(c_i = u \\mid U_t, i) \\\\\n1624: &= D_H(\\epsilon) \\cdot \\mathbb{P}(c_i \\in U_L \\mid U_t, i)\n1625: \\end{aligned}\n1626: $$\n1627: \n1628: Since $|U_L| \\geq |L_k| - k/2 \\geq k \\cdot f_L - k/2 = k(f_L - 1/2) > 0$ for $f_L > 1/2$, the probability of pairing with a low-error walker is bounded below. This gives us a worst-case lower bound by considering the minimum over all possible unpaired sets $U_t$.\n1629: \n1630: **Step 1.3: History-independence and unconditional bound.**\n1631: \n1632: Since the bound $\\mathbb{E}[d_i \\mid U_t, i \\in U_t] \\geq D_H(\\epsilon)$ holds for **every possible set** $U_t$ containing $i$, it holds regardless of the specific pairing history. Taking the expectation over all possible pairing orders:\n1633: \n1634: $$\n1635: \\mathbb{E}[d_i \\mid \\mathcal{S}_t, i \\in H_k] = \\mathbb{E}_{U_t}\\left[\\mathbb{E}[d_i \\mid U_t, i \\in U_t]\\right] \\geq \\mathbb{E}_{U_t}[D_H(\\epsilon)] = D_H(\\epsilon)\n1636: $$\n1637: \n1638: This establishes the first claim. The bound is **N-uniform** because $D_H(\\epsilon)$ is an N-uniform geometric constant (proven in Chapter 6).\n1639: \n1640: **Part 2: Rigorous Upper Bound for Low-Error Walkers.**\n1641: \n1642: **Claim:** For any low-error walker $j \\in L_k$, $\\mathbb{E}[d_j \\mid \\mathcal{S}_t, j \\in L_k] \\leq R_L(\\epsilon) + (D_{\\text{valid}} - R_L(\\epsilon)) \\cdot c_k \\exp\\left(-\\frac{[D_H(\\epsilon) - R_L(\\epsilon)]^2}{2\\epsilon_d^2}\\right)$ where $c_k$ is N-uniform.\n1643: \n1644: **Step 2.1: Geometric property of low-error walkers.**\n1645: \n1646: By {prf:ref}`def-geometric-partition`, a walker $j \\in L_k$ has a **local cluster** $C_j \\subset L_k$ with the following properties:\n1647: - $|C_j| \\geq f_c k$ for an N-uniform constant $f_c > 0$\n1648: - For all $l \\in C_j$: $d_{\\text{alg}}(j, l) \\leq R_L(\\epsilon)$\n1649: - For all $m \\notin C_j$: $d_{\\text{alg}}(j, m) \\geq D_H(\\epsilon)$\n1650: \n1651: (Note: The last property follows from the fact that walkers outside the cluster must be either in $H_k$ or in other clusters, both of which are separated by at least $D_H(\\epsilon)$ by the geometric partition structure.)\n1652: \n1653: **Step 2.2: Worst-case cluster depletion bound.**\n1654: \n1655: At any stage $t$ of the pairing process, at most $\\lfloor k/2 \\rfloor$ pairs have been formed, removing at most $k$ walkers from consideration. In the worst case, all removed walkers could have been from $C_j$. Therefore:\n1656: \n1657: $$\n1658: |U_t \\cap C_j| \\geq |C_j| - k \\geq f_c k - k = k(f_c - 1)\n1659: $$\n1660: \n1661: For the axiom $f_c > 1/2$ to be meaningful, we typically have $f_c \\geq 2/3$, giving $|U_t \\cap C_j| \\geq k/3 > 0$ (strictly positive cluster survivors).\n1662: \n1663: **Step 2.3: Partition of available companions.**\n1664: \n1665: For $j$ being paired at stage $t$ with remaining set $U_t$, partition:\n1666: - $U_{\\text{in}} := U_t \\cap C_j$ (nearby cluster members)\n1667: - $U_{\\text{out}} := U_t \\setminus C_j$ (distant walkers)\n1668: \n1669: We have $|U_{\\text{in}}| \\geq k(f_c - 1) > 0$ and $|U_{\\text{out}}| \\leq k$.\n1670: \n1671: **Step 2.4: Bounding the normalization constant.**\n1672: \n1673: The partition function for $j$ satisfies:\n1674: \n1675: $$\n1676: \\begin{aligned}\n1677: Z_j(U_t) &= \\sum_{l \\in U_t \\setminus \\{j\\}} \\exp\\left(-\\frac{d_{\\text{alg}}(j,l)^2}{2\\epsilon_d^2}\\right) \\\\\n1678: &= \\sum_{l \\in U_{\\text{in}}} \\exp\\left(-\\frac{d_{\\text{alg}}(j,l)^2}{2\\epsilon_d^2}\\right) + \\sum_{m \\in U_{\\text{out}}} \\exp\\left(-\\frac{d_{\\text{alg}}(j,m)^2}{2\\epsilon_d^2}\\right) \\\\\n1679: &\\geq \\sum_{l \\in U_{\\text{in}}} \\exp\\left(-\\frac{R_L(\\epsilon)^2}{2\\epsilon_d^2}\\right) \\\\\n1680: &= |U_{\\text{in}}| \\exp\\left(-\\frac{R_L(\\epsilon)^2}{2\\epsilon_d^2}\\right) \\\\\n1681: &\\geq k(f_c - 1) \\exp\\left(-\\frac{R_L(\\epsilon)^2}{2\\epsilon_d^2}\\right)\n1682: \\end{aligned}\n1683: $$\n1684: \n1685: using $d_{\\text{alg}}(j,l) \\leq R_L(\\epsilon)$ for $l \\in U_{\\text{in}}$.\n1686: \n1687: **Step 2.5: Bounding the tail probability.**\n1688: \n1689: The probability of $j$ being paired with a distant walker is:\n1690: \n1691: $$\n1692: \\begin{aligned}\n1693: \\mathbb{P}(c_j \\in U_{\\text{out}} \\mid U_t, j) &= \\frac{\\sum_{m \\in U_{\\text{out}}} \\exp\\left(-\\frac{d_{\\text{alg}}(j,m)^2}{2\\epsilon_d^2}\\right)}{Z_j(U_t)} \\\\\n1694: &\\leq \\frac{|U_{\\text{out}}| \\exp\\left(-\\frac{D_H(\\epsilon)^2}{2\\epsilon_d^2}\\right)}{|U_{\\text{in}}| \\exp\\left(-\\frac{R_L(\\epsilon)^2}{2\\epsilon_d^2}\\right)} \\\\\n1695: &\\leq \\frac{k}{k(f_c - 1)} \\exp\\left(-\\frac{D_H(\\epsilon)^2 - R_L(\\epsilon)^2}{2\\epsilon_d^2}\\right) \\\\\n1696: &= \\frac{1}{f_c - 1} \\exp\\left(-\\frac{[D_H(\\epsilon) + R_L(\\epsilon)][D_H(\\epsilon) - R_L(\\epsilon)]}{2\\epsilon_d^2}\\right)\n1697: \\end{aligned}\n1698: $$\n1699: \n1700: Define $c_k := 1/(f_c - 1)$, which is N-uniform. Using $D_H(\\epsilon) + R_L(\\epsilon) \\geq D_H(\\epsilon) - R_L(\\epsilon)$ (since both are positive):\n1701: \n1702: $$\n1703: \\mathbb{P}(c_j \\in U_{\\text{out}} \\mid U_t, j) \\leq c_k \\exp\\left(-\\frac{[D_H(\\epsilon) - R_L(\\epsilon)]^2}{2\\epsilon_d^2}\\right) =: p_{\\text{tail}}\n1704: $$\n1705: \n1706: **Step 2.6: Bounding the conditional expected distance.**\n1707: \n1708: $$\n1709: \\begin{aligned}\n1710: \\mathbb{E}[d_j \\mid U_t, j] &= \\sum_{l \\in U_{\\text{in}}} d_{\\text{alg}}(j,l) \\mathbb{P}(c_j = l \\mid U_t, j) + \\sum_{m \\in U_{\\text{out}}} d_{\\text{alg}}(j,m) \\mathbb{P}(c_j = m \\mid U_t, j) \\\\\n1711: &\\leq R_L(\\epsilon) \\sum_{l \\in U_{\\text{in}}} \\mathbb{P}(c_j = l \\mid U_t, j) + D_{\\text{valid}} \\sum_{m \\in U_{\\text{out}}} \\mathbb{P}(c_j = m \\mid U_t, j) \\\\\n1712: &= R_L(\\epsilon) \\cdot [1 - p_{\\text{tail}}] + D_{\\text{valid}} \\cdot p_{\\text{tail}} \\\\\n1713: &= R_L(\\epsilon) + [D_{\\text{valid}} - R_L(\\epsilon)] p_{\\text{tail}}\n1714: \\end{aligned}\n1715: $$\n1716: \n1717: **Step 2.7: History-independence and unconditional bound.**\n1718: \n1719: The bound on $\\mathbb{E}[d_j \\mid U_t, j]$ holds for every possible set $U_t$ containing $j$, with the same constants. Therefore:\n1720: \n1721: $$\n1722: \\mathbb{E}[d_j \\mid \\mathcal{S}_t, j \\in L_k] \\leq R_L(\\epsilon) + [D_{\\text{valid}} - R_L(\\epsilon)] \\cdot c_k \\exp\\left(-\\frac{[D_H(\\epsilon) - R_L(\\epsilon)]^2}{2\\epsilon_d^2}\\right)\n1723: $$\n1724: \n1725: **Conclusion:**\n1726: \n1727: Both bounds are **N-uniform** because:\n1728: - $D_H(\\epsilon), R_L(\\epsilon)$ are N-uniform geometric constants (from Chapter 6)\n1729: - $f_c$ is an N-uniform population fraction (from Chapter 6)\n1730: - $c_k = 1/(f_c - 1)$ is therefore N-uniform\n1731: - $D_{\\text{valid}}$ is a fixed environmental parameter\n1732: - $\\epsilon_d$ is a fixed algorithmic parameter\n1733: \n1734: This completes the proof that the greedy pairing algorithm reliably detects the geometric partition structure.\n1735: ",
      "strategy_summary": "The proof derives history-independent probabilistic bounds on expected pairing distances using conditional expectations and softmax probabilities, exploiting geometric separation properties of high- and low-error walker sets to ensure the greedy algorithm preserves detectable signals from the partition structure.",
      "conclusion": {
        "text": "This completes the proof that the greedy pairing algorithm reliably detects the geometric partition structure.",
        "latex": null
      },
      "assumptions": [
        {
          "text": "Geometric separation constants $D_H(\\epsilon)$ and $R_L(\\epsilon)$ are N-uniform from Chapter 6.",
          "latex": null
        },
        {
          "text": "Low-error set fraction $|L_k|/k \\geq f_L > 1/2$ is N-uniform.",
          "latex": null
        },
        {
          "text": "Local cluster size $|C_j| \\geq f_c k$ with $f_c > 1/2$ (typically $\\geq 2/3$) is N-uniform for low-error walkers.",
          "latex": null
        },
        {
          "text": "$D_{\\text{valid}}$ is a fixed environmental parameter and $\\epsilon_d$ is a fixed algorithmic parameter.",
          "latex": null
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "framework",
          "text": "Introduce the sequential greedy pairing process with conditional probabilities using softmax distribution and define conditional expected distances.",
          "latex": null,
          "references": [],
          "derived_statement": null
        },
        {
          "order": 2.0,
          "kind": "insight",
          "text": "Highlight history-independence from geometric properties providing uniform bounds on conditional expectations.",
          "latex": null,
          "references": [],
          "derived_statement": null
        },
        {
          "order": 3.0,
          "kind": "claim",
          "text": "For high-error walkers $i \\in H_k$, $\\mathbb{E}[d_i \\mid \\mathcal{S}_t, i \\in H_k] \\geq D_H(\\epsilon)$ (simplified from detailed claim).",
          "latex": null,
          "references": [
            "lem-geometric-separation-of-partition"
          ],
          "derived_statement": "$\\mathbb{E}[d_i \\mid U_t, i] \\geq D_H(\\epsilon) \\cdot \\mathbb{P}(c_i \\in U_L \\mid U_t, i)$"
        },
        {
          "order": 4.0,
          "kind": "step",
          "text": "Apply geometric bounds: distances to low-error walkers $\\geq D_H(\\epsilon)$, within high-error clusters $\\leq R_L(\\epsilon)$ (noted as deterministic).",
          "latex": null,
          "references": [
            "lem-geometric-separation-of-partition"
          ],
          "derived_statement": null
        },
        {
          "order": 5.0,
          "kind": "step",
          "text": "Decompose conditional expectation over $U_L$ and $U_H$, lower-bounding by $D_H(\\epsilon)$ times probability of pairing with $U_L$.",
          "latex": null,
          "references": [],
          "derived_statement": "$\\mathbb{E}[d_i \\mid U_t, i] \\geq D_H(\\epsilon) \\cdot \\mathbb{P}(c_i \\in U_L \\mid U_t, i)$"
        },
        {
          "order": 6.0,
          "kind": "step",
          "text": "Ensure positive probability mass in $U_L$ due to $|L_k|/k \\geq f_L > 1/2$, yielding worst-case lower bound independent of $U_t$.",
          "latex": null,
          "references": [],
          "derived_statement": null
        },
        {
          "order": 7.0,
          "kind": "step",
          "text": "Average over pairing histories using law of total expectation to get unconditional bound $\\mathbb{E}[d_i \\mid \\mathcal{S}_t, i \\in H_k] \\geq D_H(\\epsilon)$, N-uniform.",
          "latex": null,
          "references": [],
          "derived_statement": "$\\mathbb{E}[d_i] = \\mathbb{E}_{U_t}[\\mathbb{E}[d_i \\mid U_t]] \\geq D_H(\\epsilon)$"
        },
        {
          "order": 8.0,
          "kind": "claim",
          "text": "For low-error walkers $j \\in L_k$, $\\mathbb{E}[d_j \\mid \\mathcal{S}_t, j \\in L_k] \\leq R_L(\\epsilon) + (D_{\\text{valid}} - R_L(\\epsilon)) \\cdot c_k \\exp\\left(-\\frac{[D_H(\\epsilon) - R_L(\\epsilon)]^2}{2\\epsilon_d^2}\\right)$.",
          "latex": null,
          "references": [
            "def-geometric-partition"
          ],
          "derived_statement": null
        },
        {
          "order": 9.0,
          "kind": "step",
          "text": "Geometric properties: local cluster $C_j$ with $|C_j| \\geq f_c k$, intra-cluster distances $\\leq R_L(\\epsilon)$, extra-cluster $\\geq D_H(\\epsilon)$.",
          "latex": null,
          "references": [
            "def-geometric-partition"
          ],
          "derived_statement": null
        },
        {
          "order": 10.0,
          "kind": "step",
          "text": "Bound surviving cluster members $|U_t \\cap C_j| \\geq k(f_c - 1) > 0$ in worst-case depletion.",
          "latex": null,
          "references": [],
          "derived_statement": null
        },
        {
          "order": 11.0,
          "kind": "step",
          "text": "Partition $U_t$ into $U_{\\text{in}}$ (intra-cluster) and $U_{\\text{out}}$ (extra-cluster).",
          "latex": null,
          "references": [],
          "derived_statement": null
        },
        {
          "order": 12.0,
          "kind": "step",
          "text": "Lower-bound partition function $Z_j(U_t) \\geq |U_{\\text{in}}| \\exp(-R_L(\\epsilon)^2 / 2\\epsilon_d^2) \\geq k(f_c - 1) \\exp(-R_L(\\epsilon)^2 / 2\\epsilon_d^2)$.",
          "latex": null,
          "references": [],
          "derived_statement": null
        },
        {
          "order": 13.0,
          "kind": "step",
          "text": "Upper-bound tail probability $\\mathbb{P}(c_j \\in U_{\\text{out}}) \\leq c_k \\exp(-[D_H(\\epsilon) - R_L(\\epsilon)]^2 / 2\\epsilon_d^2)$ with $c_k = 1/(f_c - 1)$.",
          "latex": null,
          "references": [],
          "derived_statement": "$p_{\\text{tail}} \\leq c_k \\exp(-\\frac{[D_H - R_L]^2}{2\\epsilon_d^2})$"
        },
        {
          "order": 14.0,
          "kind": "step",
          "text": "Bound conditional expectation $\\mathbb{E}[d_j \\mid U_t] \\leq R_L(\\epsilon) + (D_{\\text{valid}} - R_L(\\epsilon)) p_{\\text{tail}}$.",
          "latex": null,
          "references": [],
          "derived_statement": null
        },
        {
          "order": 15.0,
          "kind": "step",
          "text": "Extend to unconditional bound via averaging over histories, all constants N-uniform.",
          "latex": null,
          "references": [],
          "derived_statement": null
        }
      ],
      "key_equations": [
        {
          "label": "eq-pairing-prob",
          "latex": "\\mathbb{P}(c_i = u \\mid U_t, i) = \\frac{\\exp\\left(-\\frac{d_{\\text{alg}}(i,u)^2}{2\\epsilon_d^2}\\right)}{\\sum_{l \\in U_t \\setminus \\{i\\}} \\exp\\left(-\\frac{d_{\\text{alg}}(i,l)^2}{2\\epsilon_d^2}\\right)} =: \\frac{w_{iu}}{Z_i(U_t)}",
          "role": "Defines the softmax pairing probability."
        },
        {
          "label": "eq-conditional-exp",
          "latex": "\\mathbb{E}[d_i \\mid U_t, i \\in U_t] = \\sum_{u \\in U_t \\setminus \\{i\\}} d_{\\text{alg}}(i, u) \\cdot \\mathbb{P}(c_i = u \\mid U_t, i)",
          "role": "Conditional expected distance for analysis."
        },
        {
          "label": "eq-high-lower",
          "latex": "\\mathbb{E}[d_i \\mid U_t, i] \\geq D_H(\\epsilon) \\cdot \\mathbb{P}(c_i \\in U_L \\mid U_t, i)",
          "role": "Lower bound decomposition for high-error walkers."
        },
        {
          "label": "eq-uncond-high",
          "latex": "\\mathbb{E}[d_i \\mid \\mathcal{S}_t, i \\in H_k] = \\mathbb{E}_{U_t}[\\mathbb{E}[d_i \\mid U_t]] \\geq D_H(\\epsilon)",
          "role": "Unconditional lower bound for high-error."
        },
        {
          "label": "eq-z-lower",
          "latex": "Z_j(U_t) \\geq |U_{\\text{in}}| \\exp\\left(-\\frac{R_L(\\epsilon)^2}{2\\epsilon_d^2}\\right) \\geq k(f_c - 1) \\exp\\left(-\\frac{R_L(\\epsilon)^2}{2\\epsilon_d^2}\\right)",
          "role": "Lower bound on partition function for low-error."
        },
        {
          "label": "eq-tail-prob",
          "latex": "\\mathbb{P}(c_j \\in U_{\\text{out}} \\mid U_t, j) \\leq c_k \\exp\\left(-\\frac{[D_H(\\epsilon) - R_L(\\epsilon)]^2}{2\\epsilon_d^2}\\right)",
          "role": "Upper bound on probability of distant pairing."
        },
        {
          "label": "eq-low-upper",
          "latex": "\\mathbb{E}[d_j \\mid U_t, j] \\leq R_L(\\epsilon) + [D_{\\text{valid}} - R_L(\\epsilon)] p_{\\text{tail}}",
          "role": "Conditional upper bound for low-error walkers."
        }
      ],
      "references": [
        "lem-geometric-separation-of-partition",
        "def-geometric-partition"
      ],
      "math_tools": [
        {
          "toolName": "Conditional Expectation",
          "field": "Probability Theory",
          "description": "The expected value of a random variable given partial information, used to compute averages over possible pairing outcomes.",
          "roleInProof": "Core tool for bounding expected distances at each stage of the sequential pairing process, enabling history-independent unconditional bounds.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Law of Total Expectation"
          ]
        },
        {
          "toolName": "Softmax Distribution",
          "field": "Probability and Machine Learning",
          "description": "A probability distribution derived from exponentials of negative squared distances, normalizing choices in the greedy pairing.",
          "roleInProof": "Defines the pairing probabilities in the algorithm, allowing decomposition of expectations into contributions from nearby and distant walkers.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Partition Function"
          ]
        },
        {
          "toolName": "Partition Function",
          "field": "Statistical Mechanics and Probability",
          "description": "The normalizing constant in the softmax, summing weighted exponentials over possible pairing options.",
          "roleInProof": "Facilitates bounding tail probabilities of pairing with distant walkers by lower-bounding contributions from local clusters.",
          "levelOfAbstraction": "Notation",
          "relatedTools": [
            "Softmax Distribution"
          ]
        }
      ],
      "cases": [
        {
          "name": "High-Error Walkers",
          "condition": "$i \\in H_k$",
          "summary": "Lower bound on expected distance using separation to low-error set and positive probability mass."
        },
        {
          "name": "Low-Error Walkers",
          "condition": "$j \\in L_k$",
          "summary": "Upper bound on expected distance using local cluster survival and exponential tail decay for distant pairings."
        }
      ],
      "remarks": [
        {
          "type": "key-insight",
          "text": "Geometric properties provide uniform bounds on conditional expectations independent of pairing history."
        },
        {
          "type": "framework",
          "text": "Analysis relies on sequential stochastic process with softmax probabilities."
        },
        {
          "type": "n-uniform",
          "text": "All constants and bounds are N-uniform, relying on Chapter 6 results."
        }
      ],
      "gaps": [
        {
          "description": "Proof assumes verification against Chapter 6 geometric results for full self-containment; minor dependency on 'corrected' lemma.",
          "severity": "minor",
          "location_hint": "Introductory note and references"
        }
      ],
      "tags": [
        "greedy pairing",
        "probabilistic bounds",
        "geometric partition",
        "conditional expectations",
        "softmax distribution",
        "N-uniform constants"
      ],
      "document_id": "03_cloning",
      "section": "## 5. The Measurement and Interaction Pipeline",
      "span": {
        "start_line": 1562,
        "end_line": 1735,
        "content_start": 1564,
        "content_end": 1734,
        "header_lines": [
          1563
        ]
      },
      "metadata": {
        "label": "proof-lem-greedy-preserves-signal"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 5,
        "chapter_file": "chapter_5.json",
        "section_id": "## 5. The Measurement and Interaction Pipeline"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "greedy pairing",
      "signal separation",
      "internal variance",
      "walkers",
      "phase-space distance",
      "expected distance",
      "error sets"
    ],
    "content_markdown": ":label: lem-greedy-preserves-signal\n\nLet a swarm be in a state with high internal variance, such that its alive walkers `A_k` are partitioned into a high-error set `H_k` and a low-error set `L_k` as per {prf:ref}`def-geometric-partition`.\n\nThe Sequential Stochastic Greedy Pairing Operator ({prf:ref}`def-greedy-pairing-algorithm`) guarantees a statistical separation in the expected raw phase-space distance measurements for these two populations. Specifically, there exist N-uniform, $\\varepsilon$-dependent bounds such that:\n\n1.  For any high-error walker $i \\in H_k$, its expected raw distance is large:\n\n\n$$\n\\mathbb{E}[d_i \\mid S_t, i \\in H_k] \\ge D_H(\\epsilon)\n$$\n\n2.  For any low-error walker $j \\in L_k$, its expected raw distance is small:\n\n\n$$\n\\mathbb{E}[d_j \\mid S_t, j \\in L_k] \\le R_L(\\epsilon) + D_{\\mathrm{valid}} \\cdot c_k \\exp\\left(-\\frac{D_H(\\epsilon)^2 - R_L(\\epsilon)^2}{2\\epsilon^2}\\right)\n$$",
    "raw_directive": "1528: With these formal definitions, we can now prove that the practical pairing algorithm reliably detects this structure.\n1529: \n1530: :::{prf:lemma} Greedy Pairing Guarantees Signal Separation\n1531: :label: lem-greedy-preserves-signal\n1532: \n1533: Let a swarm be in a state with high internal variance, such that its alive walkers `A_k` are partitioned into a high-error set `H_k` and a low-error set `L_k` as per {prf:ref}`def-geometric-partition`.\n1534: \n1535: The Sequential Stochastic Greedy Pairing Operator ({prf:ref}`def-greedy-pairing-algorithm`) guarantees a statistical separation in the expected raw phase-space distance measurements for these two populations. Specifically, there exist N-uniform, $\\varepsilon$-dependent bounds such that:\n1536: \n1537: 1.  For any high-error walker $i \\in H_k$, its expected raw distance is large:\n1538: \n1539: \n1540: $$\n1541: \\mathbb{E}[d_i \\mid S_t, i \\in H_k] \\ge D_H(\\epsilon)\n1542: $$\n1543: \n1544: 2.  For any low-error walker $j \\in L_k$, its expected raw distance is small:\n1545: \n1546: \n1547: $$\n1548: \\mathbb{E}[d_j \\mid S_t, j \\in L_k] \\le R_L(\\epsilon) + D_{\\mathrm{valid}} \\cdot c_k \\exp\\left(-\\frac{D_H(\\epsilon)^2 - R_L(\\epsilon)^2}{2\\epsilon^2}\\right)\n1549: $$\n1550: ",
    "document_id": "03_cloning",
    "section": "## 5. The Measurement and Interaction Pipeline",
    "span": {
      "start_line": 1528,
      "end_line": 1550,
      "content_start": 1531,
      "content_end": 1549,
      "header_lines": [
        1529
      ]
    },
    "references": [
      "def-geometric-partition",
      "def-greedy-pairing-algorithm",
      "lem-geometric-separation-of-partition"
    ],
    "metadata": {
      "label": "lem-greedy-preserves-signal"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 5,
      "chapter_file": "chapter_5.json",
      "section_id": "## 5. The Measurement and Interaction Pipeline"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-patching-properties",
    "title": "Properties of the Patching Function",
    "type": "lemma",
    "nl_statement": "Properties of the patching function.",
    "equations": [],
    "hypotheses": [],
    "conclusion": {
      "text": null,
      "latex": null
    },
    "variables": [],
    "implicit_assumptions": [],
    "local_refs": [],
    "proof": null,
    "tags": [
      "patching",
      "properties",
      "lemma"
    ],
    "content_markdown": ":::{prf:lemma} Properties of the Patching Function",
    "raw_directive": "1809: :::\n1810: \n1811: :::{prf:lemma} Properties of the Patching Function\n1812: :label: lem-patching-properties",
    "document_id": "03_cloning",
    "section": "## 5. The Measurement and Interaction Pipeline",
    "span": {
      "start_line": 1809,
      "end_line": 1812,
      "content_start": 1811,
      "content_end": 1811,
      "header_lines": [
        1810
      ]
    },
    "references": [],
    "metadata": {
      "label": "lem-patching-properties"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 5,
      "chapter_file": "chapter_5.json",
      "section_id": "## 5. The Measurement and Interaction Pipeline"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-compact-support-z-scores",
    "title": "Compact Support of Standardized Scores",
    "type": "lemma",
    "nl_statement": "The standardized scores have compact support.",
    "equations": [],
    "hypotheses": [],
    "conclusion": {
      "text": null,
      "latex": null
    },
    "variables": [],
    "implicit_assumptions": [],
    "local_refs": [],
    "proof": null,
    "tags": [
      "compact support",
      "standardized scores",
      "z-scores",
      "probability",
      "measure theory"
    ],
    "content_markdown": ":::{prf:lemma} Compact Support of Standardized Scores",
    "raw_directive": "1830: :::\n1831: \n1832: :::{prf:lemma} Compact Support of Standardized Scores\n1833: :label: lem-compact-support-z-scores",
    "document_id": "03_cloning",
    "section": "## 5. The Measurement and Interaction Pipeline",
    "span": {
      "start_line": 1830,
      "end_line": 1833,
      "content_start": 1832,
      "content_end": 1832,
      "header_lines": [
        1831
      ]
    },
    "references": [
      "lem-patching-properties"
    ],
    "metadata": {
      "label": "lem-compact-support-z-scores"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 5,
      "chapter_file": "chapter_5.json",
      "section_id": "## 5. The Measurement and Interaction Pipeline"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-logistic-properties",
    "title": "Verification of Axiomatic Properties",
    "type": "lemma",
    "nl_statement": "Verification of axiomatic properties for the logistic model.",
    "equations": [],
    "hypotheses": [],
    "conclusion": {
      "text": null,
      "latex": null
    },
    "variables": [],
    "implicit_assumptions": [],
    "local_refs": [],
    "proof": null,
    "tags": [
      "logistic",
      "axiomatic",
      "properties",
      "verification",
      "lemma"
    ],
    "content_markdown": ":::{prf:lemma} Verification of Axiomatic Properties",
    "raw_directive": "1855: :::\n1856: \n1857: :::{prf:lemma} Verification of Axiomatic Properties\n1858: :label: lem-logistic-properties",
    "document_id": "03_cloning",
    "section": "## 5. The Measurement and Interaction Pipeline",
    "span": {
      "start_line": 1855,
      "end_line": 1858,
      "content_start": 1857,
      "content_end": 1857,
      "header_lines": [
        1856
      ]
    },
    "references": [],
    "metadata": {
      "label": "lem-logistic-properties"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 5,
      "chapter_file": "chapter_5.json",
      "section_id": "## 5. The Measurement and Interaction Pipeline"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-potential-bounds",
    "title": "Uniform Bounds of the Fitness Potential",
    "type": "lemma",
    "nl_statement": "Any non-zero fitness potential \\(V_i\\) generated by this pipeline is uniformly bounded within a compact interval \\([V_{\\text{pot,min}}, V_{\\text{pot,max}}]\\), where the bounds are state-independent constants defined by the algorithmic parameters, with \\(V_{\\text{pot,min}} := \\eta^{\\alpha+\\beta}\\).",
    "equations": [
      {
        "label": "V_pot_min_def",
        "latex": "V_{\\text{pot,min}} := \\eta^{\\alpha + \\beta}"
      }
    ],
    "hypotheses": [
      {
        "text": "V_i is a non-zero fitness potential generated by the pipeline",
        "latex": "V_i \\neq 0"
      }
    ],
    "conclusion": {
      "text": "V_i is uniformly bounded within [V_{\\text{pot,min}}, V_{\\text{pot,max}}], where the bounds are state-independent constants defined by algorithmic parameters",
      "latex": "V_i \\in [V_{\\text{pot,min}}, V_{\\text{pot,max}}]"
    },
    "variables": [
      {
        "symbol": "V_i",
        "name": "fitness potential",
        "description": "Non-zero fitness potential value generated by the pipeline",
        "constraints": [
          "non-zero"
        ],
        "tags": [
          "fitness",
          "variable"
        ]
      },
      {
        "symbol": "V_{\\text{pot,min}}",
        "name": "minimum potential bound",
        "description": "State-independent lower bound for fitness potentials",
        "constraints": [],
        "tags": [
          "bound",
          "constant"
        ]
      },
      {
        "symbol": "V_{\\text{pot,max}}",
        "name": "maximum potential bound",
        "description": "State-independent upper bound for fitness potentials",
        "constraints": [],
        "tags": [
          "bound",
          "constant"
        ]
      },
      {
        "symbol": "\\eta",
        "name": "eta parameter",
        "description": "Algorithmic parameter used in bound definition",
        "constraints": [],
        "tags": [
          "parameter"
        ]
      },
      {
        "symbol": "\\alpha",
        "name": "alpha parameter",
        "description": "Algorithmic parameter used in bound definition",
        "constraints": [],
        "tags": [
          "parameter"
        ]
      },
      {
        "symbol": "\\beta",
        "name": "beta parameter",
        "description": "Algorithmic parameter used in bound definition",
        "constraints": [],
        "tags": [
          "parameter"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Algorithmic parameters \\eta, \\alpha, \\beta are positive real numbers ensuring a compact interval",
        "confidence": 0.9
      },
      {
        "text": "The pipeline generates fitness potentials V_i based on these parameters",
        "confidence": 1.0
      },
      {
        "text": "V_{\\text{pot,max}} is defined analogously to V_{\\text{pot,min}} elsewhere",
        "confidence": 0.8
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-potential-bounds",
      "title": null,
      "type": "proof",
      "proves": "lem-potential-bounds",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-potential-bounds\n\n**Proof.**\n\nThe proof follows directly from the definition of the multiplicative potential and the bounded properties of its components.\n\nThe rescaled components, $r'_i = g_A(z_{r,i}) + \\eta$ and $d'_i = g_A(z_{d,i}) + \\eta$, are strictly positive and bounded. The rescale function $g_A(z)$ has a range of $(g_{A,\\min}, g_{A,\\max}]$. Since $\\eta > 0$, the components are bounded on the interval $(g_{A,\\min} + \\eta, g_{A,\\max} + \\eta]$. For simplicity and rigor, we use the absolute bounds $(\\eta, g_{A,\\max} + \\eta]$.\n\n**Lower Bound ($V_{\\text{pot,min}}$):** The fitness potential $V_i$ is a product of positive terms raised to non-negative powers ($\\alpha, \\beta \\geq 0$). It is minimized when each component is at its minimum possible value.\n\n$$\nV_i \\ge (\\eta)^{\\beta} \\cdot (\\eta)^{\\alpha} = \\eta^{\\alpha+\\beta}\n$$\n\nTherefore, the uniform lower bound is $V_{\\text{pot,min}} := \\eta^{\\alpha+\\beta}$.\n\n**Upper Bound ($V_{\\text{pot,max}}$):** The potential is maximized when each component is at its maximum possible value.\n\n$$\nV_i \\le (g_{A,\\max} + \\eta)^{\\beta} \\cdot (g_{A,\\max} + \\eta)^{\\alpha} = (g_{A,\\max} + \\eta)^{\\alpha+\\beta}\n$$\n\nTherefore, the uniform upper bound is $V_{\\text{pot,max}} := (g_{A,\\max} + \\eta)^{\\alpha+\\beta}$.\n\n**Uniformity:**\n\nSince $g_A$ is bounded and $\\eta$ is a finite positive constant, both $V_{\\text{pot,min}}$ and $V_{\\text{pot,max}}$ are finite, positive, state-independent constants. They are independent of the swarm size $N$, the current state, or any dynamical variables.\n\nThis completes the proof.",
      "raw_directive": "1893: *   $V_{\\text{pot,max}} := (g_{A,\\max} + \\eta)^{\\alpha+\\beta}$\n1894: :::\n1895: :::{prf:proof}\n1896: :label: proof-lem-potential-bounds\n1897: \n1898: **Proof.**\n1899: \n1900: The proof follows directly from the definition of the multiplicative potential and the bounded properties of its components.\n1901: \n1902: The rescaled components, $r'_i = g_A(z_{r,i}) + \\eta$ and $d'_i = g_A(z_{d,i}) + \\eta$, are strictly positive and bounded. The rescale function $g_A(z)$ has a range of $(g_{A,\\min}, g_{A,\\max}]$. Since $\\eta > 0$, the components are bounded on the interval $(g_{A,\\min} + \\eta, g_{A,\\max} + \\eta]$. For simplicity and rigor, we use the absolute bounds $(\\eta, g_{A,\\max} + \\eta]$.\n1903: \n1904: **Lower Bound ($V_{\\text{pot,min}}$):** The fitness potential $V_i$ is a product of positive terms raised to non-negative powers ($\\alpha, \\beta \\geq 0$). It is minimized when each component is at its minimum possible value.\n1905: \n1906: $$\n1907: V_i \\ge (\\eta)^{\\beta} \\cdot (\\eta)^{\\alpha} = \\eta^{\\alpha+\\beta}\n1908: $$\n1909: \n1910: Therefore, the uniform lower bound is $V_{\\text{pot,min}} := \\eta^{\\alpha+\\beta}$.\n1911: \n1912: **Upper Bound ($V_{\\text{pot,max}}$):** The potential is maximized when each component is at its maximum possible value.\n1913: \n1914: $$\n1915: V_i \\le (g_{A,\\max} + \\eta)^{\\beta} \\cdot (g_{A,\\max} + \\eta)^{\\alpha} = (g_{A,\\max} + \\eta)^{\\alpha+\\beta}\n1916: $$\n1917: \n1918: Therefore, the uniform upper bound is $V_{\\text{pot,max}} := (g_{A,\\max} + \\eta)^{\\alpha+\\beta}$.\n1919: \n1920: **Uniformity:**\n1921: \n1922: Since $g_A$ is bounded and $\\eta$ is a finite positive constant, both $V_{\\text{pot,min}}$ and $V_{\\text{pot,max}}$ are finite, positive, state-independent constants. They are independent of the swarm size $N$, the current state, or any dynamical variables.\n1923: \n1924: This completes the proof.\n1925: ",
      "strategy_summary": "The proof derives uniform lower and upper bounds for the multiplicative potential function V_i directly from the positivity and boundedness of its components (r'_i and d'_i) and the non-negative exponents alpha and beta, minimizing and maximizing the product at the component bounds.",
      "conclusion": {
        "text": "The multiplicative potential V_i satisfies V_{pot,min} <= V_i <= V_{pot,max}, where V_{pot,min} = eta^{alpha + beta} and V_{pot,max} = (g_{A,max} + eta)^{alpha + beta}, and these bounds are finite, positive, and state-independent.",
        "latex": "V_{\\text{pot,min}} \\leq V_i \\leq V_{\\text{pot,max}}"
      },
      "assumptions": [
        {
          "text": "The rescaling function g_A(z) has range (g_{A,min}, g_{A,max}] with g_{A,min} < g_{A,max}.",
          "latex": "g_A(z) \\in (g_{A,\\min}, g_{A,\\max}]"
        },
        {
          "text": "eta > 0 is a finite positive constant.",
          "latex": "\\eta > 0"
        },
        {
          "text": "Exponents alpha >= 0 and beta >= 0.",
          "latex": "\\alpha \\geq 0, \\beta \\geq 0"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "explanation",
          "text": "The rescaled components r'_i = g_A(z_{r,i}) + eta and d'_i = g_A(z_{d,i}) + eta are strictly positive and bounded in (g_{A,min} + eta, g_{A,max} + eta], using absolute bounds (eta, g_{A,max} + eta] for rigor.",
          "latex": null,
          "references": [],
          "derived_statement": "r'_i, d'_i \\in (\\eta, g_{A,\\max} + \\eta]"
        },
        {
          "order": 2.0,
          "kind": "derivation",
          "text": "Lower bound: V_i is minimized when components are minimal, yielding V_i >= eta^beta * eta^alpha = eta^{alpha + beta}. Thus, V_{pot,min} := eta^{alpha + beta}.",
          "latex": "V_i \\ge (\\eta)^{\\beta} \\cdot (\\eta)^{\\alpha} = \\eta^{\\alpha + \\beta}",
          "references": [],
          "derived_statement": "V_{\\text{pot,min}} := \\eta^{\\alpha + \\beta}"
        },
        {
          "order": 3.0,
          "kind": "derivation",
          "text": "Upper bound: V_i is maximized when components are maximal, yielding V_i <= (g_{A,max} + eta)^beta * (g_{A,max} + eta)^alpha = (g_{A,max} + eta)^{alpha + beta}. Thus, V_{pot,max} := (g_{A,max} + eta)^{alpha + beta}.",
          "latex": "V_i \\le (g_{A,\\max} + \\eta)^{\\beta} \\cdot (g_{A,\\max} + \\eta)^{\\alpha} = (g_{A,\\max} + \\eta)^{\\alpha + \\beta}",
          "references": [],
          "derived_statement": "V_{\\text{pot,max}} := (g_{A,\\max} + \\eta)^{\\alpha + \\beta}"
        },
        {
          "order": 4.0,
          "kind": "explanation",
          "text": "Uniformity: Since g_A is bounded and eta is finite positive, both bounds are finite, positive constants independent of state, swarm size N, or dynamics.",
          "latex": null,
          "references": [],
          "derived_statement": "Bounds are state-independent constants"
        }
      ],
      "key_equations": [
        {
          "label": "eq-lower-bound",
          "latex": "V_i \\ge (\\eta)^{\\beta} \\cdot (\\eta)^{\\alpha} = \\eta^{\\alpha + \\beta}",
          "role": "Derives the uniform lower bound V_{pot,min}"
        },
        {
          "label": "eq-upper-bound",
          "latex": "V_i \\le (g_{A,\\max} + \\eta)^{\\beta} \\cdot (g_{A,\\max} + \\eta)^{\\alpha} = (g_{A,\\max} + \\eta)^{\\alpha + \\beta}",
          "role": "Derives the uniform upper bound V_{pot,max}"
        }
      ],
      "references": [],
      "math_tools": [
        {
          "toolName": "Bounded Functions",
          "field": "Real Analysis",
          "description": "Functions with values confined to a closed or open interval, allowing derivation of bounds on compositions and products.",
          "roleInProof": "Establishes the interval (eta, g_{A,max} + eta] for rescaled components to bound the potential.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Monotonicity"
          ]
        },
        {
          "toolName": "Monotonicity of Exponentiation",
          "field": "Algebra",
          "description": "For positive bases and non-negative exponents, the power function is non-decreasing, enabling min/max evaluation at endpoint bases.",
          "roleInProof": "Justifies that the potential is minimized/maximized when components are at their minimum/maximum values.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Bounded Functions"
          ]
        },
        {
          "toolName": "Product Rule for Bounds",
          "field": "Inequalities",
          "description": "For positive terms, the minimum/maximum of a product occurs at the product of minima/maxima when terms are independent.",
          "roleInProof": "Combines bounds on r'_i and d'_i to obtain overall bounds on V_i.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Monotonicity of Exponentiation"
          ]
        }
      ],
      "cases": [
        {
          "name": "Lower Bound",
          "condition": "Components at minimum value eta",
          "summary": "V_i minimized as product of minima raised to powers."
        },
        {
          "name": "Upper Bound",
          "condition": "Components at maximum value g_{A,max} + eta",
          "summary": "V_i maximized as product of maxima raised to powers."
        },
        {
          "name": "Uniformity",
          "condition": "Bounded g_A and eta > 0",
          "summary": "Bounds are finite, positive, and independent of state or parameters like N."
        }
      ],
      "remarks": [
        {
          "type": "uniformity",
          "text": "The bounds V_{pot,min} and V_{pot,max} are state-independent constants, facilitating analysis of swarm dynamics."
        }
      ],
      "gaps": [],
      "tags": [
        "potential bounds",
        "multiplicative potential",
        "uniform bounds",
        "bounded functions",
        "exponentiation",
        "swarm optimization"
      ],
      "document_id": "03_cloning",
      "section": "## 5. The Measurement and Interaction Pipeline",
      "span": {
        "start_line": 1893,
        "end_line": 1925,
        "content_start": 1896,
        "content_end": 1924,
        "header_lines": [
          1894
        ]
      },
      "metadata": {
        "label": "proof-lem-potential-bounds"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 5,
        "chapter_file": "chapter_5.json",
        "section_id": "## 5. The Measurement and Interaction Pipeline"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "uniform-bounds",
      "fitness-potential",
      "algorithmic-parameters",
      "state-independent",
      "compact-interval",
      "lemma"
    ],
    "content_markdown": ":label: lem-potential-bounds\n\nAny non-zero fitness potential $V_i$ generated by this pipeline is uniformly bounded within a compact interval $[V_{\\text{pot,min}}, V_{\\text{pot,max}}]$. The bounds are state-independent constants defined by the algorithmic parameters (using properties from {prf:ref}`lem-logistic-properties`):",
    "raw_directive": "1886: :::\n1887: \n1888: :::{prf:lemma} Uniform Bounds of the Fitness Potential\n1889: :label: lem-potential-bounds\n1890: \n1891: Any non-zero fitness potential $V_i$ generated by this pipeline is uniformly bounded within a compact interval $[V_{\\text{pot,min}}, V_{\\text{pot,max}}]$. The bounds are state-independent constants defined by the algorithmic parameters (using properties from {prf:ref}`lem-logistic-properties`):\n1892: *   $V_{\\text{pot,min}} := \\eta^{\\alpha+\\beta}$",
    "document_id": "03_cloning",
    "section": "## 5. The Measurement and Interaction Pipeline",
    "span": {
      "start_line": 1886,
      "end_line": 1892,
      "content_start": 1889,
      "content_end": 1891,
      "header_lines": [
        1887
      ]
    },
    "references": [
      "lem-logistic-properties"
    ],
    "metadata": {
      "label": "lem-potential-bounds"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 5,
      "chapter_file": "chapter_5.json",
      "section_id": "## 5. The Measurement and Interaction Pipeline"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-V_Varx-implies-variance",
    "title": "Large $V_{\\text{Var},x}$ Implies Large Single-Swarm Positional Variance",
    "type": "lemma",
    "nl_statement": "If the total intra-swarm positional variance component $V_{\\text{Var},x}(S_1, S_2)$ exceeds the threshold $R_{\\text{total_var},x}^2 > 0$, then at least one of the two swarms has an intra-swarm positional variance exceeding half that threshold.",
    "equations": [
      {
        "label": null,
        "latex": "V_{\\text{Var},x}(S_1, S_2) = \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2 + \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2"
      },
      {
        "label": null,
        "latex": "\\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2 > \\frac{R_{\\text{total_var},x}^2}{2}"
      }
    ],
    "hypotheses": [
      {
        "text": "$V_{\\text{Var},x}(S_1, S_2) > R_{\\text{total_var},x}^2$ for some $R_{\\text{total_var},x}^2 > 0$",
        "latex": "V_{\\text{Var},x} > R_{\\text{total_var},x}^2"
      }
    ],
    "conclusion": {
      "text": "At least one swarm $k \\in \\{1, 2\\}$ satisfies $\\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2 > \\frac{R_{\\text{total_var},x}^2}{2}$",
      "latex": "\\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2 > \\frac{R_{\\text{total_var},x}^2}{2}"
    },
    "variables": [
      {
        "symbol": "S_1",
        "name": "Swarm 1",
        "description": "The first swarm of agents",
        "constraints": [],
        "tags": [
          "swarm"
        ]
      },
      {
        "symbol": "S_2",
        "name": "Swarm 2",
        "description": "The second swarm of agents",
        "constraints": [],
        "tags": [
          "swarm"
        ]
      },
      {
        "symbol": "N",
        "name": "Number of agents per swarm",
        "description": "The size of each swarm",
        "constraints": [
          "N > 0"
        ],
        "tags": [
          "count",
          "size"
        ]
      },
      {
        "symbol": "\\delta_{x,k,i}",
        "name": "Positional deviation",
        "description": "The positional deviation of agent i in swarm k from the swarm mean",
        "constraints": [],
        "tags": [
          "position",
          "deviation",
          "error"
        ]
      },
      {
        "symbol": "R_{\\text{total_var},x}",
        "name": "Total variance threshold",
        "description": "A positive threshold for the total variance",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "threshold",
          "variance"
        ]
      },
      {
        "symbol": "V_{\\text{Var},x}",
        "name": "Intra-swarm positional variance component",
        "description": "The variance term in the synergistic Lyapunov function",
        "constraints": [],
        "tags": [
          "lyapunov",
          "variance"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Each swarm has exactly N agents, i.e., $|\\mathcal{A}(S_1)| = |\\mathcal{A}(S_2)| = N$",
        "confidence": null
      },
      {
        "text": "Positional deviations $\\delta_{x,k,i}$ are computed relative to the mean position of each swarm",
        "confidence": null
      }
    ],
    "local_refs": [
      "def-full-synergistic-lyapunov-function"
    ],
    "proof": {
      "label": "proof-lem-V_Varx-implies-variance",
      "title": null,
      "type": "proof",
      "proves": "lem-V_Varx-implies-variance",
      "proof_type": "contradiction",
      "proof_status": "complete",
      "content_markdown": ":::{prf:proof}\n:label: proof-lem-V_Varx-implies-variance\n**Proof.**\n\nThe proof is by contradiction. Assume the premise holds: $V_{Var,x} > R_{total\\_var,x}^2$. Assume for contradiction that the conclusion is false. This would mean that for *both* swarms (`k=1` and `k=2`):\n\n$$\n\\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2 \\le \\frac{R_{total\\_var,x}^2}{2}, \\quad \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2 \\le \\frac{R_{total\\_var,x}^2}{2}\n$$\n\nNow, we bound the total intra-swarm positional error $V_{Var,x}$ under this assumption:\n\n$$\n\\begin{aligned}\nV_{Var,x} &= \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2 + \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2 \\\\\n&\\le \\frac{R_{total\\_var,x}^2}{2} + \\frac{R_{total\\_var,x}^2}{2} = R_{total\\_var,x}^2\n\\end{aligned}\n$$\n\nThe result $V_{Var,x} \\le R_{total\\_var,x}^2$ directly contradicts our premise. Therefore, the assumption must be false, and the conclusion must be true.",
      "raw_directive": "2339: :::\n2340: \n2341: :::{prf:proof}\n2342: :label: proof-lem-V_Varx-implies-variance\n2343: **Proof.**\n2344: \n2345: The proof is by contradiction. Assume the premise holds: $V_{Var,x} > R_{total\\_var,x}^2$. Assume for contradiction that the conclusion is false. This would mean that for *both* swarms (`k=1` and `k=2`):\n2346: \n2347: $$\n2348: \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2 \\le \\frac{R_{total\\_var,x}^2}{2}, \\quad \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2 \\le \\frac{R_{total\\_var,x}^2}{2}\n2349: $$\n2350: \n2351: Now, we bound the total intra-swarm positional error $V_{Var,x}$ under this assumption:\n2352: \n2353: $$\n2354: \\begin{aligned}\n2355: V_{Var,x} &= \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2 + \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2 \\\\\n2356: &\\le \\frac{R_{total\\_var,x}^2}{2} + \\frac{R_{total\\_var,x}^2}{2} = R_{total\\_var,x}^2\n2357: \\end{aligned}\n2358: $$\n2359: \n2360: The result $V_{Var,x} \\le R_{total\\_var,x}^2$ directly contradicts our premise. Therefore, the assumption must be false, and the conclusion must be true.\n2361: ",
      "strategy_summary": "The proof assumes the premise that the total intra-swarm positional error \\(V_{Var,x}\\) exceeds \\(R_{total\\_var,x}^2\\) and, for contradiction, assumes both swarms have intra-swarm errors at most half of that bound; it then derives that \\(V_{Var,x} \\leq R_{total\\_var,x}^2\\), contradicting the premise and establishing that at least one swarm must exceed the half-bound.",
      "conclusion": {
        "text": "The assumption that both intra-swarm variances are \u2264 R_{total_var,x}^2 / 2 must be false, so at least one swarm has (1/N) \u2211 ||\u03b4_{x,k,i}||^2 > R_{total_var,x}^2 / 2.",
        "latex": null
      },
      "assumptions": [],
      "steps": [],
      "key_equations": [],
      "references": [],
      "math_tools": [
        {
          "toolName": "Proof by Contradiction",
          "field": "Mathematical Logic",
          "description": "A method where the negation of the statement to be proved is assumed true, and this leads to a logical inconsistency.",
          "roleInProof": "Structures the entire argument by assuming both intra-swarm variances are bounded and deriving a contradiction with the total variance premise.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Summation",
            "Inequality"
          ]
        },
        {
          "toolName": "Summation",
          "field": "Analysis",
          "description": "The operation of adding a sequence of terms, often used to aggregate errors or variances.",
          "roleInProof": "Used to express the total intra-swarm error as the sum of the two swarm contributions.",
          "levelOfAbstraction": "Notation",
          "relatedTools": [
            "Inequality"
          ]
        },
        {
          "toolName": "Triangle Inequality (Bounded Sum)",
          "field": "Analysis",
          "description": "A principle bounding the sum of non-negative terms by the sum of their individual bounds.",
          "roleInProof": "Bounds the total variance by adding the assumed bounds on each swarm's contribution.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Summation"
          ]
        }
      ],
      "cases": [],
      "remarks": [],
      "gaps": [],
      "tags": [
        "proof",
        "contradiction",
        "variance",
        "swarms",
        "inequality",
        "positional error"
      ],
      "document_id": "03_cloning",
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "span": {
        "start_line": 2339,
        "end_line": 2361,
        "content_start": 2341,
        "content_end": 2360,
        "header_lines": [
          2340
        ]
      },
      "metadata": {
        "label": "proof-lem-V_Varx-implies-variance"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 6,
        "chapter_file": "chapter_6.json",
        "section_id": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "variance",
      "lyapunov",
      "swarm",
      "positional",
      "cloning",
      "contractive"
    ],
    "content_markdown": ":label: lem-V_Varx-implies-variance\n\nLet $V_{Var,x}(S_1, S_2)$ be the total intra-swarm positional variance component of the Lyapunov function as defined in [](#def-full-synergistic-lyapunov-function):\n\n$$\nV_{Var,x}(S_1, S_2) = \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2 + \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2\n$$\n\nIf this component is large, such that $V_{Var,x} > R_{total\\_var,x}^2$ for some threshold $R_{total\\_var,x}^2 > 0$, then at least one swarm $k \\in \\{1, 2\\}$ must have a large sum of squared deviations:\n\n$$\n\\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2 > \\frac{R_{total\\_var,x}^2}{2}",
    "raw_directive": "2322: The first step in the Keystone analysis is to connect the relevant component of the synergistic Lyapunov function to the state of a single swarm. As this document aims to prove the contractive nature of cloning on **positional error**, the Keystone mechanism is triggered specifically by the **positional variance component, $V_{\\text{Var},x}$**. The following lemma provides the simple but necessary guarantee that if this $V_{\\text{Var},x}$ term is large, then at least one of the two swarms must have a large internal positional variance.\n2323: \n2324: :::{prf:lemma} Large $V_{\\text{Var},x}$ Implies Large Single-Swarm Positional Variance\n2325: :label: lem-V_Varx-implies-variance\n2326: \n2327: Let $V_{Var,x}(S_1, S_2)$ be the total intra-swarm positional variance component of the Lyapunov function as defined in [](#def-full-synergistic-lyapunov-function):\n2328: \n2329: $$\n2330: V_{Var,x}(S_1, S_2) = \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_1)} \\|\\delta_{x,1,i}\\|^2 + \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_2)} \\|\\delta_{x,2,i}\\|^2\n2331: $$\n2332: \n2333: If this component is large, such that $V_{Var,x} > R_{total\\_var,x}^2$ for some threshold $R_{total\\_var,x}^2 > 0$, then at least one swarm $k \\in \\{1, 2\\}$ must have a large sum of squared deviations:\n2334: \n2335: $$\n2336: \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2 > \\frac{R_{total\\_var,x}^2}{2}\n2337: $$",
    "document_id": "03_cloning",
    "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
    "span": {
      "start_line": 2322,
      "end_line": 2337,
      "content_start": 2325,
      "content_end": 2336,
      "header_lines": [
        2323
      ]
    },
    "references": [],
    "metadata": {
      "label": "lem-V_Varx-implies-variance"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 6,
      "chapter_file": "chapter_6.json",
      "section_id": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-phase-space-packing",
    "title": "The Phase-Space Packing Lemma",
    "type": "lemma",
    "nl_statement": "For a swarm of k \u2265 2 walkers in a compact phase-space domain, the fraction of close pairs under the algorithmic distance is bounded above by a monotonically decreasing function of the swarm's total hypocoercive variance, assuming \u03bb_v \u2264 \u03bb_alg.",
    "equations": [
      {
        "label": null,
        "latex": "\\mathrm{Var}_h(S_k) := \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k)"
      },
      {
        "label": null,
        "latex": "d_{\\text{alg}}(i, j)^2 := \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2"
      },
      {
        "label": null,
        "latex": "f_{\\text{close}} = N_{\\text{close}} / \\binom{k}{2}"
      },
      {
        "label": null,
        "latex": "D_{\\text{valid}}^2 := D_x^2 + \\lambda_{\\text{alg}} D_v^2"
      },
      {
        "label": null,
        "latex": "f_{\\text{close}} \\le g(\\mathrm{Var}_h(S_k)) := \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}"
      }
    ],
    "hypotheses": [
      {
        "text": "Swarm S_k consists of k \u2265 2 walkers with phase-space states {(x_i, v_i)} in a compact domain",
        "latex": null
      },
      {
        "text": "Proximity threshold d_close > 0 is chosen",
        "latex": null
      },
      {
        "text": "\u03bb_v \u2264 \u03bb_alg",
        "latex": null
      },
      {
        "text": "D_x and D_v are the spatial and velocity domain diameters",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "There exists a continuous, monotonically decreasing function g such that f_close \u2264 g(Var_h(S_k)) = (D_valid^2 - 2 Var_h(S_k)) / (D_valid^2 - d_close^2)",
      "latex": "f_{\\text{close}} \\le g(\\mathrm{Var}_h(S_k)) := \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}"
    },
    "variables": [
      {
        "symbol": "k",
        "name": "number of walkers",
        "description": "Size of the swarm",
        "constraints": [
          "k \u2265 2"
        ],
        "tags": [
          "swarm-size"
        ]
      },
      {
        "symbol": "x_i",
        "name": "position of walker i",
        "description": "Spatial coordinate in phase space",
        "constraints": [
          "within compact domain of diameter D_x"
        ],
        "tags": [
          "position"
        ]
      },
      {
        "symbol": "v_i",
        "name": "velocity of walker i",
        "description": "Velocity coordinate in phase space",
        "constraints": [
          "within compact domain of diameter D_v"
        ],
        "tags": [
          "velocity"
        ]
      },
      {
        "symbol": "d_close",
        "name": "proximity threshold",
        "description": "Threshold for close pairs under d_alg",
        "constraints": [
          "d_close > 0"
        ],
        "tags": [
          "threshold"
        ]
      },
      {
        "symbol": "\u03bb_v",
        "name": "velocity scaling in hypocoercive variance",
        "description": "Weight for velocity variance",
        "constraints": [
          "\u03bb_v \u2264 \u03bb_alg"
        ],
        "tags": [
          "scaling"
        ]
      },
      {
        "symbol": "\u03bb_alg",
        "name": "velocity scaling in algorithmic distance",
        "description": "Weight for velocity in d_alg",
        "constraints": [],
        "tags": [
          "scaling"
        ]
      },
      {
        "symbol": "D_x",
        "name": "spatial domain diameter",
        "description": "Diameter of the position domain",
        "constraints": [],
        "tags": [
          "diameter"
        ]
      },
      {
        "symbol": "D_v",
        "name": "velocity domain diameter",
        "description": "Diameter of the velocity domain",
        "constraints": [],
        "tags": [
          "diameter"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Var_x(S_k) and Var_v(S_k) are well-defined variances of positions and velocities",
        "confidence": 0.9
      },
      {
        "text": "The domain is compact, ensuring finite diameters D_x and D_v",
        "confidence": 1.0
      },
      {
        "text": "d_alg defines a metric on phase space",
        "confidence": 0.8
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-phase-space-packing",
      "title": null,
      "type": "proof",
      "proves": "lem-phase-space-packing",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":::{prf:proof}\n:label: proof-lem-phase-space-packing\n**Proof.**\n\nThe proof generalizes the classical packing argument to phase space and proceeds in four parts. First, we establish fundamental identities relating the hypocoercive variance to sums of pairwise squared distances in both position and velocity. Second, we partition pairs by their algorithmic distance and bound the hypocoercive variance. Third, we carefully account for the potentially different velocity weighting factors $\\lambda_v$ (in the variance) and $\\lambda_{\\text{alg}}$ (in the distance). Finally, we invert the relationship to derive the desired upper bound on the fraction of close pairs.\n\n**Part 1: Pairwise Identities for Hypocoercive Variance**\n\nWe begin by establishing pairwise representations for both positional and velocity variances. For the positional variance, the standard identity states:\n\n$$\n2k^2 \\mathrm{Var}_x(S_k) = \\sum_{i=1}^k \\sum_{j=1}^k \\|x_i - x_j\\|^2\n$$\n\nThis can be verified by expanding the right-hand side:\n\n$$\n\\begin{aligned}\n\\sum_{i,j} \\|x_i - x_j\\|^2 &= \\sum_{i,j} (\\|x_i\\|^2 - 2\\langle x_i, x_j \\rangle + \\|x_j\\|^2) \\\\\n&= 2k \\sum_i \\|x_i\\|^2 - 2\\langle k\\mu_x, k\\mu_x \\rangle \\\\\n&= 2k \\sum_i \\|x_i\\|^2 - 2k^2 \\|\\mu_x\\|^2 \\\\\n&= 2k(k \\cdot \\mathrm{Var}_x + k\\|\\mu_x\\|^2) - 2k^2\\|\\mu_x\\|^2 = 2k^2 \\mathrm{Var}_x\n\\end{aligned}\n$$\n\nAn identical derivation applies to the velocity variance:\n\n$$\n2k^2 \\mathrm{Var}_v(S_k) = \\sum_{i=1}^k \\sum_{j=1}^k \\|v_i - v_j\\|^2\n$$\n\nMultiplying the velocity identity by $\\lambda_v$ and adding the two identities yields:\n\n$$\n2k^2 \\mathrm{Var}_h(S_k) = 2k^2 (\\mathrm{Var}_x + \\lambda_v \\mathrm{Var}_v) = \\sum_{i,j} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right)\n$$\n\nSince the sum over all ordered pairs $(i,j)$ is twice the sum over unique pairs where $i<j$, we obtain:\n\n$$\n\\mathrm{Var}_h(S_k) = \\frac{1}{k^2} \\sum_{i<j} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right)\n$$\n\n**Part 2: Partitioning by Algorithmic Distance**\n\nWe now partition the set of unique pairs into two subsets based on the algorithmic distance $d_{\\text{alg}}(i,j)^2 = \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2$:\n- $P_{\\text{close}}$: the set of $N_{\\text{close}}$ pairs with $d_{\\text{alg}}(i,j) < d_{\\text{close}}$\n- $P_{\\text{far}}$: the set of $N_{\\text{far}}$ pairs with $d_{\\text{alg}}(i,j) \\ge d_{\\text{close}}$\n\nThe hypocoercive variance can be written as:\n\n$$\n\\mathrm{Var}_h(S_k) = \\frac{1}{k^2} \\left( \\sum_{(i,j) \\in P_{\\text{close}}} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right) + \\sum_{(i,j) \\in P_{\\text{far}}} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right) \\right)\n$$\n\n**Part 3: Bounding the Variance Terms**\n\nFor pairs in $P_{\\text{close}}$, we have $d_{\\text{alg}}(i,j)^2 = \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2 < d_{\\text{close}}^2$. Under our assumption that $\\lambda_v \\le \\lambda_{\\text{alg}}$, we can bound:\n\n$$\n\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2 \\le \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2 = d_{\\text{alg}}(i,j)^2 < d_{\\text{close}}^2\n$$\n\nFor pairs in $P_{\\text{far}}$, each component is bounded by the corresponding domain diameter:\n\n$$\n\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2 \\le D_x^2 + \\lambda_v D_v^2 \\le D_x^2 + \\lambda_{\\text{alg}} D_v^2 = D_{\\text{valid}}^2\n$$\n\nwhere we again used $\\lambda_v \\le \\lambda_{\\text{alg}}$. Therefore:\n\n$$\n\\mathrm{Var}_h(S_k) \\le \\frac{1}{k^2} \\left( N_{\\text{close}} \\cdot d_{\\text{close}}^2 + N_{\\text{far}} \\cdot D_{\\text{valid}}^2 \\right)\n$$\n\nLet $f_{\\text{close}} = N_{\\text{close}} / \\binom{k}{2}$ be the fraction of close pairs. Substituting $N_{\\text{close}} = f_{\\text{close}} \\binom{k}{2}$ and $N_{\\text{far}} = (1 - f_{\\text{close}}) \\binom{k}{2}$:\n\n$$\n\\mathrm{Var}_h(S_k) \\le \\frac{\\binom{k}{2}}{k^2} \\left( f_{\\text{close}} d_{\\text{close}}^2 + (1-f_{\\text{close}})D_{\\text{valid}}^2 \\right) = \\frac{k-1}{2k} \\left( f_{\\text{close}} (d_{\\text{close}}^2 - D_{\\text{valid}}^2) + D_{\\text{valid}}^2 \\right)\n$$\n\n**Part 4: Deriving the Upper Bound on the Fraction of Close Pairs**\n\nTo obtain a simpler bound, we use $(k-1)/(2k) < 1/2$ for $k \\ge 2$:\n\n$$\n\\mathrm{Var}_h(S_k) < \\frac{1}{2} \\left( f_{\\text{close}} (d_{\\text{close}}^2 - D_{\\text{valid}}^2) + D_{\\text{valid}}^2 \\right)\n$$\n\nSolving for $f_{\\text{close}}$:\n\n$$\n\\begin{aligned}\n2\\mathrm{Var}_h(S_k) &< f_{\\text{close}} (d_{\\text{close}}^2 - D_{\\text{valid}}^2) + D_{\\text{valid}}^2 \\\\\n2\\mathrm{Var}_h(S_k) - D_{\\text{valid}}^2 &< f_{\\text{close}}(d_{\\text{close}}^2 - D_{\\text{valid}}^2)\n\\end{aligned}\n$$\n\nSince $d_{\\text{close}} < D_{\\text{valid}}$, the term $(d_{\\text{close}}^2 - D_{\\text{valid}}^2)$ is strictly negative. Dividing by it reverses the inequality:\n\n$$\nf_{\\text{close}} < \\frac{2\\mathrm{Var}_h(S_k) - D_{\\text{valid}}^2}{d_{\\text{close}}^2 - D_{\\text{valid}}^2} = \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}\n$$\n\nThis establishes $f_{\\text{close}} \\le g(\\mathrm{Var}_h(S_k))$ where $g(V) := (D_{\\text{valid}}^2 - 2V) / (D_{\\text{valid}}^2 - d_{\\text{close}}^2)$. As an affine function of $V$ with negative coefficient, $g(V)$ is continuous and strictly decreasing.\n\nFinally, we verify that $g(\\mathrm{Var}_h) < 1$ when $\\mathrm{Var}_h > d_{\\text{close}}^2 / 2$:\n\n$$\n\\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h}{D_{\\text{valid}}^2 - d_{\\text{close}}^2} < 1 \\implies D_{\\text{valid}}^2 - 2\\mathrm{Var}_h < D_{\\text{valid}}^2 - d_{\\text{close}}^2 \\implies \\mathrm{Var}_h > \\frac{d_{\\text{close}}^2}{2}\n$$\n\nThis completes the proof.",
      "raw_directive": "2450: :::\n2451: \n2452: :::{prf:proof}\n2453: :label: proof-lem-phase-space-packing\n2454: **Proof.**\n2455: \n2456: The proof generalizes the classical packing argument to phase space and proceeds in four parts. First, we establish fundamental identities relating the hypocoercive variance to sums of pairwise squared distances in both position and velocity. Second, we partition pairs by their algorithmic distance and bound the hypocoercive variance. Third, we carefully account for the potentially different velocity weighting factors $\\lambda_v$ (in the variance) and $\\lambda_{\\text{alg}}$ (in the distance). Finally, we invert the relationship to derive the desired upper bound on the fraction of close pairs.\n2457: \n2458: **Part 1: Pairwise Identities for Hypocoercive Variance**\n2459: \n2460: We begin by establishing pairwise representations for both positional and velocity variances. For the positional variance, the standard identity states:\n2461: \n2462: $$\n2463: 2k^2 \\mathrm{Var}_x(S_k) = \\sum_{i=1}^k \\sum_{j=1}^k \\|x_i - x_j\\|^2\n2464: $$\n2465: \n2466: This can be verified by expanding the right-hand side:\n2467: \n2468: $$\n2469: \\begin{aligned}\n2470: \\sum_{i,j} \\|x_i - x_j\\|^2 &= \\sum_{i,j} (\\|x_i\\|^2 - 2\\langle x_i, x_j \\rangle + \\|x_j\\|^2) \\\\\n2471: &= 2k \\sum_i \\|x_i\\|^2 - 2\\langle k\\mu_x, k\\mu_x \\rangle \\\\\n2472: &= 2k \\sum_i \\|x_i\\|^2 - 2k^2 \\|\\mu_x\\|^2 \\\\\n2473: &= 2k(k \\cdot \\mathrm{Var}_x + k\\|\\mu_x\\|^2) - 2k^2\\|\\mu_x\\|^2 = 2k^2 \\mathrm{Var}_x\n2474: \\end{aligned}\n2475: $$\n2476: \n2477: An identical derivation applies to the velocity variance:\n2478: \n2479: $$\n2480: 2k^2 \\mathrm{Var}_v(S_k) = \\sum_{i=1}^k \\sum_{j=1}^k \\|v_i - v_j\\|^2\n2481: $$\n2482: \n2483: Multiplying the velocity identity by $\\lambda_v$ and adding the two identities yields:\n2484: \n2485: $$\n2486: 2k^2 \\mathrm{Var}_h(S_k) = 2k^2 (\\mathrm{Var}_x + \\lambda_v \\mathrm{Var}_v) = \\sum_{i,j} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right)\n2487: $$\n2488: \n2489: Since the sum over all ordered pairs $(i,j)$ is twice the sum over unique pairs where $i<j$, we obtain:\n2490: \n2491: $$\n2492: \\mathrm{Var}_h(S_k) = \\frac{1}{k^2} \\sum_{i<j} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right)\n2493: $$\n2494: \n2495: **Part 2: Partitioning by Algorithmic Distance**\n2496: \n2497: We now partition the set of unique pairs into two subsets based on the algorithmic distance $d_{\\text{alg}}(i,j)^2 = \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2$:\n2498: - $P_{\\text{close}}$: the set of $N_{\\text{close}}$ pairs with $d_{\\text{alg}}(i,j) < d_{\\text{close}}$\n2499: - $P_{\\text{far}}$: the set of $N_{\\text{far}}$ pairs with $d_{\\text{alg}}(i,j) \\ge d_{\\text{close}}$\n2500: \n2501: The hypocoercive variance can be written as:\n2502: \n2503: $$\n2504: \\mathrm{Var}_h(S_k) = \\frac{1}{k^2} \\left( \\sum_{(i,j) \\in P_{\\text{close}}} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right) + \\sum_{(i,j) \\in P_{\\text{far}}} \\left(\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2\\right) \\right)\n2505: $$\n2506: \n2507: **Part 3: Bounding the Variance Terms**\n2508: \n2509: For pairs in $P_{\\text{close}}$, we have $d_{\\text{alg}}(i,j)^2 = \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2 < d_{\\text{close}}^2$. Under our assumption that $\\lambda_v \\le \\lambda_{\\text{alg}}$, we can bound:\n2510: \n2511: $$\n2512: \\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2 \\le \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2 = d_{\\text{alg}}(i,j)^2 < d_{\\text{close}}^2\n2513: $$\n2514: \n2515: For pairs in $P_{\\text{far}}$, each component is bounded by the corresponding domain diameter:\n2516: \n2517: $$\n2518: \\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2 \\le D_x^2 + \\lambda_v D_v^2 \\le D_x^2 + \\lambda_{\\text{alg}} D_v^2 = D_{\\text{valid}}^2\n2519: $$\n2520: \n2521: where we again used $\\lambda_v \\le \\lambda_{\\text{alg}}$. Therefore:\n2522: \n2523: $$\n2524: \\mathrm{Var}_h(S_k) \\le \\frac{1}{k^2} \\left( N_{\\text{close}} \\cdot d_{\\text{close}}^2 + N_{\\text{far}} \\cdot D_{\\text{valid}}^2 \\right)\n2525: $$\n2526: \n2527: Let $f_{\\text{close}} = N_{\\text{close}} / \\binom{k}{2}$ be the fraction of close pairs. Substituting $N_{\\text{close}} = f_{\\text{close}} \\binom{k}{2}$ and $N_{\\text{far}} = (1 - f_{\\text{close}}) \\binom{k}{2}$:\n2528: \n2529: $$\n2530: \\mathrm{Var}_h(S_k) \\le \\frac{\\binom{k}{2}}{k^2} \\left( f_{\\text{close}} d_{\\text{close}}^2 + (1-f_{\\text{close}})D_{\\text{valid}}^2 \\right) = \\frac{k-1}{2k} \\left( f_{\\text{close}} (d_{\\text{close}}^2 - D_{\\text{valid}}^2) + D_{\\text{valid}}^2 \\right)\n2531: $$\n2532: \n2533: **Part 4: Deriving the Upper Bound on the Fraction of Close Pairs**\n2534: \n2535: To obtain a simpler bound, we use $(k-1)/(2k) < 1/2$ for $k \\ge 2$:\n2536: \n2537: $$\n2538: \\mathrm{Var}_h(S_k) < \\frac{1}{2} \\left( f_{\\text{close}} (d_{\\text{close}}^2 - D_{\\text{valid}}^2) + D_{\\text{valid}}^2 \\right)\n2539: $$\n2540: \n2541: Solving for $f_{\\text{close}}$:\n2542: \n2543: $$\n2544: \\begin{aligned}\n2545: 2\\mathrm{Var}_h(S_k) &< f_{\\text{close}} (d_{\\text{close}}^2 - D_{\\text{valid}}^2) + D_{\\text{valid}}^2 \\\\\n2546: 2\\mathrm{Var}_h(S_k) - D_{\\text{valid}}^2 &< f_{\\text{close}}(d_{\\text{close}}^2 - D_{\\text{valid}}^2)\n2547: \\end{aligned}\n2548: $$\n2549: \n2550: Since $d_{\\text{close}} < D_{\\text{valid}}$, the term $(d_{\\text{close}}^2 - D_{\\text{valid}}^2)$ is strictly negative. Dividing by it reverses the inequality:\n2551: \n2552: $$\n2553: f_{\\text{close}} < \\frac{2\\mathrm{Var}_h(S_k) - D_{\\text{valid}}^2}{d_{\\text{close}}^2 - D_{\\text{valid}}^2} = \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}\n2554: $$\n2555: \n2556: This establishes $f_{\\text{close}} \\le g(\\mathrm{Var}_h(S_k))$ where $g(V) := (D_{\\text{valid}}^2 - 2V) / (D_{\\text{valid}}^2 - d_{\\text{close}}^2)$. As an affine function of $V$ with negative coefficient, $g(V)$ is continuous and strictly decreasing.\n2557: \n2558: Finally, we verify that $g(\\mathrm{Var}_h) < 1$ when $\\mathrm{Var}_h > d_{\\text{close}}^2 / 2$:\n2559: \n2560: $$\n2561: \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h}{D_{\\text{valid}}^2 - d_{\\text{close}}^2} < 1 \\implies D_{\\text{valid}}^2 - 2\\mathrm{Var}_h < D_{\\text{valid}}^2 - d_{\\text{close}}^2 \\implies \\mathrm{Var}_h > \\frac{d_{\\text{close}}^2}{2}\n2562: $$\n2563: \n2564: This completes the proof.\n2565: ",
      "strategy_summary": "The proof establishes pairwise identities linking hypocoercive variance to squared distances in position and velocity spaces, partitions pairs based on algorithmic distance into close and far sets, bounds the variance contributions from each partition using the assumption \u03bb_v \u2264 \u03bb_alg, and inverts the inequality to derive an upper bound on the fraction of close pairs.",
      "conclusion": {
        "text": "f_{\\text{close}} < \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}",
        "latex": "f_{\\text{close}} < \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}"
      },
      "assumptions": [
        {
          "text": "\u03bb_v \u2264 \u03bb_alg",
          "latex": "\\lambda_v \\le \\lambda_{\\text{alg}}"
        },
        {
          "text": "k \u2265 2",
          "latex": "k \\ge 2"
        },
        {
          "text": "d_close < D_valid",
          "latex": "d_{\\text{close}} < D_{\\text{valid}}"
        },
        {
          "text": "Var_h(S_k) > d_close^2 / 2 for g(Var_h) < 1",
          "latex": "\\mathrm{Var}_h(S_k) > \\frac{d_{\\text{close}}^2}{2}"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "identity-establishment",
          "text": "Establish pairwise identities for positional and velocity variances, then combine with \u03bb_v to get the hypocoercive variance in terms of pairwise distances.",
          "latex": "2k^2 \\mathrm{Var}_h(S_k) = \\sum_{i,j} \\left( \\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2 \\right)",
          "references": [],
          "derived_statement": "\\mathrm{Var}_h(S_k) = \\frac{1}{k^2} \\sum_{i<j} \\left( \\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2 \\right)"
        },
        {
          "order": 2.0,
          "kind": "partitioning",
          "text": "Partition unique pairs into P_close (N_close pairs with d_alg(i,j) < d_close) and P_far (N_far pairs with d_alg(i,j) \u2265 d_close).",
          "latex": null,
          "references": [],
          "derived_statement": "\\mathrm{Var}_h(S_k) = \\frac{1}{k^2} \\left( \\sum_{(i,j) \\in P_{\\text{close}}} (...) + \\sum_{(i,j) \\in P_{\\text{far}}} (...) \\right)"
        },
        {
          "order": 3.0,
          "kind": "bounding",
          "text": "Bound close pairs by d_close^2 using \u03bb_v \u2264 \u03bb_alg, and far pairs by D_valid^2, leading to variance upper bound in terms of f_close.",
          "latex": "\\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2 \\le d_{\\text{alg}}(i,j)^2 < d_{\\text{close}}^2 for close; \\le D_{\\text{valid}}^2 for far",
          "references": [],
          "derived_statement": "\\mathrm{Var}_h(S_k) \\le \\frac{k-1}{2k} \\left( f_{\\text{close}} (d_{\\text{close}}^2 - D_{\\text{valid}}^2) + D_{\\text{valid}}^2 \\right)"
        },
        {
          "order": 4.0,
          "kind": "inversion",
          "text": "Use (k-1)/(2k) < 1/2 to simplify, solve for f_close by inverting the inequality (reversing due to negative factor).",
          "latex": "f_{\\text{close}} < \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}",
          "references": [],
          "derived_statement": "f_{\\text{close}} \\le g(\\mathrm{Var}_h(S_k)) where g is affine decreasing, and g < 1 when \\mathrm{Var}_h > d_{\\text{close}}^2 / 2"
        }
      ],
      "key_equations": [
        {
          "label": "eq-var-x-identity",
          "latex": "2k^2 \\mathrm{Var}_x(S_k) = \\sum_{i=1}^k \\sum_{j=1}^k \\|x_i - x_j\\|^2",
          "role": "Positional variance pairwise identity"
        },
        {
          "label": "eq-var-v-identity",
          "latex": "2k^2 \\mathrm{Var}_v(S_k) = \\sum_{i=1}^k \\sum_{j=1}^k \\|v_i - v_j\\|^2",
          "role": "Velocity variance pairwise identity"
        },
        {
          "label": "eq-var-h-identity",
          "latex": "\\mathrm{Var}_h(S_k) = \\frac{1}{k^2} \\sum_{i<j} \\left( \\|x_i - x_j\\|^2 + \\lambda_v \\|v_i - v_j\\|^2 \\right)",
          "role": "Hypocoercive variance in terms of pairs"
        },
        {
          "label": "eq-d-alg",
          "latex": "d_{\\text{alg}}(i,j)^2 = \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2",
          "role": "Definition of algorithmic distance"
        },
        {
          "label": "eq-var-h-bound",
          "latex": "\\mathrm{Var}_h(S_k) \\le \\frac{1}{k^2} \\left( N_{\\text{close}} \\cdot d_{\\text{close}}^2 + N_{\\text{far}} \\cdot D_{\\text{valid}}^2 \\right)",
          "role": "Variance bound after partitioning"
        },
        {
          "label": "eq-f-close-bound",
          "latex": "f_{\\text{close}} < \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}",
          "role": "Final upper bound on fraction of close pairs"
        }
      ],
      "references": [],
      "math_tools": [
        {
          "toolName": "Variance Pairwise Expansion",
          "field": "Statistics",
          "description": "Identity expressing the variance of a set of points as the average of squared pairwise distances.",
          "roleInProof": "Used to relate hypocoercive variance to sums of squared position and velocity differences.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Euclidean Norm"
          ]
        },
        {
          "toolName": "Set Partitioning",
          "field": "Combinatorics",
          "description": "Dividing a collection into disjoint subsets based on a criterion, here algorithmic distance.",
          "roleInProof": "Partitions pairs into close and far sets to separately bound variance contributions.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Binomial Coefficients"
          ]
        },
        {
          "toolName": "Linear Bounding",
          "field": "Analysis",
          "description": "Applying inequalities to bound linear combinations under monotonicity assumptions like \u03bb_v \u2264 \u03bb_alg.",
          "roleInProof": "Bounds the hypocoercive terms for close and far pairs using domain diameters.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Triangle Inequality"
          ]
        },
        {
          "toolName": "Inequality Inversion",
          "field": "Algebra",
          "description": "Solving inequalities by isolating variables and reversing directions when dividing by negative quantities.",
          "roleInProof": "Inverts the variance bound to obtain the upper limit on the close pair fraction.",
          "levelOfAbstraction": "Technique",
          "relatedTools": []
        }
      ],
      "cases": [
        {
          "name": "Close Pairs (P_close)",
          "condition": "d_alg(i,j) < d_close",
          "summary": "Bounded by d_close^2 using \u03bb_v \u2264 \u03bb_alg"
        },
        {
          "name": "Far Pairs (P_far)",
          "condition": "d_alg(i,j) \u2265 d_close",
          "summary": "Bounded by D_valid^2 = D_x^2 + \u03bb_alg D_v^2"
        }
      ],
      "remarks": [
        {
          "type": "verification",
          "text": "g(Var_h) < 1 when Var_h > d_close^2 / 2, ensuring the bound is meaningful."
        }
      ],
      "gaps": [],
      "tags": [
        "phase-space",
        "packing-argument",
        "hypocoercivity",
        "variance-identities",
        "pairwise-distances",
        "algorithmic-distance",
        "bounding-inequalities"
      ],
      "document_id": "03_cloning",
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "span": {
        "start_line": 2450,
        "end_line": 2565,
        "content_start": 2452,
        "content_end": 2564,
        "header_lines": [
          2451
        ]
      },
      "metadata": {
        "label": "proof-lem-phase-space-packing"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 6,
        "chapter_file": "chapter_6.json",
        "section_id": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "phase-space",
      "packing-lemma",
      "hypocoercive-variance",
      "swarm-dispersion",
      "algorithmic-distance",
      "clustering",
      "variance-bound"
    ],
    "content_markdown": ":label: lem-phase-space-packing\n\nFor a swarm `k` consisting of $k \\geq 2$ walkers with phase-space states $\\{(x_i, v_i)\\}_{i=1}^k$ within a compact domain, define the **total hypocoercive variance** of the swarm as:\n\n$$\n\\mathrm{Var}_h(S_k) := \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k)\n$$\n\nFor any chosen proximity threshold $d_{\\text{close}} > 0$, let $N_{\\text{close}}$ be the number of unique pairs $(i, j)$ with $i<j$ and $d_{\\text{alg}}(i, j) < d_{\\text{close}}$, where $d_{\\text{alg}}(i, j)^2 := \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2$ is the algorithmic phase-space distance.\n\nThe fraction of such \"close pairs in phase space\", $f_{\\text{close}} = N_{\\text{close}} / \\binom{k}{2}$, is bounded above by a function of the swarm's hypocoercive variance. Specifically, assuming $\\lambda_v \\le \\lambda_{\\text{alg}}$ and defining the phase-space diameter $D_{\\text{valid}}^2 := D_x^2 + \\lambda_{\\text{alg}} D_v^2$ where $D_x$ and $D_v$ are the spatial and velocity domain diameters, there exists a continuous, monotonically decreasing function such that:\n\n$$\nf_{\\text{close}} \\le g(\\mathrm{Var}_h(S_k)) := \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}\n$$",
    "raw_directive": "2430: Before analyzing the specific regimes of the $\\varepsilon$-dichotomy, we establish a precise, quantitative relationship between a swarm's global dispersion in phase space, as measured by its **total hypocoercive variance**, and its local phase-space clustering structure. This lemma generalizes the classical packing argument to phase space, proving that a swarm cannot be simultaneously spread out in the hypocoercive norm while being highly clustered under the algorithmic distance metric `d_alg`. This result is the foundational geometric constraint upon which both regimes of our analysis will depend.\n2431: \n2432: :::{prf:lemma} The Phase-Space Packing Lemma\n2433: :label: lem-phase-space-packing\n2434: \n2435: For a swarm `k` consisting of $k \\geq 2$ walkers with phase-space states $\\{(x_i, v_i)\\}_{i=1}^k$ within a compact domain, define the **total hypocoercive variance** of the swarm as:\n2436: \n2437: $$\n2438: \\mathrm{Var}_h(S_k) := \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k)\n2439: $$\n2440: \n2441: For any chosen proximity threshold $d_{\\text{close}} > 0$, let $N_{\\text{close}}$ be the number of unique pairs $(i, j)$ with $i<j$ and $d_{\\text{alg}}(i, j) < d_{\\text{close}}$, where $d_{\\text{alg}}(i, j)^2 := \\|x_i - x_j\\|^2 + \\lambda_{\\text{alg}} \\|v_i - v_j\\|^2$ is the algorithmic phase-space distance.\n2442: \n2443: The fraction of such \"close pairs in phase space\", $f_{\\text{close}} = N_{\\text{close}} / \\binom{k}{2}$, is bounded above by a function of the swarm's hypocoercive variance. Specifically, assuming $\\lambda_v \\le \\lambda_{\\text{alg}}$ and defining the phase-space diameter $D_{\\text{valid}}^2 := D_x^2 + \\lambda_{\\text{alg}} D_v^2$ where $D_x$ and $D_v$ are the spatial and velocity domain diameters, there exists a continuous, monotonically decreasing function such that:\n2444: \n2445: $$\n2446: f_{\\text{close}} \\le g(\\mathrm{Var}_h(S_k)) := \\frac{D_{\\text{valid}}^2 - 2\\mathrm{Var}_h(S_k)}{D_{\\text{valid}}^2 - d_{\\text{close}}^2}\n2447: $$\n2448: ",
    "document_id": "03_cloning",
    "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
    "span": {
      "start_line": 2430,
      "end_line": 2448,
      "content_start": 2433,
      "content_end": 2447,
      "header_lines": [
        2431
      ]
    },
    "references": [],
    "metadata": {
      "label": "lem-phase-space-packing"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 6,
      "chapter_file": "chapter_6.json",
      "section_id": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-var-x-implies-var-h",
    "title": "Positional Variance as a Lower Bound for Hypocoercive Variance",
    "type": "lemma",
    "nl_statement": "For any swarm k, the total hypocoercive variance Var_h(S_k) is bounded below by the positional variance Var_x(S_k).",
    "equations": [
      {
        "label": null,
        "latex": "\\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k)"
      }
    ],
    "hypotheses": [
      {
        "text": "For any swarm k",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "Var_h(S_k) \u2265 Var_x(S_k)",
      "latex": "\\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k)"
    },
    "variables": [
      {
        "symbol": "k",
        "name": "swarm index",
        "description": "Index labeling a specific swarm.",
        "constraints": [],
        "tags": [
          "index"
        ]
      },
      {
        "symbol": "S_k",
        "name": "swarm k",
        "description": "The k-th swarm configuration.",
        "constraints": [],
        "tags": [
          "swarm"
        ]
      },
      {
        "symbol": "Var_h",
        "name": "hypocoercive variance",
        "description": "Total hypocoercive variance function.",
        "constraints": [],
        "tags": [
          "variance",
          "hypocoercive"
        ]
      },
      {
        "symbol": "Var_x",
        "name": "positional variance",
        "description": "Positional variance component.",
        "constraints": [],
        "tags": [
          "variance",
          "positional"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Var_h and Var_x are defined non-negative variance measures for swarms.",
        "confidence": 0.9
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-var-x-implies-var-h",
      "title": null,
      "type": "proof",
      "proves": "lem-var-x-implies-var-h",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":::{prf:proof}\n:label: proof-lem-var-x-implies-var-h\n**Proof.**\n\nBy definition, the hypocoercive variance is:\n\n$$\n\\mathrm{Var}_h(S_k) := \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k)\n$$\n\nSince $\\lambda_v > 0$ is a positive hypocoercive parameter and $\\mathrm{Var}_v(S_k) \\ge 0$ (variance is non-negative), we immediately have:\n\n$$\n\\mathrm{Var}_h(S_k) = \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k) \\ge \\mathrm{Var}_x(S_k)\n$$\n\nThe second claim follows directly: if $\\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$, then by the above inequality, $\\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$.",
      "raw_directive": "2587: :::\n2588: \n2589: :::{prf:proof}\n2590: :label: proof-lem-var-x-implies-var-h\n2591: **Proof.**\n2592: \n2593: By definition, the hypocoercive variance is:\n2594: \n2595: $$\n2596: \\mathrm{Var}_h(S_k) := \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k)\n2597: $$\n2598: \n2599: Since $\\lambda_v > 0$ is a positive hypocoercive parameter and $\\mathrm{Var}_v(S_k) \\ge 0$ (variance is non-negative), we immediately have:\n2600: \n2601: $$\n2602: \\mathrm{Var}_h(S_k) = \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k) \\ge \\mathrm{Var}_x(S_k)\n2603: $$\n2604: \n2605: The second claim follows directly: if $\\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$, then by the above inequality, $\\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$.\n2606: ",
      "strategy_summary": "The proof establishes the inequality between hypocoercive variance and position variance using the definition and non-negativity properties, then directly applies it to the threshold condition.",
      "conclusion": {
        "text": "If $\\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$, then $\\mathrm{Var}_h(S_k) > R^2_{\\text{var}}$.",
        "latex": "\\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k) > R^2_{\\text{var}}"
      },
      "assumptions": [
        {
          "text": "$\\lambda_v > 0$",
          "latex": "\\lambda_v > 0"
        },
        {
          "text": "$\\mathrm{Var}_v(S_k) \\ge 0$",
          "latex": "\\mathrm{Var}_v(S_k) \\ge 0"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "definition",
          "text": "By definition, the hypocoercive variance is $\\mathrm{Var}_h(S_k) := \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k)$",
          "latex": "\\mathrm{Var}_h(S_k) := \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k)",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 2.0,
          "kind": "inequality",
          "text": "Since $\\lambda_v > 0$ and $\\mathrm{Var}_v(S_k) \\ge 0$, it follows that $\\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k)$",
          "latex": "\\mathrm{Var}_h(S_k) = \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k) \\ge \\mathrm{Var}_x(S_k)",
          "references": [],
          "derived_statement": "\\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k)"
        },
        {
          "order": 3.0,
          "kind": "implication",
          "text": "Therefore, if $\\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$, then $\\mathrm{Var}_h(S_k) > R^2_{\\text{var}}$",
          "latex": "\\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k) > R^2_{\\text{var}}",
          "references": [],
          "derived_statement": "\\mathrm{Var}_h(S_k) > R^2_{\\text{var}}"
        }
      ],
      "key_equations": [
        {
          "label": "def-var-h",
          "latex": "\\mathrm{Var}_h(S_k) := \\mathrm{Var}_x(S_k) + \\lambda_v \\mathrm{Var}_v(S_k)",
          "role": "definition of hypocoercive variance"
        },
        {
          "label": "ineq-var-h-x",
          "latex": "\\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k)",
          "role": "key inequality from non-negativity"
        }
      ],
      "references": [],
      "math_tools": [
        {
          "toolName": "Variance",
          "field": "Probability",
          "description": "A measure of the dispersion of a random variable around its mean.",
          "roleInProof": "Used to define hypocoercive variance and leverage its non-negativity to derive the inequality Var_h >= Var_x.",
          "levelOfAbstraction": "Concept",
          "relatedTools": []
        }
      ],
      "cases": [],
      "remarks": [],
      "gaps": [],
      "tags": [
        "hypocoercivity",
        "variance",
        "inequality",
        "non-negativity"
      ],
      "document_id": "03_cloning",
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "span": {
        "start_line": 2587,
        "end_line": 2606,
        "content_start": 2589,
        "content_end": 2605,
        "header_lines": [
          2588
        ]
      },
      "metadata": {
        "label": "proof-lem-var-x-implies-var-h"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 6,
        "chapter_file": "chapter_6.json",
        "section_id": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "variance",
      "hypocoercive",
      "positional",
      "swarm",
      "inequality",
      "lemma"
    ],
    "content_markdown": ":label: lem-var-x-implies-var-h\n\nFor any swarm `k`, its total hypocoercive variance is bounded below by its positional variance:\n\n$$\n\\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k)\n$$",
    "raw_directive": "2575: To connect this analysis back to the positional variance component of the Lyapunov function, we first establish a simple but crucial relationship.\n2576: \n2577: :::{prf:lemma} Positional Variance as a Lower Bound for Hypocoercive Variance\n2578: :label: lem-var-x-implies-var-h\n2579: \n2580: For any swarm `k`, its total hypocoercive variance is bounded below by its positional variance:\n2581: \n2582: $$\n2583: \\mathrm{Var}_h(S_k) \\ge \\mathrm{Var}_x(S_k)\n2584: $$\n2585: ",
    "document_id": "03_cloning",
    "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
    "span": {
      "start_line": 2575,
      "end_line": 2585,
      "content_start": 2578,
      "content_end": 2584,
      "header_lines": [
        2576
      ]
    },
    "references": [],
    "metadata": {
      "label": "lem-var-x-implies-var-h"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 6,
      "chapter_file": "chapter_6.json",
      "section_id": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-outlier-fraction-lower-bound",
    "title": "N-Uniform Lower Bound on the Outlier Fraction",
    "type": "lemma",
    "nl_statement": "For a swarm with k >= 2 alive walkers, if the hypocoercive variance Var_h(S_k) exceeds R_h^2 > 0, then the fraction of alive walkers in the global kinematic outlier set O_k is at least (1 - \u03b5_O) R_h^2 / D_h^2 > 0, independent of N.",
    "equations": [
      {
        "label": "ineq-outlier-fraction",
        "latex": "\\frac{|O_k|}{k} \\ge \\frac{(1-\\varepsilon_O) R^2_h}{D_h^2} =: f_O > 0"
      }
    ],
    "hypotheses": [
      {
        "text": "Swarm k has k >= 2 alive walkers.",
        "latex": null
      },
      {
        "text": "Structural parameter \u03b5_O \u2208 (0, 1).",
        "latex": "\\varepsilon_O \\in (0, 1)"
      },
      {
        "text": "Hypocoercive variance Var_h(S_k) > R_h^2 for some R_h^2 > 0.",
        "latex": "\\mathrm{Var}_h(S_k) > R^2_h \\; \\text{for some} \\; R^2_h > 0"
      }
    ],
    "conclusion": {
      "text": "The fraction of alive walkers in the outlier set is bounded below: |O_k|/k >= (1 - \u03b5_O) R_h^2 / D_h^2 =: f_O > 0, independent of N.",
      "latex": "\\frac{|O_k|}{k} \\ge \\frac{(1-\\varepsilon_O) R^2_h}{D_h^2} =: f_O > 0"
    },
    "variables": [
      {
        "symbol": "O_k",
        "name": "global kinematic outlier set",
        "description": "Set of outliers for swarm k.",
        "constraints": [
          "|O_k| <= k"
        ],
        "tags": [
          "outlier",
          "set"
        ]
      },
      {
        "symbol": "k",
        "name": "number of alive walkers",
        "description": "Size of the swarm.",
        "constraints": [
          "k >= 2"
        ],
        "tags": [
          "swarm",
          "size"
        ]
      },
      {
        "symbol": "\\varepsilon_O",
        "name": "structural parameter",
        "description": "Parameter for outlier set definition.",
        "constraints": [
          "\\varepsilon_O \\in (0,1)"
        ],
        "tags": [
          "structural",
          "parameter"
        ]
      },
      {
        "symbol": "\\mathrm{Var}_h(S_k)",
        "name": "hypocoercive variance",
        "description": "Internal variance measure for swarm S_k.",
        "constraints": [
          "> R_h^2"
        ],
        "tags": [
          "variance",
          "hypocoercive"
        ]
      },
      {
        "symbol": "R_h^2",
        "name": "variance threshold",
        "description": "Positive threshold for large variance.",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "threshold",
          "variance"
        ]
      },
      {
        "symbol": "D_h^2",
        "name": "hypocoercive diameter",
        "description": "Scaling factor in the bound.",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "diameter",
          "scaling"
        ]
      },
      {
        "symbol": "f_O",
        "name": "outlier fraction lower bound",
        "description": "Positive constant independent of N.",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "bound",
          "fraction"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "D_h^2 > 0 is a fixed positive constant from the model.",
        "confidence": 0.9
      },
      {
        "text": "The outlier set O_k is defined using \u03b5_O and kinematic properties as per Section 6.3.",
        "confidence": 1.0
      },
      {
        "text": "The hypocoercive variance relates to phase-space measures.",
        "confidence": 0.8
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-outlier-fraction-lower-bound",
      "title": null,
      "type": "proof",
      "proves": "lem-outlier-fraction-lower-bound",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-outlier-fraction-lower-bound\n\n**Proof.**\n\nThe proof establishes the lower bound by relating the total hypocoercive variance of the swarm to the maximum possible contribution of any single walker in phase space (using the packing argument from {prf:ref}`lem-phase-space-packing`), which is a fixed geometric property of the environment.\n\n**1. Recall Definitions and Outlier Set Property:**\n*   The sum of squared hypocoercive norms of the centered phase-space vectors for the `k` alive walkers is:\n\n\n$$\nT_k = \\sum_{j \\in \\mathcal{A}_k} \\left(\\|\\delta_{x,k,j}\\|^2 + \\lambda_v \\|\\delta_{v,k,j}\\|^2\\right) = k \\cdot \\mathrm{Var}_h(S_k)\n$$\n\n*   By the definition of the global kinematic outlier set $O_k$ (Section 6.3), the sum of squared hypocoercive norms over this subset is bounded below by a fixed fraction of the total sum:\n\n\n$$\n\\sum_{i \\in O_k} \\left(\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2\\right) \\ge (1-\\varepsilon_O) T_k = (1-\\varepsilon_O) k \\cdot \\mathrm{Var}_h(S_k)\n$$\n\n**2. Establish a Uniform Upper Bound on Single-Walker Contribution:**\n*   For any single alive walker `i`, its centered phase-space state is $(\\delta_{x,k,i}, \\delta_{v,k,i}) = (x_{k,i} - \\mu_{x,k}, v_{k,i} - \\mu_{v,k})$.\n*   The walker's position $x_{k,i}$ must lie within the valid domain $\\mathcal{X}_{\\text{valid}}$. If $\\mathcal{X}_{\\text{valid}}$ is convex (a standard assumption), the center of mass $\\mu_{x,k}$ must also lie within $\\mathcal{X}_{\\text{valid}}$. Therefore, $\\|\\delta_{x,k,i}\\| \\le D_x$, where $D_x$ is the positional domain diameter.\n*   Similarly, the velocity $v_{k,i}$ is bounded by the velocity domain diameter: $\\|\\delta_{v,k,i}\\| \\le D_v$.\n*   Therefore, the squared hypocoercive norm of any centered phase-space vector is uniformly bounded:\n\n\n$$\n\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2 \\le D_x^2 + \\lambda_v D_v^2 = D_h^2\n$$\n\n    This bound is a geometric property of the environment and is independent of the number of walkers `N` or `k`.\n\n**3. Bound the Sum over the Outlier Set:**\n*   The sum of squared hypocoercive norms over the outlier set can also be bounded above by multiplying the number of walkers in the set, $|O_k|$, by the maximum possible value of any single term:\n\n\n$$\n\\sum_{i \\in O_k} \\left(\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2\\right) \\le |O_k| \\cdot \\sup_{j \\in O_k} \\left(\\|\\delta_{x,k,j}\\|^2 + \\lambda_v \\|\\delta_{v,k,j}\\|^2\\right) \\le |O_k| \\cdot D_h^2\n$$\n\n**4. Combine Bounds and Finalize:**\n*   We now have both a lower and an upper bound for the same quantity. Combining them yields:\n\n\n$$\n(1-\\varepsilon_O) k \\cdot \\mathrm{Var}_h(S_k) \\le \\sum_{i \\in O_k} \\left(\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2\\right) \\le |O_k| \\cdot D_h^2\n$$\n\n*   We are given the premise that the hypocoercive variance is large: $\\mathrm{Var}_h(S_k) > R^2_h$. Substituting this into the left-hand side gives:\n\n\n$$\n(1-\\varepsilon_O) k \\cdot R^2_h < |O_k| \\cdot D_h^2\n$$\n\n*   Rearranging to find a bound on the fraction of outliers relative to the number of *alive* walkers `k`, we get:\n\n\n$$\n\\frac{|O_k|}{k} > \\frac{(1-\\varepsilon_O) R^2_h}{D_h^2}\n$$\n\n*   The resulting lower bound, $f_O := (1-\\varepsilon_O) R^2_h / D_h^2$, is a positive constant constructed entirely from `N`-independent parameters. This completes the proof that a large hypocoercive variance guarantees a non-vanishing fraction of global phase-space outliers among the alive population.",
      "raw_directive": "2623: where $D_h^2 := D_x^2 + \\lambda_v D_v^2$ is the squared **hypocoercive diameter** of the valid domain, with $D_x := \\sup_{x_1, x_2 \\in \\mathcal{X}_{\\text{valid}}} \\|x_1 - x_2\\|$ being the positional domain diameter and $D_v$ being the velocity domain diameter.\n2624: :::\n2625: :::{prf:proof}\n2626: :label: proof-lem-outlier-fraction-lower-bound\n2627: \n2628: **Proof.**\n2629: \n2630: The proof establishes the lower bound by relating the total hypocoercive variance of the swarm to the maximum possible contribution of any single walker in phase space (using the packing argument from {prf:ref}`lem-phase-space-packing`), which is a fixed geometric property of the environment.\n2631: \n2632: **1. Recall Definitions and Outlier Set Property:**\n2633: *   The sum of squared hypocoercive norms of the centered phase-space vectors for the `k` alive walkers is:\n2634: \n2635: \n2636: $$\n2637: T_k = \\sum_{j \\in \\mathcal{A}_k} \\left(\\|\\delta_{x,k,j}\\|^2 + \\lambda_v \\|\\delta_{v,k,j}\\|^2\\right) = k \\cdot \\mathrm{Var}_h(S_k)\n2638: $$\n2639: \n2640: *   By the definition of the global kinematic outlier set $O_k$ (Section 6.3), the sum of squared hypocoercive norms over this subset is bounded below by a fixed fraction of the total sum:\n2641: \n2642: \n2643: $$\n2644: \\sum_{i \\in O_k} \\left(\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2\\right) \\ge (1-\\varepsilon_O) T_k = (1-\\varepsilon_O) k \\cdot \\mathrm{Var}_h(S_k)\n2645: $$\n2646: \n2647: **2. Establish a Uniform Upper Bound on Single-Walker Contribution:**\n2648: *   For any single alive walker `i`, its centered phase-space state is $(\\delta_{x,k,i}, \\delta_{v,k,i}) = (x_{k,i} - \\mu_{x,k}, v_{k,i} - \\mu_{v,k})$.\n2649: *   The walker's position $x_{k,i}$ must lie within the valid domain $\\mathcal{X}_{\\text{valid}}$. If $\\mathcal{X}_{\\text{valid}}$ is convex (a standard assumption), the center of mass $\\mu_{x,k}$ must also lie within $\\mathcal{X}_{\\text{valid}}$. Therefore, $\\|\\delta_{x,k,i}\\| \\le D_x$, where $D_x$ is the positional domain diameter.\n2650: *   Similarly, the velocity $v_{k,i}$ is bounded by the velocity domain diameter: $\\|\\delta_{v,k,i}\\| \\le D_v$.\n2651: *   Therefore, the squared hypocoercive norm of any centered phase-space vector is uniformly bounded:\n2652: \n2653: \n2654: $$\n2655: \\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2 \\le D_x^2 + \\lambda_v D_v^2 = D_h^2\n2656: $$\n2657: \n2658:     This bound is a geometric property of the environment and is independent of the number of walkers `N` or `k`.\n2659: \n2660: **3. Bound the Sum over the Outlier Set:**\n2661: *   The sum of squared hypocoercive norms over the outlier set can also be bounded above by multiplying the number of walkers in the set, $|O_k|$, by the maximum possible value of any single term:\n2662: \n2663: \n2664: $$\n2665: \\sum_{i \\in O_k} \\left(\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2\\right) \\le |O_k| \\cdot \\sup_{j \\in O_k} \\left(\\|\\delta_{x,k,j}\\|^2 + \\lambda_v \\|\\delta_{v,k,j}\\|^2\\right) \\le |O_k| \\cdot D_h^2\n2666: $$\n2667: \n2668: **4. Combine Bounds and Finalize:**\n2669: *   We now have both a lower and an upper bound for the same quantity. Combining them yields:\n2670: \n2671: \n2672: $$\n2673: (1-\\varepsilon_O) k \\cdot \\mathrm{Var}_h(S_k) \\le \\sum_{i \\in O_k} \\left(\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2\\right) \\le |O_k| \\cdot D_h^2\n2674: $$\n2675: \n2676: *   We are given the premise that the hypocoercive variance is large: $\\mathrm{Var}_h(S_k) > R^2_h$. Substituting this into the left-hand side gives:\n2677: \n2678: \n2679: $$\n2680: (1-\\varepsilon_O) k \\cdot R^2_h < |O_k| \\cdot D_h^2\n2681: $$\n2682: \n2683: *   Rearranging to find a bound on the fraction of outliers relative to the number of *alive* walkers `k`, we get:\n2684: \n2685: \n2686: $$\n2687: \\frac{|O_k|}{k} > \\frac{(1-\\varepsilon_O) R^2_h}{D_h^2}\n2688: $$\n2689: \n2690: *   The resulting lower bound, $f_O := (1-\\varepsilon_O) R^2_h / D_h^2$, is a positive constant constructed entirely from `N`-independent parameters. This completes the proof that a large hypocoercive variance guarantees a non-vanishing fraction of global phase-space outliers among the alive population.\n2691: ",
      "strategy_summary": "The proof derives a lower bound on the fraction of outliers by lower-bounding the total hypocoercive norm sum over the outlier set using the swarm's variance and upper-bounding it via the maximum single-walker contribution times the outlier count, assuming large variance.",
      "conclusion": {
        "text": "The fraction of outliers satisfies \\frac{|O_k|}{k} > \\frac{(1-\\varepsilon_O) R^2_h}{D_h^2}, where f_O = (1-\\varepsilon_O) R^2_h / D_h^2 is a positive constant independent of the number of walkers.",
        "latex": "\\frac{|O_k|}{k} > \\frac{(1-\\varepsilon_O) R^2_h}{D_h^2}"
      },
      "assumptions": [
        {
          "text": "The valid domain \\mathcal{X}_{valid} is convex.",
          "latex": "\\mathcal{X}_{\\text{valid}} \\text{ is convex}"
        },
        {
          "text": "The hypocoercive variance satisfies \\mathrm{Var}_h(S_k) > R_h^2.",
          "latex": "\\mathrm{Var}_h(S_k) > R_h^2"
        },
        {
          "text": "Velocities are bounded by the velocity domain diameter D_v.",
          "latex": "\\|v_{k,i}\\| \\le D_v \\ \\forall i"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "recall",
          "text": "Recall the total hypocoercive sum T_k = \\sum_{j \\in \\mathcal{A}_k} (\\|\\delta_{x,k,j}\\|^2 + \\lambda_v \\|\\delta_{v,k,j}\\|^2) = k \\cdot \\mathrm{Var}_h(S_k), and the outlier property \\sum_{i \\in O_k} (\\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2) \\ge (1-\\varepsilon_O) T_k.",
          "latex": "T_k = \\sum_{j \\in \\mathcal{A}_k} \\left(\\Vert\\delta_{x,k,j}\\Vert^2 + \\lambda_v \\Vert\\delta_{v,k,j}\\Vert^2\\right) = k \\cdot \\mathrm{Var}_h(S_k) \\\\ \\sum_{i \\in O_k} \\left(\\Vert\\delta_{x,k,i}\\Vert^2 + \\lambda_v \\Vert\\delta_{v,k,i}\\Vert^2\\right) \\ge (1-\\varepsilon_O) T_k = (1-\\varepsilon_O) k \\cdot \\mathrm{Var}_h(S_k)",
          "references": [
            "def-hypocoercive-variance",
            "def-global-kinematic-outlier-set"
          ],
          "derived_statement": "(1-\\varepsilon_O) k \\cdot \\mathrm{Var}_h(S_k) lower bound on outlier sum"
        },
        {
          "order": 2.0,
          "kind": "bound",
          "text": "For any walker i, \\|\\delta_{x,k,i}\\| \\le D_x and \\|\\delta_{v,k,i}\\| \\le D_v, so \\|\\delta_{x,k,i}\\|^2 + \\lambda_v \\|\\delta_{v,k,i}\\|^2 \\le D_h^2 = D_x^2 + \\lambda_v D_v^2.",
          "latex": "\\Vert\\delta_{x,k,i}\\Vert \\le D_x, \\ \\Vert\\delta_{v,k,i}\\Vert \\le D_v \\\\ \\Vert\\delta_{x,k,i}\\Vert^2 + \\lambda_v \\Vert\\delta_{v,k,i}\\Vert^2 \\le D_x^2 + \\lambda_v D_v^2 = D_h^2",
          "references": [
            "def-valid-domain"
          ],
          "derived_statement": "Single-walker hypocoercive norm \\le D_h^2"
        },
        {
          "order": 3.0,
          "kind": "bound",
          "text": "Upper bound the outlier sum: \\sum_{i \\in O_k} (...) \\le |O_k| \\cdot D_h^2.",
          "latex": "\\sum_{i \\in O_k} \\left(\\Vert\\delta_{x,k,i}\\Vert^2 + \\lambda_v \\Vert\\delta_{v,k,i}\\Vert^2\\right) \\le |O_k| \\cdot D_h^2",
          "references": [],
          "derived_statement": "Outlier sum \\le |O_k| D_h^2"
        },
        {
          "order": 4.0,
          "kind": "combine",
          "text": "Combine bounds: (1-\\varepsilon_O) k \\cdot \\mathrm{Var}_h(S_k) \\le |O_k| \\cdot D_h^2. With \\mathrm{Var}_h(S_k) > R_h^2, get (1-\\varepsilon_O) k R_h^2 < |O_k| D_h^2, so \\frac{|O_k|}{k} > \\frac{(1-\\varepsilon_O) R_h^2}{D_h^2}.",
          "latex": "(1-\\varepsilon_O) k \\cdot \\mathrm{Var}_h(S_k) \\le |O_k| \\cdot D_h^2 \\\\ (1-\\varepsilon_O) k R_h^2 < |O_k| D_h^2 \\\\ \\frac{|O_k|}{k} > \\frac{(1-\\varepsilon_O) R_h^2}{D_h^2}",
          "references": [],
          "derived_statement": "Lower bound on outlier fraction"
        }
      ],
      "key_equations": [
        {
          "label": "eq-T_k",
          "latex": "T_k = \\sum_{j \\in \\mathcal{A}_k} \\left(\\Vert\\delta_{x,k,j}\\Vert^2 + \\lambda_v \\Vert\\delta_{v,k,j}\\Vert^2\\right) = k \\cdot \\mathrm{Var}_h(S_k)",
          "role": "Total hypocoercive variance sum"
        },
        {
          "label": "eq-outlier-lower",
          "latex": "\\sum_{i \\in O_k} \\left(\\Vert\\delta_{x,k,i}\\Vert^2 + \\lambda_v \\Vert\\delta_{v,k,i}\\Vert^2\\right) \\ge (1-\\varepsilon_O) T_k",
          "role": "Lower bound on outlier sum"
        },
        {
          "label": "eq-single-bound",
          "latex": "\\Vert\\delta_{x,k,i}\\Vert^2 + \\lambda_v \\Vert\\delta_{v,k,i}\\Vert^2 \\le D_h^2",
          "role": "Upper bound per walker"
        },
        {
          "label": "eq-outlier-upper",
          "latex": "\\sum_{i \\in O_k} (...) \\le |O_k| \\cdot D_h^2",
          "role": "Upper bound on outlier sum"
        },
        {
          "label": "eq-final-bound",
          "latex": "\\frac{|O_k|}{k} > \\frac{(1-\\varepsilon_O) R_h^2}{D_h^2}",
          "role": "Lower bound on outlier fraction"
        }
      ],
      "references": [
        "lem-phase-space-packing"
      ],
      "math_tools": [
        {
          "toolName": "Hypocoercive norm",
          "field": "Dynamical Systems",
          "description": "A norm that weights positional and velocity deviations to capture hypocoercivity in phase space.",
          "roleInProof": "Used to measure individual and collective deviations in the swarm, enabling bounds on variance and outlier contributions.",
          "levelOfAbstraction": "Notation",
          "relatedTools": [
            "Euclidean norm"
          ]
        },
        {
          "toolName": "Domain diameter",
          "field": "Geometry",
          "description": "The supremum of distances between points in a bounded domain, providing a uniform bound on deviations.",
          "roleInProof": "Establishes the upper bound D_h^2 on single-walker hypocoercive norms within the valid domain.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Euclidean norm"
          ]
        },
        {
          "toolName": "Population variance",
          "field": "Statistics",
          "description": "The average squared deviation from the mean, here adapted to hypocoercive norms in phase space.",
          "roleInProof": "Links the total hypocoercive dispersion T_k to k * Var_h(S_k), forming the lower bound on the outlier sum.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Hypocoercive norm"
          ]
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "note",
          "text": "The bound f_O is independent of the total number of walkers N and depends only on fixed parameters like \u03b5_O, R_h, and domain diameters."
        }
      ],
      "gaps": [],
      "tags": [
        "hypocoercivity",
        "variance",
        "outliers",
        "lower bound",
        "phase space",
        "geometric bound",
        "swarm dynamics"
      ],
      "document_id": "03_cloning",
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "span": {
        "start_line": 2623,
        "end_line": 2691,
        "content_start": 2626,
        "content_end": 2690,
        "header_lines": [
          2624
        ]
      },
      "metadata": {
        "label": "proof-lem-outlier-fraction-lower-bound"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 6,
        "chapter_file": "chapter_6.json",
        "section_id": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "outlier set",
      "hypocoercive variance",
      "lower bound",
      "swarm",
      "kinematic",
      "fraction",
      "uniform bound"
    ],
    "content_markdown": ":label: lem-outlier-fraction-lower-bound\n\nLet $O_k$ be the **global kinematic outlier set** for a swarm `k` with `k >= 2` alive walkers, as defined in Section 6.3, with structural parameter $\\varepsilon_O \\in (0, 1)$.\n\nIf the swarm's internal hypocoercive variance is large, such that $\\mathrm{Var}_h(S_k) > R^2_h$ for some threshold $R^2_h > 0$, then the fraction of *alive* walkers in the outlier set is bounded below by a positive constant that is independent of `N`. Specifically:\n\n$$\n\\frac{|O_k|}{k} \\ge \\frac{(1-\\varepsilon_O) R^2_h}{D_h^2} =: f_O > 0\n$$",
    "raw_directive": "2610: This lemma establishes the crucial bridge: the analysis in Section 6.2 guaranteed a large positional variance `Var_x`, and this lemma proves that such a condition is sufficient to guarantee a large hypocoercive variance `Var_h`, which is the relevant measure for the phase-space outlier set `O_k`. We can now proceed with the main result.\n2611: \n2612: :::{prf:lemma} N-Uniform Lower Bound on the Outlier Fraction\n2613: :label: lem-outlier-fraction-lower-bound\n2614: \n2615: Let $O_k$ be the **global kinematic outlier set** for a swarm `k` with `k >= 2` alive walkers, as defined in Section 6.3, with structural parameter $\\varepsilon_O \\in (0, 1)$.\n2616: \n2617: If the swarm's internal hypocoercive variance is large, such that $\\mathrm{Var}_h(S_k) > R^2_h$ for some threshold $R^2_h > 0$, then the fraction of *alive* walkers in the outlier set is bounded below by a positive constant that is independent of `N`. Specifically:\n2618: \n2619: $$\n2620: \\frac{|O_k|}{k} \\ge \\frac{(1-\\varepsilon_O) R^2_h}{D_h^2} =: f_O > 0\n2621: $$\n2622: ",
    "document_id": "03_cloning",
    "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
    "span": {
      "start_line": 2610,
      "end_line": 2622,
      "content_start": 2613,
      "content_end": 2621,
      "header_lines": [
        2611
      ]
    },
    "references": [
      "lem-phase-space-packing"
    ],
    "metadata": {
      "label": "lem-outlier-fraction-lower-bound"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 6,
      "chapter_file": "chapter_6.json",
      "section_id": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-outlier-cluster-fraction-lower-bound",
    "title": "N-Uniform Lower Bound on the Outlier-Cluster Fraction",
    "type": "lemma",
    "nl_statement": "In the local-interaction regime, using the clustering-based definition of the high-error set with maximum cluster diameter \\(D_{\\text{diam}}(\\varepsilon) = c_d \\cdot \\varepsilon\\) where \\(c_d \\varepsilon < 2\\sqrt{R^2_{\\text{var}}}\\), if the swarm's internal positional variance satisfies \\(\\mathrm{Var}_x(S_k) > R^2_{\\text{var}}\\), then the fraction of alive walkers in the high-error set \\(H_k(\\varepsilon)\\) is bounded below by a positive constant \\(f_H(\\varepsilon) > 0\\) independent of \\(N\\) and \\(k\\).",
    "equations": [
      {
        "label": null,
        "latex": "\\frac{|H_k(\\epsilon)|}{k} \\ge f_H(\\epsilon) > 0"
      }
    ],
    "hypotheses": [],
    "conclusion": {
      "text": null,
      "latex": null
    },
    "variables": [
      {
        "symbol": "\\varepsilon",
        "name": "epsilon",
        "description": "Error threshold parameter.",
        "constraints": [
          "positive"
        ],
        "tags": [
          "error",
          "threshold"
        ]
      },
      {
        "symbol": "k",
        "name": "k",
        "description": "Number of walkers in the swarm subset \\(S_k\\).",
        "constraints": [
          "positive integer"
        ],
        "tags": [
          "walkers",
          "count"
        ]
      },
      {
        "symbol": "H_k(\\varepsilon)",
        "name": "high-error set",
        "description": "Set of walkers in high-error clusters based on phase-space clustering.",
        "constraints": [],
        "tags": [
          "set",
          "high-error",
          "clustering"
        ]
      },
      {
        "symbol": "c_d",
        "name": "c_d",
        "description": "Fixed positive constant scaling the cluster diameter.",
        "constraints": [
          "positive"
        ],
        "tags": [
          "constant",
          "diameter"
        ]
      },
      {
        "symbol": "R^2_{\\text{var}}",
        "name": "variance threshold",
        "description": "Threshold for internal positional variance.",
        "constraints": [
          "positive"
        ],
        "tags": [
          "variance",
          "threshold"
        ]
      },
      {
        "symbol": "S_k",
        "name": "swarm subset",
        "description": "Subset of k walkers in the swarm.",
        "constraints": [],
        "tags": [
          "swarm",
          "subset"
        ]
      },
      {
        "symbol": "\\mathrm{Var}_x(S_k)",
        "name": "positional variance",
        "description": "Internal positional variance of the swarm subset \\(S_k\\).",
        "constraints": [],
        "tags": [
          "variance",
          "positional"
        ]
      },
      {
        "symbol": "f_H(\\varepsilon)",
        "name": "lower bound constant",
        "description": "Positive constant bounding the high-error fraction, independent of N and k.",
        "constraints": [
          "positive"
        ],
        "tags": [
          "constant",
          "lower-bound"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The swarm consists of alive walkers.",
        "confidence": 0.9
      },
      {
        "text": "N represents a large system size, uniform across bounds.",
        "confidence": 0.8
      },
      {
        "text": "The phase-space clustering approach is well-defined in the local-interaction regime.",
        "confidence": 1.0
      }
    ],
    "local_refs": [
      "def-unified-high-low-error-sets"
    ],
    "proof": {
      "label": "proof-lem-outlier-cluster-fraction-lower-bound",
      "title": null,
      "type": "proof",
      "proves": "lem-outlier-cluster-fraction-lower-bound",
      "proof_type": "construction",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-outlier-cluster-fraction-lower-bound\n\n**Proof.**\n\nThe proof is constructive. We use the Law of Total Variance to show that a large global variance forces a large variance *between* the cluster centers. We then apply the same logic used in the mean-field regime ({prf:ref}`lem-outlier-fraction-lower-bound`) to this set of cluster centers to prove that a non-vanishing fraction of the population must reside in these outlier clusters.\n\n**1. Decomposing the Total Variance.**\nThe Law of Total Variance provides an exact identity for the swarm's variance based on the cluster partition `{G_1, ..., G_M}`. Let $\\mu$ be the global center of mass of the `k` alive walkers, $\\mu_m$ be the center of mass of cluster `G_m`, and `|G_m|` be the number of walkers in it. The total sum of squared deviations can be decomposed as:\n\n$$\nk \\cdot \\mathrm{Var}_k(x) = \\sum_{m=1}^M \\sum_{i \\in G_m} \\|x_i - \\mu\\|^2 = \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) + \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n$$\n\nThe first term is the \"within-cluster\" sum of squares, and the second is the size-weighted \"between-cluster\" sum of squares.\n\n**2. A Uniform Upper Bound on the Within-Cluster Variance.**\nBy the definition of our clustering algorithm, the diameter of any cluster `G_m` is at most $D_diam(\\varepsilon)$. The maximum possible internal variance for any set of points with a given diameter is achieved when the points are at the extremes of an interval, which gives $\\text{Var}(G_m) \\leq (D_diam(\\varepsilon)/2)^{2}$. This provides a uniform, N-independent upper bound for the within-cluster variance of any cluster.\nThe total within-cluster sum of squares is therefore bounded:\n\n$$\n\\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) \\le \\sum_{m=1}^M |G_m| \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2 = k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2\n$$\n\n**3. A Uniform Lower Bound on the Between-Cluster Variance.**\nWe can now find a lower bound for the between-cluster sum of squares. Rearranging the identity from Step 1 and using our premise `Var_k(x) > R^{2}_var`:\n\n$$\n\\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 = k \\cdot \\mathrm{Var}_k(x) - \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) > k \\cdot R^2_{\\mathrm{var}} - k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2\n$$\n\nLet's define a new positive, N-uniform constant $R^{2}_means := R^{2}_var - (D_diam(\\varepsilon)/2)^{2}$. The premise of this lemma requires that we choose $D_diam(\\varepsilon)$ small enough to ensure `R^{2}_means > 0`. With this, we have a guaranteed lower bound on the size-weighted variance of the cluster means:\n\n$$\n\\frac{1}{k}\\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 > R^2_{\\mathrm{means}} > 0\n$$\n\n**4. Applying the Outlier Argument to the Cluster Centers.**\nWe have now reduced the problem to one that is formally identical to the mean-field case. We have a set of `M` \"meta-particles\" (the cluster centers $\\mu_m$) with associated weights (`|G_m|`) whose size-weighted variance is guaranteed to be large.\n\nBy the definition of the high-error set $H_k(\\varepsilon)$, it is the union of all walkers in the \"outlier clusters\" `O_M`. These are the clusters whose weighted contribution to the between-cluster variance sums to at least $(1-\\varepsilon_O)$ of the total.\n\n$$\n\\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 > (1-\\varepsilon_O) k \\cdot R^2_{\\mathrm{means}}\n$$\n\nAt the same time, we can find an upper bound for this sum. The maximum squared distance of any cluster mean from the global mean is bounded by `D_valid^{2}`.\n\n$$\n\\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\le \\sum_{m \\in O_M} |G_m|D_{\\mathrm{valid}}^2 = D_{\\mathrm{valid}}^2 \\sum_{m \\in O_M} |G_m|\n$$\n\nThe term $\\Sigma_{m\\inO_M} |G_m|$ is, by definition, the total number of walkers in the high-error set, $|H_k(\\varepsilon)|$. Combining the inequalities:\n\n$$\n(1-\\varepsilon_O) k \\cdot R^2_{\\mathrm{means}} < |H_k(\\epsilon)| \\cdot D_{\\mathrm{valid}}^2\n$$\n\n**5. Conclusion.**\nRearranging the final inequality gives the desired N-uniform lower bound on the high-error fraction:\n\n$$\n\\frac{|H_k(\\epsilon)|}{k} > \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{D_{\\mathrm{valid}}^2} = \\frac{(1-\\varepsilon_O) \\left(R^2_{\\mathrm{var}} - (D_{\\mathrm{diam}}(\\epsilon)/2)^2\\right)}{D_{\\mathrm{valid}}^2}\n$$\n\nWe define the right-hand side as our N-uniform constant $f_H(\\varepsilon)$. It is strictly positive by our choice of $D_diam(\\varepsilon)$, and it is constructed entirely from N-independent system parameters ($\\varepsilon_O$, `R^{2}_var`, `D_diam`, `D_valid`). This completes the N-uniform proof.",
      "raw_directive": "2730: \n2731: :::\n2732: :::{prf:proof}\n2733: :label: proof-lem-outlier-cluster-fraction-lower-bound\n2734: \n2735: **Proof.**\n2736: \n2737: The proof is constructive. We use the Law of Total Variance to show that a large global variance forces a large variance *between* the cluster centers. We then apply the same logic used in the mean-field regime ({prf:ref}`lem-outlier-fraction-lower-bound`) to this set of cluster centers to prove that a non-vanishing fraction of the population must reside in these outlier clusters.\n2738: \n2739: **1. Decomposing the Total Variance.**\n2740: The Law of Total Variance provides an exact identity for the swarm's variance based on the cluster partition `{G_1, ..., G_M}`. Let $\\mu$ be the global center of mass of the `k` alive walkers, $\\mu_m$ be the center of mass of cluster `G_m`, and `|G_m|` be the number of walkers in it. The total sum of squared deviations can be decomposed as:\n2741: \n2742: $$\n2743: k \\cdot \\mathrm{Var}_k(x) = \\sum_{m=1}^M \\sum_{i \\in G_m} \\|x_i - \\mu\\|^2 = \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) + \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n2744: $$\n2745: \n2746: The first term is the \"within-cluster\" sum of squares, and the second is the size-weighted \"between-cluster\" sum of squares.\n2747: \n2748: **2. A Uniform Upper Bound on the Within-Cluster Variance.**\n2749: By the definition of our clustering algorithm, the diameter of any cluster `G_m` is at most $D_diam(\\varepsilon)$. The maximum possible internal variance for any set of points with a given diameter is achieved when the points are at the extremes of an interval, which gives $\\text{Var}(G_m) \\leq (D_diam(\\varepsilon)/2)^{2}$. This provides a uniform, N-independent upper bound for the within-cluster variance of any cluster.\n2750: The total within-cluster sum of squares is therefore bounded:\n2751: \n2752: $$\n2753: \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) \\le \\sum_{m=1}^M |G_m| \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2 = k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2\n2754: $$\n2755: \n2756: **3. A Uniform Lower Bound on the Between-Cluster Variance.**\n2757: We can now find a lower bound for the between-cluster sum of squares. Rearranging the identity from Step 1 and using our premise `Var_k(x) > R^{2}_var`:\n2758: \n2759: $$\n2760: \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 = k \\cdot \\mathrm{Var}_k(x) - \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) > k \\cdot R^2_{\\mathrm{var}} - k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2\n2761: $$\n2762: \n2763: Let's define a new positive, N-uniform constant $R^{2}_means := R^{2}_var - (D_diam(\\varepsilon)/2)^{2}$. The premise of this lemma requires that we choose $D_diam(\\varepsilon)$ small enough to ensure `R^{2}_means > 0`. With this, we have a guaranteed lower bound on the size-weighted variance of the cluster means:\n2764: \n2765: $$\n2766: \\frac{1}{k}\\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 > R^2_{\\mathrm{means}} > 0\n2767: $$\n2768: \n2769: **4. Applying the Outlier Argument to the Cluster Centers.**\n2770: We have now reduced the problem to one that is formally identical to the mean-field case. We have a set of `M` \"meta-particles\" (the cluster centers $\\mu_m$) with associated weights (`|G_m|`) whose size-weighted variance is guaranteed to be large.\n2771: \n2772: By the definition of the high-error set $H_k(\\varepsilon)$, it is the union of all walkers in the \"outlier clusters\" `O_M`. These are the clusters whose weighted contribution to the between-cluster variance sums to at least $(1-\\varepsilon_O)$ of the total.\n2773: \n2774: $$\n2775: \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 > (1-\\varepsilon_O) k \\cdot R^2_{\\mathrm{means}}\n2776: $$\n2777: \n2778: At the same time, we can find an upper bound for this sum. The maximum squared distance of any cluster mean from the global mean is bounded by `D_valid^{2}`.\n2779: \n2780: $$\n2781: \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\le \\sum_{m \\in O_M} |G_m|D_{\\mathrm{valid}}^2 = D_{\\mathrm{valid}}^2 \\sum_{m \\in O_M} |G_m|\n2782: $$\n2783: \n2784: The term $\\Sigma_{m\\inO_M} |G_m|$ is, by definition, the total number of walkers in the high-error set, $|H_k(\\varepsilon)|$. Combining the inequalities:\n2785: \n2786: $$\n2787: (1-\\varepsilon_O) k \\cdot R^2_{\\mathrm{means}} < |H_k(\\epsilon)| \\cdot D_{\\mathrm{valid}}^2\n2788: $$\n2789: \n2790: **5. Conclusion.**\n2791: Rearranging the final inequality gives the desired N-uniform lower bound on the high-error fraction:\n2792: \n2793: $$\n2794: \\frac{|H_k(\\epsilon)|}{k} > \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{D_{\\mathrm{valid}}^2} = \\frac{(1-\\varepsilon_O) \\left(R^2_{\\mathrm{var}} - (D_{\\mathrm{diam}}(\\epsilon)/2)^2\\right)}{D_{\\mathrm{valid}}^2}\n2795: $$\n2796: \n2797: We define the right-hand side as our N-uniform constant $f_H(\\varepsilon)$. It is strictly positive by our choice of $D_diam(\\varepsilon)$, and it is constructed entirely from N-independent system parameters ($\\varepsilon_O$, `R^{2}_var`, `D_diam`, `D_valid`). This completes the N-uniform proof.\n2798: ",
      "strategy_summary": "The proof decomposes the total variance using the Law of Total Variance into within-cluster and between-cluster components, establishes an upper bound on within-cluster variance via cluster diameters, derives a lower bound on the weighted variance of cluster centers, and applies an outlier argument to these centers to obtain a positive lower bound on the fraction of walkers in outlier clusters.",
      "conclusion": {
        "text": "The fraction of walkers in the high-error set satisfies \\frac{|H_k(\\epsilon)|}{k} > f_H(\\varepsilon), where f_H(\\varepsilon) = \\frac{(1-\\varepsilon_O) \\left(R^2_{\\mathrm{var}} - (D_{\\mathrm{diam}}(\\epsilon)/2)^2\\right)}{D_{\\mathrm{valid}}^2} > 0 is an N-uniform constant.",
        "latex": "\\frac{|H_k(\\epsilon)|}{k} > \\frac{(1-\\varepsilon_O) \\left(R^2_{\\mathrm{var}} - \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2\\right)}{D_{\\mathrm{valid}}^2}"
      },
      "assumptions": [
        {
          "text": "The global variance satisfies \\mathrm{Var}_k(x) > R^2_{\\mathrm{var}}.",
          "latex": "\\mathrm{Var}_k(x) > R^2_{\\mathrm{var}}"
        },
        {
          "text": "Each cluster G_m has diameter at most D_{\\mathrm{diam}}(\\varepsilon), chosen small enough so that R^2_{\\mathrm{var}} - (D_{\\mathrm{diam}}(\\varepsilon)/2)^2 > 0.",
          "latex": "\\mathrm{diam}(G_m) \\le D_{\\mathrm{diam}}(\\varepsilon) \\quad \\text{and} \\quad R^2_{\\mathrm{var}} - \\left(\\frac{D_{\\mathrm{diam}}(\\varepsilon)}{2}\\right)^2 > 0"
        },
        {
          "text": "Cluster centers \\mu_m satisfy \\|\\mu_m - \\mu\\| \\le D_{\\mathrm{valid}}.",
          "latex": "\\|\\mu_m - \\mu\\| \\le D_{\\mathrm{valid}}"
        },
        {
          "text": "The high-error set H_k(\\varepsilon) is the union of outlier clusters O_M, where \\sum_{m \\in O_M} |G_m| \\|\\mu_m - \\mu\\|^2 \\ge (1 - \\varepsilon_O) \\sum_{m=1}^M |G_m| \\|\\mu_m - \\mu\\|^2.",
          "latex": "\\sum_{m \\in O_M} |G_m| \\|\\mu_m - \\mu\\|^2 \\ge (1 - \\varepsilon_O) \\sum_{m=1}^M |G_m| \\|\\mu_m - \\mu\\|^2"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "decomposition",
          "text": "Apply the Law of Total Variance to decompose the total sum of squared deviations: k \\cdot \\mathrm{Var}_k(x) = \\sum_{m=1}^M |G_m| \\mathrm{Var}(G_m) + \\sum_{m=1}^M |G_m| \\|\\mu_m - \\mu\\|^2.",
          "latex": "k \\cdot \\mathrm{Var}_k(x) = \\sum_{m=1}^M \\sum_{i \\in G_m} \\|x_i - \\mu\\|^2 = \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) + \\sum_{m=1}^M |G_m| \\|\\mu_m - \\mu\\|^2",
          "references": [],
          "derived_statement": "Total variance = within-cluster + between-cluster variance."
        },
        {
          "order": 2.0,
          "kind": "bound",
          "text": "Bound the within-cluster variance: \\mathrm{Var}(G_m) \\le (D_{\\mathrm{diam}}(\\varepsilon)/2)^2, so \\sum_{m=1}^M |G_m| \\mathrm{Var}(G_m) \\le k (D_{\\mathrm{diam}}(\\varepsilon)/2)^2.",
          "latex": "\\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) \\le k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2",
          "references": [],
          "derived_statement": "Within-cluster sum of squares \\le k (D_{\\mathrm{diam}}(\\varepsilon)/2)^2."
        },
        {
          "order": 3.0,
          "kind": "lower-bound",
          "text": "Derive lower bound on between-cluster variance: \\sum_{m=1}^M |G_m| \\|\\mu_m - \\mu\\|^2 > k R^2_{\\mathrm{var}} - k (D_{\\mathrm{diam}}(\\varepsilon)/2)^2 = k R^2_{\\mathrm{means}}, with R^2_{\\mathrm{means}} > 0.",
          "latex": "\\frac{1}{k} \\sum_{m=1}^M |G_m| \\|\\mu_m - \\mu\\|^2 > R^2_{\\mathrm{means}} > 0 \\quad \\text{where} \\quad R^2_{\\mathrm{means}} = R^2_{\\mathrm{var}} - \\left(\\frac{D_{\\mathrm{diam}}(\\varepsilon)}{2}\\right)^2",
          "references": [],
          "derived_statement": "Weighted variance of cluster centers > R^2_{\\mathrm{means}} > 0."
        },
        {
          "order": 4.0,
          "kind": "application",
          "text": "Apply outlier argument to cluster centers: For outlier clusters O_M, \\sum_{m \\in O_M} |G_m| \\|\\mu_m - \\mu\\|^2 \\ge (1 - \\varepsilon_O) k R^2_{\\mathrm{means}}, and bound above by D^2_{\\mathrm{valid}} |H_k(\\varepsilon)|.",
          "latex": "(1 - \\varepsilon_O) k R^2_{\\mathrm{means}} < |H_k(\\varepsilon)| D^2_{\\mathrm{valid}}",
          "references": [
            "lem-outlier-fraction-lower-bound"
          ],
          "derived_statement": "Size of high-error set bounded below via outlier contribution."
        },
        {
          "order": 5.0,
          "kind": "conclusion",
          "text": "Rearrange to obtain the lower bound on the fraction: \\frac{|H_k(\\varepsilon)|}{k} > \\frac{(1 - \\varepsilon_O) R^2_{\\mathrm{means}}}{D^2_{\\mathrm{valid}}} = f_H(\\varepsilon) > 0.",
          "latex": "\\frac{|H_k(\\epsilon)|}{k} > \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{D_{\\mathrm{valid}}^2}",
          "references": [],
          "derived_statement": "N-uniform lower bound on high-error fraction established."
        }
      ],
      "key_equations": [
        {
          "label": "eq-total-variance-decomp",
          "latex": "k \\cdot \\mathrm{Var}_k(x) = \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) + \\sum_{m=1}^M |G_m| \\|\\mu_m - \\mu\\|^2",
          "role": "Decomposition of total variance into within and between components."
        },
        {
          "label": "eq-within-bound",
          "latex": "\\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) \\le k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2",
          "role": "Upper bound on within-cluster sum of squares."
        },
        {
          "label": "eq-between-lower",
          "latex": "\\sum_{m=1}^M |G_m| \\|\\mu_m - \\mu\\|^2 > k \\left( R^2_{\\mathrm{var}} - \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2 \\right)",
          "role": "Lower bound on between-cluster sum of squares."
        },
        {
          "label": "eq-outlier-contribution",
          "latex": "\\sum_{m \\in O_M} |G_m| \\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m| \\|\\mu_m - \\mu\\|^2 > (1-\\varepsilon_O) k R^2_{\\mathrm{means}}",
          "role": "Lower bound on outlier clusters' variance contribution."
        },
        {
          "label": "eq-final-inequality",
          "latex": "\\frac{|H_k(\\epsilon)|}{k} > \\frac{(1-\\varepsilon_O) \\left(R^2_{\\mathrm{var}} - (D_{\\mathrm{diam}}(\\epsilon)/2)^2\\right)}{D_{\\mathrm{valid}}^2}",
          "role": "Final lower bound on the high-error fraction."
        }
      ],
      "references": [
        "lem-outlier-fraction-lower-bound"
      ],
      "math_tools": [
        {
          "toolName": "Law of Total Variance",
          "field": "Statistics",
          "description": "Decomposes the total variance of a random variable into the expected variance within subpopulations plus the variance of the subpopulation means.",
          "roleInProof": "Used to separate the swarm's total variance into within-cluster and between-cluster sums of squares, enabling isolation of large between-cluster variance.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": []
        },
        {
          "toolName": "Variance Bound by Diameter",
          "field": "Geometry",
          "description": "For a set of points with diameter D, the variance is at most (D/2)^2, achieved when points are at the extremes.",
          "roleInProof": "Provides a uniform upper bound on within-cluster variance based on the clustering algorithm's diameter guarantee.",
          "levelOfAbstraction": "Technique",
          "relatedTools": []
        },
        {
          "toolName": "Outlier Detection via Weighted Variance",
          "field": "Statistics",
          "description": "Identifies a subset of weighted points (outliers) whose contribution to total weighted variance is at least a fixed fraction, bounded by maximum distance.",
          "roleInProof": "Applied to the weighted cluster centers to bound the size of the high-error set from below.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Law of Total Variance"
          ]
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "uniformity",
          "text": "The bound f_H(\\varepsilon) is N-uniform, depending only on system parameters \\varepsilon_O, R^2_{\\mathrm{var}}, D_{\\mathrm{diam}}(\\varepsilon), D_{\\mathrm{valid}}."
        },
        {
          "type": "reference",
          "text": "The outlier argument in Step 4 mirrors the mean-field case from lem-outlier-fraction-lower-bound."
        }
      ],
      "gaps": [],
      "tags": [
        "variance decomposition",
        "law of total variance",
        "clustering",
        "outlier fraction",
        "constructive proof",
        "between-cluster variance",
        "N-uniform bound"
      ],
      "document_id": "03_cloning",
      "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
      "span": {
        "start_line": 2730,
        "end_line": 2798,
        "content_start": 2733,
        "content_end": 2797,
        "header_lines": [
          2731
        ]
      },
      "metadata": {
        "label": "proof-lem-outlier-cluster-fraction-lower-bound"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 6,
        "chapter_file": "chapter_6.json",
        "section_id": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "clustering",
      "high-error-set",
      "lower-bound",
      "outlier-fraction",
      "variance",
      "swarm",
      "local-interaction"
    ],
    "content_markdown": ":label: lem-outlier-cluster-fraction-lower-bound\n\nLet the high-error set $H_k(\\varepsilon)$ be defined via the phase-space clustering-based approach (as $C_k(\\varepsilon)$ in {prf:ref}`def-unified-high-low-error-sets`) for the local-interaction regime, with maximum cluster diameter $D_diam(\\varepsilon) = c_d \u00b7 \\varepsilon$ where $c_d > 0$ is a fixed constant.\n\nFor any choice of $c_d$ and variance threshold $R^2_{\\text{var}}$ satisfying $c_d \u00b7 \\epsilon < 2\\sqrt{R^2_{\\text{var}}}$, there exists a positive constant $f_H(\\epsilon) > 0$, independent of `N` and `k`, such that:\n\nIf the swarm's internal positional variance is large, $\\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$, then the fraction of *alive* walkers in the high-error set is bounded below:\n\n$$\n\\frac{|H_k(\\epsilon)|}{k} \\ge f_H(\\epsilon) > 0",
    "raw_directive": "2716: With the clustering-based definition from Section 6.3 established, we can now prove the main result for this regime.\n2717: \n2718: :::{prf:lemma} N-Uniform Lower Bound on the Outlier-Cluster Fraction\n2719: :label: lem-outlier-cluster-fraction-lower-bound\n2720: \n2721: Let the high-error set $H_k(\\varepsilon)$ be defined via the phase-space clustering-based approach (as $C_k(\\varepsilon)$ in {prf:ref}`def-unified-high-low-error-sets`) for the local-interaction regime, with maximum cluster diameter $D_diam(\\varepsilon) = c_d \u00b7 \\varepsilon$ where $c_d > 0$ is a fixed constant.\n2722: \n2723: For any choice of $c_d$ and variance threshold $R^2_{\\text{var}}$ satisfying $c_d \u00b7 \\epsilon < 2\\sqrt{R^2_{\\text{var}}}$, there exists a positive constant $f_H(\\epsilon) > 0$, independent of `N` and `k`, such that:\n2724: \n2725: If the swarm's internal positional variance is large, $\\mathrm{Var}_x(S_k) > R^2_{\\text{var}}$, then the fraction of *alive* walkers in the high-error set is bounded below:\n2726: \n2727: $$\n2728: \\frac{|H_k(\\epsilon)|}{k} \\ge f_H(\\epsilon) > 0\n2729: $$",
    "document_id": "03_cloning",
    "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
    "span": {
      "start_line": 2716,
      "end_line": 2729,
      "content_start": 2719,
      "content_end": 2728,
      "header_lines": [
        2717
      ]
    },
    "references": [
      "def-unified-high-low-error-sets",
      "lem-outlier-fraction-lower-bound"
    ],
    "metadata": {
      "label": "lem-outlier-cluster-fraction-lower-bound"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 6,
      "chapter_file": "chapter_6.json",
      "section_id": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-geometric-separation-of-partition",
    "title": "Geometric Separation of the Partition",
    "type": "lemma",
    "nl_statement": "Under large positional variance Var(x) > R_var\u00b2, the high-error set H_k(\u03b5) and low-error set L_k(\u03b5) for swarm k are geometrically separated in algorithmic phase space by D_H(\u03b5) > R_L(\u03b5) > 0, with low-error walkers forming clusters of size at least f_c k within radius R_L(\u03b5).",
    "equations": [
      {
        "label": null,
        "latex": "d_{\\text{alg}}(i, j) \\ge D_H(\\epsilon)"
      },
      {
        "label": null,
        "latex": "d_{\\text{alg}}(j, \\ell) \\le R_L(\\epsilon)"
      },
      {
        "label": null,
        "latex": "D_H(\\epsilon) > R_L(\\epsilon) > 0"
      },
      {
        "label": null,
        "latex": "\\mathrm{Var}(x) > R^2_{\\mathrm{var}}"
      }
    ],
    "hypotheses": [
      {
        "text": "H_k(\u03b5) and L_k(\u03b5) are the unified high-error and low-error sets for swarm k as defined in def-unified-high-low-error-sets",
        "latex": null
      },
      {
        "text": "The swarm's internal positional variance is large: Var(x) > R_var\u00b2",
        "latex": "\\mathrm{Var}(x) > R^2_{\\mathrm{var}}"
      }
    ],
    "conclusion": {
      "text": "There exist N-uniform, \u03b5-dependent constants D_H(\u03b5) > R_L(\u03b5) > 0 and a fractional constant f_c > 0 such that: (1) for any i \u2208 H_k(\u03b5) and j \u2208 L_k(\u03b5), d_alg(i, j) \u2265 D_H(\u03b5); (2) for any j \u2208 L_k(\u03b5), there exists C_j \u2282 L_k(\u03b5) \\ {j} with |C_j| \u2265 f_c k and d_alg(j, \u2113) \u2264 R_L(\u03b5) for all \u2113 \u2208 C_j. The separation D_H(\u03b5) > R_L(\u03b5) ensures non-overlapping geometric signatures in algorithmic phase space.",
      "latex": null
    },
    "variables": [
      {
        "symbol": "H_k(\\epsilon)",
        "name": "H_k(\u03b5)",
        "description": "Unified high-error set for swarm k",
        "constraints": [
          "subset of walkers"
        ],
        "tags": [
          "set",
          "high-error"
        ]
      },
      {
        "symbol": "L_k(\\epsilon)",
        "name": "L_k(\u03b5)",
        "description": "Unified low-error set for swarm k",
        "constraints": [
          "subset of walkers"
        ],
        "tags": [
          "set",
          "low-error"
        ]
      },
      {
        "symbol": "\\epsilon",
        "name": "\u03b5",
        "description": "Error threshold parameter",
        "constraints": [
          "positive real"
        ],
        "tags": [
          "parameter",
          "error"
        ]
      },
      {
        "symbol": "k",
        "name": "k",
        "description": "Swarm index or size",
        "constraints": [
          "positive integer"
        ],
        "tags": [
          "swarm",
          "index"
        ]
      },
      {
        "symbol": "D_H(\\epsilon)",
        "name": "D_H(\u03b5)",
        "description": "Lower bound constant for separation between high and low-error sets",
        "constraints": [
          "> R_L(\u03b5) > 0",
          "N-uniform, \u03b5-dependent"
        ],
        "tags": [
          "constant",
          "separation"
        ]
      },
      {
        "symbol": "R_L(\\epsilon)",
        "name": "R_L(\u03b5)",
        "description": "Upper bound radius for low-error clustering",
        "constraints": [
          "> 0",
          "N-uniform, \u03b5-dependent"
        ],
        "tags": [
          "constant",
          "radius"
        ]
      },
      {
        "symbol": "f_c",
        "name": "f_c",
        "description": "Fractional constant for minimum cluster size",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "constant",
          "fractional"
        ]
      },
      {
        "symbol": "d_{\\text{alg}}",
        "name": "d_alg",
        "description": "Algorithmic distance metric",
        "constraints": [],
        "tags": [
          "distance",
          "algorithmic"
        ]
      },
      {
        "symbol": "R_{\\mathrm{var}}",
        "name": "R_var",
        "description": "Variance threshold radius",
        "constraints": [],
        "tags": [
          "variance",
          "threshold"
        ]
      },
      {
        "symbol": "C_j",
        "name": "C_j",
        "description": "Companion subset for low-error walker j",
        "constraints": [
          "|C_j| \u2265 f_c k",
          "subset of L_k(\u03b5) \\ {j}"
        ],
        "tags": [
          "set",
          "cluster"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Constants D_H(\u03b5) and R_L(\u03b5) are N-uniform and \u03b5-dependent",
        "confidence": 1.0
      },
      {
        "text": "f_c is a positive fractional constant independent of \u03b5 and N",
        "confidence": 1.0
      },
      {
        "text": "Swarm consists of N walkers with positional variance defined",
        "confidence": 0.9
      },
      {
        "text": "Algorithmic phase space is well-defined for distance metric d_alg",
        "confidence": 1.0
      }
    ],
    "local_refs": [
      "def-unified-high-low-error-sets"
    ],
    "proof": null,
    "tags": [
      "geometric-separation",
      "swarm-partition",
      "high-error",
      "low-error",
      "algorithmic-distance",
      "clustering",
      "variance"
    ],
    "content_markdown": ":label: lem-geometric-separation-of-partition\n\nLet $H_k(\\epsilon)$ and $L_k(\\epsilon)$ be the unified high-error and low-error sets for swarm $k$ as defined in {prf:ref}`def-unified-high-low-error-sets`. Assume the swarm's internal positional variance is large: $\\mathrm{Var}(x) > R^2_{\\mathrm{var}}$.\n\nThen there exist N-uniform, $\\epsilon$-dependent constants $D_H(\\epsilon) > R_L(\\epsilon) > 0$ and a fractional constant $f_c > 0$ such that:\n\n**Part 1 (Separation Between Sets):** For any walker $i \\in H_k(\\epsilon)$ from a high-error cluster and any walker $j \\in L_k(\\epsilon)$ from a low-error cluster, their algorithmic distance is bounded below:\n\n$$\nd_{\\text{alg}}(i, j) \\ge D_H(\\epsilon)\n$$\n\n**Part 2 (Clustering of Low-Error Walkers):** For any walker $j \\in L_k(\\epsilon)$, there exists a non-empty subset of companion walkers $C_j \\subset L_k(\\epsilon) \\setminus \\{j\\}$ of minimum size $|C_j| \\ge f_c k$ such that all members of this cluster are within a small algorithmic radius:\n\n$$\nd_{\\text{alg}}(j, \\ell) \\le R_L(\\epsilon) \\quad \\text{for all } \\ell \\in C_j\n$$\n\nThe separation property $D_H(\\epsilon) > R_L(\\epsilon)$ ensures that the geometric signatures of the two sets are fundamentally distinct and non-overlapping **in the algorithmic phase space**.",
    "raw_directive": "2890: #### 6.5.1. Main Lemma: Statement of Geometric Separation\n2891: \n2892: :::{prf:lemma} Geometric Separation of the Partition\n2893: :label: lem-geometric-separation-of-partition\n2894: \n2895: Let $H_k(\\epsilon)$ and $L_k(\\epsilon)$ be the unified high-error and low-error sets for swarm $k$ as defined in {prf:ref}`def-unified-high-low-error-sets`. Assume the swarm's internal positional variance is large: $\\mathrm{Var}(x) > R^2_{\\mathrm{var}}$.\n2896: \n2897: Then there exist N-uniform, $\\epsilon$-dependent constants $D_H(\\epsilon) > R_L(\\epsilon) > 0$ and a fractional constant $f_c > 0$ such that:\n2898: \n2899: **Part 1 (Separation Between Sets):** For any walker $i \\in H_k(\\epsilon)$ from a high-error cluster and any walker $j \\in L_k(\\epsilon)$ from a low-error cluster, their algorithmic distance is bounded below:\n2900: \n2901: $$\n2902: d_{\\text{alg}}(i, j) \\ge D_H(\\epsilon)\n2903: $$\n2904: \n2905: **Part 2 (Clustering of Low-Error Walkers):** For any walker $j \\in L_k(\\epsilon)$, there exists a non-empty subset of companion walkers $C_j \\subset L_k(\\epsilon) \\setminus \\{j\\}$ of minimum size $|C_j| \\ge f_c k$ such that all members of this cluster are within a small algorithmic radius:\n2906: \n2907: $$\n2908: d_{\\text{alg}}(j, \\ell) \\le R_L(\\epsilon) \\quad \\text{for all } \\ell \\in C_j\n2909: $$\n2910: \n2911: The separation property $D_H(\\epsilon) > R_L(\\epsilon)$ ensures that the geometric signatures of the two sets are fundamentally distinct and non-overlapping **in the algorithmic phase space**.\n2912: ",
    "document_id": "03_cloning",
    "section": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure",
    "span": {
      "start_line": 2890,
      "end_line": 2912,
      "content_start": 2893,
      "content_end": 2911,
      "header_lines": [
        2891
      ]
    },
    "references": [
      "def-unified-high-low-error-sets"
    ],
    "metadata": {
      "label": "lem-geometric-separation-of-partition"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 6,
      "chapter_file": "chapter_6.json",
      "section_id": "## 6. The Geometry of Error: From System Error to a Guaranteed Geometric Structure"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-variance-to-gap",
    "title": "From Bounded Variance to a Guaranteed Gap",
    "type": "lemma",
    "nl_statement": "For a set of at least two real numbers with empirical variance bounded below by a positive constant \u03ba, the maximum absolute difference between any two numbers is at least \u221a(2\u03ba).",
    "equations": [
      {
        "label": null,
        "latex": "\\max_{i,j} |v_i - v_j| \\ge \\sqrt{2\\kappa}"
      }
    ],
    "hypotheses": [],
    "conclusion": {
      "text": "There exists at least one pair of indices $(i, j)$ such that the maximum gap is bounded below: $\\max_{i,j} |v_i - v_j| \\ge \\sqrt{2\\kappa}$",
      "latex": "\\max_{i,j} |v_i - v_j| \\ge \\sqrt{2\\kappa}"
    },
    "variables": [
      {
        "symbol": "k",
        "name": "number of elements",
        "description": "The number of real numbers in the set, an integer at least 2.",
        "constraints": [
          "k \\ge 2"
        ],
        "tags": [
          "integer",
          "size"
        ]
      },
      {
        "symbol": "v_i",
        "name": "set values",
        "description": "The real numbers in the set, indexed from 1 to k.",
        "constraints": [
          "real numbers"
        ],
        "tags": [
          "variables",
          "values"
        ]
      },
      {
        "symbol": "\\kappa",
        "name": "variance lower bound",
        "description": "A strictly positive constant bounding the empirical variance from below.",
        "constraints": [
          "\\kappa > 0"
        ],
        "tags": [
          "positive",
          "bound"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The empirical variance is defined as $\\frac{1}{k} \\sum_{i=1}^k (v_i - \\bar{v})^2$, where $\\bar{v}$ is the mean.",
        "confidence": 1.0
      }
    ],
    "local_refs": [
      "thm-geometry-guarantees-variance"
    ],
    "proof": {
      "label": "proof-lem-variance-to-gap",
      "title": null,
      "type": "proof",
      "proves": "lem-variance-to-gap",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-variance-to-gap\n\n**Proof.**\n\nThe proof relies on a standard identity that relates the empirical variance of a set to the sum of its pairwise squared differences.\n\n**1. The Pairwise Variance Identity.**\nThe empirical variance, $\\text{Var}(\\{v_i\\}) = \\frac{1}{k}\\sum_i v_i^2 - (\\frac{1}{k}\\sum_i v_i)^2$, can be expressed as:\n\n$$\n\\mathrm{Var}(\\{v_i\\}) = \\frac{1}{2k^2} \\sum_{i=1}^k \\sum_{j=1}^k (v_i - v_j)^2\n$$\n\nThis identity is established by expanding the squared term in the double summation.\n\n**2. Bounding the Variance by the Maximum Gap.**\nLet $\\Delta_{\\text{max}} := \\max_{i,j} |v_i - v_j|$. By definition, every term in the summation is bounded above by this maximum: $(v_i - v_j)^2 \\le \\Delta_{\\max}^2$. The double summation contains $k^2$ such terms. We can therefore bound the sum:\n\n$$\n\\sum_{i=1}^k \\sum_{j=1}^k (v_i - v_j)^2 \\le \\sum_{i=1}^k \\sum_{j=1}^k \\Delta_{\\max}^2 = k^2 \\Delta_{\\max}^2\n$$\n\nSubstituting this into the identity from Step 1 gives an upper bound on the variance in terms of the maximum gap:\n\n$$\n\\mathrm{Var}(\\{v_i\\}) \\le \\frac{1}{2k^2} (k^2 \\Delta_{\\max}^2) = \\frac{1}{2} \\Delta_{\\max}^2\n$$\n\n**3. Final Derivation.**\nWe are given the premise that $\\mathrm{Var}(\\{v_i\\}) \\geq \\kappa$. Combining this with the result from Step 2:\n\n$$\n\\kappa \\le \\mathrm{Var}(\\{v_i\\}) \\le \\frac{1}{2} \\Delta_{\\max}^2\n$$\n\nRearranging the inequality $\\kappa \\le \\frac{1}{2} \\Delta_{\\max}^2$ gives $\\Delta_{\\max}^2 \\ge 2\\kappa$. Taking the square root of both sides yields the desired result.",
      "raw_directive": "3603: \n3604: :::\n3605: :::{prf:proof}\n3606: :label: proof-lem-variance-to-gap\n3607: \n3608: **Proof.**\n3609: \n3610: The proof relies on a standard identity that relates the empirical variance of a set to the sum of its pairwise squared differences.\n3611: \n3612: **1. The Pairwise Variance Identity.**\n3613: The empirical variance, $\\text{Var}(\\{v_i\\}) = \\frac{1}{k}\\sum_i v_i^2 - (\\frac{1}{k}\\sum_i v_i)^2$, can be expressed as:\n3614: \n3615: $$\n3616: \\mathrm{Var}(\\{v_i\\}) = \\frac{1}{2k^2} \\sum_{i=1}^k \\sum_{j=1}^k (v_i - v_j)^2\n3617: $$\n3618: \n3619: This identity is established by expanding the squared term in the double summation.\n3620: \n3621: **2. Bounding the Variance by the Maximum Gap.**\n3622: Let $\\Delta_{\\text{max}} := \\max_{i,j} |v_i - v_j|$. By definition, every term in the summation is bounded above by this maximum: $(v_i - v_j)^2 \\le \\Delta_{\\max}^2$. The double summation contains $k^2$ such terms. We can therefore bound the sum:\n3623: \n3624: $$\n3625: \\sum_{i=1}^k \\sum_{j=1}^k (v_i - v_j)^2 \\le \\sum_{i=1}^k \\sum_{j=1}^k \\Delta_{\\max}^2 = k^2 \\Delta_{\\max}^2\n3626: $$\n3627: \n3628: Substituting this into the identity from Step 1 gives an upper bound on the variance in terms of the maximum gap:\n3629: \n3630: $$\n3631: \\mathrm{Var}(\\{v_i\\}) \\le \\frac{1}{2k^2} (k^2 \\Delta_{\\max}^2) = \\frac{1}{2} \\Delta_{\\max}^2\n3632: $$\n3633: \n3634: **3. Final Derivation.**\n3635: We are given the premise that $\\mathrm{Var}(\\{v_i\\}) \\geq \\kappa$. Combining this with the result from Step 2:\n3636: \n3637: $$\n3638: \\kappa \\le \\mathrm{Var}(\\{v_i\\}) \\le \\frac{1}{2} \\Delta_{\\max}^2\n3639: $$\n3640: \n3641: Rearranging the inequality $\\kappa \\le \\frac{1}{2} \\Delta_{\\max}^2$ gives $\\Delta_{\\max}^2 \\ge 2\\kappa$. Taking the square root of both sides yields the desired result.\n3642: ",
      "strategy_summary": "The proof establishes an identity linking empirical variance to the average of squared pairwise differences, bounds this sum using the maximum pairwise gap, and combines the bound with the given lower bound on variance to derive the inequality relating the maximum gap to the variance threshold.",
      "conclusion": {
        "text": "\u0394_max \u2265 \u221a(2\u03ba)",
        "latex": "\\Delta_{\\max} \\geq \\sqrt{2\\kappa}"
      },
      "assumptions": [
        {
          "text": "Var({v_i}) \u2265 \u03ba for a set {v_i} of k real numbers",
          "latex": "\\mathrm{Var}(\\{v_i\\}) \\geq \\kappa"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "identity",
          "text": "The empirical variance Var({v_i}) = (1/k)\u2211_i v_i\u00b2 - ((1/k)\u2211_i v_i)\u00b2 can be expressed as Var({v_i}) = (1/(2k\u00b2)) \u2211_{i=1}^k \u2211_{j=1}^k (v_i - v_j)\u00b2. This identity is established by expanding the squared term in the double summation.",
          "latex": "\\mathrm{Var}(\\{v_i\\}) = \\frac{1}{2k^2} \\sum_{i=1}^k \\sum_{j=1}^k (v_i - v_j)^2",
          "references": [],
          "derived_statement": "Pairwise variance identity"
        },
        {
          "order": 2.0,
          "kind": "bounding",
          "text": "Let \u0394_max := max_{i,j} |v_i - v_j|. Each (v_i - v_j)\u00b2 \u2264 \u0394_max\u00b2, so the double sum \u2264 k\u00b2 \u0394_max\u00b2, yielding Var({v_i}) \u2264 (1/2) \u0394_max\u00b2.",
          "latex": "\\sum_{i=1}^k \\sum_{j=1}^k (v_i - v_j)^2 \\le k^2 \\Delta_{\\max}^2 \\implies \\mathrm{Var}(\\{v_i\\}) \\le \\frac{1}{2} \\Delta_{\\max}^2",
          "references": [],
          "derived_statement": "Variance upper bound in terms of maximum gap"
        },
        {
          "order": 3.0,
          "kind": "derivation",
          "text": "Given Var({v_i}) \u2265 \u03ba, combine with the upper bound: \u03ba \u2264 Var({v_i}) \u2264 (1/2) \u0394_max\u00b2, so \u0394_max\u00b2 \u2265 2\u03ba, and thus \u0394_max \u2265 \u221a(2\u03ba).",
          "latex": "\\kappa \\le \\frac{1}{2} \\Delta_{\\max}^2 \\implies \\Delta_{\\max}^2 \\ge 2\\kappa \\implies \\Delta_{\\max} \\ge \\sqrt{2\\kappa}",
          "references": [],
          "derived_statement": "Desired inequality"
        }
      ],
      "key_equations": [
        {
          "label": "eq-var-empirical",
          "latex": "\\mathrm{Var}(\\{v_i\\}) = \\frac{1}{k}\\sum_i v_i^2 - \\left(\\frac{1}{k}\\sum_i v_i\\right)^2",
          "role": "Standard empirical variance formula"
        },
        {
          "label": "eq-var-pairwise",
          "latex": "\\mathrm{Var}(\\{v_i\\}) = \\frac{1}{2k^2} \\sum_{i=1}^k \\sum_{j=1}^k (v_i - v_j)^2",
          "role": "Pairwise identity for variance"
        },
        {
          "label": "eq-bound-sum",
          "latex": "\\sum_{i=1}^k \\sum_{j=1}^k (v_i - v_j)^2 \\le k^2 \\Delta_{\\max}^2",
          "role": "Bounding the double sum"
        },
        {
          "label": "eq-var-bound",
          "latex": "\\mathrm{Var}(\\{v_i\\}) \\le \\frac{1}{2} \\Delta_{\\max}^2",
          "role": "Upper bound on variance"
        },
        {
          "label": "eq-final-ineq",
          "latex": "\\Delta_{\\max}^2 \\ge 2\\kappa",
          "role": "Key inequality before taking square root"
        }
      ],
      "references": [],
      "math_tools": [
        {
          "toolName": "Empirical Variance",
          "field": "Statistics",
          "description": "The variance of a finite sample, computed as the average squared deviation from the mean.",
          "roleInProof": "Serves as the starting point for relating variance to pairwise differences and applying the maximum gap bound.",
          "levelOfAbstraction": "Concept",
          "relatedTools": []
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "note",
          "text": "The pairwise variance identity is a standard result obtained by algebraic expansion."
        }
      ],
      "gaps": [],
      "tags": [
        "variance",
        "empirical variance",
        "pairwise differences",
        "bounding",
        "inequality",
        "gap"
      ],
      "document_id": "03_cloning",
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "span": {
        "start_line": 3603,
        "end_line": 3642,
        "content_start": 3606,
        "content_end": 3641,
        "header_lines": [
          3604
        ]
      },
      "metadata": {
        "label": "proof-lem-variance-to-gap"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "variance",
      "gap",
      "separation",
      "bounded",
      "real numbers",
      "empirical"
    ],
    "content_markdown": ":label: lem-variance-to-gap\n\nLet $\\{v_i\\}_{i=1}^k$ be a set of $k \\ge 2$ real numbers. If the empirical variance of this set is bounded below by a strictly positive constant, $\\text{Var}(\\{v_i\\}) \\geq \\kappa > 0$, then there must exist at least one pair of indices $(i, j)$ such that the gap between their values is bounded below:\n\n$$\n\\max_{i,j} |v_i - v_j| \\ge \\sqrt{2\\kappa}",
    "raw_directive": "3593: The first step in the signal integrity proof is to show that the statistical property of variance, now proven in [](#thm-geometry-guarantees-variance), has a direct, concrete consequence: it forces a measurable separation between the raw values of at least two walkers.\n3594: \n3595: :::{prf:lemma} From Bounded Variance to a Guaranteed Gap\n3596: :label: lem-variance-to-gap\n3597: \n3598: Let $\\{v_i\\}_{i=1}^k$ be a set of $k \\ge 2$ real numbers. If the empirical variance of this set is bounded below by a strictly positive constant, $\\text{Var}(\\{v_i\\}) \\geq \\kappa > 0$, then there must exist at least one pair of indices $(i, j)$ such that the gap between their values is bounded below:\n3599: \n3600: $$\n3601: \\max_{i,j} |v_i - v_j| \\ge \\sqrt{2\\kappa}\n3602: $$",
    "document_id": "03_cloning",
    "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
    "span": {
      "start_line": 3593,
      "end_line": 3602,
      "content_start": 3596,
      "content_end": 3601,
      "header_lines": [
        3594
      ]
    },
    "references": [],
    "metadata": {
      "label": "lem-variance-to-gap"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 7,
      "chapter_file": "chapter_7.json",
      "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-rescale-derivative-lower-bound",
    "title": "Positive Derivative Bound for the Rescale Function",
    "type": "lemma",
    "nl_statement": "The first derivative of the canonical logistic rescale function g_A(z) is uniformly bounded below by a strictly positive constant g'_min over the operational range Z_supp.",
    "equations": [
      {
        "label": null,
        "latex": "\\inf_{z \\in Z_{\\mathrm{supp}}} g'_A(z) = g'_{\\min} > 0"
      }
    ],
    "hypotheses": [],
    "conclusion": {
      "text": "there exists a constant g'_{\\min} > 0 such that \\inf_{z \\in Z_{\\mathrm{supp}}} g'_A(z) = g'_{\\min} > 0",
      "latex": "\\inf_{z \\in Z_{\\mathrm{supp}}} g'_A(z) = g'_{\\min} > 0"
    },
    "variables": [
      {
        "symbol": "g'_A",
        "name": "first derivative of rescale function",
        "description": "derivative of the canonical logistic rescale function g_A",
        "constraints": [],
        "tags": [
          "derivative",
          "rescale"
        ]
      },
      {
        "symbol": "z",
        "name": "z-score",
        "description": "input variable in the operational range",
        "constraints": [
          "z \\in Z_{\\mathrm{supp}}"
        ],
        "tags": [
          "z-score",
          "input"
        ]
      },
      {
        "symbol": "Z_{\\mathrm{supp}}",
        "name": "operational range",
        "description": "domain of z-scores where g_A operates",
        "constraints": [],
        "tags": [
          "domain",
          "support"
        ]
      },
      {
        "symbol": "g'_{\\min}",
        "name": "minimum derivative bound",
        "description": "strictly positive lower bound for g'_A(z)",
        "constraints": [
          "g'_{\\min} > 0"
        ],
        "tags": [
          "bound",
          "positive"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The canonical logistic rescale function g_A is differentiable on Z_supp",
        "confidence": 1.0
      },
      {
        "text": "Z_supp is a non-empty interval where g_A is defined and operational",
        "confidence": 1.0
      },
      {
        "text": "g_A is the canonical logistic function used for rescaling",
        "confidence": 1.0
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-rescale-derivative-lower-bound",
      "title": null,
      "type": "proof",
      "proves": "lem-rescale-derivative-lower-bound",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-rescale-derivative-lower-bound\n\n**Proof.**\n1.  **Compactness of the Domain:** Any standardized score `z\u1d62` must lie within the interval `Z_supp` (from {prf:ref}`lem-compact-support-z-scores`). This interval is defined by the uniform constants `V_max` and $\\sigma'_min,patch$, making `Z_supp` a compact set that is independent of the swarm state.\n\n2.  **Properties of the Derivative:** The Canonical Logistic Rescale function (see {prf:ref}`def-logistic-rescale`) is $g_A(z) = 2 / (1 + e^{-z})$. Its derivative, $g'_A(z) = 2e^{-z} / (1+e^{-z})^2$, is continuous and strictly positive for all $z \\in \\mathbb{R}$.\n\n3.  **Application of the Extreme Value Theorem:** By the Extreme Value Theorem, a continuous function ($g'_A(z)$) must attain its minimum value on a compact set ($Z_{\\text{supp}}$).\n\n4.  **Conclusion:** Since `g'_A(z)` is strictly positive on its entire domain, its minimum value on the compact subset `Z_supp`, which we define as `g'_min`, must also be a strictly positive constant.",
      "raw_directive": "3681: where $Z_{\\text{supp}} := \\left[ -2V_{\\max}/\\sigma'_{\\min,\\text{patch}}, 2V_{\\max}/\\sigma'_{\\min,\\text{patch}} \\right]$ is the compact support of all possible standardized scores.\n3682: :::\n3683: :::{prf:proof}\n3684: :label: proof-lem-rescale-derivative-lower-bound\n3685: \n3686: **Proof.**\n3687: 1.  **Compactness of the Domain:** Any standardized score `z\u1d62` must lie within the interval `Z_supp` (from {prf:ref}`lem-compact-support-z-scores`). This interval is defined by the uniform constants `V_max` and $\\sigma'_min,patch$, making `Z_supp` a compact set that is independent of the swarm state.\n3688: \n3689: 2.  **Properties of the Derivative:** The Canonical Logistic Rescale function (see {prf:ref}`def-logistic-rescale`) is $g_A(z) = 2 / (1 + e^{-z})$. Its derivative, $g'_A(z) = 2e^{-z} / (1+e^{-z})^2$, is continuous and strictly positive for all $z \\in \\mathbb{R}$.\n3690: \n3691: 3.  **Application of the Extreme Value Theorem:** By the Extreme Value Theorem, a continuous function ($g'_A(z)$) must attain its minimum value on a compact set ($Z_{\\text{supp}}$).\n3692: \n3693: 4.  **Conclusion:** Since `g'_A(z)` is strictly positive on its entire domain, its minimum value on the compact subset `Z_supp`, which we define as `g'_min`, must also be a strictly positive constant.\n3694: ",
      "strategy_summary": "The proof demonstrates a uniform positive lower bound on the derivative of the canonical logistic rescaling function by establishing the compactness of the domain of standardized scores and applying the Extreme Value Theorem to the continuous and strictly positive derivative.",
      "conclusion": {
        "text": "The minimum value of g'_A(z) on Z_supp, denoted g'_min, is a strictly positive constant.",
        "latex": null
      },
      "assumptions": [],
      "steps": [],
      "key_equations": [
        {
          "label": "g_A",
          "latex": "g_A(z) = \\frac{2}{1 + e^{-z}}",
          "role": "canonical logistic rescaling function"
        },
        {
          "label": "g'_A",
          "latex": "g'_A(z) = \\frac{2 e^{-z}}{(1 + e^{-z})^2}",
          "role": "derivative of the rescaling function"
        },
        {
          "label": "Z_supp",
          "latex": "Z_{\\text{supp}} = \\left[ -\\frac{2 V_{\\max}}{\\sigma'_{\\min,\\text{patch}}}, \\frac{2 V_{\\max}}{\\sigma'_{\\min,\\text{patch}}} \\right]",
          "role": "compact support interval for standardized scores"
        }
      ],
      "references": [
        "lem-compact-support-z-scores",
        "def-logistic-rescale"
      ],
      "math_tools": [
        {
          "toolName": "Extreme Value Theorem",
          "field": "Real Analysis",
          "description": "A continuous real-valued function on a compact set in Euclidean space attains its maximum and minimum values.",
          "roleInProof": "Used to ensure that the continuous derivative g'_A attains a minimum on the compact interval Z_supp.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": []
        }
      ],
      "cases": [],
      "remarks": [],
      "gaps": [],
      "tags": [
        "compactness",
        "extreme value theorem",
        "logistic function",
        "derivative",
        "lower bound",
        "continuity",
        "real analysis"
      ],
      "document_id": "03_cloning",
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "span": {
        "start_line": 3681,
        "end_line": 3694,
        "content_start": 3684,
        "content_end": 3693,
        "header_lines": [
          3682
        ]
      },
      "metadata": {
        "label": "proof-lem-rescale-derivative-lower-bound"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "rescale",
      "derivative",
      "lower bound",
      "logistic",
      "canonical",
      "infimum",
      "z-score",
      "sensitivity"
    ],
    "content_markdown": ":label: lem-rescale-derivative-lower-bound\n\nFor the Canonical Logistic Rescale function (see {prf:ref}`def-logistic-rescale`), the first derivative $g'_A(z)$ is uniformly bounded below by a strictly positive constant for all z-scores in the operational range $Z_{\\text{supp}}$. That is, there exists a constant $g'_{\\min} > 0$ such that:\n\n$$\n\\inf_{z \\in Z_{\\mathrm{supp}}} g'_A(z) = g'_{\\min} > 0\n$$",
    "raw_directive": "3670: Next, we must prove that the rescale function `g_A` is sufficiently sensitive to preserve a standardized gap. This requires showing that its derivative is uniformly bounded below by a positive constant over its entire operational domain.\n3671: \n3672: :::{prf:lemma} Positive Derivative Bound for the Rescale Function\n3673: :label: lem-rescale-derivative-lower-bound\n3674: \n3675: For the Canonical Logistic Rescale function (see {prf:ref}`def-logistic-rescale`), the first derivative $g'_A(z)$ is uniformly bounded below by a strictly positive constant for all z-scores in the operational range $Z_{\\text{supp}}$. That is, there exists a constant $g'_{\\min} > 0$ such that:\n3676: \n3677: $$\n3678: \\inf_{z \\in Z_{\\mathrm{supp}}} g'_A(z) = g'_{\\min} > 0\n3679: $$\n3680: ",
    "document_id": "03_cloning",
    "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
    "span": {
      "start_line": 3670,
      "end_line": 3680,
      "content_start": 3673,
      "content_end": 3679,
      "header_lines": [
        3671
      ]
    },
    "references": [
      "def-logistic-rescale",
      "lem-compact-support-z-scores"
    ],
    "metadata": {
      "label": "lem-rescale-derivative-lower-bound"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 7,
      "chapter_file": "chapter_7.json",
      "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-raw-gap-to-rescaled-gap",
    "title": "From Raw Measurement Gap to Rescaled Value Gap",
    "type": "lemma",
    "nl_statement": "For fixed system parameters and any swarm state S with k \u2265 2 alive walkers, if raw measurement values have a gap |v_a - v_b| \u2265 \u03ba_raw > 0, then rescaled values have |g_A(z_a) - g_A(z_b)| \u2265 \u03ba_rescaled(\u03ba_raw) > 0, where \u03ba_rescaled(\u03ba_raw) = (g'_min / \u03c3'_max) \u22c5 \u03ba_raw is independent of S and k.",
    "equations": [
      {
        "label": null,
        "latex": "|g_A(z_a) - g_A(z_b)| \\ge \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}}) > 0"
      },
      {
        "label": null,
        "latex": "\\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}}) := \\frac{g'_{\\min}}{\\sigma'_{\\max}} \\cdot \\kappa_{\\mathrm{raw}}"
      }
    ],
    "hypotheses": [
      {
        "text": "System parameters are fixed.",
        "latex": null
      },
      {
        "text": "Swarm state S has k \u2265 2 alive walkers.",
        "latex": null
      },
      {
        "text": "Raw measurement gap |v_a - v_b| \u2265 \u03ba_raw > 0.",
        "latex": "|v_a - v_b| \\ge \\kappa_{\\mathrm{raw}} > 0"
      }
    ],
    "conclusion": {
      "text": "Rescaled value gap |g_A(z_a) - g_A(z_b)| \u2265 \u03ba_rescaled(\u03ba_raw) > 0 with \u03ba_rescaled independent of S and k.",
      "latex": "|g_A(z_a) - g_A(z_b)| \\ge \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}}) > 0"
    },
    "variables": [
      {
        "symbol": "\\kappa_{\\mathrm{raw}}",
        "name": "raw gap threshold",
        "description": "Positive threshold for raw measurement gap.",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "threshold",
          "raw"
        ]
      },
      {
        "symbol": "\\kappa_{\\mathrm{rescaled}}",
        "name": "rescaled gap function",
        "description": "Function mapping raw gap to guaranteed rescaled gap.",
        "constraints": [],
        "tags": [
          "function",
          "rescaled"
        ]
      },
      {
        "symbol": "k",
        "name": "number of alive walkers",
        "description": "Size of swarm state with alive agents.",
        "constraints": [
          ">= 2"
        ],
        "tags": [
          "swarm",
          "size"
        ]
      },
      {
        "symbol": "S",
        "name": "swarm state",
        "description": "State of the walker swarm.",
        "constraints": [],
        "tags": [
          "swarm",
          "state"
        ]
      },
      {
        "symbol": "v_a, v_b",
        "name": "raw measurements",
        "description": "Raw measurement values for agents a and b.",
        "constraints": [],
        "tags": [
          "raw",
          "value"
        ]
      },
      {
        "symbol": "z_a, z_b",
        "name": "rescaled positions",
        "description": "Rescaled positions corresponding to measurements.",
        "constraints": [],
        "tags": [
          "rescaled",
          "position"
        ]
      },
      {
        "symbol": "g_A",
        "name": "rescaling function",
        "description": "Function g_A applied to rescaled positions.",
        "constraints": [],
        "tags": [
          "rescaling",
          "function"
        ]
      },
      {
        "symbol": "g'_{\\min}",
        "name": "minimum derivative of g",
        "description": "Uniform lower bound on derivative of rescaling function.",
        "constraints": [],
        "tags": [
          "derivative",
          "min bound"
        ]
      },
      {
        "symbol": "\\sigma'_{\\max}",
        "name": "maximum derivative of sigma",
        "description": "Uniform upper bound on derivative of sigma function.",
        "constraints": [],
        "tags": [
          "derivative",
          "max bound"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Uniform bounds g'_min and \u03c3'_max on pipeline components exist and are positive.",
        "confidence": 1.0
      },
      {
        "text": "Rescaling pipeline transforms raw measurements v to rescaled positions z via invertible functions.",
        "confidence": 0.9
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-raw-gap-to-rescaled-gap",
      "title": null,
      "type": "proof",
      "proves": "lem-raw-gap-to-rescaled-gap",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-raw-gap-to-rescaled-gap\n\n**Proof.**\n\nThe proof follows the signal gap as it propagates through the two main steps of the pipeline.\n\n**Stage 1: From Raw Value Gap to a Uniform Lower Bound on the Z-Score Gap**\nWe seek a uniform lower bound for the gap between standardized scores, `|z\u2090 - z\u1d66|`.\n\n$$\n|z_a - z_b| = \\left| \\frac{v_a - \\mu}{\\sigma'} - \\frac{v_b - \\mu}{\\sigma'} \\right| = \\frac{|v_a - v_b|}{\\sigma'}\n$$\n\nWe are given the premise that the numerator is bounded below by $\\kappa_raw$. The denominator $\\sigma'$ is the patched standard deviation (see {prf:ref}`def-patched-std-dev-function`) of the full set of `k` raw values. By Definition {prf:ref}`def-max-patched-std`, $\\sigma'$ is uniformly bounded above by the state-independent constant $\\sigma'_max$. Combining these gives a uniform lower bound on the z-score gap:\n\n$$\n|z_a - z_b| \\ge \\frac{\\kappa_{\\mathrm{raw}}}{\\sigma'_{\\max}} =: \\kappa_z > 0\n$$\n\n**Stage 2: From Z-Score Gap to Rescaled Value Gap**\nThe rescale function `g_A(z)` is continuously differentiable. By the Mean Value Theorem, there exists a point `c` on the line segment between `z\u2090` and `z\u1d66` such that:\n\n$$\n|g_A(z_a) - g_A(z_b)| = |g'_A(c)| \\cdot |z_a - z_b|\n$$\n\nThe points `z\u2090`, `z\u1d66`, and `c` are all within the compact operational range `Z_supp`. By Lemma {prf:ref}`lem-rescale-derivative-lower-bound`, the derivative at `c` is uniformly bounded below by the positive constant `g'_min`. Substituting the lower bounds for both terms on the right-hand side gives:\n\n$$\n|g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\kappa_z\n$$\n\n**Conclusion**\nSubstituting the definition of $\\kappa_z$ from Stage 1 yields the final result:\n\n$$\n|g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\left(\\frac{\\kappa_{\\mathrm{raw}}}{\\sigma'_{\\max}}\\right) = \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}})\n$$\n\nSince `g'_min` and $\\sigma'_max$ are positive, N-uniform constants, the function $\\kappa_rescaled(\\kappa_raw)$ provides a strictly positive, N-uniform lower bound for any $\\kappa_raw > 0$. This completes the proof that a raw measurement gap robustly propagates to a guaranteed rescaled value gap.",
      "raw_directive": "3716: \n3717: :::\n3718: :::{prf:proof}\n3719: :label: proof-lem-raw-gap-to-rescaled-gap\n3720: \n3721: **Proof.**\n3722: \n3723: The proof follows the signal gap as it propagates through the two main steps of the pipeline.\n3724: \n3725: **Stage 1: From Raw Value Gap to a Uniform Lower Bound on the Z-Score Gap**\n3726: We seek a uniform lower bound for the gap between standardized scores, `|z\u2090 - z\u1d66|`.\n3727: \n3728: $$\n3729: |z_a - z_b| = \\left| \\frac{v_a - \\mu}{\\sigma'} - \\frac{v_b - \\mu}{\\sigma'} \\right| = \\frac{|v_a - v_b|}{\\sigma'}\n3730: $$\n3731: \n3732: We are given the premise that the numerator is bounded below by $\\kappa_raw$. The denominator $\\sigma'$ is the patched standard deviation (see {prf:ref}`def-patched-std-dev-function`) of the full set of `k` raw values. By Definition {prf:ref}`def-max-patched-std`, $\\sigma'$ is uniformly bounded above by the state-independent constant $\\sigma'_max$. Combining these gives a uniform lower bound on the z-score gap:\n3733: \n3734: $$\n3735: |z_a - z_b| \\ge \\frac{\\kappa_{\\mathrm{raw}}}{\\sigma'_{\\max}} =: \\kappa_z > 0\n3736: $$\n3737: \n3738: **Stage 2: From Z-Score Gap to Rescaled Value Gap**\n3739: The rescale function `g_A(z)` is continuously differentiable. By the Mean Value Theorem, there exists a point `c` on the line segment between `z\u2090` and `z\u1d66` such that:\n3740: \n3741: $$\n3742: |g_A(z_a) - g_A(z_b)| = |g'_A(c)| \\cdot |z_a - z_b|\n3743: $$\n3744: \n3745: The points `z\u2090`, `z\u1d66`, and `c` are all within the compact operational range `Z_supp`. By Lemma {prf:ref}`lem-rescale-derivative-lower-bound`, the derivative at `c` is uniformly bounded below by the positive constant `g'_min`. Substituting the lower bounds for both terms on the right-hand side gives:\n3746: \n3747: $$\n3748: |g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\kappa_z\n3749: $$\n3750: \n3751: **Conclusion**\n3752: Substituting the definition of $\\kappa_z$ from Stage 1 yields the final result:\n3753: \n3754: $$\n3755: |g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\left(\\frac{\\kappa_{\\mathrm{raw}}}{\\sigma'_{\\max}}\\right) = \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}})\n3756: $$\n3757: \n3758: Since `g'_min` and $\\sigma'_max$ are positive, N-uniform constants, the function $\\kappa_rescaled(\\kappa_raw)$ provides a strictly positive, N-uniform lower bound for any $\\kappa_raw > 0$. This completes the proof that a raw measurement gap robustly propagates to a guaranteed rescaled value gap.\n3759: ",
      "strategy_summary": "The proof establishes a uniform lower bound on the z-score gap from the raw value gap using the bounded patched standard deviation, then propagates this to the rescaled gap via the Mean Value Theorem and a uniform lower bound on the rescaling function's derivative over its compact support.",
      "conclusion": {
        "text": "|g_A(z_a) - g_A(z_b)| \u2265 \u03ba_rescaled(\u03ba_raw), where \u03ba_rescaled(\u03ba_raw) = g'_min \u00b7 (\u03ba_raw / \u03c3'_max) provides a strictly positive, N-uniform lower bound for any \u03ba_raw > 0.",
        "latex": "|g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\left(\\frac{\\kappa_{\\mathrm{raw}}}{\\sigma'_{\\max}}\\right) = \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}})"
      },
      "assumptions": [
        {
          "text": "The raw value gap |v_a - v_b| is bounded below by \u03ba_raw > 0.",
          "latex": null
        },
        {
          "text": "The patched standard deviation \u03c3' is bounded above by the constant \u03c3'_max.",
          "latex": null
        },
        {
          "text": "The rescaling function g_A is continuously differentiable with derivative bounded below by g'_min > 0 on the compact operational range Z_supp.",
          "latex": null
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "derivation",
          "text": "Express the z-score gap in terms of the raw value gap and patched standard deviation.",
          "latex": "|z_a - z_b| = \\frac{|v_a - v_b|}{\\sigma'}",
          "references": [],
          "derived_statement": "|z_a - z_b| = \\frac{|v_a - v_b|}{\\sigma'}"
        },
        {
          "order": 2.0,
          "kind": "bounding",
          "text": "Apply the lower bound on the numerator and upper bound on the denominator to obtain a uniform lower bound \u03ba_z on the z-score gap.",
          "latex": "|z_a - z_b| \\ge \\frac{\\kappa_{\\mathrm{raw}}}{\\sigma'_{\\max}} =: \\kappa_z > 0",
          "references": [
            "def-max-patched-std"
          ],
          "derived_statement": "|z_a - z_b| \\ge \\kappa_z"
        },
        {
          "order": 3.0,
          "kind": "application",
          "text": "Apply the Mean Value Theorem to the rescaling function g_A between z_a and z_b, introducing an intermediate point c.",
          "latex": "|g_A(z_a) - g_A(z_b)| = |g'_A(c)| \\cdot |z_a - z_b|",
          "references": [],
          "derived_statement": "|g_A(z_a) - g_A(z_b)| = |g'_A(c)| \\cdot |z_a - z_b|"
        },
        {
          "order": 4.0,
          "kind": "bounding",
          "text": "Use the uniform lower bound on the derivative g'_A(c) \u2265 g'_min > 0 since c is in the compact Z_supp, combined with the z-score gap bound.",
          "latex": "|g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\kappa_z",
          "references": [
            "lem-rescale-derivative-lower-bound"
          ],
          "derived_statement": "|g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\kappa_z"
        },
        {
          "order": 5.0,
          "kind": "substitution",
          "text": "Substitute \u03ba_z to obtain the final rescaled gap bound in terms of \u03ba_raw.",
          "latex": "|g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\left(\\frac{\\kappa_{\\mathrm{raw}}}{\\sigma'_{\\max}}\\right) = \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}})",
          "references": [],
          "derived_statement": "|g_A(z_a) - g_A(z_b)| \\ge \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}})"
        },
        {
          "order": 6.0,
          "kind": "conclusion",
          "text": "Since g'_min and \u03c3'_max are positive N-uniform constants, the bound is strictly positive and uniform for any \u03ba_raw > 0.",
          "latex": null,
          "references": [],
          "derived_statement": "Raw gap propagates to guaranteed rescaled gap."
        }
      ],
      "key_equations": [
        {
          "label": "eq-zscore-gap",
          "latex": "|z_a - z_b| = \\frac{|v_a - v_b|}{\\sigma'}",
          "role": "Expresses z-score difference in terms of raw values"
        },
        {
          "label": "eq-kappa-z",
          "latex": "|z_a - z_b| \\ge \\frac{\\kappa_{\\mathrm{raw}}}{\\sigma'_{\\max}} =: \\kappa_z > 0",
          "role": "Uniform lower bound on z-score gap"
        },
        {
          "label": "eq-mvt-rescale",
          "latex": "|g_A(z_a) - g_A(z_b)| = |g'_A(c)| \\cdot |z_a - z_b|",
          "role": "Mean Value Theorem application"
        },
        {
          "label": "eq-rescaled-bound",
          "latex": "|g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\kappa_z",
          "role": "Intermediate rescaled gap bound"
        },
        {
          "label": "eq-final-rescaled",
          "latex": "|g_A(z_a) - g_A(z_b)| \\ge g'_{\\min} \\cdot \\left(\\frac{\\kappa_{\\mathrm{raw}}}{\\sigma'_{\\max}}\\right) = \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}})",
          "role": "Final propagated gap bound"
        }
      ],
      "references": [
        "def-patched-std-dev-function",
        "def-max-patched-std",
        "lem-rescale-derivative-lower-bound"
      ],
      "math_tools": [
        {
          "toolName": "Mean Value Theorem",
          "field": "Calculus",
          "description": "If a function is continuous on the closed interval [a, b] and differentiable on the open interval (a, b), then there exists at least one point c in (a, b) such that f'(c) = (f(b) - f(a)) / (b - a).",
          "roleInProof": "Applied to the continuously differentiable rescaling function g_A on the compact interval between z_a and z_b to bound the difference |g_A(z_a) - g_A(z_b)| by the absolute value of the derivative at some intermediate point times |z_a - z_b|.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": []
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "concluding",
          "text": "This completes the proof that a raw measurement gap robustly propagates to a guaranteed rescaled value gap."
        }
      ],
      "gaps": [],
      "tags": [
        "signal gap",
        "z-score standardization",
        "rescaling",
        "mean value theorem",
        "uniform bound",
        "derivative lower bound",
        "patched standard deviation"
      ],
      "document_id": "03_cloning",
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "span": {
        "start_line": 3716,
        "end_line": 3759,
        "content_start": 3719,
        "content_end": 3758,
        "header_lines": [
          3717
        ]
      },
      "metadata": {
        "label": "proof-lem-raw-gap-to-rescaled-gap"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "swarm",
      "measurement",
      "gap",
      "rescaling",
      "uniform bounds",
      "kappa"
    ],
    "content_markdown": ":label: lem-raw-gap-to-rescaled-gap\n\nLet the system parameters be fixed. There exists a function $\\kappa_rescaled(\\kappa_raw)$ such that for *any* swarm state `S` with $k \\geq 2$ alive walkers, if the raw measurement values contain a gap $|v\u2090 - v\u1d66| \\geq \\kappa_raw > 0$, then the corresponding rescaled values are guaranteed to have a gap:\n\n$$\n|g_A(z_a) - g_A(z_b)| \\ge \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}}) > 0\n$$\n\nThe function $\\kappa_rescaled$ is independent of the swarm state `S` and its size `k`, and is defined as:\n\n$$\n\\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}}) := \\frac{g'_{\\min}}{\\sigma'_{\\max}} \\cdot \\kappa_{\\mathrm{raw}}",
    "raw_directive": "3700: With the uniform bounds on the pipeline's components now established, we can prove the main result of this section: a guaranteed raw measurement gap is reliably transformed into a guaranteed rescaled value gap.\n3701: \n3702: :::{prf:lemma} From Raw Measurement Gap to Rescaled Value Gap\n3703: :label: lem-raw-gap-to-rescaled-gap\n3704: \n3705: Let the system parameters be fixed. There exists a function $\\kappa_rescaled(\\kappa_raw)$ such that for *any* swarm state `S` with $k \\geq 2$ alive walkers, if the raw measurement values contain a gap $|v\u2090 - v\u1d66| \\geq \\kappa_raw > 0$, then the corresponding rescaled values are guaranteed to have a gap:\n3706: \n3707: $$\n3708: |g_A(z_a) - g_A(z_b)| \\ge \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}}) > 0\n3709: $$\n3710: \n3711: The function $\\kappa_rescaled$ is independent of the swarm state `S` and its size `k`, and is defined as:\n3712: \n3713: $$\n3714: \\kappa_{\\mathrm{rescaled}}(\\kappa_{\\mathrm{raw}}) := \\frac{g'_{\\min}}{\\sigma'_{\\max}} \\cdot \\kappa_{\\mathrm{raw}}\n3715: $$",
    "document_id": "03_cloning",
    "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
    "span": {
      "start_line": 3700,
      "end_line": 3715,
      "content_start": 3703,
      "content_end": 3714,
      "header_lines": [
        3701
      ]
    },
    "references": [
      "def-patched-std-dev-function",
      "def-max-patched-std",
      "lem-rescale-derivative-lower-bound"
    ],
    "metadata": {
      "label": "lem-raw-gap-to-rescaled-gap"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 7,
      "chapter_file": "chapter_7.json",
      "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-variance-to-mean-separation",
    "title": "From Total Variance to Mean Separation",
    "type": "lemma",
    "nl_statement": "A set of real numbers in a compact interval with sufficiently large empirical variance, when partitioned into two substantial non-empty subsets, guarantees a statistically significant separation between the subset means, bounded below by an expression involving the variance and maximum possible variance.",
    "equations": [
      {
        "label": null,
        "latex": "\\operatorname{Var}_{\\mathrm{max}} := \\frac{1}{4}(V_{\\max} - V_{\\min})^2"
      },
      {
        "label": null,
        "latex": "(\\mu_H - \\mu_L)^2 \\ge \\frac{1}{f_H f_L} \\left( \\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}} \\right)"
      },
      {
        "label": null,
        "latex": "|\\mu_H - \\mu_L| \\ge \\frac{1}{\\sqrt{f_H f_L}} \\sqrt{\\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}}} > 0"
      }
    ],
    "hypotheses": [
      {
        "text": "Set $\\mathcal{V} = \\{v_i\\}_{i=1}^k$ with $k \\ge 2$ real numbers, each $v_i \\in [V_{\\min}, V_{\\max}]$ (compact interval).",
        "latex": "\\mathcal{V} = \\{v_i\\}_{i=1}^k, \\, k \\ge 2, \\, v_i \\in [V_{\\min}, V_{\\max}]"
      },
      {
        "text": "$\\mathcal{V}$ partitioned into disjoint non-empty subsets $H$ and $L$ with means $\\mu_H$ and $\\mu_L$.",
        "latex": "\\mathcal{V} = H \\sqcup L, \\, H, L \\neq \\emptyset, \\, \\mu_H = \\frac{1}{|H|} \\sum_{v \\in H} v, \\, \\mu_L = \\frac{1}{|L|} \\sum_{v \\in L} v"
      },
      {
        "text": "Fractional sizes $f_H = |H|/k \\ge f_{\\min} > 0$ and $f_L = |L|/k \\ge f_{\\min}$, with $f_{\\min} \\in (0, 1/2]$.",
        "latex": "f_H = |H|/k \\ge f_{\\min}, \\, f_L = |L|/k \\ge f_{\\min}, \\, f_{\\min} \\in (0, 1/2]"
      },
      {
        "text": "Empirical variance $\\operatorname{Var}(\\mathcal{V}) \\ge \\kappa_{\\mathrm{var}} > 0$.",
        "latex": "\\operatorname{Var}(\\mathcal{V}) \\ge \\kappa_{\\mathrm{var}} > 0"
      }
    ],
    "conclusion": {
      "text": "Squared mean difference bounded below: $(\\mu_H - \\mu_L)^2 \\ge \\frac{1}{f_H f_L} (\\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}})$. If $\\kappa_{\\mathrm{var}} > \\operatorname{Var}_{\\mathrm{max}}$, then positive separation: $|\\mu_H - \\mu_L| \\ge \\frac{1}{\\sqrt{f_H f_L}} \\sqrt{\\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}}} > 0$.",
      "latex": "(\\mu_H - \\mu_L)^2 \\ge \\frac{1}{f_H f_L} (\\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}}); \\, \\text{if } \\kappa_{\\mathrm{var}} > \\operatorname{Var}_{\\mathrm{max}}, \\, |\\mu_H - \\mu_L| \\ge \\frac{1}{\\sqrt{f_H f_L}} \\sqrt{\\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}}} > 0"
    },
    "variables": [
      {
        "symbol": "k",
        "name": "population size",
        "description": "Number of elements in the set $\\mathcal{V}$.",
        "constraints": [
          "k \\ge 2"
        ],
        "tags": [
          "size",
          "integer"
        ]
      },
      {
        "symbol": "\\mathcal{V}",
        "name": "set of values",
        "description": "Finite set of real numbers $\\mathcal{V} = \\{v_i\\}_{i=1}^k$.",
        "constraints": [
          "finite",
          "real-valued"
        ],
        "tags": [
          "set",
          "data"
        ]
      },
      {
        "symbol": "v_i",
        "name": "element",
        "description": "Individual value in $\\mathcal{V}$.",
        "constraints": [
          "v_i \\in [V_{\\min}, V_{\\max}]"
        ],
        "tags": [
          "value",
          "bounded"
        ]
      },
      {
        "symbol": "V_{\\min}",
        "name": "minimum bound",
        "description": "Lower bound of the compact interval containing all $v_i$.",
        "constraints": [
          "real"
        ],
        "tags": [
          "bound",
          "interval"
        ]
      },
      {
        "symbol": "V_{\\max}",
        "name": "maximum bound",
        "description": "Upper bound of the compact interval containing all $v_i$.",
        "constraints": [
          "real",
          "V_{\\max} \\ge V_{\\min}"
        ],
        "tags": [
          "bound",
          "interval"
        ]
      },
      {
        "symbol": "H",
        "name": "high subset",
        "description": "One non-empty subset in the partition of $\\mathcal{V}$.",
        "constraints": [
          "non-empty",
          "disjoint from L"
        ],
        "tags": [
          "subset",
          "partition"
        ]
      },
      {
        "symbol": "L",
        "name": "low subset",
        "description": "The other non-empty subset in the partition of $\\mathcal{V}$.",
        "constraints": [
          "non-empty",
          "disjoint from H"
        ],
        "tags": [
          "subset",
          "partition"
        ]
      },
      {
        "symbol": "\\mu_H",
        "name": "mean of H",
        "description": "Arithmetic mean of values in subset H.",
        "constraints": [],
        "tags": [
          "mean",
          "statistic"
        ]
      },
      {
        "symbol": "\\mu_L",
        "name": "mean of L",
        "description": "Arithmetic mean of values in subset L.",
        "constraints": [],
        "tags": [
          "mean",
          "statistic"
        ]
      },
      {
        "symbol": "f_H",
        "name": "fractional size of H",
        "description": "Proportion $|H|/k$.",
        "constraints": [
          "f_H \\ge f_{\\min} > 0",
          "f_H + f_L = 1"
        ],
        "tags": [
          "fraction",
          "size"
        ]
      },
      {
        "symbol": "f_L",
        "name": "fractional size of L",
        "description": "Proportion $|L|/k$.",
        "constraints": [
          "f_L \\ge f_{\\min} > 0",
          "f_H + f_L = 1"
        ],
        "tags": [
          "fraction",
          "size"
        ]
      },
      {
        "symbol": "f_{\\min}",
        "name": "minimum fraction",
        "description": "Strictly positive lower bound on subset fractions.",
        "constraints": [
          "f_{\\min} \\in (0, 1/2]"
        ],
        "tags": [
          "bound",
          "fraction"
        ]
      },
      {
        "symbol": "\\kappa_{\\mathrm{var}}",
        "name": "variance lower bound",
        "description": "Strictly positive constant bounding variance from below.",
        "constraints": [
          "\\kappa_{\\mathrm{var}} > 0"
        ],
        "tags": [
          "variance",
          "threshold"
        ]
      },
      {
        "symbol": "\\operatorname{Var}(\\mathcal{V})",
        "name": "empirical variance",
        "description": "Variance of the set $\\mathcal{V}$.",
        "constraints": [
          "\\ge \\kappa_{\\mathrm{var}}"
        ],
        "tags": [
          "variance",
          "empirical"
        ]
      },
      {
        "symbol": "\\operatorname{Var}_{\\mathrm{max}}",
        "name": "maximum variance",
        "description": "Upper bound on variance for values in [V_min, V_max], equal to (1/4)(V_max - V_min)^2.",
        "constraints": [],
        "tags": [
          "variance",
          "bound"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The subsets H and L cover the entire set V (i.e., V = H union L).",
        "confidence": 1.0
      },
      {
        "text": "The means mu_H and mu_L are well-defined since H and L are non-empty.",
        "confidence": 1.0
      },
      {
        "text": "Empirical variance is the standard sample variance (possibly population variance, as k is total size).",
        "confidence": 0.9
      },
      {
        "text": "The signal-to-noise condition kappa_var > Var_max ensures positive separation.",
        "confidence": 1.0
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-variance-to-mean-separation",
      "title": null,
      "type": "proof",
      "proves": "lem-variance-to-mean-separation",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-variance-to-mean-separation\n\n**Proof.**\n\nThe proof is based on the decomposition of the total variance provided by the Law of Total Variance. We will establish a precise identity relating the total variance to the difference in subset means, find a sharp upper bound on the confounding variance term, and combine these results to derive the desired lower bound.\n\n**Step 1: The Law of Total Variance.**\nLet $\\mu_{\\mathcal{V}}$ be the mean of the entire set $\\mathcal{V}$. The total empirical variance, $\\operatorname{Var}(\\mathcal{V}) := \\frac{1}{k}\\sum_{i \\in \\mathcal{V}} (v_i - \\mu_{\\mathcal{V}})^2$, can be decomposed into two components: the between-group variance ($\\operatorname{Var}_B$) and the within-group variance ($\\operatorname{Var}_W$).\n\n$$\n\\operatorname{Var}(\\mathcal{V}) = \\operatorname{Var}_B(\\mathcal{V}) + \\operatorname{Var}_W(\\mathcal{V})\n$$\n\nThe **within-group variance** is the weighted average of the variances of the subsets:\n\n$$\n\\operatorname{Var}_W(\\mathcal{V}) := f_H \\operatorname{Var}(H) + f_L \\operatorname{Var}(L)\n$$\n\nThe **between-group variance** is the variance of the subset means around the total mean:\n\n$$\n\\operatorname{Var}_B(\\mathcal{V}) := f_H(\\mu_H - \\mu_{\\mathcal{V}})^2 + f_L(\\mu_L - \\mu_{\\mathcal{V}})^2\n$$\n\n**Step 2: Relating Between-Group Variance to the Mean Separation.**\nWe will now prove that the between-group variance is directly proportional to $(\\mu_H - \\mu_L)^2$. The total mean is the weighted average of the subset means: $\\mu_{\\mathcal{V}} = f_H \\mu_H + f_L \\mu_L$. Substituting this into the definition of $\\operatorname{Var}_B(\\mathcal{V})$:\n\n$$\n\\begin{aligned}\n\\mu_H - \\mu_{\\mathcal{V}} &= \\mu_H - (f_H \\mu_H + f_L \\mu_L) = (1-f_H)\\mu_H - f_L \\mu_L = f_L \\mu_H - f_L \\mu_L = f_L(\\mu_H - \\mu_L) \\\\\n\\mu_L - \\mu_{\\mathcal{V}} &= \\mu_L - (f_H \\mu_H + f_L \\mu_L) = -f_H \\mu_H + (1-f_L)\\mu_L = -f_H \\mu_H + f_H \\mu_L = -f_H(\\mu_H - \\mu_L)\n\\end{aligned}\n$$\n\nSubstituting these expressions back into the formula for $\\operatorname{Var}_B(\\mathcal{V})$ yields:\n\n$$\n\\begin{aligned}\n\\operatorname{Var}_B(\\mathcal{V}) &= f_H (f_L(\\mu_H - \\mu_L))^2 + f_L (-f_H(\\mu_H - \\mu_L))^2 \\\\\n&= f_H f_L^2 (\\mu_H - \\mu_L)^2 + f_L f_H^2 (\\mu_H - \\mu_L)^2 \\\\\n&= (f_H f_L^2 + f_L f_H^2)(\\mu_H - \\mu_L)^2 \\\\\n&= f_H f_L (f_L + f_H)(\\mu_H - \\mu_L)^2\n\\end{aligned}\n$$\n\nSince $f_H + f_L = 1$, we arrive at the exact identity:\n\n$$\n\\operatorname{Var}_B(\\mathcal{V}) = f_H f_L (\\mu_H - \\mu_L)^2\n$$\n\n**Step 3: A Uniform Upper Bound on the Within-Group Variance.**\nThe within-group variance, $\\operatorname{Var}_W(\\mathcal{V}) = f_H \\operatorname{Var}(H) + f_L \\operatorname{Var}(L)$, represents the noise that can mask the signal from the mean separation. We seek a sharp, state-independent upper bound. For any set of numbers on a compact interval $[a, b]$, the maximum possible variance is given by Popoviciu's inequality:\n\n$$\n\\operatorname{Var}(S) \\le \\frac{1}{4}(\\max(S) - \\min(S))^2\n$$\n\nSince for any subset $S \\subseteq \\mathcal{V}$, its elements are contained in $[V_{\\min}, V_{\\max}]$, we have $\\operatorname{Var}(H) \\le \\frac{1}{4}(V_{\\max} - V_{\\min})^2$ and $\\operatorname{Var}(L) \\le \\frac{1}{4}(V_{\\max} - V_{\\min})^2$.\nLet $\\operatorname{Var}_{\\mathrm{max}} := \\frac{1}{4}(V_{\\max} - V_{\\min})^2$. The within-group variance is therefore uniformly bounded above:\n\n$$\n\\operatorname{Var}_W(\\mathcal{V}) \\le f_H \\operatorname{Var}_{\\mathrm{max}} + f_L \\operatorname{Var}_{\\mathrm{max}} = (f_H+f_L)\\operatorname{Var}_{\\mathrm{max}} = \\operatorname{Var}_{\\mathrm{max}}\n$$\n\nThis upper bound is sharp; it is attained if both subsets consist of values located only at the endpoints of the interval.\n\n**Step 4: Assembling the Final Inequality.**\nWe rearrange the Law of Total Variance from Step 1:\n\n$$\n\\operatorname{Var}_B(\\mathcal{V}) = \\operatorname{Var}(\\mathcal{V}) - \\operatorname{Var}_W(\\mathcal{V})\n$$\n\nWe substitute our identity for $\\operatorname{Var}_B(\\mathcal{V})$ from Step 2. Then, we use our premise, $\\operatorname{Var}(\\mathcal{V}) \\ge \\kappa_{\\mathrm{var}}$, and our upper bound for the within-group variance from Step 3:\n\n$$\nf_H f_L (\\mu_H - \\mu_L)^2 \\ge \\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}}\n$$\n\nSince the fractional sizes $f_H$ and $f_L$ are strictly positive, dividing by their product preserves the inequality:\n\n$$\n(\\mu_H - \\mu_L)^2 \\ge \\frac{1}{f_H f_L} \\left( \\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}} \\right)\n$$\n\nThis proves the main inequality of the lemma. The final conclusion follows directly. If $\\kappa_{\\mathrm{var}} > \\operatorname{Var}_{\\mathrm{max}}$, the right-hand side is strictly positive. Taking the square root gives the lower bound on $|\\mu_H - \\mu_L|$. The pre-factor $1/\\sqrt{f_H f_L}$ is well-defined and uniformly bounded above because the premises guarantee $f_H, f_L \\ge f_{\\min} > 0$. The entire lower bound is therefore a strictly positive constant.",
      "raw_directive": "3803: \n3804: :::\n3805: :::{prf:proof}\n3806: :label: proof-lem-variance-to-mean-separation\n3807: \n3808: **Proof.**\n3809: \n3810: The proof is based on the decomposition of the total variance provided by the Law of Total Variance. We will establish a precise identity relating the total variance to the difference in subset means, find a sharp upper bound on the confounding variance term, and combine these results to derive the desired lower bound.\n3811: \n3812: **Step 1: The Law of Total Variance.**\n3813: Let $\\mu_{\\mathcal{V}}$ be the mean of the entire set $\\mathcal{V}$. The total empirical variance, $\\operatorname{Var}(\\mathcal{V}) := \\frac{1}{k}\\sum_{i \\in \\mathcal{V}} (v_i - \\mu_{\\mathcal{V}})^2$, can be decomposed into two components: the between-group variance ($\\operatorname{Var}_B$) and the within-group variance ($\\operatorname{Var}_W$).\n3814: \n3815: $$\n3816: \\operatorname{Var}(\\mathcal{V}) = \\operatorname{Var}_B(\\mathcal{V}) + \\operatorname{Var}_W(\\mathcal{V})\n3817: $$\n3818: \n3819: The **within-group variance** is the weighted average of the variances of the subsets:\n3820: \n3821: $$\n3822: \\operatorname{Var}_W(\\mathcal{V}) := f_H \\operatorname{Var}(H) + f_L \\operatorname{Var}(L)\n3823: $$\n3824: \n3825: The **between-group variance** is the variance of the subset means around the total mean:\n3826: \n3827: $$\n3828: \\operatorname{Var}_B(\\mathcal{V}) := f_H(\\mu_H - \\mu_{\\mathcal{V}})^2 + f_L(\\mu_L - \\mu_{\\mathcal{V}})^2\n3829: $$\n3830: \n3831: **Step 2: Relating Between-Group Variance to the Mean Separation.**\n3832: We will now prove that the between-group variance is directly proportional to $(\\mu_H - \\mu_L)^2$. The total mean is the weighted average of the subset means: $\\mu_{\\mathcal{V}} = f_H \\mu_H + f_L \\mu_L$. Substituting this into the definition of $\\operatorname{Var}_B(\\mathcal{V})$:\n3833: \n3834: $$\n3835: \\begin{aligned}\n3836: \\mu_H - \\mu_{\\mathcal{V}} &= \\mu_H - (f_H \\mu_H + f_L \\mu_L) = (1-f_H)\\mu_H - f_L \\mu_L = f_L \\mu_H - f_L \\mu_L = f_L(\\mu_H - \\mu_L) \\\\\n3837: \\mu_L - \\mu_{\\mathcal{V}} &= \\mu_L - (f_H \\mu_H + f_L \\mu_L) = -f_H \\mu_H + (1-f_L)\\mu_L = -f_H \\mu_H + f_H \\mu_L = -f_H(\\mu_H - \\mu_L)\n3838: \\end{aligned}\n3839: $$\n3840: \n3841: Substituting these expressions back into the formula for $\\operatorname{Var}_B(\\mathcal{V})$ yields:\n3842: \n3843: $$\n3844: \\begin{aligned}\n3845: \\operatorname{Var}_B(\\mathcal{V}) &= f_H (f_L(\\mu_H - \\mu_L))^2 + f_L (-f_H(\\mu_H - \\mu_L))^2 \\\\\n3846: &= f_H f_L^2 (\\mu_H - \\mu_L)^2 + f_L f_H^2 (\\mu_H - \\mu_L)^2 \\\\\n3847: &= (f_H f_L^2 + f_L f_H^2)(\\mu_H - \\mu_L)^2 \\\\\n3848: &= f_H f_L (f_L + f_H)(\\mu_H - \\mu_L)^2\n3849: \\end{aligned}\n3850: $$\n3851: \n3852: Since $f_H + f_L = 1$, we arrive at the exact identity:\n3853: \n3854: $$\n3855: \\operatorname{Var}_B(\\mathcal{V}) = f_H f_L (\\mu_H - \\mu_L)^2\n3856: $$\n3857: \n3858: **Step 3: A Uniform Upper Bound on the Within-Group Variance.**\n3859: The within-group variance, $\\operatorname{Var}_W(\\mathcal{V}) = f_H \\operatorname{Var}(H) + f_L \\operatorname{Var}(L)$, represents the noise that can mask the signal from the mean separation. We seek a sharp, state-independent upper bound. For any set of numbers on a compact interval $[a, b]$, the maximum possible variance is given by Popoviciu's inequality:\n3860: \n3861: $$\n3862: \\operatorname{Var}(S) \\le \\frac{1}{4}(\\max(S) - \\min(S))^2\n3863: $$\n3864: \n3865: Since for any subset $S \\subseteq \\mathcal{V}$, its elements are contained in $[V_{\\min}, V_{\\max}]$, we have $\\operatorname{Var}(H) \\le \\frac{1}{4}(V_{\\max} - V_{\\min})^2$ and $\\operatorname{Var}(L) \\le \\frac{1}{4}(V_{\\max} - V_{\\min})^2$.\n3866: Let $\\operatorname{Var}_{\\mathrm{max}} := \\frac{1}{4}(V_{\\max} - V_{\\min})^2$. The within-group variance is therefore uniformly bounded above:\n3867: \n3868: $$\n3869: \\operatorname{Var}_W(\\mathcal{V}) \\le f_H \\operatorname{Var}_{\\mathrm{max}} + f_L \\operatorname{Var}_{\\mathrm{max}} = (f_H+f_L)\\operatorname{Var}_{\\mathrm{max}} = \\operatorname{Var}_{\\mathrm{max}}\n3870: $$\n3871: \n3872: This upper bound is sharp; it is attained if both subsets consist of values located only at the endpoints of the interval.\n3873: \n3874: **Step 4: Assembling the Final Inequality.**\n3875: We rearrange the Law of Total Variance from Step 1:\n3876: \n3877: $$\n3878: \\operatorname{Var}_B(\\mathcal{V}) = \\operatorname{Var}(\\mathcal{V}) - \\operatorname{Var}_W(\\mathcal{V})\n3879: $$\n3880: \n3881: We substitute our identity for $\\operatorname{Var}_B(\\mathcal{V})$ from Step 2. Then, we use our premise, $\\operatorname{Var}(\\mathcal{V}) \\ge \\kappa_{\\mathrm{var}}$, and our upper bound for the within-group variance from Step 3:\n3882: \n3883: $$\n3884: f_H f_L (\\mu_H - \\mu_L)^2 \\ge \\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}}\n3885: $$\n3886: \n3887: Since the fractional sizes $f_H$ and $f_L$ are strictly positive, dividing by their product preserves the inequality:\n3888: \n3889: $$\n3890: (\\mu_H - \\mu_L)^2 \\ge \\frac{1}{f_H f_L} \\left( \\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}} \\right)\n3891: $$\n3892: \n3893: This proves the main inequality of the lemma. The final conclusion follows directly. If $\\kappa_{\\mathrm{var}} > \\operatorname{Var}_{\\mathrm{max}}$, the right-hand side is strictly positive. Taking the square root gives the lower bound on $|\\mu_H - \\mu_L|$. The pre-factor $1/\\sqrt{f_H f_L}$ is well-defined and uniformly bounded above because the premises guarantee $f_H, f_L \\ge f_{\\min} > 0$. The entire lower bound is therefore a strictly positive constant.\n3894: ",
      "strategy_summary": "The proof applies the Law of Total Variance to decompose the empirical variance into between-group and within-group terms, derives an exact expression for the between-group variance in terms of the squared mean difference, bounds the within-group variance using Popoviciu's inequality, and combines these to establish a lower bound on the mean separation under the given premises.",
      "conclusion": {
        "text": null,
        "latex": null
      },
      "assumptions": [],
      "steps": [],
      "key_equations": [],
      "references": [],
      "math_tools": [
        {
          "toolName": "Law of Total Variance",
          "field": "Statistics",
          "description": "Decomposes the total variance of a dataset into the variance between subset means and the average variance within subsets.",
          "roleInProof": "Provides the key decomposition used to isolate the between-group variance related to mean separation.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Popoviciu Inequality"
          ]
        },
        {
          "toolName": "Popoviciu's Inequality",
          "field": "Statistics",
          "description": "States that for any bounded set of numbers in [a, b], the variance is at most (b - a)^2 / 4.",
          "roleInProof": "Yields a uniform upper bound on the within-group variances to subtract from the total variance.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Law of Total Variance"
          ]
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "sharpness",
          "text": "The upper bound on within-group variance is sharp, attained when subsets consist of values at the interval endpoints."
        },
        {
          "type": "positivity",
          "text": "The lower bound is strictly positive when \u03ba_var > Var_max, and the prefactor is uniformly bounded due to f_min > 0."
        }
      ],
      "gaps": [],
      "tags": [
        "variance decomposition",
        "law of total variance",
        "popoviciu inequality",
        "mean separation",
        "empirical statistics",
        "upper bound",
        "lower bound"
      ],
      "document_id": "03_cloning",
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "span": {
        "start_line": 3803,
        "end_line": 3894,
        "content_start": 3806,
        "content_end": 3893,
        "header_lines": [
          3804
        ]
      },
      "metadata": {
        "label": "proof-lem-variance-to-mean-separation"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "variance",
      "mean-separation",
      "partition",
      "statistical",
      "signal-noise",
      "population",
      "subsets"
    ],
    "content_markdown": ":label: lem-variance-to-mean-separation\n\nLet $\\mathcal{V} = \\{v_i\\}_{i=1}^k$ be a set of $k \\ge 2$ real numbers, with each element $v_i$ contained in the compact interval $[V_{\\min}, V_{\\max}]$. Let $\\mathcal{V}$ be partitioned into two disjoint, non-empty subsets, $H$ and $L$, with corresponding means $\\mu_H$ and $\\mu_L$. Let their fractional population sizes, $f_H = |H|/k$ and $f_L = |L|/k$, be bounded below by a strictly positive constant $f_{\\min} \\in (0, 1/2]$, such that $f_H \\ge f_{\\min}$ and $f_L \\ge f_{\\min}$.\n\nIf the empirical variance of the total set, $\\operatorname{Var}(\\mathcal{V})$, is bounded below by a strictly positive constant $\\kappa_{\\mathrm{var}} > 0$, then the squared difference between the subset means is bounded below by:\n\n$$\n(\\mu_H - \\mu_L)^2 \\ge \\frac{1}{f_H f_L} \\left( \\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}} \\right)\n$$\n\nwhere $\\operatorname{Var}_{\\mathrm{max}} := \\frac{1}{4}(V_{\\max} - V_{\\min})^2$ is the maximum possible variance for any set of values on the interval.\n\nConsequently, if the guaranteed variance $\\kappa_{\\mathrm{var}}$ is sufficiently large to satisfy the **Signal-to-Noise Condition**, $\\kappa_{\\mathrm{var}} > \\operatorname{Var}_{\\mathrm{max}}$, then the mean separation is guaranteed to be positive:\n\n$$\n|\\mu_H - \\mu_L| \\ge \\frac{1}{\\sqrt{f_H f_L}} \\sqrt{\\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}}} > 0",
    "raw_directive": "3783: The following lemma provides this fundamental link. It proves, from first principles, that a sufficiently large variance within a population partitioned into two substantial subsets necessitates a statistically significant separation between the means of those subsets.\n3784: \n3785: :::{prf:lemma} **(From Total Variance to Mean Separation)**\n3786: :label: lem-variance-to-mean-separation\n3787: \n3788: Let $\\mathcal{V} = \\{v_i\\}_{i=1}^k$ be a set of $k \\ge 2$ real numbers, with each element $v_i$ contained in the compact interval $[V_{\\min}, V_{\\max}]$. Let $\\mathcal{V}$ be partitioned into two disjoint, non-empty subsets, $H$ and $L$, with corresponding means $\\mu_H$ and $\\mu_L$. Let their fractional population sizes, $f_H = |H|/k$ and $f_L = |L|/k$, be bounded below by a strictly positive constant $f_{\\min} \\in (0, 1/2]$, such that $f_H \\ge f_{\\min}$ and $f_L \\ge f_{\\min}$.\n3789: \n3790: If the empirical variance of the total set, $\\operatorname{Var}(\\mathcal{V})$, is bounded below by a strictly positive constant $\\kappa_{\\mathrm{var}} > 0$, then the squared difference between the subset means is bounded below by:\n3791: \n3792: $$\n3793: (\\mu_H - \\mu_L)^2 \\ge \\frac{1}{f_H f_L} \\left( \\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}} \\right)\n3794: $$\n3795: \n3796: where $\\operatorname{Var}_{\\mathrm{max}} := \\frac{1}{4}(V_{\\max} - V_{\\min})^2$ is the maximum possible variance for any set of values on the interval.\n3797: \n3798: Consequently, if the guaranteed variance $\\kappa_{\\mathrm{var}}$ is sufficiently large to satisfy the **Signal-to-Noise Condition**, $\\kappa_{\\mathrm{var}} > \\operatorname{Var}_{\\mathrm{max}}$, then the mean separation is guaranteed to be positive:\n3799: \n3800: $$\n3801: |\\mu_H - \\mu_L| \\ge \\frac{1}{\\sqrt{f_H f_L}} \\sqrt{\\kappa_{\\mathrm{var}} - \\operatorname{Var}_{\\mathrm{max}}} > 0\n3802: $$",
    "document_id": "03_cloning",
    "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
    "span": {
      "start_line": 3783,
      "end_line": 3802,
      "content_start": 3786,
      "content_end": 3801,
      "header_lines": [
        3784
      ]
    },
    "references": [],
    "metadata": {
      "label": "lem-variance-to-mean-separation"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 7,
      "chapter_file": "chapter_7.json",
      "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-log-gap-lower-bound",
    "title": "Lower Bound on Logarithmic Mean Gap",
    "type": "lemma",
    "nl_statement": "If two random variables X and Y take values in the compact interval [V_min, V_max] with V_min > 0 and their means satisfy E[X] >= E[Y] + kappa for kappa > 0, then the difference in their expected logarithms satisfies E[ln X] - E[ln Y] >= ln(1 + kappa / V_max).",
    "equations": [
      {
        "label": null,
        "latex": "\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\ge \\ln\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right)"
      }
    ],
    "hypotheses": [
      {
        "text": "X and Y are random variables taking values in the compact interval [V_min, V_max] with V_min > 0.",
        "latex": null
      },
      {
        "text": "The means satisfy mu_X = E[X] >= mu_Y = E[Y] + kappa for some constant kappa > 0.",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "E[ln(X)] - E[ln(Y)] >= ln(1 + kappa / V_max)",
      "latex": "\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\ge \\ln\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right)"
    },
    "variables": [
      {
        "symbol": "X",
        "name": "X",
        "description": "First random variable.",
        "constraints": [
          "Supported on [V_min, V_max]"
        ],
        "tags": [
          "random-variable"
        ]
      },
      {
        "symbol": "Y",
        "name": "Y",
        "description": "Second random variable.",
        "constraints": [
          "Supported on [V_min, V_max]"
        ],
        "tags": [
          "random-variable"
        ]
      },
      {
        "symbol": "V_{\\min}",
        "name": "V_min",
        "description": "Lower bound of the support interval.",
        "constraints": [
          "V_min > 0"
        ],
        "tags": [
          "bound"
        ]
      },
      {
        "symbol": "V_{\\max}",
        "name": "V_max",
        "description": "Upper bound of the support interval.",
        "constraints": [],
        "tags": [
          "bound"
        ]
      },
      {
        "symbol": "\\mu_X",
        "name": "mu_X",
        "description": "Mean of X.",
        "constraints": [],
        "tags": [
          "mean"
        ]
      },
      {
        "symbol": "\\mu_Y",
        "name": "mu_Y",
        "description": "Mean of Y.",
        "constraints": [],
        "tags": [
          "mean"
        ]
      },
      {
        "symbol": "\\kappa",
        "name": "kappa",
        "description": "Positive constant for mean separation.",
        "constraints": [
          "kappa > 0"
        ],
        "tags": [
          "separation"
        ]
      },
      {
        "symbol": "\\mathbb{E}",
        "name": "E",
        "description": "Expectation operator.",
        "constraints": [],
        "tags": [
          "expectation"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The supports ensure X and Y are positive, allowing the logarithm to be well-defined.",
        "confidence": 1.0
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-log-gap-lower-bound",
      "title": null,
      "type": "proof",
      "proves": "lem-log-gap-lower-bound",
      "proof_type": "variational",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-log-gap-lower-bound\n\n**Proof.**\n\nThe proof uses the theory of extremal distributions for concave functions. We establish tight bounds on each term by identifying the distributions that minimize $\\mathbb{E}[\\ln(X)]$ and maximize $\\mathbb{E}[\\ln(Y)]$, then find the minimum of their difference over all valid mean pairs.\n\n**Step 1: Extremal Distributions for the Logarithm.**\n\nSince $f(t) = \\ln(t)$ is strictly concave for $t > 0$, the extremal distributions are well-known:\n- For a **fixed mean** $\\mu$, the minimum of $\\mathbb{E}[\\ln(X)]$ is achieved by a **two-point distribution** with mass only at the endpoints $\\{V_{\\min}, V_{\\max}\\}$.\n- For a **fixed mean** $\\mu$, the maximum of $\\mathbb{E}[\\ln(Y)]$ is achieved by a **deterministic distribution**: $Y = \\mu$ with probability 1. By Jensen's inequality, $\\mathbb{E}[\\ln(Y)] \\le \\ln(\\mu_Y)$, with equality when $Y$ is deterministic.\n\n**Step 2: Bounding the Difference Using Extremal Cases.**\n\nThe difference $\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]$ is minimized in the worst-case scenario where:\n- $\\mathbb{E}[\\ln(X)]$ is as small as possible for mean $\\mu_X$ \u2192 Use the extremal two-point distribution $X_{\\min}$\n- $\\mathbb{E}[\\ln(Y)]$ is as large as possible for mean $\\mu_Y$ \u2192 Use the deterministic distribution $Y = \\mu_Y$\n\nTherefore, for any distributions $X$, $Y$ with means $\\mu_X$, $\\mu_Y$:\n\n$$\n\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\ge \\mathbb{E}[\\ln(X_{\\min})] - \\ln(\\mu_Y)\n$$\n\nwhere $X_{\\min}$ is the two-point distribution with mean $\\mu_X$:\n\n$$\nP(X_{\\min} = V_{\\min}) = \\frac{V_{\\max} - \\mu_X}{V_{\\max} - V_{\\min}}, \\quad P(X_{\\min} = V_{\\max}) = \\frac{\\mu_X - V_{\\min}}{V_{\\max} - V_{\\min}}\n$$\n\n**Step 3: Reduction to a One-Dimensional Optimization Problem.**\n\nWe now minimize $\\mathbb{E}[\\ln(X_{\\min})] - \\ln(\\mu_Y)$ over all valid pairs $(\\mu_X, \\mu_Y)$ satisfying:\n- $\\mu_X \\ge \\mu_Y + \\kappa$\n- $\\mu_X, \\mu_Y \\in [V_{\\min}, V_{\\max}]$\n\nFirst, observe that for any fixed $\\mu_Y$, the expected value $\\mathbb{E}[\\ln(X_{\\min})]$ is an increasing function of $\\mu_X$. Therefore, to minimize the difference, we should choose $\\mu_X$ as small as possible, which places us on the boundary: $\\mu_X = \\mu_Y + \\kappa$.\n\nThe problem reduces to minimizing the one-dimensional function:\n\n$$\nh(\\mu_Y) := \\mathbb{E}[\\ln(X_{\\min,\\mu_Y+\\kappa})] - \\ln(\\mu_Y)\n$$\n\nfor $\\mu_Y \\in [V_{\\min}, V_{\\max} - \\kappa]$.\n\nNow we prove that this function is **convex**. The expected log of the two-point extremal distribution is a linear function of its mean:\n\n$$\n\\mathbb{E}[\\ln(X_{\\min,\\mu})] = \\ln(V_{\\max}) + \\frac{V_{\\max} - \\mu}{V_{\\max} - V_{\\min}}(\\ln(V_{\\min}) - \\ln(V_{\\max}))\n$$\n\nThis can be written as $C_0 + C_1 \\mu$ where $C_1 = (\\ln(V_{\\max}) - \\ln(V_{\\min}))/(V_{\\max} - V_{\\min}) > 0$. Substituting $\\mu = \\mu_Y + \\kappa$:\n\n$$\nh(\\mu_Y) = [C_0 + C_1(\\mu_Y + \\kappa)] - \\ln(\\mu_Y)\n$$\n\nThe function $h(\\mu_Y)$ is the sum of a linear function (in $\\mu_Y$) and the function $-\\ln(\\mu_Y)$, which is strictly convex. Therefore, $h(\\mu_Y)$ is strictly convex.\n\n**A convex function on a closed interval attains its minimum at one of the endpoints.** We must check the values at $\\mu_Y = V_{\\min}$ and $\\mu_Y = V_{\\max} - \\kappa$.\n\n**Key insight:** The logarithm becomes flatter as its argument increases (decreasing derivative). For a fixed gap $\\kappa$ between means, the logarithmic gap is smaller when the means are at higher values. This suggests the minimum occurs at the right endpoint: $\\mu_Y = V_{\\max} - \\kappa$.\n\nThe worst-case configuration is therefore:\n- $\\mu_Y = V_{\\max} - \\kappa$ (right endpoint)\n- $\\mu_X = V_{\\max}$ (forced by the boundary constraint)\n\n**Step 4: Computing the Lower Bound for the Worst Case.**\n\nAt this worst-case configuration:\n- For $X$ with mean $\\mu_X = V_{\\max}$, the two-point extremal distribution degenerates to a deterministic distribution: $X = V_{\\max}$ with probability 1. Thus:\n\n$$\n\\mathbb{E}[\\ln(X)] = \\ln(V_{\\max})\n$$\n\n- For $Y$, the extremal case (maximum expected log) is deterministic: $Y = \\mu_Y = V_{\\max} - \\kappa$. Thus:\n\n$$\n\\mathbb{E}[\\ln(Y)] = \\ln(V_{\\max} - \\kappa)\n$$\n\nThe worst-case lower bound is:\n\n$$\n\\ln(V_{\\max}) - \\ln(V_{\\max} - \\kappa) = \\ln\\left(\\frac{V_{\\max}}{V_{\\max} - \\kappa}\\right) = \\ln\\left(1 + \\frac{\\kappa}{V_{\\max} - \\kappa}\\right)\n$$\n\n**Step 5: Simplification to the Stated Bound.**\n\nThe tight bound from Step 4 is $\\ln(1 + \\kappa/(V_{\\max} - \\kappa))$. The lemma states the slightly looser but simpler bound $\\ln(1 + \\kappa/V_{\\max})$.\n\nTo verify this is valid, note that for $\\kappa < V_{\\max}$:\n\n$$\n\\frac{\\kappa}{V_{\\max}} < \\frac{\\kappa}{V_{\\max} - \\kappa}\n$$\n\nSince $\\ln(1+t)$ is strictly increasing in $t$:\n\n$$\n\\ln\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right) < \\ln\\left(1 + \\frac{\\kappa}{V_{\\max} - \\kappa}\\right)\n$$\n\nTherefore, $\\ln(1 + \\kappa/V_{\\max})$ is a valid (conservative) lower bound that is simpler to use in subsequent analysis.",
      "raw_directive": "4045: :::\n4046: \n4047: :::{prf:proof}\n4048: :label: proof-lem-log-gap-lower-bound\n4049: \n4050: **Proof.**\n4051: \n4052: The proof uses the theory of extremal distributions for concave functions. We establish tight bounds on each term by identifying the distributions that minimize $\\mathbb{E}[\\ln(X)]$ and maximize $\\mathbb{E}[\\ln(Y)]$, then find the minimum of their difference over all valid mean pairs.\n4053: \n4054: **Step 1: Extremal Distributions for the Logarithm.**\n4055: \n4056: Since $f(t) = \\ln(t)$ is strictly concave for $t > 0$, the extremal distributions are well-known:\n4057: - For a **fixed mean** $\\mu$, the minimum of $\\mathbb{E}[\\ln(X)]$ is achieved by a **two-point distribution** with mass only at the endpoints $\\{V_{\\min}, V_{\\max}\\}$.\n4058: - For a **fixed mean** $\\mu$, the maximum of $\\mathbb{E}[\\ln(Y)]$ is achieved by a **deterministic distribution**: $Y = \\mu$ with probability 1. By Jensen's inequality, $\\mathbb{E}[\\ln(Y)] \\le \\ln(\\mu_Y)$, with equality when $Y$ is deterministic.\n4059: \n4060: **Step 2: Bounding the Difference Using Extremal Cases.**\n4061: \n4062: The difference $\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]$ is minimized in the worst-case scenario where:\n4063: - $\\mathbb{E}[\\ln(X)]$ is as small as possible for mean $\\mu_X$ \u2192 Use the extremal two-point distribution $X_{\\min}$\n4064: - $\\mathbb{E}[\\ln(Y)]$ is as large as possible for mean $\\mu_Y$ \u2192 Use the deterministic distribution $Y = \\mu_Y$\n4065: \n4066: Therefore, for any distributions $X$, $Y$ with means $\\mu_X$, $\\mu_Y$:\n4067: \n4068: $$\n4069: \\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\ge \\mathbb{E}[\\ln(X_{\\min})] - \\ln(\\mu_Y)\n4070: $$\n4071: \n4072: where $X_{\\min}$ is the two-point distribution with mean $\\mu_X$:\n4073: \n4074: $$\n4075: P(X_{\\min} = V_{\\min}) = \\frac{V_{\\max} - \\mu_X}{V_{\\max} - V_{\\min}}, \\quad P(X_{\\min} = V_{\\max}) = \\frac{\\mu_X - V_{\\min}}{V_{\\max} - V_{\\min}}\n4076: $$\n4077: \n4078: **Step 3: Reduction to a One-Dimensional Optimization Problem.**\n4079: \n4080: We now minimize $\\mathbb{E}[\\ln(X_{\\min})] - \\ln(\\mu_Y)$ over all valid pairs $(\\mu_X, \\mu_Y)$ satisfying:\n4081: - $\\mu_X \\ge \\mu_Y + \\kappa$\n4082: - $\\mu_X, \\mu_Y \\in [V_{\\min}, V_{\\max}]$\n4083: \n4084: First, observe that for any fixed $\\mu_Y$, the expected value $\\mathbb{E}[\\ln(X_{\\min})]$ is an increasing function of $\\mu_X$. Therefore, to minimize the difference, we should choose $\\mu_X$ as small as possible, which places us on the boundary: $\\mu_X = \\mu_Y + \\kappa$.\n4085: \n4086: The problem reduces to minimizing the one-dimensional function:\n4087: \n4088: $$\n4089: h(\\mu_Y) := \\mathbb{E}[\\ln(X_{\\min,\\mu_Y+\\kappa})] - \\ln(\\mu_Y)\n4090: $$\n4091: \n4092: for $\\mu_Y \\in [V_{\\min}, V_{\\max} - \\kappa]$.\n4093: \n4094: Now we prove that this function is **convex**. The expected log of the two-point extremal distribution is a linear function of its mean:\n4095: \n4096: $$\n4097: \\mathbb{E}[\\ln(X_{\\min,\\mu})] = \\ln(V_{\\max}) + \\frac{V_{\\max} - \\mu}{V_{\\max} - V_{\\min}}(\\ln(V_{\\min}) - \\ln(V_{\\max}))\n4098: $$\n4099: \n4100: This can be written as $C_0 + C_1 \\mu$ where $C_1 = (\\ln(V_{\\max}) - \\ln(V_{\\min}))/(V_{\\max} - V_{\\min}) > 0$. Substituting $\\mu = \\mu_Y + \\kappa$:\n4101: \n4102: $$\n4103: h(\\mu_Y) = [C_0 + C_1(\\mu_Y + \\kappa)] - \\ln(\\mu_Y)\n4104: $$\n4105: \n4106: The function $h(\\mu_Y)$ is the sum of a linear function (in $\\mu_Y$) and the function $-\\ln(\\mu_Y)$, which is strictly convex. Therefore, $h(\\mu_Y)$ is strictly convex.\n4107: \n4108: **A convex function on a closed interval attains its minimum at one of the endpoints.** We must check the values at $\\mu_Y = V_{\\min}$ and $\\mu_Y = V_{\\max} - \\kappa$.\n4109: \n4110: **Key insight:** The logarithm becomes flatter as its argument increases (decreasing derivative). For a fixed gap $\\kappa$ between means, the logarithmic gap is smaller when the means are at higher values. This suggests the minimum occurs at the right endpoint: $\\mu_Y = V_{\\max} - \\kappa$.\n4111: \n4112: The worst-case configuration is therefore:\n4113: - $\\mu_Y = V_{\\max} - \\kappa$ (right endpoint)\n4114: - $\\mu_X = V_{\\max}$ (forced by the boundary constraint)\n4115: \n4116: **Step 4: Computing the Lower Bound for the Worst Case.**\n4117: \n4118: At this worst-case configuration:\n4119: - For $X$ with mean $\\mu_X = V_{\\max}$, the two-point extremal distribution degenerates to a deterministic distribution: $X = V_{\\max}$ with probability 1. Thus:\n4120: \n4121: $$\n4122: \\mathbb{E}[\\ln(X)] = \\ln(V_{\\max})\n4123: $$\n4124: \n4125: - For $Y$, the extremal case (maximum expected log) is deterministic: $Y = \\mu_Y = V_{\\max} - \\kappa$. Thus:\n4126: \n4127: $$\n4128: \\mathbb{E}[\\ln(Y)] = \\ln(V_{\\max} - \\kappa)\n4129: $$\n4130: \n4131: The worst-case lower bound is:\n4132: \n4133: $$\n4134: \\ln(V_{\\max}) - \\ln(V_{\\max} - \\kappa) = \\ln\\left(\\frac{V_{\\max}}{V_{\\max} - \\kappa}\\right) = \\ln\\left(1 + \\frac{\\kappa}{V_{\\max} - \\kappa}\\right)\n4135: $$\n4136: \n4137: **Step 5: Simplification to the Stated Bound.**\n4138: \n4139: The tight bound from Step 4 is $\\ln(1 + \\kappa/(V_{\\max} - \\kappa))$. The lemma states the slightly looser but simpler bound $\\ln(1 + \\kappa/V_{\\max})$.\n4140: \n4141: To verify this is valid, note that for $\\kappa < V_{\\max}$:\n4142: \n4143: $$\n4144: \\frac{\\kappa}{V_{\\max}} < \\frac{\\kappa}{V_{\\max} - \\kappa}\n4145: $$\n4146: \n4147: Since $\\ln(1+t)$ is strictly increasing in $t$:\n4148: \n4149: $$\n4150: \\ln\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right) < \\ln\\left(1 + \\frac{\\kappa}{V_{\\max} - \\kappa}\\right)\n4151: $$\n4152: \n4153: Therefore, $\\ln(1 + \\kappa/V_{\\max})$ is a valid (conservative) lower bound that is simpler to use in subsequent analysis.\n4154: ",
      "strategy_summary": "The proof leverages extremal distributions for the concave logarithm function to establish tight lower bounds on E[ln(X)] - E[ln(Y)] under mean constraints, reduces the problem to minimizing a one-dimensional convex function over valid \u03bc_Y, identifies the minimum at the boundary \u03bc_Y = V_max - \u03ba, and simplifies to the stated conservative bound.",
      "conclusion": {
        "text": "For random variables X, Y supported on [V_min, V_max] with E[X] \u2265 E[Y] + \u03ba, we have E[ln(X)] - E[ln(Y)] \u2265 ln(1 + \u03ba / V_max).",
        "latex": "\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\ge \\ln\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right)"
      },
      "assumptions": [
        {
          "text": "X and Y are random variables taking values in the interval [V_min, V_max].",
          "latex": "X, Y \\in [V_{\\min}, V_{\\max}]"
        },
        {
          "text": "The means satisfy E[X] \u2265 E[Y] + \u03ba with \u03ba > 0.",
          "latex": "\\mathbb{E}[X] \\ge \\mathbb{E}[Y] + \\kappa"
        },
        {
          "text": "V_min > 0 to ensure the logarithm is defined.",
          "latex": "V_{\\min} > 0"
        },
        {
          "text": "\u03ba < V_max to ensure feasible means.",
          "latex": "\\kappa < V_{\\max}"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "explanation",
          "text": "Recall that for the strictly concave function ln(t), the minimum E[ln(X)] for fixed mean \u03bc_X is achieved by a two-point distribution on {V_min, V_max}, and the maximum E[ln(Y)] for fixed mean \u03bc_Y is achieved by the deterministic Y = \u03bc_Y via Jensen's inequality.",
          "latex": null,
          "references": [],
          "derived_statement": "Extremal distributions: min E[ln(X)] via two-point, max E[ln(Y)] = ln(\u03bc_Y)."
        },
        {
          "order": 2.0,
          "kind": "bounding",
          "text": "The difference E[ln(X)] - E[ln(Y)] is at least E[ln(X_min)] - ln(\u03bc_Y), where X_min is the two-point distribution with mean \u03bc_X and probabilities P(X_min = V_min) = (V_max - \u03bc_X)/(V_max - V_min), P(X_min = V_max) = (\u03bc_X - V_min)/(V_max - V_min).",
          "latex": "P(X_{\\min} = V_{\\min}) = \\frac{V_{\\max} - \\mu_X}{V_{\\max} - V_{\\min}}, \\quad P(X_{\\min} = V_{\\max}) = \\frac{\\mu_X - V_{\\min}}{V_{\\max} - V_{\\min}}",
          "references": [],
          "derived_statement": "Lower bound on difference using extremals."
        },
        {
          "order": 3.0,
          "kind": "reduction",
          "text": "Minimize E[ln(X_min)] - ln(\u03bc_Y) over \u03bc_X \u2265 \u03bc_Y + \u03ba, \u03bc_X, \u03bc_Y \u2208 [V_min, V_max]. Since E[ln(X_min)] increases in \u03bc_X, set \u03bc_X = \u03bc_Y + \u03ba, reducing to min h(\u03bc_Y) = E[ln(X_min, \u03bc_Y + \u03ba)] - ln(\u03bc_Y) for \u03bc_Y \u2208 [V_min, V_max - \u03ba].",
          "latex": null,
          "references": [],
          "derived_statement": "One-dimensional optimization on boundary."
        },
        {
          "order": 4.0,
          "kind": "analysis",
          "text": "Show h(\u03bc_Y) is strictly convex: E[ln(X_min, \u03bc)] = C_0 + C_1 \u03bc with C_1 > 0, so h(\u03bc_Y) = linear in \u03bc_Y minus ln(\u03bc_Y), hence strictly convex. Thus, minimum at endpoint \u03bc_Y = V_min or V_max - \u03ba.",
          "latex": "\\mathbb{E}[\\ln(X_{\\min,\\mu})] = \\ln(V_{\\max}) + \\frac{V_{\\max} - \\mu}{V_{\\max} - V_{\\min}}(\\ln(V_{\\min}) - \\ln(V_{\\max}))",
          "references": [],
          "derived_statement": "h(\u03bc_Y) strictly convex, min at endpoint."
        },
        {
          "order": 5.0,
          "kind": "evaluation",
          "text": "The minimum occurs at \u03bc_Y = V_max - \u03ba (due to flattening of log at higher values), with \u03bc_X = V_max. Here, both extremals degenerate to deterministic: E[ln(X)] = ln(V_max), E[ln(Y)] = ln(V_max - \u03ba), so difference ln(V_max / (V_max - \u03ba)) = ln(1 + \u03ba/(V_max - \u03ba)).",
          "latex": "\\ln\\left(1 + \\frac{\\kappa}{V_{\\max} - \\kappa}\\right)",
          "references": [],
          "derived_statement": "Tight bound at right endpoint."
        },
        {
          "order": 6.0,
          "kind": "simplification",
          "text": "The lemma uses the looser bound ln(1 + \u03ba/V_max), which is valid since \u03ba/V_max < \u03ba/(V_max - \u03ba) and ln(1 + \u00b7) is increasing.",
          "latex": "\\ln\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right) < \\ln\\left(1 + \\frac{\\kappa}{V_{\\max} - \\kappa}\\right)",
          "references": [],
          "derived_statement": "Conservative simplification to stated bound."
        }
      ],
      "key_equations": [
        {
          "label": "eq-two-point-probs",
          "latex": "P(X_{\\min} = V_{\\min}) = \\frac{V_{\\max} - \\mu_X}{V_{\\max} - V_{\\min}}, \\quad P(X_{\\min} = V_{\\max}) = \\frac{\\mu_X - V_{\\min}}{V_{\\max} - V_{\\min}}",
          "role": "Defines the extremal two-point distribution for min E[ln(X)]."
        },
        {
          "label": "eq-e-log-xmin",
          "latex": "\\mathbb{E}[\\ln(X_{\\min,\\mu})] = \\ln(V_{\\max}) + \\frac{V_{\\max} - \\mu}{V_{\\max} - V_{\\min}}(\\ln(V_{\\min}) - \\ln(V_{\\max}))",
          "role": "Explicit linear form of E[ln(X_min)] in \u03bc, used to show convexity of h."
        },
        {
          "label": "eq-h-muy",
          "latex": "h(\\mu_Y) = [C_0 + C_1(\\mu_Y + \\kappa)] - \\ln(\\mu_Y)",
          "role": "Reduced objective function for minimization."
        },
        {
          "label": "eq-tight-bound",
          "latex": "\\ln\\left(\\frac{V_{\\max}}{V_{\\max} - \\kappa}\\right) = \\ln\\left(1 + \\frac{\\kappa}{V_{\\max} - \\kappa}\\right)",
          "role": "Tight worst-case lower bound at boundary."
        },
        {
          "label": "eq-loose-bound",
          "latex": "\\ln\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right)",
          "role": "Simplified conservative lower bound stated in the lemma."
        }
      ],
      "references": [],
      "math_tools": [
        {
          "toolName": "Jensen's Inequality",
          "field": "Convex Analysis",
          "description": "For a concave function f, the expectation satisfies E[f(X)] \u2264 f(E[X]), with equality if X is constant.",
          "roleInProof": "Used to show that the maximum of E[ln(Y)] for fixed mean \u03bc_Y is achieved by the deterministic distribution Y = \u03bc_Y.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Concavity"
          ]
        },
        {
          "toolName": "Extremal Distributions",
          "field": "Probability Theory",
          "description": "Distributions that achieve the minimum or maximum expectation of a concave or convex function over random variables with fixed mean and support constraints.",
          "roleInProof": "Applied to identify the two-point distribution minimizing E[ln(X)] for fixed mean \u03bc_X on [V_min, V_max].",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Jensen's Inequality",
            "Two-Point Distributions"
          ]
        },
        {
          "toolName": "Two-Point Distributions",
          "field": "Probability Theory",
          "description": "Distributions supported on exactly two points that extremize linear functionals or expectations under moment constraints.",
          "roleInProof": "Specifically used as the minimizer for E[ln(X)] due to the strict concavity of the logarithm.",
          "levelOfAbstraction": "Notation",
          "relatedTools": [
            "Extremal Distributions"
          ]
        },
        {
          "toolName": "Convex Functions",
          "field": "Real Analysis",
          "description": "A function h is convex if for all x, y and \u03bb \u2208 [0,1], h(\u03bbx + (1-\u03bb)y) \u2264 \u03bb h(x) + (1-\u03bb) h(y); minima on intervals occur at endpoints for strictly convex functions.",
          "roleInProof": "Demonstrates that the reduced function h(\u03bc_Y) is strictly convex, so its minimum is at an endpoint of [V_min, V_max - \u03ba].",
          "levelOfAbstraction": "Concept",
          "relatedTools": []
        }
      ],
      "cases": [
        {
          "name": "Endpoint \u03bc_Y = V_min",
          "condition": "\u03bc_Y = V_min, \u03bc_X = V_min + \u03ba",
          "summary": "Evaluated but not the minimum; logarithmic gap larger due to steeper log at low values."
        },
        {
          "name": "Endpoint \u03bc_Y = V_max - \u03ba",
          "condition": "\u03bc_Y = V_max - \u03ba, \u03bc_X = V_max",
          "summary": "Achieves the minimum difference ln(1 + \u03ba/(V_max - \u03ba)) due to flatter log at high values."
        }
      ],
      "remarks": [
        {
          "type": "insight",
          "text": "The worst-case occurs at high values because the derivative of ln(t) decreases, making the gap smaller for fixed mean difference \u03ba."
        },
        {
          "type": "simplification",
          "text": "The bound ln(1 + \u03ba/V_max) is looser but analytically simpler, valid since \u03ba/(V_max - \u03ba) > \u03ba/V_max."
        }
      ],
      "gaps": [],
      "tags": [
        "extremal distributions",
        "concave functions",
        "Jensen inequality",
        "two-point distribution",
        "convex optimization",
        "lower bound",
        "logarithmic gap",
        "probability bounds"
      ],
      "document_id": "03_cloning",
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "span": {
        "start_line": 4045,
        "end_line": 4154,
        "content_start": 4048,
        "content_end": 4153,
        "header_lines": [
          4046
        ]
      },
      "metadata": {
        "label": "proof-lem-log-gap-lower-bound"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "log-gap",
      "lower-bound",
      "random-variables",
      "mean-separation",
      "extremal-distributions",
      "compact-support"
    ],
    "content_markdown": ":label: lem-log-gap-lower-bound\n\nLet $X$ and $Y$ be two random variables whose values are contained in the compact interval $[V_{\\min}, V_{\\max}]$, where $V_{\\min} > 0$. Let their means, $\\mu_X = \\mathbb{E}[X]$ and $\\mu_Y = \\mathbb{E}[Y]$, satisfy $\\mu_X \\ge \\mu_Y + \\kappa$ for some constant $\\kappa > 0$.\n\nThen the difference of their expected logarithms is bounded below as follows:\n\n$$\n\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\ge \\ln\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right)",
    "raw_directive": "4032: This lemma establishes that a guaranteed separation in the means of two distributions implies a guaranteed, non-vanishing separation in their expected logarithms. The proof uses extremal distribution theory to find the worst-case scenario.\n4033: \n4034: :::{prf:lemma} Lower Bound on Logarithmic Mean Gap\n4035: :label: lem-log-gap-lower-bound\n4036: \n4037: Let $X$ and $Y$ be two random variables whose values are contained in the compact interval $[V_{\\min}, V_{\\max}]$, where $V_{\\min} > 0$. Let their means, $\\mu_X = \\mathbb{E}[X]$ and $\\mu_Y = \\mathbb{E}[Y]$, satisfy $\\mu_X \\ge \\mu_Y + \\kappa$ for some constant $\\kappa > 0$.\n4038: \n4039: Then the difference of their expected logarithms is bounded below as follows:\n4040: \n4041: $$\n4042: \\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\ge \\ln\\left(1 + \\frac{\\kappa}{V_{\\max}}\\right)\n4043: $$",
    "document_id": "03_cloning",
    "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
    "span": {
      "start_line": 4032,
      "end_line": 4043,
      "content_start": 4035,
      "content_end": 4042,
      "header_lines": [
        4033
      ]
    },
    "references": [],
    "metadata": {
      "label": "lem-log-gap-lower-bound"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 7,
      "chapter_file": "chapter_7.json",
      "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-log-gap-upper-bound",
    "title": "Upper Bound on Logarithmic Mean Gap",
    "type": "lemma",
    "nl_statement": "For random variables X and Y supported on the compact interval [V_min, V_max] with V_min > 0 and means satisfying |E[X] - E[Y]| \u2264 \u03ba for some \u03ba > 0, the absolute difference of their expected logarithms is at most ln(1 + \u03ba / V_min).",
    "equations": [
      {
        "label": null,
        "latex": "|\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]| \\le \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)"
      }
    ],
    "hypotheses": [
      {
        "text": "X and Y are random variables whose values are contained in the compact interval [V_min, V_max], where V_min > 0.",
        "latex": null
      },
      {
        "text": "Their means \u03bc_X = E[X] and \u03bc_Y = E[Y] satisfy |\u03bc_X - \u03bc_Y| \u2264 \u03ba for some constant \u03ba > 0.",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "|E[ln(X)] - E[ln(Y)]| \u2264 ln(1 + \u03ba / V_min)",
      "latex": "|\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]| \\le \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)"
    },
    "variables": [
      {
        "symbol": "X",
        "name": "X",
        "description": "random variable with values in [V_min, V_max]",
        "constraints": [
          "values in [V_min, V_max]"
        ],
        "tags": [
          "random variable"
        ]
      },
      {
        "symbol": "Y",
        "name": "Y",
        "description": "random variable with values in [V_min, V_max]",
        "constraints": [
          "values in [V_min, V_max]"
        ],
        "tags": [
          "random variable"
        ]
      },
      {
        "symbol": "\\mu_X",
        "name": "\u03bc_X",
        "description": "expected value of X",
        "constraints": [],
        "tags": [
          "mean"
        ]
      },
      {
        "symbol": "\\mu_Y",
        "name": "\u03bc_Y",
        "description": "expected value of Y",
        "constraints": [],
        "tags": [
          "mean"
        ]
      },
      {
        "symbol": "V_{\\min}",
        "name": "V_min",
        "description": "minimum value of the support interval, positive",
        "constraints": [
          "V_min > 0"
        ],
        "tags": [
          "lower bound"
        ]
      },
      {
        "symbol": "V_{\\max}",
        "name": "V_max",
        "description": "maximum value of the support interval",
        "constraints": [],
        "tags": [
          "upper bound"
        ]
      },
      {
        "symbol": "\\kappa",
        "name": "\u03ba",
        "description": "constant bounding the absolute difference of means",
        "constraints": [
          "\u03ba > 0"
        ],
        "tags": [
          "constant",
          "bound"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The expectations E[X], E[Y], E[ln(X)], and E[ln(Y)] exist (guaranteed by compact support).",
        "confidence": 1.0
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-log-gap-upper-bound",
      "title": null,
      "type": "proof",
      "proves": "lem-log-gap-upper-bound",
      "proof_type": "variational",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-log-gap-upper-bound\n\n**Proof.**\n\nThe proof uses extremal distribution theory to find the configuration that maximizes the logarithmic gap. By symmetry, it suffices to bound the one-sided difference $\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]$; the bound on the absolute value follows immediately.\n\n**Step 1: Extremal Distributions for the Logarithm.**\n\nFor the concave function $f(t) = \\ln(t)$:\n- To **maximize** $\\mathbb{E}[\\ln(X)]$ for a fixed mean $\\mu_X$: use a **deterministic distribution** $X = \\mu_X$. By Jensen's inequality, $\\mathbb{E}[\\ln(X)] \\le \\ln(\\mu_X)$, with equality achieved when $X$ is deterministic.\n- To **minimize** $\\mathbb{E}[\\ln(Y)]$ for a fixed mean $\\mu_Y$: use a **two-point distribution** $Y_{\\min}$ with mass only at the endpoints $\\{V_{\\min}, V_{\\max}\\}$. This is the extremal distribution for concave functions.\n\n**Step 2: Bounding the Difference Using Extremal Cases.**\n\nThe difference $\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]$ is maximized when:\n- $\\mathbb{E}[\\ln(X)]$ is as large as possible for mean $\\mu_X$ \u2192 Use deterministic $X = \\mu_X$\n- $\\mathbb{E}[\\ln(Y)]$ is as small as possible for mean $\\mu_Y$ \u2192 Use extremal two-point distribution $Y_{\\min}$\n\nTherefore, for any distributions $X$, $Y$ with means $\\mu_X$, $\\mu_Y$:\n\n$$\n\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\le \\ln(\\mu_X) - \\mathbb{E}[\\ln(Y_{\\min})]\n$$\n\nwhere $Y_{\\min}$ has probability masses:\n\n$$\nP(Y_{\\min} = V_{\\min}) = \\frac{V_{\\max} - \\mu_Y}{V_{\\max} - V_{\\min}}, \\quad P(Y_{\\min} = V_{\\max}) = \\frac{\\mu_Y - V_{\\min}}{V_{\\max} - V_{\\min}}\n$$\n\nThe expected logarithm of $Y_{\\min}$ is:\n\n$$\n\\mathbb{E}[\\ln(Y_{\\min})] = \\frac{V_{\\max} - \\mu_Y}{V_{\\max} - V_{\\min}} \\ln(V_{\\min}) + \\frac{\\mu_Y - V_{\\min}}{V_{\\max} - V_{\\min}} \\ln(V_{\\max})\n$$\n\n**Step 3: Finding the Worst-Case Mean Configuration.**\n\nWe now maximize $\\ln(\\mu_X) - \\mathbb{E}[\\ln(Y_{\\min})]$ over all valid pairs $(\\mu_X, \\mu_Y)$ satisfying:\n- $|\\mu_X - \\mu_Y| \\le \\kappa$\n- $\\mu_X, \\mu_Y \\in [V_{\\min}, V_{\\max}]$\n\n**Key insight:** The logarithm function has the steepest slope (greatest curvature) near $V_{\\min}$. Therefore, the gap $\\ln(\\mu_X) - \\mathbb{E}[\\ln(Y_{\\min})]$ is maximized when both means are located at the bottom of the allowable range.\n\nWithout loss of generality, assume $\\mu_X \\ge \\mu_Y$ (by symmetry). The constraint $|\\mu_X - \\mu_Y| \\le \\kappa$ allows $\\mu_X = \\mu_Y + \\kappa$.\n\nThe worst-case configuration is:\n- $\\mu_Y = V_{\\min}$ (minimum possible value)\n- $\\mu_X = V_{\\min} + \\kappa$ (maximum separation at the bottom of the range)\n\n**Step 4: Computing the Upper Bound for the Worst Case.**\n\nWith $\\mu_Y = V_{\\min}$, the extremal two-point distribution $Y_{\\min}$ degenerates to a **deterministic distribution** with all mass at $V_{\\min}$:\n\n$$\nP(Y_{\\min} = V_{\\min}) = 1\n$$\n\nTherefore:\n\n$$\n\\mathbb{E}[\\ln(Y_{\\min})] = \\ln(V_{\\min})\n$$\n\nFor $X$ deterministic at $\\mu_X = V_{\\min} + \\kappa$:\n\n$$\n\\ln(\\mu_X) = \\ln(V_{\\min} + \\kappa)\n$$\n\nThe worst-case upper bound is:\n\n$$\n\\ln(V_{\\min} + \\kappa) - \\ln(V_{\\min}) = \\ln\\left(\\frac{V_{\\min} + \\kappa}{V_{\\min}}\\right) = \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)\n$$\n\n**Step 5: Extension to the Absolute Value.**\n\nBy symmetry (swapping the roles of $X$ and $Y$), the bound also holds for $\\mathbb{E}[\\ln(Y)] - \\mathbb{E}[\\ln(X)]$. Therefore:\n\n$$\n|\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]| \\le \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)\n$$\n\nThis completes the proof.",
      "raw_directive": "4186: :::\n4187: \n4188: :::{prf:proof}\n4189: :label: proof-lem-log-gap-upper-bound\n4190: \n4191: **Proof.**\n4192: \n4193: The proof uses extremal distribution theory to find the configuration that maximizes the logarithmic gap. By symmetry, it suffices to bound the one-sided difference $\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]$; the bound on the absolute value follows immediately.\n4194: \n4195: **Step 1: Extremal Distributions for the Logarithm.**\n4196: \n4197: For the concave function $f(t) = \\ln(t)$:\n4198: - To **maximize** $\\mathbb{E}[\\ln(X)]$ for a fixed mean $\\mu_X$: use a **deterministic distribution** $X = \\mu_X$. By Jensen's inequality, $\\mathbb{E}[\\ln(X)] \\le \\ln(\\mu_X)$, with equality achieved when $X$ is deterministic.\n4199: - To **minimize** $\\mathbb{E}[\\ln(Y)]$ for a fixed mean $\\mu_Y$: use a **two-point distribution** $Y_{\\min}$ with mass only at the endpoints $\\{V_{\\min}, V_{\\max}\\}$. This is the extremal distribution for concave functions.\n4200: \n4201: **Step 2: Bounding the Difference Using Extremal Cases.**\n4202: \n4203: The difference $\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]$ is maximized when:\n4204: - $\\mathbb{E}[\\ln(X)]$ is as large as possible for mean $\\mu_X$ \u2192 Use deterministic $X = \\mu_X$\n4205: - $\\mathbb{E}[\\ln(Y)]$ is as small as possible for mean $\\mu_Y$ \u2192 Use extremal two-point distribution $Y_{\\min}$\n4206: \n4207: Therefore, for any distributions $X$, $Y$ with means $\\mu_X$, $\\mu_Y$:\n4208: \n4209: $$\n4210: \\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\le \\ln(\\mu_X) - \\mathbb{E}[\\ln(Y_{\\min})]\n4211: $$\n4212: \n4213: where $Y_{\\min}$ has probability masses:\n4214: \n4215: $$\n4216: P(Y_{\\min} = V_{\\min}) = \\frac{V_{\\max} - \\mu_Y}{V_{\\max} - V_{\\min}}, \\quad P(Y_{\\min} = V_{\\max}) = \\frac{\\mu_Y - V_{\\min}}{V_{\\max} - V_{\\min}}\n4217: $$\n4218: \n4219: The expected logarithm of $Y_{\\min}$ is:\n4220: \n4221: $$\n4222: \\mathbb{E}[\\ln(Y_{\\min})] = \\frac{V_{\\max} - \\mu_Y}{V_{\\max} - V_{\\min}} \\ln(V_{\\min}) + \\frac{\\mu_Y - V_{\\min}}{V_{\\max} - V_{\\min}} \\ln(V_{\\max})\n4223: $$\n4224: \n4225: **Step 3: Finding the Worst-Case Mean Configuration.**\n4226: \n4227: We now maximize $\\ln(\\mu_X) - \\mathbb{E}[\\ln(Y_{\\min})]$ over all valid pairs $(\\mu_X, \\mu_Y)$ satisfying:\n4228: - $|\\mu_X - \\mu_Y| \\le \\kappa$\n4229: - $\\mu_X, \\mu_Y \\in [V_{\\min}, V_{\\max}]$\n4230: \n4231: **Key insight:** The logarithm function has the steepest slope (greatest curvature) near $V_{\\min}$. Therefore, the gap $\\ln(\\mu_X) - \\mathbb{E}[\\ln(Y_{\\min})]$ is maximized when both means are located at the bottom of the allowable range.\n4232: \n4233: Without loss of generality, assume $\\mu_X \\ge \\mu_Y$ (by symmetry). The constraint $|\\mu_X - \\mu_Y| \\le \\kappa$ allows $\\mu_X = \\mu_Y + \\kappa$.\n4234: \n4235: The worst-case configuration is:\n4236: - $\\mu_Y = V_{\\min}$ (minimum possible value)\n4237: - $\\mu_X = V_{\\min} + \\kappa$ (maximum separation at the bottom of the range)\n4238: \n4239: **Step 4: Computing the Upper Bound for the Worst Case.**\n4240: \n4241: With $\\mu_Y = V_{\\min}$, the extremal two-point distribution $Y_{\\min}$ degenerates to a **deterministic distribution** with all mass at $V_{\\min}$:\n4242: \n4243: $$\n4244: P(Y_{\\min} = V_{\\min}) = 1\n4245: $$\n4246: \n4247: Therefore:\n4248: \n4249: $$\n4250: \\mathbb{E}[\\ln(Y_{\\min})] = \\ln(V_{\\min})\n4251: $$\n4252: \n4253: For $X$ deterministic at $\\mu_X = V_{\\min} + \\kappa$:\n4254: \n4255: $$\n4256: \\ln(\\mu_X) = \\ln(V_{\\min} + \\kappa)\n4257: $$\n4258: \n4259: The worst-case upper bound is:\n4260: \n4261: $$\n4262: \\ln(V_{\\min} + \\kappa) - \\ln(V_{\\min}) = \\ln\\left(\\frac{V_{\\min} + \\kappa}{V_{\\min}}\\right) = \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)\n4263: $$\n4264: \n4265: **Step 5: Extension to the Absolute Value.**\n4266: \n4267: By symmetry (swapping the roles of $X$ and $Y$), the bound also holds for $\\mathbb{E}[\\ln(Y)] - \\mathbb{E}[\\ln(X)]$. Therefore:\n4268: \n4269: $$\n4270: |\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]| \\le \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)\n4271: $$\n4272: \n4273: This completes the proof.\n4274: ",
      "strategy_summary": "The proof maximizes the difference E[ln(X)] - E[ln(Y)] by using Jensen's inequality for the maximum (deterministic X) and extremal two-point distributions for the minimum (Y), then optimizes over means mu_X and mu_Y under the constraint |mu_X - mu_Y| <= kappa, finding the worst case at the lower support boundary.",
      "conclusion": {
        "text": "|E[ln(X)] - E[ln(Y)]| <= ln(1 + kappa / V_min)",
        "latex": "|\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]| \\le \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)"
      },
      "assumptions": [
        {
          "text": "X and Y are random variables with support in [V_min, V_max]",
          "latex": "X, Y \\in [V_{\\min}, V_{\\max}]"
        },
        {
          "text": "The means satisfy |mu_X - mu_Y| <= kappa",
          "latex": "|\\mu_X - \\mu_Y| \\le \\kappa"
        },
        {
          "text": "mu_X, mu_Y in [V_min, V_max]",
          "latex": "\\mu_X, \\mu_Y \\in [V_{\\min}, V_{\\max}]"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "explanation",
          "text": "Use extremal distribution theory for the concave function ln(t). To maximize E[ln(X)] for fixed mu_X, use deterministic X = mu_X by Jensen's inequality. To minimize E[ln(Y)] for fixed mu_Y, use two-point distribution Y_min on {V_min, V_max}.",
          "latex": null,
          "references": [],
          "derived_statement": "E[ln(X)] <= ln(mu_X); E[ln(Y)] >= E[ln(Y_min)]"
        },
        {
          "order": 2.0,
          "kind": "bounding",
          "text": "The difference E[ln(X)] - E[ln(Y)] <= ln(mu_X) - E[ln(Y_min)], where Y_min has masses P(Y_min = V_min) = (V_max - mu_Y)/(V_max - V_min), P(Y_min = V_max) = (mu_Y - V_min)/(V_max - V_min).",
          "latex": null,
          "references": [],
          "derived_statement": "E[ln(X)] - E[ln(Y)] <= ln(mu_X) - [(V_max - mu_Y)/(V_max - V_min) ln(V_min) + (mu_Y - V_min)/(V_max - V_min) ln(V_max)]"
        },
        {
          "order": 3.0,
          "kind": "optimization",
          "text": "Maximize ln(mu_X) - E[ln(Y_min)] over mu_X, mu_Y in [V_min, V_max] with |mu_X - mu_Y| <= kappa. The maximum occurs at mu_Y = V_min, mu_X = V_min + kappa, due to the curvature of ln near V_min.",
          "latex": null,
          "references": [],
          "derived_statement": "Worst-case: mu_Y = V_min, mu_X = V_min + kappa"
        },
        {
          "order": 4.0,
          "kind": "computation",
          "text": "For mu_Y = V_min, Y_min is deterministic at V_min, so E[ln(Y_min)] = ln(V_min). For X deterministic at mu_X, ln(mu_X) = ln(V_min + kappa). Thus, the bound is ln((V_min + kappa)/V_min) = ln(1 + kappa/V_min).",
          "latex": null,
          "references": [],
          "derived_statement": "ln(1 + kappa/V_min)"
        },
        {
          "order": 5.0,
          "kind": "extension",
          "text": "By symmetry, the bound holds for the reverse difference, yielding the absolute value bound.",
          "latex": null,
          "references": [],
          "derived_statement": "|E[ln(X)] - E[ln(Y)]| <= ln(1 + kappa/V_min)"
        }
      ],
      "key_equations": [
        {
          "label": "eq-y-min-masses",
          "latex": "P(Y_{\\min} = V_{\\min}) = \\frac{V_{\\max} - \\mu_Y}{V_{\\max} - V_{\\min}}, \\quad P(Y_{\\min} = V_{\\max}) = \\frac{\\mu_Y - V_{\\min}}{V_{\\max} - V_{\\min}}",
          "role": "Probabilities for the extremal two-point distribution"
        },
        {
          "label": "eq-e-ln-y-min",
          "latex": "\\mathbb{E}[\\ln(Y_{\\min})] = \\frac{V_{\\max} - \\mu_Y}{V_{\\max} - V_{\\min}} \\ln(V_{\\min}) + \\frac{\\mu_Y - V_{\\min}}{V_{\\max} - V_{\\min}} \\ln(V_{\\max})",
          "role": "Expectation of ln under extremal distribution"
        },
        {
          "label": "eq-upper-bound",
          "latex": "\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)] \\le \\ln(\\mu_X) - \\mathbb{E}[\\ln(Y_{\\min})]",
          "role": "Key inequality from extremal cases"
        },
        {
          "label": "eq-final-bound",
          "latex": "\\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)",
          "role": "Computed worst-case upper bound"
        }
      ],
      "references": [],
      "math_tools": [
        {
          "toolName": "Jensen's Inequality",
          "field": "Convex Analysis",
          "description": "For a concave function f, the expectation E[f(X)] is at most f(E[X]), with equality for deterministic X.",
          "roleInProof": "Bounds the maximum of E[ln(X)] by ln(mu_X) for concave ln.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Concavity"
          ]
        },
        {
          "toolName": "Extremal Distributions",
          "field": "Probability Theory",
          "description": "For concave objectives, extremal expectations under fixed mean and support constraints are achieved at distributions with mass on boundaries, often two-point.",
          "roleInProof": "Identifies the minimizing distribution for E[ln(Y)] as a two-point mass on V_min and V_max.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Jensen's Inequality",
            "Moment Constraints"
          ]
        },
        {
          "toolName": "Symmetry Argument",
          "field": "Mathematical Analysis",
          "description": "Exploiting symmetry in the problem to reduce to one-sided cases and extend bounds to absolute values.",
          "roleInProof": "Simplifies to bounding one-sided difference and mirrors for the absolute gap.",
          "levelOfAbstraction": "Technique",
          "relatedTools": []
        }
      ],
      "cases": [
        {
          "name": "Worst-case configuration",
          "condition": "mu_Y = V_min, mu_X = V_min + kappa",
          "summary": "The extremal two-point for Y degenerates to deterministic at V_min, maximizing the gap."
        },
        {
          "name": "Symmetric case",
          "condition": "mu_X = V_min, mu_Y = V_min + kappa",
          "summary": "Mirrors the bound for E[ln(Y)] - E[ln(X)]."
        }
      ],
      "remarks": [
        {
          "type": "insight",
          "text": "The steepest curvature of ln near V_min drives the worst case to the lower boundary."
        },
        {
          "type": "symmetry",
          "text": "The one-sided bound extends to absolute value by swapping X and Y."
        }
      ],
      "gaps": [],
      "tags": [
        "extremal distributions",
        "Jensen's inequality",
        "concave functions",
        "logarithmic gap",
        "two-point distribution",
        "symmetry",
        "variational optimization"
      ],
      "document_id": "03_cloning",
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "span": {
        "start_line": 4186,
        "end_line": 4274,
        "content_start": 4189,
        "content_end": 4273,
        "header_lines": [
          4187
        ]
      },
      "metadata": {
        "label": "proof-lem-log-gap-upper-bound"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "upper bound",
      "logarithmic gap",
      "random variables",
      "mean difference",
      "compact support"
    ],
    "content_markdown": ":label: lem-log-gap-upper-bound\n\nLet $X$ and $Y$ be two random variables whose values are contained in the compact interval $[V_{\\min}, V_{\\max}]$, where $V_{\\min} > 0$. Let their means, $\\mu_X = \\mathbb{E}[X]$ and $\\mu_Y = \\mathbb{E}[Y]$, satisfy $|\\mu_X - \\mu_Y| \\le \\kappa$ for some constant $\\kappa > 0$.\n\nThen the absolute difference of their expected logarithms is bounded above as follows:\n\n$$\n|\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]| \\le \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)",
    "raw_directive": "4173: This lemma provides a symmetric result, capping the maximum possible logarithmic gap. This is essential for bounding the influence of the potentially adversarial reward signal, ensuring it cannot overwhelm the corrective diversity signal.\n4174: \n4175: :::{prf:lemma} Upper Bound on Logarithmic Mean Gap\n4176: :label: lem-log-gap-upper-bound\n4177: \n4178: Let $X$ and $Y$ be two random variables whose values are contained in the compact interval $[V_{\\min}, V_{\\max}]$, where $V_{\\min} > 0$. Let their means, $\\mu_X = \\mathbb{E}[X]$ and $\\mu_Y = \\mathbb{E}[Y]$, satisfy $|\\mu_X - \\mu_Y| \\le \\kappa$ for some constant $\\kappa > 0$.\n4179: \n4180: Then the absolute difference of their expected logarithms is bounded above as follows:\n4181: \n4182: $$\n4183: |\\mathbb{E}[\\ln(X)] - \\mathbb{E}[\\ln(Y)]| \\le \\ln\\left(1 + \\frac{\\kappa}{V_{\\min}}\\right)\n4184: $$",
    "document_id": "03_cloning",
    "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
    "span": {
      "start_line": 4173,
      "end_line": 4184,
      "content_start": 4176,
      "content_end": 4183,
      "header_lines": [
        4174
      ]
    },
    "references": [],
    "metadata": {
      "label": "lem-log-gap-upper-bound"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 7,
      "chapter_file": "chapter_7.json",
      "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-unfit-fraction-lower-bound",
    "title": "N-Uniform Lower Bound on the Unfit Fraction",
    "type": "lemma",
    "nl_statement": "For a swarm of k \u2265 2 alive walkers whose fitness potential range is bounded below by the positive \u03b5-dependent constant \u03ba_{V,gap}(\u03b5), the fraction of alive walkers in the unfit set U_k is bounded below by the positive N-uniform \u03b5-dependent constant f_U(\u03b5) = \u03ba_{V,gap}(\u03b5) / (2(V_{pot,max} - V_{pot,min})).",
    "equations": [
      {
        "label": null,
        "latex": "V_{\\max,k} - V_{\\min,k} \\ge \\kappa_{V,\\text{gap}}(\\epsilon) > 0"
      },
      {
        "label": null,
        "latex": "\\frac{|U_k|}{k} \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} - V_{\\text{pot,min}})} =: f_U(\\epsilon)"
      }
    ],
    "hypotheses": [
      {
        "text": "Swarm of k \u2265 2 alive walkers with fitness potential range bounded below by positive \u03b5-dependent constant",
        "latex": "V_{\\max,k} - V_{\\min,k} \\ge \\kappa_{V,\\text{gap}}(\\epsilon) > 0"
      }
    ],
    "conclusion": {
      "text": "Fraction of alive walkers in unfit set U_k bounded below by positive N-uniform \u03b5-dependent constant f_U(\u03b5) > 0",
      "latex": "\\frac{|U_k|}{k} \\ge f_U(\\epsilon) = \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} - V_{\\text{pot,min}})}"
    },
    "variables": [
      {
        "symbol": "k",
        "name": "swarm size",
        "description": "Number of alive walkers in the swarm",
        "constraints": [
          "k \\ge 2"
        ],
        "tags": [
          "swarm",
          "alive-walkers"
        ]
      },
      {
        "symbol": "\\epsilon",
        "name": "epsilon",
        "description": "Small positive parameter for \u03b5-dependence",
        "constraints": [],
        "tags": [
          "parameter",
          "dependence"
        ]
      },
      {
        "symbol": "U_k",
        "name": "unfit set",
        "description": "Set of unfit walkers in the swarm",
        "constraints": [],
        "tags": [
          "unfit",
          "set"
        ]
      },
      {
        "symbol": "V_{\\max,k}",
        "name": "max fitness potential",
        "description": "Maximum fitness potential in swarm k",
        "constraints": [],
        "tags": [
          "fitness",
          "potential",
          "max"
        ]
      },
      {
        "symbol": "V_{\\min,k}",
        "name": "min fitness potential",
        "description": "Minimum fitness potential in swarm k",
        "constraints": [],
        "tags": [
          "fitness",
          "potential",
          "min"
        ]
      },
      {
        "symbol": "\\kappa_{V,\\text{gap}}(\\epsilon)",
        "name": "fitness gap constant",
        "description": "Positive \u03b5-dependent lower bound on potential range",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "gap",
          "constant"
        ]
      },
      {
        "symbol": "f_U(\\epsilon)",
        "name": "unfit fraction constant",
        "description": "Positive N-uniform \u03b5-dependent lower bound on unfit fraction",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "fraction",
          "lower-bound"
        ]
      },
      {
        "symbol": "V_{\\text{pot,max}}",
        "name": "global max potential",
        "description": "Global maximum fitness potential",
        "constraints": [],
        "tags": [
          "global",
          "potential",
          "max"
        ]
      },
      {
        "symbol": "V_{\\text{pot,min}}",
        "name": "global min potential",
        "description": "Global minimum fitness potential",
        "constraints": [],
        "tags": [
          "global",
          "potential",
          "min"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Fitness potentials are real-valued and bounded globally by V_{pot,max} and V_{pot,min}",
        "confidence": 0.9
      },
      {
        "text": "The unfit set U_k is a subset of the alive walkers",
        "confidence": 1.0
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-unfit-fraction-lower-bound",
      "title": null,
      "type": "proof",
      "proves": "lem-unfit-fraction-lower-bound",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-unfit-fraction-lower-bound\n\n**Proof.**\n\nThe proof establishes the bound by analyzing the balance of deviations from the mean fitness, a fundamental statistical property.\n\n1.  **The Principle of Balanced Deviations:** By the definition of the mean $\\mu_{V,k}$, the sum of all deviations from the mean is zero. Let $F_k$ be the \"fit\" set (the complement of $U_k$). Partitioning the sum over these two sets shows that the total positive deviation equals the magnitude of the total negative deviation:\n\n\n$$\n\\sum_{i \\in F_k} (V_{k,i} - \\mu_{V,k}) = \\sum_{j \\in U_k} (\\mu_{V,k} - V_{k,j})\n$$\n\n2.  **Bounding the Total Deviation:** The total range of fitness values, $V_{\\max,k} - V_{\\min,k}$, can be partitioned at the mean: $V_{\\max,k} - V_{\\min,k} = (V_{\\max,k} - \\mu_{V,k}) + (\\mu_{V,k} - V_{\\min,k})$. Since both terms on the right are non-negative, at least one of them must be greater than or equal to half of the total range. Using the premise, we have:\n\n\n$$\n\\max\\left( (V_{\\max,k} - \\mu_{V,k}), (\\mu_{V,k} - V_{\\min,k}) \\right) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}\n$$\n\n3.  **Case Analysis:**\n    *   **Case A:** If $(V_{\\max,k} - \\mu_{V,k}) \\ge \\kappa_{V,\\text{gap}}(\\epsilon)/2$. The sum of positive deviations, $\\sum_{i \\in F_k} (V_{k,i} - \\mu_{V,k})$, must be at least this large. By the balance of deviations, the sum of negative deviations must also satisfy this bound. We can then bound this sum by its size, $|U_k|$, multiplied by the maximum possible value for any single term, which is bounded by the total potential range $V_{\\text{pot,max}} - V_{\\text{pot,min}}$:\n\n\n$$\n|U_k| \\cdot (V_{\\text{pot,max}} - V_{\\text{pot,min}}) \\ge \\sum_{j \\in U_k} (\\mu_{V,k} - V_{k,j}) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}\n$$\n\n        This directly yields the desired lower bound on $|U_k|$.\n\n    *   **Case B:** If $(\\mu_{V,k} - V_{\\min,k}) \\ge \\kappa_{V,\\text{gap}}(\\epsilon)/2$. By a symmetric argument, the sum of negative deviations is at least this large. This implies the sum of positive deviations is also this large. Bounding the sum of positive deviations:\n\n\n$$\n|F_k| \\cdot (V_{\\text{pot,max}} - V_{\\text{pot,min}}) \\ge \\sum_{i \\in F_k} (V_{i,k} - \\mu_{V,k}) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}\n$$\n\n        This gives a lower bound on the size of the *fit* set: $|F_k|/k \\ge \\kappa_{V,\\text{gap}} / (2(V_{\\text{pot,max}} - V_{\\text{pot,min}}))$. Since $|U_k| + |F_k| = k$, this implies an *upper bound* on the size of the unfit set. However, a simpler argument is to note that if the unfit set were vanishingly small, the mean would be pulled towards $V_{\\max,k}$, making $(\\mu_{V,k} - V_{\\min,k})$ large and contradicting the mean's location. The bound from Case A thus represents the worst-case scenario for the size of the unfit set, providing a valid global lower bound.\n\n4.  **Conclusion:** Rearranging the inequality from Step 3 and dividing by `k` gives the final result for the fraction of unfit walkers. The lower bound, $f_U(\\epsilon)$, is a strictly positive, N-uniform, and $\\varepsilon$-dependent constant, as it is constructed from other N-uniform constants.",
      "raw_directive": "4583: where $V_{\\text{pot,max}}$ and $V_{\\text{pot,min}}$ are the N-uniform bounds on the fitness potential from [](#lem-potential-bounds).\n4584: :::\n4585: :::{prf:proof}\n4586: :label: proof-lem-unfit-fraction-lower-bound\n4587: \n4588: **Proof.**\n4589: \n4590: The proof establishes the bound by analyzing the balance of deviations from the mean fitness, a fundamental statistical property.\n4591: \n4592: 1.  **The Principle of Balanced Deviations:** By the definition of the mean $\\mu_{V,k}$, the sum of all deviations from the mean is zero. Let $F_k$ be the \"fit\" set (the complement of $U_k$). Partitioning the sum over these two sets shows that the total positive deviation equals the magnitude of the total negative deviation:\n4593: \n4594: \n4595: $$\n4596: \\sum_{i \\in F_k} (V_{k,i} - \\mu_{V,k}) = \\sum_{j \\in U_k} (\\mu_{V,k} - V_{k,j})\n4597: $$\n4598: \n4599: 2.  **Bounding the Total Deviation:** The total range of fitness values, $V_{\\max,k} - V_{\\min,k}$, can be partitioned at the mean: $V_{\\max,k} - V_{\\min,k} = (V_{\\max,k} - \\mu_{V,k}) + (\\mu_{V,k} - V_{\\min,k})$. Since both terms on the right are non-negative, at least one of them must be greater than or equal to half of the total range. Using the premise, we have:\n4600: \n4601: \n4602: $$\n4603: \\max\\left( (V_{\\max,k} - \\mu_{V,k}), (\\mu_{V,k} - V_{\\min,k}) \\right) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}\n4604: $$\n4605: \n4606: 3.  **Case Analysis:**\n4607:     *   **Case A:** If $(V_{\\max,k} - \\mu_{V,k}) \\ge \\kappa_{V,\\text{gap}}(\\epsilon)/2$. The sum of positive deviations, $\\sum_{i \\in F_k} (V_{k,i} - \\mu_{V,k})$, must be at least this large. By the balance of deviations, the sum of negative deviations must also satisfy this bound. We can then bound this sum by its size, $|U_k|$, multiplied by the maximum possible value for any single term, which is bounded by the total potential range $V_{\\text{pot,max}} - V_{\\text{pot,min}}$:\n4608: \n4609: \n4610: $$\n4611: |U_k| \\cdot (V_{\\text{pot,max}} - V_{\\text{pot,min}}) \\ge \\sum_{j \\in U_k} (\\mu_{V,k} - V_{k,j}) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}\n4612: $$\n4613: \n4614:         This directly yields the desired lower bound on $|U_k|$.\n4615: \n4616:     *   **Case B:** If $(\\mu_{V,k} - V_{\\min,k}) \\ge \\kappa_{V,\\text{gap}}(\\epsilon)/2$. By a symmetric argument, the sum of negative deviations is at least this large. This implies the sum of positive deviations is also this large. Bounding the sum of positive deviations:\n4617: \n4618: \n4619: $$\n4620: |F_k| \\cdot (V_{\\text{pot,max}} - V_{\\text{pot,min}}) \\ge \\sum_{i \\in F_k} (V_{i,k} - \\mu_{V,k}) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}\n4621: $$\n4622: \n4623:         This gives a lower bound on the size of the *fit* set: $|F_k|/k \\ge \\kappa_{V,\\text{gap}} / (2(V_{\\text{pot,max}} - V_{\\text{pot,min}}))$. Since $|U_k| + |F_k| = k$, this implies an *upper bound* on the size of the unfit set. However, a simpler argument is to note that if the unfit set were vanishingly small, the mean would be pulled towards $V_{\\max,k}$, making $(\\mu_{V,k} - V_{\\min,k})$ large and contradicting the mean's location. The bound from Case A thus represents the worst-case scenario for the size of the unfit set, providing a valid global lower bound.\n4624: \n4625: 4.  **Conclusion:** Rearranging the inequality from Step 3 and dividing by `k` gives the final result for the fraction of unfit walkers. The lower bound, $f_U(\\epsilon)$, is a strictly positive, N-uniform, and $\\varepsilon$-dependent constant, as it is constructed from other N-uniform constants.\n4626: ",
      "strategy_summary": "The proof leverages the zero-sum property of deviations from the mean fitness to equate positive and negative deviations, then uses case analysis on the mean's position within the fitness range to derive a lower bound on the unfit set size via uniform potential bounds.",
      "conclusion": {
        "text": "The fraction of unfit walkers satisfies |U_k|/k \u2265 f_U(\u03b5), where f_U(\u03b5) = \u03ba_{V,gap}(\u03b5) / (2 (V_{pot,max} - V_{pot,min})) is a strictly positive, N-uniform, and \u03b5-dependent constant.",
        "latex": "$\\frac{|U_k|}{k} \\ge f_U(\\epsilon) = \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} - V_{\\text{pot,min}})}$"
      },
      "assumptions": [
        {
          "text": "V_{pot,max} and V_{pot,min} are N-uniform bounds on the fitness potential from Lemma on potential bounds.",
          "latex": "$V_{\\text{pot,max}}$ and $V_{\\text{pot,min}}$ are the N-uniform bounds on the fitness potential from [](#lem-potential-bounds)."
        },
        {
          "text": "\u03ba_{V,gap}(\u03b5) is an \u03b5-dependent gap in the fitness range, with V_{max,k} - V_{min,k} \u2265 \u03ba_{V,gap}(\u03b5).",
          "latex": "$V_{\\max,k} - V_{\\min,k} \\ge \\kappa_{V,\\text{gap}}(\\epsilon)$"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "principle",
          "text": "By the definition of the mean \u03bc_{V,k}, the sum of all deviations from the mean is zero. Let F_k be the fit set (complement of U_k). Partitioning the sum over these two sets shows that the total positive deviation equals the magnitude of the total negative deviation.",
          "latex": "\\sum_{i \\in F_k} (V_{k,i} - \\mu_{V,k}) = \\sum_{j \\in U_k} (\\mu_{V,k} - V_{k,j})",
          "references": [],
          "derived_statement": "Balanced deviations principle."
        },
        {
          "order": 2.0,
          "kind": "bounding",
          "text": "The total range of fitness values, V_{max,k} - V_{min,k}, can be partitioned at the mean: V_{max,k} - V_{min,k} = (V_{max,k} - \u03bc_{V,k}) + (\u03bc_{V,k} - V_{min,k}). Since both terms are non-negative, at least one is \u2265 half the range. Using the premise, max((V_{max,k} - \u03bc_{V,k}), (\u03bc_{V,k} - V_{min,k})) \u2265 \u03ba_{V,gap}(\u03b5)/2.",
          "latex": "\\max\\left( (V_{\\max,k} - \\mu_{V,k}), (\\mu_{V,k} - V_{\\min,k}) \\right) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}",
          "references": [],
          "derived_statement": "Dominant deviation bound."
        },
        {
          "order": 3.0,
          "kind": "case",
          "text": "Case A: If (V_{max,k} - \u03bc_{V,k}) \u2265 \u03ba_{V,gap}(\u03b5)/2. The sum of positive deviations \u2265 this value, so sum of negative deviations \u2265 \u03ba_{V,gap}(\u03b5)/2. Bound by |U_k| times max deviation (V_{pot,max} - V_{pot,min}): |U_k| \u00b7 (V_{pot,max} - V_{pot,min}) \u2265 \u03ba_{V,gap}(\u03b5)/2.",
          "latex": "|U_k| \\cdot (V_{\\text{pot,max}} - V_{\\text{pot,min}}) \\ge \\sum_{j \\in U_k} (\\mu_{V,k} - V_{k,j}) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}",
          "references": [
            "lem-potential-bounds"
          ],
          "derived_statement": "Lower bound on |U_k| in Case A."
        },
        {
          "order": 4.0,
          "kind": "case",
          "text": "Case B: If (\u03bc_{V,k} - V_{min,k}) \u2265 \u03ba_{V,gap}(\u03b5)/2. Symmetric argument gives lower bound on |F_k|, implying upper bound on |U_k|. The Case A bound provides the worst-case lower bound for |U_k|.",
          "latex": "|F_k| \\cdot (V_{\\text{pot,max}} - V_{\\text{pot,min}}) \\ge \\sum_{i \\in F_k} (V_{i,k} - \\mu_{V,k}) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}",
          "references": [
            "lem-potential-bounds"
          ],
          "derived_statement": "Upper bound on |U_k| via |F_k| in Case B; Case A is tighter for lower bound."
        },
        {
          "order": 5.0,
          "kind": "conclusion",
          "text": "Rearranging the inequality from Case A and dividing by k gives |U_k|/k \u2265 \u03ba_{V,gap}(\u03b5) / (2 (V_{pot,max} - V_{pot,min})), a positive N-uniform constant f_U(\u03b5).",
          "latex": "\\frac{|U_k|}{k} \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} - V_{\\text{pot,min}})}",
          "references": [],
          "derived_statement": "Final fraction bound."
        }
      ],
      "key_equations": [
        {
          "label": "eq-balanced-deviations",
          "latex": "\\sum_{i \\in F_k} (V_{k,i} - \\mu_{V,k}) = \\sum_{j \\in U_k} (\\mu_{V,k} - V_{k,j})",
          "role": "Equates positive and negative deviations from the mean."
        },
        {
          "label": "eq-dominant-deviation",
          "latex": "\\max\\left( (V_{\\max,k} - \\mu_{V,k}), (\\mu_{V,k} - V_{\\min,k}) \\right) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}",
          "role": "Bounds the larger half of the fitness range relative to the mean."
        },
        {
          "label": "eq-unfit-bound",
          "latex": "|U_k| \\cdot (V_{\\text{pot,max}} - V_{\\text{pot,min}}) \\ge \\sum_{j \\in U_k} (\\mu_{V,k} - V_{k,j}) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}",
          "role": "Lower bounds the size of the unfit set in Case A."
        },
        {
          "label": "eq-fit-bound",
          "latex": "|F_k| \\cdot (V_{\\text{pot,max}} - V_{\\text{pot,min}}) \\ge \\sum_{i \\in F_k} (V_{i,k} - \\mu_{V,k}) \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2}",
          "role": "Lower bounds the fit set size in Case B."
        },
        {
          "label": "eq-final-fraction",
          "latex": "\\frac{|U_k|}{k} \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} - V_{\\text{pot,min}})}",
          "role": "Concludes the lower bound on the unfit fraction."
        }
      ],
      "references": [
        "lem-potential-bounds"
      ],
      "math_tools": [
        {
          "toolName": "Principle of Balanced Deviations",
          "field": "Statistics",
          "description": "The sum of deviations from the arithmetic mean over a set is zero, allowing partitioning into positive and negative components that balance each other.",
          "roleInProof": "Equates the total positive deviations in the fit set to the total negative deviations in the unfit set, enabling bounds on set sizes.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Arithmetic Mean"
          ]
        },
        {
          "toolName": "Case Analysis",
          "field": "Mathematical Proof Techniques",
          "description": "Dividing a proof into exhaustive cases based on conditions to handle different scenarios separately.",
          "roleInProof": "Handles the two possible dominant halves of the fitness range relative to the mean.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Max Function"
          ]
        },
        {
          "toolName": "Uniform Bounds",
          "field": "Analysis",
          "description": "Constants independent of certain parameters (e.g., dimension N) that bound quantities uniformly.",
          "roleInProof": "Provides N-uniform limits on potential range to ensure the lower bound is N-uniform.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Epsilon-Dependence"
          ]
        },
        {
          "toolName": "Arithmetic Mean",
          "field": "Statistics",
          "description": "The average value of a set of numbers, serving as a central tendency measure.",
          "roleInProof": "Defines the reference point \u03bc_{V,k} for deviations in the fitness values.",
          "levelOfAbstraction": "Notation",
          "relatedTools": [
            "Principle of Balanced Deviations"
          ]
        },
        {
          "toolName": "Max Function",
          "field": "Mathematical Analysis",
          "description": "The maximum of a set of values, used to capture the larger component.",
          "roleInProof": "Identifies the dominant deviation direction to bound the total deviation.",
          "levelOfAbstraction": "Notation",
          "relatedTools": [
            "Case Analysis"
          ]
        }
      ],
      "cases": [
        {
          "name": "Case A",
          "condition": "(V_{max,k} - \u03bc_{V,k}) \u2265 \u03ba_{V,gap}(\u03b5)/2",
          "summary": "Dominant positive deviation; directly bounds |U_k| via negative deviations and potential range."
        },
        {
          "name": "Case B",
          "condition": "(\u03bc_{V,k} - V_{min,k}) \u2265 \u03ba_{V,gap}(\u03b5)/2",
          "summary": "Dominant negative deviation; bounds |F_k|, implying |U_k| is not too large, but Case A gives the global lower bound."
        }
      ],
      "remarks": [
        {
          "type": "explanation",
          "text": "Case B provides an upper bound on the unfit set, but the proof relies on Case A for the worst-case lower bound, ensuring positivity of f_U(\u03b5)."
        },
        {
          "type": "uniformity",
          "text": "The bound f_U(\u03b5) inherits N-uniformity from the potential bounds in Lemma lem-potential-bounds and the gap function."
        }
      ],
      "gaps": [],
      "tags": [
        "balanced deviations",
        "mean fitness",
        "case analysis",
        "lower bound",
        "unfit fraction",
        "fitness potential"
      ],
      "document_id": "03_cloning",
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "span": {
        "start_line": 4583,
        "end_line": 4626,
        "content_start": 4586,
        "content_end": 4625,
        "header_lines": [
          4584
        ]
      },
      "metadata": {
        "label": "proof-lem-unfit-fraction-lower-bound"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "unfit-fraction",
      "lower-bound",
      "swarm",
      "walkers",
      "fitness-potential",
      "epsilon-dependent"
    ],
    "content_markdown": ":label: lem-unfit-fraction-lower-bound\n\nLet a swarm `k` with `k >= 2` alive walkers have a fitness potential range that is bounded below by a positive, $\\varepsilon$-dependent constant: $V_{\\max,k} - V_{\\min,k} \\ge \\kappa_{V,\\text{gap}}(\\epsilon) > 0$.\n\nThe fraction of alive walkers in the unfit set $U_k$ is bounded below by a positive, N-uniform, $\\varepsilon$-dependent constant $f_U(\\epsilon) > 0$:\n\n$$\n\\frac{|U_k|}{k} \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} - V_{\\text{pot,min}})} =: f_U(\\epsilon)\n$$",
    "raw_directive": "4570: :::\n4571: \n4572: :::{prf:lemma} N-Uniform Lower Bound on the Unfit Fraction\n4573: :label: lem-unfit-fraction-lower-bound\n4574: \n4575: Let a swarm `k` with `k >= 2` alive walkers have a fitness potential range that is bounded below by a positive, $\\varepsilon$-dependent constant: $V_{\\max,k} - V_{\\min,k} \\ge \\kappa_{V,\\text{gap}}(\\epsilon) > 0$.\n4576: \n4577: The fraction of alive walkers in the unfit set $U_k$ is bounded below by a positive, N-uniform, $\\varepsilon$-dependent constant $f_U(\\epsilon) > 0$:\n4578: \n4579: $$\n4580: \\frac{|U_k|}{k} \\ge \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} - V_{\\text{pot,min}})} =: f_U(\\epsilon)\n4581: $$\n4582: ",
    "document_id": "03_cloning",
    "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
    "span": {
      "start_line": 4570,
      "end_line": 4582,
      "content_start": 4573,
      "content_end": 4581,
      "header_lines": [
        4571
      ]
    },
    "references": [
      "lem-potential-bounds"
    ],
    "metadata": {
      "label": "lem-unfit-fraction-lower-bound"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 7,
      "chapter_file": "chapter_7.json",
      "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-quantitative-keystone",
    "title": "The N-Uniform Quantitative Keystone Lemma",
    "type": "lemma",
    "nl_statement": "Under foundational axioms, there exist constants independent of N such that for any pair of swarms, the averaged weighted squared error from both swarms lower bounds the structural variance minus a constant.",
    "equations": [
      {
        "label": null,
        "latex": "\\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\||^2 \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)"
      }
    ],
    "hypotheses": [
      {
        "text": "Foundational axioms from Chapter 4 hold",
        "latex": null
      },
      {
        "text": "For any pair of swarms (S_1, S_2)",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "There exist R^2_{spread} > 0, \\chi(\\epsilon) > 0, g_{max}(\\epsilon) \\ge 0 independent of N such that the inequality holds",
      "latex": "there exist: * a structural error threshold $R^2_{\\text{spread}} > 0$, * a minimum feedback coefficient $\\chi(\\epsilon) > 0$, * and a constant offset $g_{\\max}(\\epsilon) \\ge 0$, all of which may depend on $\\epsilon$ but are independent of $N$, such that for any pair of swarms $(S_1, S_2)$: $$ \\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\||^2 \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon) $$"
    },
    "variables": [
      {
        "symbol": "N",
        "name": "Number of agents",
        "description": "Dimension or size parameter, independent constants do not depend on it",
        "constraints": [
          "positive integer"
        ],
        "tags": [
          "dimension",
          "size",
          "uniform"
        ]
      },
      {
        "symbol": "\\\\epsilon",
        "name": "Error parameter",
        "description": "Small error tolerance parameter",
        "constraints": [
          "0 < \\\\epsilon < 1"
        ],
        "tags": [
          "error",
          "tolerance",
          "feedback"
        ]
      },
      {
        "symbol": "p_{1,i}",
        "name": "Cloning probability swarm 1",
        "description": "Cloning probability for agent i in swarm 1",
        "constraints": [
          "0 \\\\le p_{1,i} \\\\le 1"
        ],
        "tags": [
          "cloning",
          "probability",
          "swarm"
        ]
      },
      {
        "symbol": "p_{2,i}",
        "name": "Cloning probability swarm 2",
        "description": "Cloning probability for agent i in swarm 2",
        "constraints": [
          "0 \\\\le p_{2,i} \\\\le 1"
        ],
        "tags": [
          "cloning",
          "probability",
          "swarm"
        ]
      },
      {
        "symbol": "\\\\Delta \\\\delta_{x,i}",
        "name": "Error deviation",
        "description": "Delta in position deviation for agent i",
        "constraints": [
          "vector in R^d"
        ],
        "tags": [
          "error",
          "deviation",
          "position"
        ]
      },
      {
        "symbol": "V_{struct}",
        "name": "Structural variance",
        "description": "Measure of structural error or spread",
        "constraints": [
          "non-negative"
        ],
        "tags": [
          "variance",
          "structural",
          "error"
        ]
      },
      {
        "symbol": "R^2_{spread}",
        "name": "Structural error threshold",
        "description": "Threshold for structural spread",
        "constraints": [
          "positive"
        ],
        "tags": [
          "threshold",
          "spread",
          "error"
        ]
      },
      {
        "symbol": "\\\\chi(\\\\epsilon)",
        "name": "Feedback coefficient",
        "description": "Minimum feedback strength depending on epsilon",
        "constraints": [
          "positive"
        ],
        "tags": [
          "feedback",
          "coefficient",
          "minimum"
        ]
      },
      {
        "symbol": "g_{max}(\\\\epsilon)",
        "name": "Constant offset",
        "description": "Maximum growth or offset term depending on epsilon",
        "constraints": [
          "non-negative"
        ],
        "tags": [
          "offset",
          "constant",
          "bound"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Cloning pressure from one swarm suffices even if the other is in low-error state",
        "confidence": 0.9
      },
      {
        "text": "I_{11} represents a relevant index set for high-error agents",
        "confidence": 0.8
      },
      {
        "text": "Norms and sums are well-defined under the axioms",
        "confidence": 1.0
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-prop-n-uniformity-keystone-addendum",
      "title": null,
      "type": "proof",
      "proves": "lem-quantitative-keystone",
      "proof_type": "construction",
      "proof_status": "complete",
      "content_markdown": ":::{prf:proof}\n:label: proof-prop-n-uniformity-keystone-addendum\n**Proof of the N-Uniform Quantitative Keystone Lemma ({prf:ref}`lem-quantitative-keystone`).**\n\nThe proof establishes the inequality for the high-error regime ($V_{\\text{struct}} > R^2_{\\text{spread}}$) and then defines the global offset $g_{\\max}(\\epsilon)$ to ensure it holds everywhere, as per the strategy outlined in Section 8.1.\n\n**1. Setup for the High-Error Regime.**\nAssume the initial state $(S_1, S_2)$ is in the high-error regime. Without loss of generality, let swarm $k=1$ be the high-variance swarm. This guarantees the existence of a non-empty **critical target set** $I_{\\text{target}} = I_{11} \\cap U_1 \\cap H_1(\\epsilon)$. We seek a lower bound for the error-weighted cloning activity, $E_w$:\n\n$$\nE_w := \\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2\n$$\n\n**2. Lower-Bound the Sum by the Critical Target Set.**\nThe sum $E_w$ consists of non-negative terms and is bounded below by the sum over the critical target set $I_{\\text{target}} \\subseteq I_{11}$:\n\n$$\nE_w \\ge \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2\n$$\n\nWe focus on the cloning probability `p_1,i` because swarm 1 is the high-variance swarm for which our guarantees on the unfit and high-error sets hold.\n\n**3. Decompose the Sum using Average Properties.**\nInstead of factoring out the minimum probability, we use a standard statistical decomposition. Let\n\n$$\n\\bar{p}_{target} = \\frac{1}{|I_{target}|}\\sum_{i \\in I_{target}} p_{1,i}\n$$\n\n be the average cloning probability over the target set, and\n\n$$\n\\bar{E}_{target} = \\frac{1}{|I_{target}|}\\sum_{i \\in I_{target}} \\|\\Delta\\delta_{x,i}\\|^2\n$$\n\n be the average error. The sum can be written as:\n\n$$\n\\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2 = |I_{target}| \\left( \\bar{p}_{target} \\cdot \\bar{E}_{target} + \\text{Cov}(p_{1,i}, \\|\\Delta\\delta_{x,i}\\|^2) \\right)\n$$\n\nwhere `Cov` is the covariance between the cloning probability and the error within the target set. We can establish a lower bound by using the average properties and bounding the covariance term.\n\nThe covariance term can be negative if walkers with larger errors happen to have smaller cloning probabilities. However, we can establish a robust lower bound by noting that $p_{1,i} \\geq 0$ for all `i`. We can use the lower bound on the *average* probability.\n\nLet's use a simpler, more direct argument. The sum is bounded below by the sum where each `p_{1,i}` is replaced by its lower bound. From **{prf:ref}`lem-mean-companion-fitness-gap` (`lem-unfit-cloning-pressure`)**, every walker $i \\in U_k$ (and therefore every walker in `I_target`) has a probability $p_{1,i} \\geq p_u(\\varepsilon)$. This allows us to use the minimum probability correctly.\n\n$$\nE_w \\ge \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2 \\ge \\frac{p_u(\\epsilon)}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2\n$$\n\n**4. Substitute the Error Concentration Bound.**\nWe now have an expression that is the product of two N-uniform lower bounds.\n*   **Cloning Pressure:** The term $p_u(\\varepsilon)$ is the N-uniform minimum cloning probability from **{prf:ref}`lem-mean-companion-fitness-gap`**.\n*   **Error Concentration:** The term\n\n$$\n\\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2\n$$\n\n is exactly the quantity lower-bounded by the **Error Concentration Lemma (8.4.1)**.\n\nSubstituting the bound from {prf:ref}`lem-variance-concentration-Hk` gives:\n\n$$\nE_w \\ge p_u(\\epsilon) \\cdot \\left( c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon) \\right)\n$$\n\n**5. Define N-Uniform Constants for the High-Error Regime.**\nSubstituting these two bounds into the inequality from Step 3 gives:\n\n$$\nE_w \\ge p_u(\\epsilon) \\cdot \\left( c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon) \\right)\n$$\n\nWe define the N-uniform, $\\varepsilon$-dependent constants that emerge from this constructive proof:\n*   The **feedback coefficient:** $\\chi(\\epsilon) := p_u(\\epsilon) \\cdot c_{err}(\\epsilon) > 0$\n*   The **partial offset:** $g_{\\text{partial}}(\\epsilon) := p_u(\\epsilon) \\cdot g_{err}(\\epsilon) \\ge 0$\n\nThis establishes the desired linear lower bound for any state in the high-error regime:\n\n$$\nE_w \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\text{partial}}(\\epsilon)\n$$\n\n**6. Finalize the Global Inequality.**\nAs outlined in the proof strategy (Section 8.1), we define the global offset constant $g_{\\max}(\\epsilon)$ to ensure the inequality holds for all states by taking the maximum of the offsets required for the low-error and high-error regimes:\n\n$$\ng_{\\max}(\\epsilon) := \\max\\bigl(g_{\\text{partial}}(\\epsilon),\\, \\chi(\\epsilon) R^2_{\\text{spread}}\\bigr)\n$$\n\nThis choice ensures the inequality is satisfied everywhere. Since $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are constructed entirely from N-uniform constants, they are themselves independent of $N$.\n\nThis completes the rigorous, constructive proof of the N-Uniform Quantitative Keystone Lemma.",
      "raw_directive": "5280: We now assemble these results to provide the final, rigorous proof of the main theorem of this analysis. The strategy is to show that the large error concentrated in the target set, when weighted by the strong average cloning probability of that same set, produces a collective corrective force that is proportional to the total system error.\n5281: \n5282: :::{prf:proof}\n5283: :label: proof-prop-n-uniformity-keystone-addendum\n5284: **Proof of the N-Uniform Quantitative Keystone Lemma ({prf:ref}`lem-quantitative-keystone`).**\n5285: \n5286: The proof establishes the inequality for the high-error regime ($V_{\\text{struct}} > R^2_{\\text{spread}}$) and then defines the global offset $g_{\\max}(\\epsilon)$ to ensure it holds everywhere, as per the strategy outlined in Section 8.1.\n5287: \n5288: **1. Setup for the High-Error Regime.**\n5289: Assume the initial state $(S_1, S_2)$ is in the high-error regime. Without loss of generality, let swarm $k=1$ be the high-variance swarm. This guarantees the existence of a non-empty **critical target set** $I_{\\text{target}} = I_{11} \\cap U_1 \\cap H_1(\\epsilon)$. We seek a lower bound for the error-weighted cloning activity, $E_w$:\n5290: \n5291: $$\n5292: E_w := \\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2\n5293: $$\n5294: \n5295: **2. Lower-Bound the Sum by the Critical Target Set.**\n5296: The sum $E_w$ consists of non-negative terms and is bounded below by the sum over the critical target set $I_{\\text{target}} \\subseteq I_{11}$:\n5297: \n5298: $$\n5299: E_w \\ge \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2\n5300: $$\n5301: \n5302: We focus on the cloning probability `p_1,i` because swarm 1 is the high-variance swarm for which our guarantees on the unfit and high-error sets hold.\n5303: \n5304: **3. Decompose the Sum using Average Properties.**\n5305: Instead of factoring out the minimum probability, we use a standard statistical decomposition. Let\n5306: \n5307: $$\n5308: \\bar{p}_{target} = \\frac{1}{|I_{target}|}\\sum_{i \\in I_{target}} p_{1,i}\n5309: $$\n5310: \n5311:  be the average cloning probability over the target set, and\n5312: \n5313: $$\n5314: \\bar{E}_{target} = \\frac{1}{|I_{target}|}\\sum_{i \\in I_{target}} \\|\\Delta\\delta_{x,i}\\|^2\n5315: $$\n5316: \n5317:  be the average error. The sum can be written as:\n5318: \n5319: $$\n5320: \\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2 = |I_{target}| \\left( \\bar{p}_{target} \\cdot \\bar{E}_{target} + \\text{Cov}(p_{1,i}, \\|\\Delta\\delta_{x,i}\\|^2) \\right)\n5321: $$\n5322: \n5323: where `Cov` is the covariance between the cloning probability and the error within the target set. We can establish a lower bound by using the average properties and bounding the covariance term.\n5324: \n5325: The covariance term can be negative if walkers with larger errors happen to have smaller cloning probabilities. However, we can establish a robust lower bound by noting that $p_{1,i} \\geq 0$ for all `i`. We can use the lower bound on the *average* probability.\n5326: \n5327: Let's use a simpler, more direct argument. The sum is bounded below by the sum where each `p_{1,i}` is replaced by its lower bound. From **{prf:ref}`lem-mean-companion-fitness-gap` (`lem-unfit-cloning-pressure`)**, every walker $i \\in U_k$ (and therefore every walker in `I_target`) has a probability $p_{1,i} \\geq p_u(\\varepsilon)$. This allows us to use the minimum probability correctly.\n5328: \n5329: $$\n5330: E_w \\ge \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2 \\ge \\frac{p_u(\\epsilon)}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2\n5331: $$\n5332: \n5333: **4. Substitute the Error Concentration Bound.**\n5334: We now have an expression that is the product of two N-uniform lower bounds.\n5335: *   **Cloning Pressure:** The term $p_u(\\varepsilon)$ is the N-uniform minimum cloning probability from **{prf:ref}`lem-mean-companion-fitness-gap`**.\n5336: *   **Error Concentration:** The term\n5337: \n5338: $$\n5339: \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2\n5340: $$\n5341: \n5342:  is exactly the quantity lower-bounded by the **Error Concentration Lemma (8.4.1)**.\n5343: \n5344: Substituting the bound from {prf:ref}`lem-variance-concentration-Hk` gives:\n5345: \n5346: $$\n5347: E_w \\ge p_u(\\epsilon) \\cdot \\left( c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon) \\right)\n5348: $$\n5349: \n5350: **5. Define N-Uniform Constants for the High-Error Regime.**\n5351: Substituting these two bounds into the inequality from Step 3 gives:\n5352: \n5353: $$\n5354: E_w \\ge p_u(\\epsilon) \\cdot \\left( c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon) \\right)\n5355: $$\n5356: \n5357: We define the N-uniform, $\\varepsilon$-dependent constants that emerge from this constructive proof:\n5358: *   The **feedback coefficient:** $\\chi(\\epsilon) := p_u(\\epsilon) \\cdot c_{err}(\\epsilon) > 0$\n5359: *   The **partial offset:** $g_{\\text{partial}}(\\epsilon) := p_u(\\epsilon) \\cdot g_{err}(\\epsilon) \\ge 0$\n5360: \n5361: This establishes the desired linear lower bound for any state in the high-error regime:\n5362: \n5363: $$\n5364: E_w \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\text{partial}}(\\epsilon)\n5365: $$\n5366: \n5367: **6. Finalize the Global Inequality.**\n5368: As outlined in the proof strategy (Section 8.1), we define the global offset constant $g_{\\max}(\\epsilon)$ to ensure the inequality holds for all states by taking the maximum of the offsets required for the low-error and high-error regimes:\n5369: \n5370: $$\n5371: g_{\\max}(\\epsilon) := \\max\\bigl(g_{\\text{partial}}(\\epsilon),\\, \\chi(\\epsilon) R^2_{\\text{spread}}\\bigr)\n5372: $$\n5373: \n5374: This choice ensures the inequality is satisfied everywhere. Since $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are constructed entirely from N-uniform constants, they are themselves independent of $N$.\n5375: \n5376: This completes the rigorous, constructive proof of the N-Uniform Quantitative Keystone Lemma.\n5377: ",
      "strategy_summary": "The proof constructs a lower bound on the error-weighted cloning activity Ew in the high-error regime by leveraging N-uniform guarantees on minimum cloning probabilities in the critical target set and error concentration within that set, yielding a linear relation to the structural variance V_struct minus an offset. A global offset g_max(\u03b5) is then defined to extend this inequality uniformly across all regimes, ensuring N-independence of the constants.",
      "conclusion": {
        "text": "E_w \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon), where \\chi(\\epsilon) and g_{\\max}(\\epsilon) are N-uniform constants.",
        "latex": "E_w \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)"
      },
      "assumptions": [
        {
          "text": "High-error regime: V_{\\text{struct}} > R^2_{\\text{spread}}",
          "latex": "V_{\\text{struct}} > R^2_{\\text{spread}}"
        },
        {
          "text": "Swarm k=1 is the high-variance swarm",
          "latex": null
        },
        {
          "text": "Non-empty critical target set I_{\\text{target}} = I_{11} \\cap U_1 \\cap H_1(\\epsilon)",
          "latex": "I_{\\text{target}} = I_{11} \\cap U_1 \\cap H_1(\\epsilon)"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "setup",
          "text": "Assume the initial state (S_1, S_2) is in the high-error regime. Without loss of generality, let swarm k=1 be the high-variance swarm, guaranteeing a non-empty critical target set I_{\\text{target}}. Define E_w as the error-weighted cloning activity.",
          "latex": null,
          "references": [],
          "derived_statement": "E_w := \\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2"
        },
        {
          "order": 2.0,
          "kind": "bounding",
          "text": "Bound E_w below by the sum over the critical target set I_{\\text{target}} \\subseteq I_{11}, focusing on p_{1,i} since swarm 1 is high-variance.",
          "latex": null,
          "references": [],
          "derived_statement": "E_w \\ge \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2"
        },
        {
          "order": 3.0,
          "kind": "decomposition",
          "text": "Decompose the sum using averages \\bar{p}_{target} and \\bar{E}_{target}, including covariance, but simplify to a direct lower bound using the N-uniform minimum p_{1,i} \\ge p_u(\\epsilon) from the unfit cloning pressure lemma.",
          "latex": null,
          "references": [
            "lem-unfit-cloning-pressure"
          ],
          "derived_statement": "E_w \\ge \\frac{p_u(\\epsilon)}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2"
        },
        {
          "order": 4.0,
          "kind": "substitution",
          "text": "Substitute the error concentration bound from the lemma on H_k to relate the sum to V_{\\text{struct}}.",
          "latex": null,
          "references": [
            "lem-variance-concentration-Hk"
          ],
          "derived_statement": "E_w \\ge p_u(\\epsilon) \\cdot (c_{\\text{err}}(\\epsilon) V_{\\mathrm{struct}} - g_{\\text{err}}(\\epsilon))"
        },
        {
          "order": 5.0,
          "kind": "definition",
          "text": "Define N-uniform constants \\chi(\\epsilon) = p_u(\\epsilon) c_{\\text{err}}(\\epsilon) and g_{\\text{partial}}(\\epsilon) = p_u(\\epsilon) g_{\\text{err}}(\\epsilon) for the high-error regime.",
          "latex": null,
          "references": [],
          "derived_statement": "E_w \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\text{partial}}(\\epsilon)"
        },
        {
          "order": 6.0,
          "kind": "globalization",
          "text": "Define global offset g_{\\max}(\\epsilon) = \\max(g_{\\text{partial}}(\\epsilon), \\chi(\\epsilon) R^2_{\\text{spread}}) to extend the inequality to all states.",
          "latex": null,
          "references": [],
          "derived_statement": "E_w \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon) \\text{ for all states}"
        }
      ],
      "key_equations": [
        {
          "label": "eq-Ew",
          "latex": "E_w := \\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2",
          "role": "Definition of error-weighted cloning activity"
        },
        {
          "label": "eq-avg-p",
          "latex": "\\bar{p}_{\\text{target}} = \\frac{1}{|I_{\\text{target}}|}\\sum_{i \\in I_{\\text{target}}} p_{1,i}",
          "role": "Average cloning probability over target set"
        },
        {
          "label": "eq-avg-E",
          "latex": "\\bar{E}_{\\text{target}} = \\frac{1}{|I_{\\text{target}}|}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2",
          "role": "Average error over target set"
        },
        {
          "label": "eq-decomp",
          "latex": "\\sum_{i \\in I_{\\text{target}}} p_{1,i}\\|\\Delta\\delta_{x,i}\\|^2 = |I_{\\text{target}}| \\left( \\bar{p}_{\\text{target}} \\cdot \\bar{E}_{\\text{target}} + \\text{Cov}(p_{1,i}, \\|\\Delta\\delta_{x,i}\\|^2) \\right)",
          "role": "Statistical decomposition of the sum"
        },
        {
          "label": "eq-lower-pu",
          "latex": "E_w \\ge \\frac{p_u(\\epsilon)}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2",
          "role": "Lower bound using minimum cloning probability"
        },
        {
          "label": "eq-subst-err",
          "latex": "E_w \\ge p_u(\\epsilon) \\cdot (c_{\\text{err}}(\\epsilon)V_{\\mathrm{struct}} - g_{\\text{err}}(\\epsilon))",
          "role": "Substitution of error concentration bound"
        },
        {
          "label": "eq-chi-g",
          "latex": "E_w \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\text{partial}}(\\epsilon)",
          "role": "High-error regime inequality with defined constants"
        },
        {
          "label": "eq-gmax",
          "latex": "g_{\\max}(\\epsilon) := \\max\\bigl(g_{\\text{partial}}(\\epsilon),\\, \\chi(\\epsilon) R^2_{\\text{spread}}\\bigr)",
          "role": "Global offset definition"
        }
      ],
      "references": [
        "lem-quantitative-keystone",
        "lem-mean-companion-fitness-gap",
        "lem-variance-concentration-Hk",
        "lem-unfit-cloning-pressure"
      ],
      "math_tools": [
        {
          "toolName": "Lower Bound Inequality",
          "field": "Analysis",
          "description": "Technique for establishing minimum values of sums or expressions using non-negativity and minimum terms.",
          "roleInProof": "Applied to bound the weighted sum Ew by replacing cloning probabilities with their N-uniform minimum p_u(\u03b5) and using error concentration results.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Error Concentration"
          ]
        },
        {
          "toolName": "Statistical Decomposition",
          "field": "Statistics",
          "description": "Decomposition of sums into averages and covariance terms to analyze joint behavior of variables.",
          "roleInProof": "Used to express the sum over the target set in terms of average cloning probability, average error, and covariance, though simplified to direct bounding for robustness.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Covariance"
          ]
        },
        {
          "toolName": "Covariance",
          "field": "Statistics",
          "description": "Measure of the linear correlation between two variables, which can be positive or negative.",
          "roleInProof": "Appears in the decomposition but is bounded away by switching to a minimum probability argument to avoid negative impacts.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Statistical Decomposition"
          ]
        },
        {
          "toolName": "Error Concentration",
          "field": "Probability",
          "description": "Bounding the proportion of total error captured by a subset like the high-error set.",
          "roleInProof": "Directly substitutes the bound from the Error Concentration Lemma to link the target set error sum to V_struct.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Lower Bound Inequality"
          ]
        }
      ],
      "cases": [
        {
          "name": "High-Error Regime",
          "condition": "V_{\\text{struct}} > R^2_{\\text{spread}}",
          "summary": "Constructive lower bound on Ew using target set concentration and unfit cloning guarantees, yielding \\chi V_{\\text{struct}} - g_{\\text{partial}}"
        },
        {
          "name": "Global Extension",
          "condition": "All system states",
          "summary": "Define g_{\\max} to cover low-error regime by offsetting up to \\chi R^2_{\\text{spread}}, ensuring uniform inequality"
        }
      ],
      "remarks": [
        {
          "type": "construction",
          "text": "All constants \\chi(\\epsilon) and g_{\\max}(\\epsilon) are N-uniform and derived from prior lemmas, independent of system size N."
        },
        {
          "type": "simplification",
          "text": "Covariance decomposition is introduced but replaced by direct minimum bounding for robustness against negative covariance."
        }
      ],
      "gaps": [],
      "tags": [
        "n-uniform",
        "keystone-lemma",
        "high-error-regime",
        "error-concentration",
        "cloning-probability",
        "lower-bound",
        "constructive-proof"
      ],
      "document_id": "03_cloning",
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "span": {
        "start_line": 5280,
        "end_line": 5377,
        "content_start": 5282,
        "content_end": 5376,
        "header_lines": [
          5281
        ]
      },
      "metadata": {
        "label": "proof-prop-n-uniformity-keystone-addendum"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 8,
        "chapter_file": "chapter_8.json",
        "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "keystone",
      "quantitative",
      "uniform",
      "swarms",
      "error",
      "cloning",
      "drift",
      "feedback"
    ],
    "content_markdown": ":label: lem-quantitative-keystone\n\nUnder the foundational axioms laid out in Chapter 4, there exist:\n*   a structural error threshold $R^2_{\\text{spread}} > 0$,\n*   a minimum feedback coefficient $\\chi(\\epsilon) > 0$,\n*   and a constant offset $g_{\\max}(\\epsilon) \\ge 0$,\n\nall of which may depend on $\\epsilon$ but are independent of $N$, such that for any pair of swarms $(S_1, S_2)$:\n\n$$\n\\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\n$$",
    "raw_directive": "4712: We begin by formally stating the main theorem. This lemma provides the quantitative link between the system's error and its corrective response, which will be the primary tool for the drift analysis in the subsequent chapters. The lemma considers the summed cloning probability from both swarms, $(p_{1,i} + p_{2,i})$, to capture the total corrective pressure. The proof will demonstrate that even when only one swarm is in a high-error state, the cloning pressure from that single swarm is sufficient to ensure the inequality holds.\n4713: \n4714: :::{prf:lemma} The N-Uniform Quantitative Keystone Lemma\n4715: :label: lem-quantitative-keystone\n4716: \n4717: Under the foundational axioms laid out in Chapter 4, there exist:\n4718: *   a structural error threshold $R^2_{\\text{spread}} > 0$,\n4719: *   a minimum feedback coefficient $\\chi(\\epsilon) > 0$,\n4720: *   and a constant offset $g_{\\max}(\\epsilon) \\ge 0$,\n4721: \n4722: all of which may depend on $\\epsilon$ but are independent of $N$, such that for any pair of swarms $(S_1, S_2)$:\n4723: \n4724: $$\n4725: \\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\ge \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\n4726: $$\n4727: ",
    "document_id": "03_cloning",
    "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
    "span": {
      "start_line": 4712,
      "end_line": 4727,
      "content_start": 4715,
      "content_end": 4726,
      "header_lines": [
        4713
      ]
    },
    "references": [
      "lem-quantitative-keystone",
      "lem-mean-companion-fitness-gap",
      "lem-variance-concentration-Hk",
      "lem-unfit-cloning-pressure"
    ],
    "metadata": {
      "label": "lem-quantitative-keystone"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 8,
      "chapter_file": "chapter_8.json",
      "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-mean-companion-fitness-gap",
    "title": "Lower Bound on Mean Companion Fitness Gap",
    "type": "lemma",
    "nl_statement": "In a swarm of k \u2265 2 alive walkers with non-degenerate fitness potential range \u03ba_{V,gap}(\u03b5) > 0, partitioned into unfit and fit sets with positive fractions f_U and f_F, for any unfit walker i, the mean companion fitness exceeds its own fitness by at least (f_F/(k-1))(\u03bc_F - \u03bc_U) > 0, where \u03bc_F - \u03bc_U \u2265 (f_U/(f_F + f_U\u00b2/f_F)) \u03ba_{V,gap}(\u03b5), yielding an overall lower bound \u0394_min(\u03b5, f_U, f_F, k) > 0.",
    "equations": [
      {
        "label": null,
        "latex": "\\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} (\\mu_F - \\mu_U) > 0"
      },
      {
        "label": null,
        "latex": "\\mu_F - \\mu_U \\geq \\frac{f_U}{f_F + f_U^2/f_F} \\kappa_{V,\\text{gap}}(\\epsilon)"
      },
      {
        "label": null,
        "latex": "\\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon) =: \\Delta_{\\min}(\\epsilon, f_U, f_F, k)"
      }
    ],
    "hypotheses": [
      {
        "text": "Swarm of k \u2265 2 alive walkers with non-degenerate fitness potential range \u03ba_{V,gap}(\u03b5) > 0.",
        "latex": null
      },
      {
        "text": "Unfit set U_k and fit set F_k with fractions f_U = |U_k|/k > 0, f_F = |F_k|/k > 0, f_U + f_F = 1.",
        "latex": null
      },
      {
        "text": "Mean fitness values \u03bc_U for U_k and \u03bc_F for F_k.",
        "latex": null
      },
      {
        "text": "For any walker i \u2208 U_k.",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "The mean companion fitness gap for unfit walkers is bounded below by \u0394_min(\u03b5, f_U, f_F, k) > 0.",
      "latex": "\\mu_{\\text{comp},i} - V_{k,i} \\geq \\Delta_{\\min}(\\epsilon, f_U, f_F, k) > 0"
    },
    "variables": [
      {
        "symbol": "k",
        "name": "swarm size",
        "description": "Number of alive walkers, integer \u2265 2.",
        "constraints": [
          "k \u2265 2",
          "integer"
        ],
        "tags": [
          "swarm",
          "population"
        ]
      },
      {
        "symbol": "\u03b5",
        "name": "epsilon",
        "description": "Parameter for fitness potential range.",
        "constraints": [],
        "tags": [
          "parameter",
          "gap"
        ]
      },
      {
        "symbol": "\u03ba_{V,gap}(\u03b5)",
        "name": "fitness gap",
        "description": "Non-degenerate fitness potential range, >0.",
        "constraints": [
          "\u03ba_{V,gap}(\u03b5) > 0"
        ],
        "tags": [
          "fitness",
          "range"
        ]
      },
      {
        "symbol": "U_k",
        "name": "unfit set",
        "description": "Set of unfit walkers.",
        "constraints": [
          "|U_k| / k = f_U > 0"
        ],
        "tags": [
          "unfit",
          "set"
        ]
      },
      {
        "symbol": "F_k",
        "name": "fit set",
        "description": "Set of fit walkers.",
        "constraints": [
          "|F_k| / k = f_F > 0",
          "f_U + f_F = 1"
        ],
        "tags": [
          "fit",
          "set"
        ]
      },
      {
        "symbol": "f_U",
        "name": "unfit fraction",
        "description": "Population fraction of unfit walkers.",
        "constraints": [
          "f_U > 0"
        ],
        "tags": [
          "fraction",
          "unfit"
        ]
      },
      {
        "symbol": "f_F",
        "name": "fit fraction",
        "description": "Population fraction of fit walkers.",
        "constraints": [
          "f_F > 0"
        ],
        "tags": [
          "fraction",
          "fit"
        ]
      },
      {
        "symbol": "\u03bc_U",
        "name": "mean unfit fitness",
        "description": "Mean fitness of unfit set.",
        "constraints": [],
        "tags": [
          "mean",
          "fitness"
        ]
      },
      {
        "symbol": "\u03bc_F",
        "name": "mean fit fitness",
        "description": "Mean fitness of fit set.",
        "constraints": [],
        "tags": [
          "mean",
          "fitness"
        ]
      },
      {
        "symbol": "V_{k,i}",
        "name": "walker fitness",
        "description": "Fitness of walker i in swarm k.",
        "constraints": [
          "i \u2208 U_k"
        ],
        "tags": [
          "fitness",
          "walker"
        ]
      },
      {
        "symbol": "\u03bc_{comp,i}",
        "name": "mean companion fitness",
        "description": "Mean fitness of companions of walker i.",
        "constraints": [],
        "tags": [
          "mean",
          "companion"
        ]
      },
      {
        "symbol": "\u0394_min(\u03b5, f_U, f_F, k)",
        "name": "minimum gap",
        "description": "N-uniform lower bound on fitness gap.",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "lower bound",
          "gap"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Walkers have assigned fitness values V_{k,i}.",
        "confidence": 1.0
      },
      {
        "text": "Companions for walker i are the other k-1 walkers.",
        "confidence": 1.0
      },
      {
        "text": "Fitness potential range is defined and non-degenerate for the given \u03b5.",
        "confidence": 0.9
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-mean-companion-fitness-gap",
      "title": null,
      "type": "proof",
      "proves": "lem-mean-companion-fitness-gap",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-mean-companion-fitness-gap\n\n**Proof.**\n\nThe proof proceeds in three steps: (1) express the mean companion fitness algebraically, (2) bound it from below using population fractions, and (3) relate the inter-set mean difference to the fitness range.\n\n**Step 1: Algebraic Expression for Mean Companion Fitness**\n\nFor walker $i \\in U_k$, the set of potential companions is all alive walkers except $i$ itself: $\\{j \\in \\mathcal{A}_k : j \\neq i\\}$. The mean fitness of these companions is:\n\n$$\n\\mu_{\\text{comp},i} = \\frac{1}{k-1} \\sum_{j \\neq i} V_{k,j} = \\frac{1}{k-1} \\left( k \\mu_{V,k} - V_{k,i} \\right)\n$$\n\nwhere $\\mu_{V,k} = \\frac{1}{k} \\sum_{j \\in \\mathcal{A}_k} V_{k,j}$ is the mean fitness of all alive walkers.\n\nThe difference we seek to bound is:\n\n$$\n\\mu_{\\text{comp},i} - V_{k,i} = \\frac{k \\mu_{V,k} - V_{k,i}}{k-1} - V_{k,i} = \\frac{k \\mu_{V,k} - k V_{k,i}}{k-1} = \\frac{k}{k-1} (\\mu_{V,k} - V_{k,i})\n$$\n\n**Step 2: Bound on the Gap Using Population Structure**\n\nThe overall mean $\\mu_{V,k}$ can be decomposed using the partition into unfit and fit sets:\n\n$$\n\\mu_{V,k} = f_U \\mu_U + f_F \\mu_F\n$$\n\nwhere $\\mu_U := \\frac{1}{|U_k|} \\sum_{j \\in U_k} V_{k,j}$ and $\\mu_F := \\frac{1}{|F_k|} \\sum_{j \\in F_k} V_{k,j}$.\n\nFor any walker $i \\in U_k$, we have $V_{k,i} \\leq \\mu_U$ (by definition of the unfit set: $V_{k,i} \\leq \\mu_{V,k}$, and most unfit walkers have fitness at or below their group mean). In the worst case, assume $V_{k,i} = \\mu_U$. Then:\n\n$$\n\\mu_{V,k} - V_{k,i} \\geq \\mu_{V,k} - \\mu_U = f_U \\mu_U + f_F \\mu_F - \\mu_U = f_F (\\mu_F - \\mu_U)\n$$\n\nSubstituting into our expression from Step 1:\n\n$$\n\\mu_{\\text{comp},i} - V_{k,i} = \\frac{k}{k-1} (\\mu_{V,k} - V_{k,i}) \\geq \\frac{k}{k-1} \\cdot f_F (\\mu_F - \\mu_U)\n$$\n\nFor $k \\geq 2$, we have $\\frac{k}{k-1} \\geq 1$, so we obtain the conservative bound:\n\n$$\n\\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} (\\mu_F - \\mu_U)\n$$\n\nNote that $\\frac{1}{k-1}$ appears because we're averaging over $k-1$ companions, not $k$ walkers.\n\n**Step 3: Relating $\\mu_F - \\mu_U$ to the Fitness Range**\n\nBy definition of the fitness potential range:\n\n$$\n\\kappa_{V,\\text{gap}}(\\epsilon) := V_{\\max,k} - V_{\\min,k}\n$$\n\nThe means $\\mu_U$ and $\\mu_F$ satisfy:\n\n$$\nV_{\\min,k} \\leq \\mu_U \\leq \\mu_{V,k} \\leq \\mu_F \\leq V_{\\max,k}\n$$\n\nTo obtain a lower bound on $\\mu_F - \\mu_U$, we use the constraint that the overall mean is a weighted average. The maximum separation between group means occurs when one group is concentrated near the minimum and the other near the maximum. However, we must be more careful.\n\nConsider the sum of squared deviations from the overall mean:\n\n$$\nk \\cdot \\text{Var}_{V,k} = \\sum_{j \\in \\mathcal{A}_k} (V_{k,j} - \\mu_{V,k})^2 = \\sum_{j \\in U_k} (V_{k,j} - \\mu_{V,k})^2 + \\sum_{j \\in F_k} (V_{k,j} - \\mu_{V,k})^2\n$$\n\nUsing the decomposition of variance formula:\n\n$$\n\\text{Var}_{V,k} = f_U \\text{Var}_U + f_F \\text{Var}_F + f_U f_F (\\mu_U - \\mu_F)^2\n$$\n\nwhere $\\text{Var}_U$ and $\\text{Var}_F$ are the within-group variances. Since variances are non-negative:\n\n$$\n\\text{Var}_{V,k} \\geq f_U f_F (\\mu_F - \\mu_U)^2\n$$\n\nThe fitness range provides an upper bound on the variance:\n\n$$\n\\text{Var}_{V,k} \\leq \\frac{1}{4} \\kappa_{V,\\text{gap}}^2(\\epsilon)\n$$\n\n(This is the standard bound for bounded random variables: variance \\leq  (range/2)^2.)\n\nCombining these:\n\n$$\nf_U f_F (\\mu_F - \\mu_U)^2 \\leq \\text{Var}_{V,k} \\leq \\frac{1}{4} \\kappa_{V,\\text{gap}}^2(\\epsilon)\n$$\n\nFrom the variance inequality, we have established:\n\n$$\n\\mu_F - \\mu_U \\geq \\frac{1}{2} \\sqrt{\\frac{1}{f_U f_F}} \\kappa_{V,\\text{gap}}(\\epsilon)\n$$\n\nThis bound is sufficient for our purposes. To obtain the specific form stated in the lemma, note that for $f_U, f_F \\in (0,1)$ with $f_U + f_F = 1$, we can simplify using the identity:\n\n$$\n\\frac{1}{\\sqrt{f_U f_F}} = \\frac{\\sqrt{f_U + f_F}}{\\sqrt{f_U f_F}} = \\sqrt{\\frac{1}{f_U f_F}} \\geq \\frac{2}{\\sqrt{(f_U + f_F)^2}} = 2\n$$\n\nwith equality when $f_U = f_F = 1/2$.  A more refined analysis using the extremal configuration (unfit set concentrated near $\\mu_{V,k}$ and fit set dispersed toward $V_{\\max,k}$, subject to the weighted-average and range constraints) yields the tighter bound:\n\n$$\n\\mu_F - \\mu_U \\geq \\frac{f_U}{f_F + f_U^2/f_F} \\kappa_{V,\\text{gap}}(\\epsilon)\n$$\n\n**Justification:** For $f_U + f_F = 1$, the expression $f_F + f_U^2/f_F = f_F + f_U^2/f_F$ can be rewritten as $(f_F^2 + f_U^2)/f_F$. The factor $\\frac{f_U}{f_F^2 + f_U^2} \\cdot f_F = \\frac{f_U f_F}{f_F^2 + f_U^2}$ arises from the weighted-average constraint: when the unfit set (with mass $f_U$) is pushed maximally toward $\\mu_{V,k}$ and the fit set (with mass $f_F$) must balance to maintain the overall mean, the minimum separation is achieved when both sets are as concentrated as possible while spanning the range $\\kappa_{V,\\text{gap}}$. This gives the coefficient stated above. For balanced populations ($f_U = f_F = 1/2$), this yields $\\mu_F - \\mu_U \\geq \\frac{1/4}{1/4 + 1/4} \\kappa_{V,\\text{gap}} = \\frac{\\kappa_{V,\\text{gap}}}{2}$, which is the intuitively correct result.\n\n**Step 4: Final Assembly**\n\nCombining the results from Steps 2 and 3:\n\n$$\n\\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} \\cdot \\frac{f_U}{f_F + f_U^2/f_F} \\kappa_{V,\\text{gap}}(\\epsilon) = \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon)\n$$\n\nSince $f_U, f_F > 0$ and $f_U + f_F = 1$, this bound is strictly positive. For $k \\geq 2$, the factor $\\frac{1}{k-1} \\leq 1$ but remains positive, ensuring the bound is N-uniform (depends on $k$ but doesn't vanish as $k \\to \\infty$ when the fractions are bounded away from zero).",
      "raw_directive": "4807: :::\n4808: \n4809: :::{prf:proof}\n4810: :label: proof-lem-mean-companion-fitness-gap\n4811: \n4812: **Proof.**\n4813: \n4814: The proof proceeds in three steps: (1) express the mean companion fitness algebraically, (2) bound it from below using population fractions, and (3) relate the inter-set mean difference to the fitness range.\n4815: \n4816: **Step 1: Algebraic Expression for Mean Companion Fitness**\n4817: \n4818: For walker $i \\in U_k$, the set of potential companions is all alive walkers except $i$ itself: $\\{j \\in \\mathcal{A}_k : j \\neq i\\}$. The mean fitness of these companions is:\n4819: \n4820: $$\n4821: \\mu_{\\text{comp},i} = \\frac{1}{k-1} \\sum_{j \\neq i} V_{k,j} = \\frac{1}{k-1} \\left( k \\mu_{V,k} - V_{k,i} \\right)\n4822: $$\n4823: \n4824: where $\\mu_{V,k} = \\frac{1}{k} \\sum_{j \\in \\mathcal{A}_k} V_{k,j}$ is the mean fitness of all alive walkers.\n4825: \n4826: The difference we seek to bound is:\n4827: \n4828: $$\n4829: \\mu_{\\text{comp},i} - V_{k,i} = \\frac{k \\mu_{V,k} - V_{k,i}}{k-1} - V_{k,i} = \\frac{k \\mu_{V,k} - k V_{k,i}}{k-1} = \\frac{k}{k-1} (\\mu_{V,k} - V_{k,i})\n4830: $$\n4831: \n4832: **Step 2: Bound on the Gap Using Population Structure**\n4833: \n4834: The overall mean $\\mu_{V,k}$ can be decomposed using the partition into unfit and fit sets:\n4835: \n4836: $$\n4837: \\mu_{V,k} = f_U \\mu_U + f_F \\mu_F\n4838: $$\n4839: \n4840: where $\\mu_U := \\frac{1}{|U_k|} \\sum_{j \\in U_k} V_{k,j}$ and $\\mu_F := \\frac{1}{|F_k|} \\sum_{j \\in F_k} V_{k,j}$.\n4841: \n4842: For any walker $i \\in U_k$, we have $V_{k,i} \\leq \\mu_U$ (by definition of the unfit set: $V_{k,i} \\leq \\mu_{V,k}$, and most unfit walkers have fitness at or below their group mean). In the worst case, assume $V_{k,i} = \\mu_U$. Then:\n4843: \n4844: $$\n4845: \\mu_{V,k} - V_{k,i} \\geq \\mu_{V,k} - \\mu_U = f_U \\mu_U + f_F \\mu_F - \\mu_U = f_F (\\mu_F - \\mu_U)\n4846: $$\n4847: \n4848: Substituting into our expression from Step 1:\n4849: \n4850: $$\n4851: \\mu_{\\text{comp},i} - V_{k,i} = \\frac{k}{k-1} (\\mu_{V,k} - V_{k,i}) \\geq \\frac{k}{k-1} \\cdot f_F (\\mu_F - \\mu_U)\n4852: $$\n4853: \n4854: For $k \\geq 2$, we have $\\frac{k}{k-1} \\geq 1$, so we obtain the conservative bound:\n4855: \n4856: $$\n4857: \\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} (\\mu_F - \\mu_U)\n4858: $$\n4859: \n4860: Note that $\\frac{1}{k-1}$ appears because we're averaging over $k-1$ companions, not $k$ walkers.\n4861: \n4862: **Step 3: Relating $\\mu_F - \\mu_U$ to the Fitness Range**\n4863: \n4864: By definition of the fitness potential range:\n4865: \n4866: $$\n4867: \\kappa_{V,\\text{gap}}(\\epsilon) := V_{\\max,k} - V_{\\min,k}\n4868: $$\n4869: \n4870: The means $\\mu_U$ and $\\mu_F$ satisfy:\n4871: \n4872: $$\n4873: V_{\\min,k} \\leq \\mu_U \\leq \\mu_{V,k} \\leq \\mu_F \\leq V_{\\max,k}\n4874: $$\n4875: \n4876: To obtain a lower bound on $\\mu_F - \\mu_U$, we use the constraint that the overall mean is a weighted average. The maximum separation between group means occurs when one group is concentrated near the minimum and the other near the maximum. However, we must be more careful.\n4877: \n4878: Consider the sum of squared deviations from the overall mean:\n4879: \n4880: $$\n4881: k \\cdot \\text{Var}_{V,k} = \\sum_{j \\in \\mathcal{A}_k} (V_{k,j} - \\mu_{V,k})^2 = \\sum_{j \\in U_k} (V_{k,j} - \\mu_{V,k})^2 + \\sum_{j \\in F_k} (V_{k,j} - \\mu_{V,k})^2\n4882: $$\n4883: \n4884: Using the decomposition of variance formula:\n4885: \n4886: $$\n4887: \\text{Var}_{V,k} = f_U \\text{Var}_U + f_F \\text{Var}_F + f_U f_F (\\mu_U - \\mu_F)^2\n4888: $$\n4889: \n4890: where $\\text{Var}_U$ and $\\text{Var}_F$ are the within-group variances. Since variances are non-negative:\n4891: \n4892: $$\n4893: \\text{Var}_{V,k} \\geq f_U f_F (\\mu_F - \\mu_U)^2\n4894: $$\n4895: \n4896: The fitness range provides an upper bound on the variance:\n4897: \n4898: $$\n4899: \\text{Var}_{V,k} \\leq \\frac{1}{4} \\kappa_{V,\\text{gap}}^2(\\epsilon)\n4900: $$\n4901: \n4902: (This is the standard bound for bounded random variables: variance \\leq  (range/2)^2.)\n4903: \n4904: Combining these:\n4905: \n4906: $$\n4907: f_U f_F (\\mu_F - \\mu_U)^2 \\leq \\text{Var}_{V,k} \\leq \\frac{1}{4} \\kappa_{V,\\text{gap}}^2(\\epsilon)\n4908: $$\n4909: \n4910: From the variance inequality, we have established:\n4911: \n4912: $$\n4913: \\mu_F - \\mu_U \\geq \\frac{1}{2} \\sqrt{\\frac{1}{f_U f_F}} \\kappa_{V,\\text{gap}}(\\epsilon)\n4914: $$\n4915: \n4916: This bound is sufficient for our purposes. To obtain the specific form stated in the lemma, note that for $f_U, f_F \\in (0,1)$ with $f_U + f_F = 1$, we can simplify using the identity:\n4917: \n4918: $$\n4919: \\frac{1}{\\sqrt{f_U f_F}} = \\frac{\\sqrt{f_U + f_F}}{\\sqrt{f_U f_F}} = \\sqrt{\\frac{1}{f_U f_F}} \\geq \\frac{2}{\\sqrt{(f_U + f_F)^2}} = 2\n4920: $$\n4921: \n4922: with equality when $f_U = f_F = 1/2$.  A more refined analysis using the extremal configuration (unfit set concentrated near $\\mu_{V,k}$ and fit set dispersed toward $V_{\\max,k}$, subject to the weighted-average and range constraints) yields the tighter bound:\n4923: \n4924: $$\n4925: \\mu_F - \\mu_U \\geq \\frac{f_U}{f_F + f_U^2/f_F} \\kappa_{V,\\text{gap}}(\\epsilon)\n4926: $$\n4927: \n4928: **Justification:** For $f_U + f_F = 1$, the expression $f_F + f_U^2/f_F = f_F + f_U^2/f_F$ can be rewritten as $(f_F^2 + f_U^2)/f_F$. The factor $\\frac{f_U}{f_F^2 + f_U^2} \\cdot f_F = \\frac{f_U f_F}{f_F^2 + f_U^2}$ arises from the weighted-average constraint: when the unfit set (with mass $f_U$) is pushed maximally toward $\\mu_{V,k}$ and the fit set (with mass $f_F$) must balance to maintain the overall mean, the minimum separation is achieved when both sets are as concentrated as possible while spanning the range $\\kappa_{V,\\text{gap}}$. This gives the coefficient stated above. For balanced populations ($f_U = f_F = 1/2$), this yields $\\mu_F - \\mu_U \\geq \\frac{1/4}{1/4 + 1/4} \\kappa_{V,\\text{gap}} = \\frac{\\kappa_{V,\\text{gap}}}{2}$, which is the intuitively correct result.\n4929: \n4930: **Step 4: Final Assembly**\n4931: \n4932: Combining the results from Steps 2 and 3:\n4933: \n4934: $$\n4935: \\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} \\cdot \\frac{f_U}{f_F + f_U^2/f_F} \\kappa_{V,\\text{gap}}(\\epsilon) = \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon)\n4936: $$\n4937: \n4938: Since $f_U, f_F > 0$ and $f_U + f_F = 1$, this bound is strictly positive. For $k \\geq 2$, the factor $\\frac{1}{k-1} \\leq 1$ but remains positive, ensuring the bound is N-uniform (depends on $k$ but doesn't vanish as $k \\to \\infty$ when the fractions are bounded away from zero).\n4939: ",
      "strategy_summary": "The proof derives an algebraic expression for the mean companion fitness and its difference from an individual's fitness, bounds this gap using the population fractions of unfit and fit groups, and lower-bounds the inter-group mean difference via variance decomposition and the fitness range constraint.",
      "conclusion": {
        "text": "For i \u2208 U_k, \u03bc_{comp,i} - V_{k,i} \u2265 \frac{f_U f_F}{(k-1)(f_F + f_U^2 / f_F)} \u03ba_{V,gap}(\u03b5)",
        "latex": "\\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_U f_F}{(k-1)(f_F + f_U^2 / f_F)} \\kappa_{V,\\text{gap}}(\\epsilon)"
      },
      "assumptions": [
        {
          "text": "k \u2265 2 (number of alive walkers)",
          "latex": null
        },
        {
          "text": "f_U + f_F = 1, f_U, f_F > 0 (population fractions of unfit and fit sets)",
          "latex": null
        },
        {
          "text": "V_{k,j} \u2208 [V_{min,k}, V_{max,k}] for all j \u2208 A_k, with range \u03ba_{V,gap}(\u03b5) = V_{max,k} - V_{min,k}",
          "latex": null
        },
        {
          "text": "U_k and F_k partition the alive walkers A_k, with V_{k,i} \u2264 \u03bc_{V,k} for i \u2208 U_k",
          "latex": null
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "algebraic derivation",
          "text": "Express the mean companion fitness \u03bc_{comp,i} for i \u2208 U_k as the average fitness of other alive walkers, yielding \u03bc_{comp,i} = (1/(k-1)) (k \u03bc_{V,k} - V_{k,i}).",
          "latex": "\\mu_{\\text{comp},i} = \\frac{1}{k-1} \\left( k \\mu_{V,k} - V_{k,i} \\right)",
          "references": [],
          "derived_statement": "\u03bc_{comp,i} - V_{k,i} = \\frac{k}{k-1} (\\mu_{V,k} - V_{k,i})"
        },
        {
          "order": 2.0,
          "kind": "inequality bound",
          "text": "Decompose \u03bc_{V,k} = f_U \u03bc_U + f_F \u03bc_F and assume worst case V_{k,i} = \u03bc_U for i \u2208 U_k, leading to \u03bc_{V,k} - V_{k,i} \u2265 f_F (\u03bc_F - \u03bc_U), hence \u03bc_{comp,i} - V_{k,i} \u2265 (k/(k-1)) f_F (\u03bc_F - \u03bc_U) \u2265 f_F / (k-1) (\u03bc_F - \u03bc_U).",
          "latex": "\\mu_{V,k} = f_U \\mu_U + f_F \\mu_F \\quad \\Rightarrow \\quad \\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} (\\mu_F - \\mu_U)",
          "references": [],
          "derived_statement": "Lower bound using population fractions"
        },
        {
          "order": 3.0,
          "kind": "variance analysis",
          "text": "Use variance decomposition Var_{V,k} \u2265 f_U f_F (\u03bc_F - \u03bc_U)^2 and bound Var_{V,k} \u2264 (1/4) \u03ba_{V,gap}^2(\u03b5) to get \u03bc_F - \u03bc_U \u2265 (1/2) \u221a(1/(f_U f_F)) \u03ba_{V,gap}(\u03b5), refined to \u03bc_F - \u03bc_U \u2265 (f_U / (f_F + f_U^2 / f_F)) \u03ba_{V,gap}(\u03b5) via weighted average optimization.",
          "latex": "\\text{Var}_{V,k} \\geq f_U f_F (\\mu_F - \\mu_U)^2 \\leq \\frac{1}{4} \\kappa_{V,\\text{gap}}^2(\\epsilon) \\quad \\Rightarrow \\quad \\mu_F - \\mu_U \\geq \\frac{f_U}{f_F + f_U^2 / f_F} \\kappa_{V,\\text{gap}}(\\epsilon)",
          "references": [],
          "derived_statement": "Bound on inter-group mean difference"
        },
        {
          "order": 4.0,
          "kind": "assembly",
          "text": "Combine bounds from previous steps to obtain the final lower bound on the companion fitness gap.",
          "latex": "\\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_U f_F}{(k-1)(f_F + f_U^2 / f_F)} \\kappa_{V,\\text{gap}}(\\epsilon)",
          "references": [],
          "derived_statement": "Final positive bound ensuring N-uniformity"
        }
      ],
      "key_equations": [
        {
          "label": "eq-mu-comp",
          "latex": "\\mu_{\\text{comp},i} = \\frac{1}{k-1} \\sum_{j \\neq i} V_{k,j} = \\frac{1}{k-1} \\left( k \\mu_{V,k} - V_{k,i} \\right)",
          "role": "Algebraic expression for mean companion fitness"
        },
        {
          "label": "eq-gap-diff",
          "latex": "\\mu_{\\text{comp},i} - V_{k,i} = \\frac{k}{k-1} (\\mu_{V,k} - V_{k,i})",
          "role": "Difference between companion mean and individual fitness"
        },
        {
          "label": "eq-mean-decomp",
          "latex": "\\mu_{V,k} = f_U \\mu_U + f_F \\mu_F",
          "role": "Decomposition of overall mean using group fractions"
        },
        {
          "label": "eq-var-decomp",
          "latex": "\\text{Var}_{V,k} = f_U \\text{Var}_U + f_F \\text{Var}_F + f_U f_F (\\mu_U - \\mu_F)^2",
          "role": "Variance decomposition for between-group bound"
        },
        {
          "label": "eq-var-bound",
          "latex": "\\text{Var}_{V,k} \\leq \\frac{1}{4} \\kappa_{V,\\text{gap}}^2(\\epsilon)",
          "role": "Upper bound on variance from fitness range"
        },
        {
          "label": "eq-final-bound",
          "latex": "\\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_U f_F}{(k-1)(f_F + f_U^2 / f_F)} \\kappa_{V,\\text{gap}}(\\epsilon)",
          "role": "Final lower bound on the fitness gap"
        }
      ],
      "references": [],
      "math_tools": [
        {
          "toolName": "Algebraic manipulation of means",
          "field": "Algebra",
          "description": "Rearranging sums and averages to express differences between individual and group means.",
          "roleInProof": "Used in Step 1 to derive the difference \u03bc_comp,i - V_{k,i} as (k/(k-1))(\u03bc_{V,k} - V_{k,i}).",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Weighted average decomposition"
          ]
        },
        {
          "toolName": "Variance decomposition",
          "field": "Statistics",
          "description": "Decomposes total variance into within-group variances and between-group variance term f_U f_F (\u03bc_U - \u03bc_F)^2.",
          "roleInProof": "Applied in Step 3 to bound (\u03bc_F - \u03bc_U)^2 from below using total variance.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Total variance",
            "Group variances"
          ]
        },
        {
          "toolName": "Popov\u2019s inequality for bounded variables",
          "field": "Probability",
          "description": "For a random variable bounded in [m, M], Var(X) \u2264 ((M - m)/2)^2.",
          "roleInProof": "Used to upper-bound total variance Var_{V,k} \u2264 (1/4) \u03ba_{V,gap}^2(\u03b5) in Step 3.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Bounded variance"
          ]
        },
        {
          "toolName": "Weighted average constraint",
          "field": "Analysis",
          "description": "Expresses overall mean as convex combination of group means, used to optimize group mean separation under range constraints.",
          "roleInProof": "Employed in Step 3 to derive the tight lower bound on \u03bc_F - \u03bc_U involving f_U and f_F.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Convex combination"
          ]
        }
      ],
      "cases": [
        {
          "name": "Worst-case assumption for unfit walker",
          "condition": "V_{k,i} = \\mu_U for i \\in U_k",
          "summary": "Assumes the unfit walker's fitness equals the unfit group mean to minimize the gap \u03bc_{V,k} - V_{k,i}."
        }
      ],
      "remarks": [
        {
          "type": "conservative bound",
          "text": "The factor k/(k-1) \u2265 1 for k \u2265 2 allows a looser bound \u03bc_{comp,i} - V_{k,i} \u2265 (f_F /(k-1)) (\u03bc_F - \u03bc_U), but the tighter assembly uses the refined inter-group bound."
        },
        {
          "type": "justification of tight bound",
          "text": "The refined bound on \u03bc_F - \u03bc_U arises from extremal configuration where unfit set concentrates near \u03bc_{V,k} and fit set balances the mean while spanning the range, yielding the coefficient f_U / (f_F + f_U^2 / f_F). For balanced fractions f_U = f_F = 1/2, it simplifies to \u03ba_{V,gap}/2."
        },
        {
          "type": "N-uniformity",
          "text": "The bound is positive and depends on k via 1/(k-1) but does not vanish as k \u2192 \u221e if f_U, f_F bounded away from 0."
        }
      ],
      "gaps": [
        {
          "description": "The justification for the tight bound on \u03bc_F - \u03bc_U in Step 3 is sketched via extremal configuration but lacks a full optimization proof; it relies on intuitive weighted-average constraints.",
          "severity": "minor",
          "location_hint": "Step 3, refined bound paragraph"
        }
      ],
      "tags": [
        "companion fitness",
        "fitness gap",
        "population fractions",
        "variance decomposition",
        "bounded variables",
        "algebraic bound",
        "group means"
      ],
      "document_id": "03_cloning",
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "span": {
        "start_line": 4807,
        "end_line": 4939,
        "content_start": 4810,
        "content_end": 4938,
        "header_lines": [
          4808
        ]
      },
      "metadata": {
        "label": "proof-lem-mean-companion-fitness-gap"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 8,
        "chapter_file": "chapter_8.json",
        "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "swarm",
      "walkers",
      "fitness",
      "companion",
      "lower bound",
      "gap",
      "unfit",
      "degenerate"
    ],
    "content_markdown": ":label: lem-mean-companion-fitness-gap\n\nLet a swarm $k$ with $k \\geq 2$ alive walkers have a non-degenerate fitness potential range $\\kappa_{V,\\text{gap}}(\\epsilon) > 0$. Let $U_k$ and $F_k$ denote the unfit and fit sets, with respective population fractions $f_U := |U_k|/k$ and $f_F := |F_k|/k$, where $f_U, f_F > 0$ and $f_U + f_F = 1$. Denote by $\\mu_U$ and $\\mu_F$ the mean fitness values of the two sets.\n\nFor any walker $i \\in U_k$ (unfit set), the difference between its mean companion fitness and its own fitness is bounded below by:\n\n$$\n\\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} (\\mu_F - \\mu_U) > 0\n$$\n\nFurthermore, the gap between the mean fitness values of the two sets can be bounded in terms of the fitness range:\n\n$$\n\\mu_F - \\mu_U \\geq \\frac{f_U}{f_F + f_U^2/f_F} \\kappa_{V,\\text{gap}}(\\epsilon)\n$$\n\nCombining these yields an N-uniform lower bound:\n\n$$\n\\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon) =: \\Delta_{\\min}(\\epsilon, f_U, f_F, k)\n$$",
    "raw_directive": "4781: To establish a lower bound on cloning probabilities, we must first prove that unfit walkers have companions whose fitness is systematically higher. The following lemma provides the required quantitative guarantee.\n4782: \n4783: :::{prf:lemma} Lower Bound on Mean Companion Fitness Gap\n4784: :label: lem-mean-companion-fitness-gap\n4785: \n4786: Let a swarm $k$ with $k \\geq 2$ alive walkers have a non-degenerate fitness potential range $\\kappa_{V,\\text{gap}}(\\epsilon) > 0$. Let $U_k$ and $F_k$ denote the unfit and fit sets, with respective population fractions $f_U := |U_k|/k$ and $f_F := |F_k|/k$, where $f_U, f_F > 0$ and $f_U + f_F = 1$. Denote by $\\mu_U$ and $\\mu_F$ the mean fitness values of the two sets.\n4787: \n4788: For any walker $i \\in U_k$ (unfit set), the difference between its mean companion fitness and its own fitness is bounded below by:\n4789: \n4790: $$\n4791: \\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F}{k-1} (\\mu_F - \\mu_U) > 0\n4792: $$\n4793: \n4794: Furthermore, the gap between the mean fitness values of the two sets can be bounded in terms of the fitness range:\n4795: \n4796: $$\n4797: \\mu_F - \\mu_U \\geq \\frac{f_U}{f_F + f_U^2/f_F} \\kappa_{V,\\text{gap}}(\\epsilon)\n4798: $$\n4799: \n4800: Combining these yields an N-uniform lower bound:\n4801: \n4802: $$\n4803: \\mu_{\\text{comp},i} - V_{k,i} \\geq \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon) =: \\Delta_{\\min}(\\epsilon, f_U, f_F, k)\n4804: $$\n4805: ",
    "document_id": "03_cloning",
    "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
    "span": {
      "start_line": 4781,
      "end_line": 4805,
      "content_start": 4784,
      "content_end": 4804,
      "header_lines": [
        4782
      ]
    },
    "references": [],
    "metadata": {
      "label": "lem-mean-companion-fitness-gap"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 8,
      "chapter_file": "chapter_8.json",
      "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-unfit-cloning-pressure",
    "title": "Guaranteed Cloning Pressure on the Unfit Set",
    "type": "lemma",
    "nl_statement": "For a swarm of at least two alive walkers with fitness potential range bounded below by \u03ba_{V,gap}(\u03b5) > 0, every walker in the unfit set has total cloning probability at least the positive constant p_u(\u03b5) > 0.",
    "equations": [
      {
        "label": null,
        "latex": "p_{k,i} = \\mathbb{E}_{c \\sim \\mathbb{C}_i(S_k)}[\\pi(S(V_{k,c}, V_{k,i}))] \\ge p_u(\\epsilon) > 0"
      }
    ],
    "hypotheses": [
      {
        "text": "Swarm k has k \u2265 2 alive walkers.",
        "latex": "k \\geq 2"
      },
      {
        "text": "The fitness potential range of the swarm is bounded below by \u03ba_{V,gap}(\u03b5) > 0.",
        "latex": "\\kappa_{V,\\text{gap}}(\\epsilon) > 0"
      },
      {
        "text": "Walker i is in the unfit set U_k.",
        "latex": "i \\in U_k"
      }
    ],
    "conclusion": {
      "text": "The total cloning probability p_{k,i} for walker i satisfies p_{k,i} \u2265 p_u(\u03b5) > 0, where p_u(\u03b5) is a positive, N-uniform, \u03b5-dependent constant.",
      "latex": "p_{k,i} = \\mathbb{E}_{c \\sim \\mathbb{C}_i(S_k)}[\\pi(S(V_{k,c}, V_{k,i}))] \\ge p_u(\\epsilon) > 0"
    },
    "variables": [
      {
        "symbol": "k",
        "name": "swarm index",
        "description": "Index of the swarm.",
        "constraints": [
          "k \u2265 2"
        ],
        "tags": [
          "swarm"
        ]
      },
      {
        "symbol": "i",
        "name": "walker index",
        "description": "Index of an unfit walker.",
        "constraints": [
          "i \u2208 U_k"
        ],
        "tags": [
          "walker",
          "unfit"
        ]
      },
      {
        "symbol": "\u03b5",
        "name": "error parameter",
        "description": "Small positive parameter controlling approximations.",
        "constraints": [
          "\u03b5 > 0"
        ],
        "tags": [
          "epsilon",
          "approximation"
        ]
      },
      {
        "symbol": "p_u(\u03b5)",
        "name": "lower bound constant",
        "description": "Positive, N-uniform, \u03b5-dependent lower bound on cloning probability.",
        "constraints": [
          "p_u(\u03b5) > 0"
        ],
        "tags": [
          "bound",
          "cloning"
        ]
      },
      {
        "symbol": "\u03ba_{V,gap}(\u03b5)",
        "name": "fitness gap",
        "description": "Lower bound on the fitness potential range.",
        "constraints": [
          "\u03ba_{V,gap}(\u03b5) > 0"
        ],
        "tags": [
          "fitness",
          "gap"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The bound p_u(\u03b5) is independent of the swarm size N.",
        "confidence": 0.9
      },
      {
        "text": "All walkers have defined fitness potentials V_{k,j}.",
        "confidence": 1.0
      },
      {
        "text": "The cloning distribution \u2102_i(S_k) is well-defined and supports expectation.",
        "confidence": 1.0
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-unfit-cloning-pressure",
      "title": null,
      "type": "proof",
      "proves": "lem-unfit-cloning-pressure",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-unfit-cloning-pressure\n\n**Proof.**\n\nThe proof establishes that for any walker $i$ in the unfit set, the average fitness of its potential companions is guaranteed to be strictly greater than its own fitness. This ensures a positive average cloning score, which in turn guarantees a positive cloning probability via Jensen's inequality.\n\n**1. Average Companion Fitness vs. Unfit Walker Fitness.**\nLet $i$ be an arbitrary walker in the unfit set $U_k$. By definition, its fitness satisfies $V_{k,i} \\le \\mu_{V,k}$, where $\\mu_{V,k}$ is the mean fitness of all $k$ alive walkers. [Lemma 8.3.1](#lem-mean-companion-fitness-gap) establishes that the gap between the average companion fitness and the walker's own fitness is bounded below by:\n\n$$\n\\mu_{\\text{comp},i} - V_{k,i} \\ge \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon) =: \\Delta_{\\min}(\\epsilon, f_U, f_F, k) > 0\n$$\n\nwhere $f_U$ and $f_F$ are the population fractions of the unfit and fit sets, and $\\kappa_{V,\\text{gap}}(\\epsilon)$ is the fitness potential range. This bound is N-uniform and strictly positive for all $k \\geq 2$ and all fitness distributions satisfying the non-degeneracy condition.\n\n**2. Guaranteed Positive Average Score.**\nThe average cloning score for walker $i$ is $S_{\\text{avg},i} = \\mathbb{E}_c[S(V_c, V_i)] = (\\mu_{\\text{comp},i} - V_i) / (V_i + \\varepsilon_{\\text{clone}})$. Using the bound from Step 1, the numerator satisfies:\n\n$$\n\\mu_{\\text{comp},i} - V_i \\geq \\Delta_{\\min}(\\epsilon, f_U, f_F, k)\n$$\n\nThe denominator is uniformly bounded above by $V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}$. Therefore, the average score is uniformly bounded below by:\n\n$$\nS_{\\text{avg},i} \\ge \\frac{\\Delta_{\\min}(\\epsilon, f_U, f_F, k)}{V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}} =: S_u(\\epsilon, k) > 0\n$$\n\nThis bound is N-uniform: it depends on $k$ through the factor $1/(k-1)$ in $\\Delta_{\\min}$, but remains strictly positive for all $k \\geq 2$.\n\n**3. From Average Score to Probability via Jensen's Inequality.**\nThe total cloning probability is $p_{k,i} = \\mathbb{E}_c[\\pi(S(V_c, V_i))]$. The function $\\pi(S) = \\min(1, \\max(0, S/p_{\\max}))$ is concave for the non-negative scores we are considering. By Jensen's inequality for concave functions, the expectation of the function is greater than or equal to the function of the expectation:\n\n$$\np_{k,i} = \\mathbb{E}_c[\\pi(S(V_c, V_i))] \\ge \\pi(\\mathbb{E}_c[S(V_c, V_i)]) = \\pi(S_{\\text{avg},i})\n$$\n\nSince $S_{\\text{avg},i} \\ge S_u(\\epsilon, k) > 0$ (from Step 2) and the function $\\pi(S)$ is strictly increasing for positive scores, we have a final N-uniform lower bound:\n\n$$\np_{k,i} \\ge \\pi(S_u(\\epsilon, k)) =: p_u(\\epsilon, k) > 0\n$$",
      "raw_directive": "4958: \n4959: :::\n4960: :::{prf:proof}\n4961: :label: proof-lem-unfit-cloning-pressure\n4962: \n4963: **Proof.**\n4964: \n4965: The proof establishes that for any walker $i$ in the unfit set, the average fitness of its potential companions is guaranteed to be strictly greater than its own fitness. This ensures a positive average cloning score, which in turn guarantees a positive cloning probability via Jensen's inequality.\n4966: \n4967: **1. Average Companion Fitness vs. Unfit Walker Fitness.**\n4968: Let $i$ be an arbitrary walker in the unfit set $U_k$. By definition, its fitness satisfies $V_{k,i} \\le \\mu_{V,k}$, where $\\mu_{V,k}$ is the mean fitness of all $k$ alive walkers. [Lemma 8.3.1](#lem-mean-companion-fitness-gap) establishes that the gap between the average companion fitness and the walker's own fitness is bounded below by:\n4969: \n4970: $$\n4971: \\mu_{\\text{comp},i} - V_{k,i} \\ge \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon) =: \\Delta_{\\min}(\\epsilon, f_U, f_F, k) > 0\n4972: $$\n4973: \n4974: where $f_U$ and $f_F$ are the population fractions of the unfit and fit sets, and $\\kappa_{V,\\text{gap}}(\\epsilon)$ is the fitness potential range. This bound is N-uniform and strictly positive for all $k \\geq 2$ and all fitness distributions satisfying the non-degeneracy condition.\n4975: \n4976: **2. Guaranteed Positive Average Score.**\n4977: The average cloning score for walker $i$ is $S_{\\text{avg},i} = \\mathbb{E}_c[S(V_c, V_i)] = (\\mu_{\\text{comp},i} - V_i) / (V_i + \\varepsilon_{\\text{clone}})$. Using the bound from Step 1, the numerator satisfies:\n4978: \n4979: $$\n4980: \\mu_{\\text{comp},i} - V_i \\geq \\Delta_{\\min}(\\epsilon, f_U, f_F, k)\n4981: $$\n4982: \n4983: The denominator is uniformly bounded above by $V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}$. Therefore, the average score is uniformly bounded below by:\n4984: \n4985: $$\n4986: S_{\\text{avg},i} \\ge \\frac{\\Delta_{\\min}(\\epsilon, f_U, f_F, k)}{V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}} =: S_u(\\epsilon, k) > 0\n4987: $$\n4988: \n4989: This bound is N-uniform: it depends on $k$ through the factor $1/(k-1)$ in $\\Delta_{\\min}$, but remains strictly positive for all $k \\geq 2$.\n4990: \n4991: **3. From Average Score to Probability via Jensen's Inequality.**\n4992: The total cloning probability is $p_{k,i} = \\mathbb{E}_c[\\pi(S(V_c, V_i))]$. The function $\\pi(S) = \\min(1, \\max(0, S/p_{\\max}))$ is concave for the non-negative scores we are considering. By Jensen's inequality for concave functions, the expectation of the function is greater than or equal to the function of the expectation:\n4993: \n4994: $$\n4995: p_{k,i} = \\mathbb{E}_c[\\pi(S(V_c, V_i))] \\ge \\pi(\\mathbb{E}_c[S(V_c, V_i)]) = \\pi(S_{\\text{avg},i})\n4996: $$\n4997: \n4998: Since $S_{\\text{avg},i} \\ge S_u(\\epsilon, k) > 0$ (from Step 2) and the function $\\pi(S)$ is strictly increasing for positive scores, we have a final N-uniform lower bound:\n4999: \n5000: $$\n5001: p_{k,i} \\ge \\pi(S_u(\\epsilon, k)) =: p_u(\\epsilon, k) > 0\n5002: $$\n5003: ",
      "strategy_summary": "The proof demonstrates that unfit walkers have strictly higher average companion fitness than their own, ensuring a positive average cloning score bounded below uniformly. Applying Jensen's inequality to the concave cloning probability function then guarantees a positive cloning probability for these walkers.",
      "conclusion": {
        "text": "For any unfit walker i, the cloning probability satisfies p_{k,i} \u2265 p_u(\u03b5, k) > 0, where p_u is an N-uniform lower bound.",
        "latex": "$p_{k,i} \\ge \\pi(S_u(\\epsilon, k)) =: p_u(\\epsilon, k) > 0$"
      },
      "assumptions": [
        {
          "text": "Walker i is in the unfit set U_k, so V_{k,i} \u2264 \u03bc_{V,k}.",
          "latex": null
        },
        {
          "text": "k \u2265 2 alive walkers.",
          "latex": null
        },
        {
          "text": "Fitness distributions satisfy non-degeneracy condition ensuring \u03ba_{V,gap}(\u03b5) > 0.",
          "latex": null
        },
        {
          "text": "Positive unfit and fit population fractions f_U, f_F > 0.",
          "latex": null
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "bound",
          "text": "Establish lower bound on average companion fitness gap for unfit walker i: \u03bc_{comp,i} - V_{k,i} \u2265 \u0394_min(\u03b5, f_U, f_F, k) > 0, via referenced lemma.",
          "latex": "\\mu_{\\text{comp},i} - V_{k,i} \\ge \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon) =: \\Delta_{\\min}(\\epsilon, f_U, f_F, k) > 0",
          "references": [
            "lem-mean-companion-fitness-gap"
          ],
          "derived_statement": "Fitness gap bound \u0394_min > 0."
        },
        {
          "order": 2.0,
          "kind": "inequality",
          "text": "Bound average cloning score below: S_{avg,i} \u2265 \u0394_min / (V_{pot,max} + \u03b5_{clone}) =: S_u(\u03b5, k) > 0.",
          "latex": "S_{\\text{avg},i} \\ge \\frac{\\Delta_{\\min}(\\epsilon, f_U, f_F, k)}{V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}} =: S_u(\\epsilon, k) > 0",
          "references": [],
          "derived_statement": "Positive average score S_u > 0."
        },
        {
          "order": 3.0,
          "kind": "application",
          "text": "Apply Jensen's inequality to concave \u03c0: p_{k,i} = E[\u03c0(S(V_c, V_i))] \u2265 \u03c0(S_{avg,i}) \u2265 \u03c0(S_u(\u03b5, k)) =: p_u(\u03b5, k) > 0.",
          "latex": "p_{k,i} = \\mathbb{E}_c[\\pi(S(V_c, V_i))] \\ge \\pi(\\mathbb{E}_c[S(V_c, V_i)]) = \\pi(S_{\\text{avg},i}) \\ge \\pi(S_u(\\epsilon, k)) =: p_u(\\epsilon, k) > 0",
          "references": [],
          "derived_statement": "Positive cloning probability p_u > 0."
        }
      ],
      "key_equations": [
        {
          "label": "eq-delta-min",
          "latex": "\\mu_{\\text{comp},i} - V_{k,i} \\ge \\frac{f_F f_U}{(k-1)(f_F + f_U^2/f_F)} \\kappa_{V,\\text{gap}}(\\epsilon) =: \\Delta_{\\min}(\\epsilon, f_U, f_F, k) > 0",
          "role": "Lower bound on fitness gap."
        },
        {
          "label": "eq-s-avg",
          "latex": "S_{\\text{avg},i} = (\\mu_{\\text{comp},i} - V_i) / (V_i + \\varepsilon_{\\text{clone}})",
          "role": "Average cloning score definition."
        },
        {
          "label": "eq-s-u",
          "latex": "S_{\\text{avg},i} \\ge \\frac{\\Delta_{\\min}(\\epsilon, f_U, f_F, k)}{V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}} =: S_u(\\epsilon, k) > 0",
          "role": "Lower bound on average score."
        },
        {
          "label": "eq-pi-jensen",
          "latex": "p_{k,i} \\ge \\pi(S_{\\text{avg},i})",
          "role": "Jensen's application to \u03c0."
        },
        {
          "label": "eq-p-u",
          "latex": "p_{k,i} \\ge \\pi(S_u(\\epsilon, k)) =: p_u(\\epsilon, k) > 0",
          "role": "Final positive probability bound."
        }
      ],
      "references": [
        "lem-mean-companion-fitness-gap"
      ],
      "math_tools": [
        {
          "toolName": "Jensen's Inequality",
          "field": "Real Analysis",
          "description": "For a concave function f and random variable X, E[f(X)] \u2264 f(E[X]).",
          "roleInProof": "Used to bound the expected cloning probability below by the probability of the expected score, since the clipping function \u03c0 is concave on non-negative scores.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": []
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "uniformity",
          "text": "All bounds are N-uniform and strictly positive for k \u2265 2 under non-degeneracy."
        },
        {
          "type": "concavity",
          "text": "The function \u03c0(S) is concave for non-negative S, enabling Jensen's inequality."
        }
      ],
      "gaps": [],
      "tags": [
        "unfit walkers",
        "cloning probability",
        "fitness gap",
        "Jensen's inequality",
        "N-uniform bound",
        "companion fitness",
        "average score"
      ],
      "document_id": "03_cloning",
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "span": {
        "start_line": 4958,
        "end_line": 5003,
        "content_start": 4961,
        "content_end": 5002,
        "header_lines": [
          4959
        ]
      },
      "metadata": {
        "label": "proof-lem-unfit-cloning-pressure"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 8,
        "chapter_file": "chapter_8.json",
        "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "swarm",
      "walkers",
      "unfit set",
      "cloning probability",
      "fitness potential",
      "epsilon-dependent"
    ],
    "content_markdown": ":label: lem-unfit-cloning-pressure\n\nLet a swarm $k$ with $k \\geq 2$ alive walkers be in a state such that its fitness potential range is bounded below by $\\kappa_{V,\\text{gap}}(\\epsilon) > 0$. For any walker $i$ in the unfit set $U_k$, its total cloning probability is bounded below by a positive, N-uniform, and $\\epsilon$-dependent constant $p_u(\\epsilon) > 0$:\n\n$$\np_{k,i} = \\mathbb{E}_{c \\sim \\mathbb{C}_i(S_k)}[\\pi(S(V_{k,c}, V_{k,i}))] \\ge p_u(\\epsilon) > 0",
    "raw_directive": "4948: :::\n4949: \n4950: :::{prf:lemma} Guaranteed Cloning Pressure on the Unfit Set\n4951: :label: lem-unfit-cloning-pressure\n4952: \n4953: Let a swarm $k$ with $k \\geq 2$ alive walkers be in a state such that its fitness potential range is bounded below by $\\kappa_{V,\\text{gap}}(\\epsilon) > 0$. For any walker $i$ in the unfit set $U_k$, its total cloning probability is bounded below by a positive, N-uniform, and $\\epsilon$-dependent constant $p_u(\\epsilon) > 0$:\n4954: \n4955: $$\n4956: p_{k,i} = \\mathbb{E}_{c \\sim \\mathbb{C}_i(S_k)}[\\pi(S(V_{k,c}, V_{k,i}))] \\ge p_u(\\epsilon) > 0\n4957: $$",
    "document_id": "03_cloning",
    "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
    "span": {
      "start_line": 4948,
      "end_line": 4957,
      "content_start": 4951,
      "content_end": 4956,
      "header_lines": [
        4949
      ]
    },
    "references": [
      "lem-mean-companion-fitness-gap"
    ],
    "metadata": {
      "label": "lem-unfit-cloning-pressure"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 8,
      "chapter_file": "chapter_8.json",
      "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-variance-concentration-Hk",
    "title": "Variance Concentration in the High-Error Set",
    "type": "lemma",
    "nl_statement": "For a swarm in a high-error state with variance exceeding R\u00b2_var, the sum of squared deviations in the high-error set H_k(\u03b5) is at least a fixed positive fraction c_H of the total sum of squared deviations across the swarm.",
    "equations": [
      {
        "label": null,
        "latex": "\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge c_H \\sum_{i \\in \\mathcal{A}_k} \\|\\delta_{x,k,i}\\|^2"
      }
    ],
    "hypotheses": [
      {
        "text": "Swarm k is in a high-error state, i.e., Var_k(x) > R\u00b2_var",
        "latex": "\\text{Var}_k(x) > R^2_{\\text{var}}"
      },
      {
        "text": "There exists a strictly positive N-uniform constant c_H in (0,1]",
        "latex": "c_H \\in (0, 1]"
      }
    ],
    "conclusion": {
      "text": "The sum of squared deviations from the mean for walkers in H_k(\u03b5) is bounded below by c_H times the total sum of squared deviations",
      "latex": "\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge c_H \\sum_{i \\in \\mathcal{A}_k} \\|\\delta_{x,k,i}\\|^2"
    },
    "variables": [
      {
        "symbol": "k",
        "name": "swarm index",
        "description": "Index identifying the swarm",
        "constraints": [],
        "tags": [
          "swarm"
        ]
      },
      {
        "symbol": "H_k(\\epsilon)",
        "name": "high-error set",
        "description": "Unified set of walkers in swarm k with errors exceeding \u03b5",
        "constraints": [
          "subset of A_k"
        ],
        "tags": [
          "high-error",
          "set"
        ]
      },
      {
        "symbol": "c_H",
        "name": "concentration constant",
        "description": "Strictly positive N-uniform constant bounding the variance fraction",
        "constraints": [
          "c_H \u2208 (0,1]"
        ],
        "tags": [
          "constant",
          "N-uniform"
        ]
      },
      {
        "symbol": "\\delta_{x,k,i}",
        "name": "position deviation",
        "description": "Squared deviation of walker i's position from the swarm mean",
        "constraints": [],
        "tags": [
          "deviation",
          "position"
        ]
      },
      {
        "symbol": "\\mathcal{A}_k",
        "name": "swarm agents",
        "description": "Set of all agents/walkers in swarm k",
        "constraints": [],
        "tags": [
          "agents",
          "swarm"
        ]
      },
      {
        "symbol": "R^2_{\\text{var}}",
        "name": "variance threshold",
        "description": "Threshold defining high-error state",
        "constraints": [],
        "tags": [
          "threshold",
          "variance"
        ]
      },
      {
        "symbol": "\\epsilon",
        "name": "error parameter",
        "description": "Error level parameter for defining H_k",
        "constraints": [],
        "tags": [
          "error",
          "parameter"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "H_k(\u03b5) is non-empty and well-defined based on error thresholds from prior geometric analysis",
        "confidence": 0.9
      },
      {
        "text": "\u03b4_{x,k,i} represents deviations within Euclidean space, with norm ||\u00b7||\u00b2 being the squared Euclidean norm",
        "confidence": 1.0
      },
      {
        "text": "N refers to the number of agents, and N-uniform means independent of N as N\u2192\u221e",
        "confidence": 0.8
      },
      {
        "text": "The swarm model assumes independent or structured walker positions contributing to variance",
        "confidence": 0.7
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-variance-concentration-Hk",
      "title": null,
      "type": "proof",
      "proves": "lem-variance-concentration-Hk",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-variance-concentration-Hk\n\n**Proof.**\nThis follows from the definition of $H_k(\\epsilon)$ in the two regimes established by the $\\epsilon$-dichotomy. We prove each regime separately.\n\n**1. Mean-Field Regime ($\\epsilon > D_{\\text{swarm}}$):**\n\n$H_k(\\epsilon)$ is the global outlier set $O_k$. By its definition ({prf:ref}`def-unified-high-low-error-sets`), this set is constructed specifically to contain at least a fraction $(1-\\varepsilon_O)$ of the total sum of squared deviations. In this case, $c_H = 1-\\varepsilon_O$.\n\n**2. Local-Interaction Regime ($\\epsilon \\leq D_{\\text{swarm}}$):**\n\nIn this regime, $H_k(\\epsilon)$ is the union of outlier clusters. We must prove that these clusters contribute a non-vanishing fraction of the total variance. The proof proceeds in three steps: (1) decompose variance using Law of Total Variance, (2) bound the within-cluster contribution, (3) show the outlier clusters capture most of the between-cluster contribution.\n\n**Step 1: Variance Decomposition.**\n\nFrom the Law of Total Variance (as used in the proof of [](#lem-outlier-cluster-fraction-lower-bound)), the total sum of squared deviations decomposes as:\n\n$$\nS_k = k \\cdot \\mathrm{Var}_k(x) = \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) + \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n$$\n\nwhere $\\{G_1, \\ldots, G_M\\}$ are the clusters, $\\mu$ is the global center of mass, and $\\mu_m$ is the center of mass of cluster $G_m$.\n\n**Step 2: Bounding Within-Cluster Contributions.**\n\nEach cluster has diameter at most $D_{\\text{diam}}(\\epsilon)$, so its internal variance satisfies $\\text{Var}(G_m) \\leq (D_{\\text{diam}}(\\epsilon)/2)^2$. The total within-cluster contribution for all clusters is:\n\n$$\n\\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) \\le k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2\n$$\n\n**Step 3: Outlier Clusters Capture the Between-Cluster Variance.**\n\nBy definition, the outlier clusters $O_M$ are chosen to capture at least a fraction $(1-\\varepsilon_O)$ of the between-cluster variance:\n\n$$\n\\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n$$\n\nNow, for any walker $i$ in an outlier cluster $G_m$ (where $m \\in O_M$), we decompose its squared deviation from the global mean:\n\n$$\n\\|\\delta_{x,k,i}\\|^2 = \\|x_i - \\mu\\|^2 = \\|x_i - \\mu_m + \\mu_m - \\mu\\|^2\n$$\n\nExpanding:\n\n$$\n\\|x_i - \\mu\\|^2 = \\|x_i - \\mu_m\\|^2 + \\|\\mu_m - \\mu\\|^2 + 2\\langle x_i - \\mu_m, \\mu_m - \\mu \\rangle\n$$\n\nSumming over all walkers in outlier clusters:\n\n$$\n\\sum_{m \\in O_M} \\sum_{i \\in G_m} \\|x_i - \\mu\\|^2 = \\sum_{m \\in O_M} \\sum_{i \\in G_m} \\|x_i - \\mu_m\\|^2 + \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 + 2\\sum_{m \\in O_M} \\left\\langle \\sum_{i \\in G_m}(x_i - \\mu_m), \\mu_m - \\mu \\right\\rangle\n$$\n\nThe cross-term vanishes because $\\sum_{i \\in G_m}(x_i - \\mu_m) = 0$ (by definition of cluster center of mass). Thus:\n\n$$\n\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 = \\sum_{m \\in O_M} |G_m|\\mathrm{Var}(G_m) + \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2\n$$\n\nThe first term is bounded above by the total within-cluster variance (Step 2), and the second term is bounded below by the outlier cluster guarantee (Step 3):\n\n$$\n\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n$$\n\nFrom Step 1, we know:\n\n$$\n\\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 = S_k - \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m)\n$$\n\nFor the high-variance regime, we have $\\text{Var}_k(x) > R^2_{\\text{var}}$, which gives:\n\n$$\n\\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 > k R^2_{\\mathrm{var}} - k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2 = k R^2_{\\mathrm{means}}\n$$\n\nwhere $R^2_{\\text{means}} := R^2_{\\text{var}} - (D_{\\text{diam}}(\\epsilon)/2)^2 > 0$ (by the premise of [](#lem-outlier-cluster-fraction-lower-bound)).\n\nCombining these results:\n\n$$\n\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge (1-\\varepsilon_O) k R^2_{\\mathrm{means}}\n$$\n\nSince the total sum of squared deviations is $S_k = k \\cdot \\text{Var}_k(x) > k R^2_{\\text{var}}$, we have:\n\n$$\n\\frac{\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2}{S_k} \\ge \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{R^2_{\\mathrm{var}}}\n$$\n\nThis establishes a positive, N-uniform constant:\n\n$$\nc_H := \\min\\left\\{1-\\varepsilon_O, \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{R^2_{\\mathrm{var}}}\\right\\} > 0\n$$",
      "raw_directive": "5042: \n5043: :::\n5044: :::{prf:proof}\n5045: :label: proof-lem-variance-concentration-Hk\n5046: \n5047: **Proof.**\n5048: This follows from the definition of $H_k(\\epsilon)$ in the two regimes established by the $\\epsilon$-dichotomy. We prove each regime separately.\n5049: \n5050: **1. Mean-Field Regime ($\\epsilon > D_{\\text{swarm}}$):**\n5051: \n5052: $H_k(\\epsilon)$ is the global outlier set $O_k$. By its definition ({prf:ref}`def-unified-high-low-error-sets`), this set is constructed specifically to contain at least a fraction $(1-\\varepsilon_O)$ of the total sum of squared deviations. In this case, $c_H = 1-\\varepsilon_O$.\n5053: \n5054: **2. Local-Interaction Regime ($\\epsilon \\leq D_{\\text{swarm}}$):**\n5055: \n5056: In this regime, $H_k(\\epsilon)$ is the union of outlier clusters. We must prove that these clusters contribute a non-vanishing fraction of the total variance. The proof proceeds in three steps: (1) decompose variance using Law of Total Variance, (2) bound the within-cluster contribution, (3) show the outlier clusters capture most of the between-cluster contribution.\n5057: \n5058: **Step 1: Variance Decomposition.**\n5059: \n5060: From the Law of Total Variance (as used in the proof of [](#lem-outlier-cluster-fraction-lower-bound)), the total sum of squared deviations decomposes as:\n5061: \n5062: $$\n5063: S_k = k \\cdot \\mathrm{Var}_k(x) = \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) + \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n5064: $$\n5065: \n5066: where $\\{G_1, \\ldots, G_M\\}$ are the clusters, $\\mu$ is the global center of mass, and $\\mu_m$ is the center of mass of cluster $G_m$.\n5067: \n5068: **Step 2: Bounding Within-Cluster Contributions.**\n5069: \n5070: Each cluster has diameter at most $D_{\\text{diam}}(\\epsilon)$, so its internal variance satisfies $\\text{Var}(G_m) \\leq (D_{\\text{diam}}(\\epsilon)/2)^2$. The total within-cluster contribution for all clusters is:\n5071: \n5072: $$\n5073: \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) \\le k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2\n5074: $$\n5075: \n5076: **Step 3: Outlier Clusters Capture the Between-Cluster Variance.**\n5077: \n5078: By definition, the outlier clusters $O_M$ are chosen to capture at least a fraction $(1-\\varepsilon_O)$ of the between-cluster variance:\n5079: \n5080: $$\n5081: \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n5082: $$\n5083: \n5084: Now, for any walker $i$ in an outlier cluster $G_m$ (where $m \\in O_M$), we decompose its squared deviation from the global mean:\n5085: \n5086: $$\n5087: \\|\\delta_{x,k,i}\\|^2 = \\|x_i - \\mu\\|^2 = \\|x_i - \\mu_m + \\mu_m - \\mu\\|^2\n5088: $$\n5089: \n5090: Expanding:\n5091: \n5092: $$\n5093: \\|x_i - \\mu\\|^2 = \\|x_i - \\mu_m\\|^2 + \\|\\mu_m - \\mu\\|^2 + 2\\langle x_i - \\mu_m, \\mu_m - \\mu \\rangle\n5094: $$\n5095: \n5096: Summing over all walkers in outlier clusters:\n5097: \n5098: $$\n5099: \\sum_{m \\in O_M} \\sum_{i \\in G_m} \\|x_i - \\mu\\|^2 = \\sum_{m \\in O_M} \\sum_{i \\in G_m} \\|x_i - \\mu_m\\|^2 + \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 + 2\\sum_{m \\in O_M} \\left\\langle \\sum_{i \\in G_m}(x_i - \\mu_m), \\mu_m - \\mu \\right\\rangle\n5100: $$\n5101: \n5102: The cross-term vanishes because $\\sum_{i \\in G_m}(x_i - \\mu_m) = 0$ (by definition of cluster center of mass). Thus:\n5103: \n5104: $$\n5105: \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 = \\sum_{m \\in O_M} |G_m|\\mathrm{Var}(G_m) + \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2\n5106: $$\n5107: \n5108: The first term is bounded above by the total within-cluster variance (Step 2), and the second term is bounded below by the outlier cluster guarantee (Step 3):\n5109: \n5110: $$\n5111: \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2\n5112: $$\n5113: \n5114: From Step 1, we know:\n5115: \n5116: $$\n5117: \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 = S_k - \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m)\n5118: $$\n5119: \n5120: For the high-variance regime, we have $\\text{Var}_k(x) > R^2_{\\text{var}}$, which gives:\n5121: \n5122: $$\n5123: \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2 > k R^2_{\\mathrm{var}} - k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2 = k R^2_{\\mathrm{means}}\n5124: $$\n5125: \n5126: where $R^2_{\\text{means}} := R^2_{\\text{var}} - (D_{\\text{diam}}(\\epsilon)/2)^2 > 0$ (by the premise of [](#lem-outlier-cluster-fraction-lower-bound)).\n5127: \n5128: Combining these results:\n5129: \n5130: $$\n5131: \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge (1-\\varepsilon_O) k R^2_{\\mathrm{means}}\n5132: $$\n5133: \n5134: Since the total sum of squared deviations is $S_k = k \\cdot \\text{Var}_k(x) > k R^2_{\\text{var}}$, we have:\n5135: \n5136: $$\n5137: \\frac{\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2}{S_k} \\ge \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{R^2_{\\mathrm{var}}}\n5138: $$\n5139: \n5140: This establishes a positive, N-uniform constant:\n5141: \n5142: $$\n5143: c_H := \\min\\left\\{1-\\varepsilon_O, \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{R^2_{\\mathrm{var}}}\\right\\} > 0\n5144: $$\n5145: ",
      "strategy_summary": "The proof analyzes the contribution of H_k(\u03b5) to total variance in two regimes: mean-field, where it directly follows from the outlier set definition, and local-interaction, where variance decomposition isolates between-cluster effects captured by outlier clusters.",
      "conclusion": {
        "text": null,
        "latex": null
      },
      "assumptions": [],
      "steps": [],
      "key_equations": [
        {
          "label": "eq-variance-decomp",
          "latex": "S_k = k \\cdot \\mathrm{Var}_k(x) = \\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) + \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2",
          "role": "Decomposition of total variance into within and between cluster components"
        },
        {
          "label": "eq-within-bound",
          "latex": "\\sum_{m=1}^M |G_m|\\mathrm{Var}(G_m) \\le k \\left(\\frac{D_{\\mathrm{diam}}(\\epsilon)}{2}\\right)^2",
          "role": "Upper bound on total within-cluster variance contribution"
        },
        {
          "label": "eq-outlier-between",
          "latex": "\\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2 \\ge (1-\\varepsilon_O) \\sum_{m=1}^M |G_m|\\|\\mu_m - \\mu\\|^2",
          "role": "Guarantee on outlier clusters capturing between-cluster variance"
        },
        {
          "label": "eq-deviation-expansion",
          "latex": "\\|x_i - \\mu\\|^2 = \\|x_i - \\mu_m\\|^2 + \\|\\mu_m - \\mu\\|^2 + 2\\langle x_i - \\mu_m, \\mu_m - \\mu \\rangle",
          "role": "Expansion of squared deviation from global mean via cluster mean"
        },
        {
          "label": "eq-sum-Hk",
          "latex": "\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\||^2 = \\sum_{m \\in O_M} |G_m|\\mathrm{Var}(G_m) + \\sum_{m \\in O_M} |G_m|\\|\\mu_m - \\mu\\|^2",
          "role": "Sum of squared deviations over outlier clusters after cross-term vanishes"
        },
        {
          "label": "eq-lower-bound",
          "latex": "\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\||^2 \\ge (1-\\varepsilon_O) k R^2_{\\mathrm{means}}",
          "role": "Lower bound on contribution from H_k(\u03b5) to total variance"
        },
        {
          "label": "eq-fraction",
          "latex": "\\frac{\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\||^2}{S_k} \\ge \\frac{(1-\\varepsilon_O) R^2_{\\mathrm{means}}}{R^2_{\\mathrm{var}}}",
          "role": "Fraction of total variance captured by H_k(\u03b5)"
        }
      ],
      "references": [
        "def-unified-high-low-error-sets",
        "lem-outlier-cluster-fraction-lower-bound"
      ],
      "math_tools": [
        {
          "toolName": "Law of Total Variance",
          "field": "Probability and Statistics",
          "description": "Decomposes the total variance of a random variable into the expected variance within subpopulations plus the variance of subpopulation means.",
          "roleInProof": "Decomposes total sum of squared deviations into within-cluster and between-cluster components to isolate the contribution from outlier clusters.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "ANOVA",
            "Variance Decomposition",
            "Clustering"
          ]
        },
        {
          "toolName": "Variance Bound via Diameter",
          "field": "Geometry and Statistics",
          "description": "Provides an upper bound on the variance within a bounded set using the square of half its diameter.",
          "roleInProof": "Bounds the within-cluster variance contributions to demonstrate that between-cluster variance dominates in the high-variance regime.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Euclidean Diameter",
            "Chebyshev Inequality",
            "Bounded Variance"
          ]
        },
        {
          "toolName": "Cross-Term Vanishing in Expansion",
          "field": "Linear Algebra",
          "description": "In the expansion of squared norms involving deviations from cluster and global means, the cross-term sums to zero due to the definition of the center of mass.",
          "roleInProof": "Simplifies the sum of squared deviations over outlier clusters by eliminating the cross-term, allowing clean separation of within and between components.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Vector Expansion",
            "Center of Mass",
            "Zero-Mean Property"
          ]
        }
      ],
      "cases": [
        {
          "name": "Mean-Field Regime",
          "condition": "\\epsilon > D_{\\text{swarm}}",
          "summary": "H_k(\\epsilon) = O_k by definition, capturing at least (1-\\varepsilon_O) of total sum of squared deviations, so c_H = 1-\\varepsilon_O."
        },
        {
          "name": "Local-Interaction Regime",
          "condition": "\\epsilon \\leq D_{\\text{swarm}}",
          "summary": "Variance decomposition and bounds show outlier clusters in H_k(\\epsilon) capture a positive fraction c_H > 0 of total variance, with explicit lower bound involving R^2_means and R^2_var."
        }
      ],
      "remarks": [
        {
          "type": "note",
          "text": "The constant c_H is N-uniform, independent of system size."
        },
        {
          "type": "definition",
          "text": "R^2_means := R^2_var - (D_diam(\u03b5)/2)^2 > 0 by high-variance assumption."
        }
      ],
      "gaps": [],
      "tags": [
        "variance-decomposition",
        "outlier-clusters",
        "law-total-variance",
        "regime-analysis",
        "cluster-bounds",
        "between-within-variance",
        "high-variance-regime"
      ],
      "document_id": "03_cloning",
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "span": {
        "start_line": 5042,
        "end_line": 5145,
        "content_start": 5045,
        "content_end": 5144,
        "header_lines": [
          5043
        ]
      },
      "metadata": {
        "label": "proof-lem-variance-concentration-Hk"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 8,
        "chapter_file": "chapter_8.json",
        "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "variance",
      "high-error set",
      "swarm",
      "concentration",
      "deviations",
      "N-uniform"
    ],
    "content_markdown": ":label: lem-variance-concentration-Hk\n\nLet a swarm $k$ be in a high-error state, $\\text{Var}_k(x) > R^2_{\\text{var}}$. There exists a strictly positive, N-uniform constant $c_H \\in (0, 1]$ such that the sum of squared deviations from the mean for the walkers in the unified high-error set $H_k(\\epsilon)$ is bounded below by a fixed fraction of the total sum of squared deviations:\n\n$$\n\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge c_H \\sum_{i \\in \\mathcal{A}_k} \\|\\delta_{x,k,i}\\|^2",
    "raw_directive": "5032: To make the proof self-contained, we first formalize a direct consequence of the geometric analysis in Chapter 6: that the high-error set, by its very nature, is responsible for a non-vanishing fraction of a swarm's internal variance.\n5033: \n5034: :::{prf:lemma} **(Variance Concentration in the High-Error Set)**\n5035: :label: lem-variance-concentration-Hk\n5036: \n5037: Let a swarm $k$ be in a high-error state, $\\text{Var}_k(x) > R^2_{\\text{var}}$. There exists a strictly positive, N-uniform constant $c_H \\in (0, 1]$ such that the sum of squared deviations from the mean for the walkers in the unified high-error set $H_k(\\epsilon)$ is bounded below by a fixed fraction of the total sum of squared deviations:\n5038: \n5039: $$\n5040: \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 \\ge c_H \\sum_{i \\in \\mathcal{A}_k} \\|\\delta_{x,k,i}\\|^2\n5041: $$",
    "document_id": "03_cloning",
    "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
    "span": {
      "start_line": 5032,
      "end_line": 5041,
      "content_start": 5035,
      "content_end": 5040,
      "header_lines": [
        5033
      ]
    },
    "references": [
      "def-unified-high-low-error-sets",
      "lem-outlier-cluster-fraction-lower-bound"
    ],
    "metadata": {
      "label": "lem-variance-concentration-Hk"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 8,
      "chapter_file": "chapter_8.json",
      "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-error-concentration-target-set",
    "title": "Error Concentration in the Target Set",
    "type": "lemma",
    "nl_statement": "For a swarm state (S\u2081, S\u2082) in the high-error regime with V_struct > R\u00b2_spread, where k is the high-variance swarm and I_target = I_{11} \u2229 U_k \u2229 H_k(\u03b5) is the critical target set, the average positional structural error in I_target is bounded below by c_err(\u03b5) V_struct - g_err(\u03b5).",
    "equations": [
      {
        "label": "ineq-lem-error-concentration",
        "latex": "\\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2 \\ge c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon)"
      }
    ],
    "hypotheses": [
      {
        "text": "Swarm state (S\u2081, S\u2082) is in the high-error regime",
        "latex": null
      },
      {
        "text": "V_struct > R\u00b2_spread",
        "latex": "V_{\\mathrm{struct}} > R^2_{\\mathrm{spread}}"
      },
      {
        "text": "k is the high-variance swarm",
        "latex": null
      },
      {
        "text": "I_target := I_{11} \u2229 U_k \u2229 H_k(\u03b5) is the critical target set",
        "latex": "I_{\\text{target}} := I_{11} \\cap U_k \\cap H_k(\\epsilon)"
      }
    ],
    "conclusion": {
      "text": "The positional structural error concentrated within this target set is bounded below by a linear function of the total structural error",
      "latex": "\\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2 \\ge c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon)"
    },
    "variables": [
      {
        "symbol": "S_1",
        "name": "S\u2081",
        "description": "First swarm component",
        "constraints": [],
        "tags": [
          "swarm"
        ]
      },
      {
        "symbol": "S_2",
        "name": "S\u2082",
        "description": "Second swarm component",
        "constraints": [],
        "tags": [
          "swarm"
        ]
      },
      {
        "symbol": "V_struct",
        "name": "V_struct",
        "description": "Total structural error variance",
        "constraints": [
          "> R\u00b2_spread"
        ],
        "tags": [
          "variance",
          "structural"
        ]
      },
      {
        "symbol": "R_spread",
        "name": "R_spread",
        "description": "Spread radius",
        "constraints": [],
        "tags": [
          "spread",
          "radius"
        ]
      },
      {
        "symbol": "k",
        "name": "k",
        "description": "High-variance swarm index",
        "constraints": [
          "high-variance"
        ],
        "tags": [
          "swarm",
          "index"
        ]
      },
      {
        "symbol": "I_target",
        "name": "I_target",
        "description": "Critical target set",
        "constraints": [
          "I_{11} \u2229 U_k \u2229 H_k(\u03b5)"
        ],
        "tags": [
          "set",
          "target"
        ]
      },
      {
        "symbol": "\u03b5",
        "name": "epsilon",
        "description": "Error parameter",
        "constraints": [],
        "tags": [
          "epsilon",
          "error"
        ]
      },
      {
        "symbol": "\u0394\u03b4_{x,i}",
        "name": "Delta delta_x,i",
        "description": "Positional structural error for agent i",
        "constraints": [],
        "tags": [
          "error",
          "positional"
        ]
      },
      {
        "symbol": "N",
        "name": "N",
        "description": "Total number of agents",
        "constraints": [],
        "tags": [
          "agents",
          "total"
        ]
      },
      {
        "symbol": "c_err(\u03b5)",
        "name": "c_err(epsilon)",
        "description": "Error concentration coefficient depending on \u03b5",
        "constraints": [],
        "tags": [
          "coefficient",
          "error"
        ]
      },
      {
        "symbol": "g_err(\u03b5)",
        "name": "g_err(epsilon)",
        "description": "Error gap term depending on \u03b5",
        "constraints": [],
        "tags": [
          "gap",
          "error"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Swarm state (S\u2081, S\u2082) satisfies necessary positivity and boundedness conditions for errors and variances",
        "confidence": 0.8
      },
      {
        "text": "Sets I_{11}, U_k, H_k(\u03b5) are non-empty and well-defined in the context",
        "confidence": 0.9
      },
      {
        "text": "N is the total number of agents in the swarm",
        "confidence": 1.0
      },
      {
        "text": "c_err(\u03b5) and g_err(\u03b5) are positive functions derived from prior analyses",
        "confidence": 0.7
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-error-concentration-target-set",
      "title": null,
      "type": "proof",
      "proves": "lem-error-concentration-target-set",
      "proof_type": "construction",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-error-concentration-target-set\n\n**Proof.**\n\nThe proof is constructive and proceeds in four steps. We first establish a linear relationship between the total system error $V_{\\text{struct}}$ and the internal variance of the high-variance swarm $k$. Second, we use this to find a linear lower bound on the error concentrated within the high-error set $H_k(\\epsilon)$. Third, we subtract the maximum possible error that can exist in the part of $H_k(\\epsilon)$ that is *not* our target set. Finally, we assemble these results to derive the N-uniform constants $c_{\\text{err}}$ and $g_{\\text{err}}$.\n\n**Notation and Scaling:** Let $k$ be the index of the high-variance swarm and $j$ be the index of the other swarm. Following {prf:ref}`def-variance-conversions`, we use:\n- $S_k = \\sum_{i \\in \\mathcal{A}_k} \\|\\delta_{x,k,i}\\|^2$: Un-normalized sum (total variance)\n- $V_{\\text{struct}}$: N-normalized structural error (Lyapunov component)\n- $E(S) := \\sum_{i \\in S} \\|\\Delta\\delta_{x,i}\\|^2$: Un-normalized error in set $S$\n\n**Key conversions used in this proof:**\n\n$$\nS_k = N \\cdot V_{\\text{Var},x}(S_k), \\quad \\frac{E(S)}{N} = \\text{(N-normalized error in set } S\\text{)}\n$$\n\n**Step 1: From Total System Error to Internal Swarm Variance.**\n\nFrom the proof of {prf:ref}`lem-V_Varx-implies-variance`, we have the inequality on the total sums of squared deviations: $N \\cdot V_{\\text{struct}} \\leq 2(S_k + S_j)$. This gives a lower bound on $S_k$:\n\n$$\nS_k \\ge \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - S_j\n$$\n\nThe positions of all walkers lie in the valid domain $\\mathcal{X}_{\\text{valid}}$ of diameter $D_{\\text{valid}}$. Thus, the maximum possible deviation from the mean for any walker is $D_{\\text{valid}}$. This provides a uniform upper bound on $S_j$: $S_j = \\sum_{i \\in \\mathcal{A}_j} \\|\\delta_{x,j,i}\\|^2 \\leq k_j \\cdot D_{\\text{valid}}^2 \\leq N \\cdot D_{\\text{valid}}^2$. Substituting this gives our first key inequality:\n\n$$\nS_k \\ge \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - N \\cdot D_{\\mathrm{valid}}^2 \\quad (*_1)\n$$\n\n**Step 2: From Internal Variance to Error in the High-Error Set $H_k$.**\n\nUsing the vector inequality $\\|a-b\\|^2 \\geq (1/2)\\|a\\|^2 - \\|b\\|^2$, we have $\\|\\Delta\\delta_{x,i}\\|^2 \\geq (1/2)\\|\\delta_{x,k,i}\\|^2 - \\|\\delta_{x,j,i}\\|^2$. Summing over the indices $i \\in H_k(\\epsilon)$:\n\n$$\nE(H_k) \\ge \\frac{1}{2}\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 - \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,j,i}\\|^2\n$$\n\nUsing **{prf:ref}`lem-variance-concentration-Hk`** on the first term and uniformly bounding the second term gives:\n\n$$\nE(H_k) \\ge \\frac{c_H}{2} S_k - |H_k(\\epsilon)| \\cdot D_{\\mathrm{valid}}^2 \\ge \\frac{c_H}{2} S_k - N \\cdot D_{\\mathrm{valid}}^2\n$$\n\nNow, substitute the lower bound for $S_k$ from inequality $(*_1)$:\n\n$$\n\\begin{aligned}\nE(H_k) &\\ge \\frac{c_H}{2} \\left( \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - N \\cdot D_{\\mathrm{valid}}^2 \\right) - N \\cdot D_{\\mathrm{valid}}^2 \\\\\n&= \\frac{c_H}{4} N \\cdot V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) N \\cdot D_{\\mathrm{valid}}^2\n\\end{aligned}\n$$\n\nDividing by $N$ gives the per-walker average error in $H_k(\\epsilon)$:\n\n$$\n\\frac{1}{N}E(H_k) \\ge \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2 \\quad (*_2)\n$$\n\nThis establishes that the error in $H_k$ is linearly bounded below by $V_{\\text{struct}}$.\n\n**Step 3: Bounding the Error Outside the Target Set.**\n\nThe error in our target set is $E(I_{\\text{target}}) = E(H_k) - E(H_k \\setminus I_{\\text{target}})$. We need a uniform upper bound for the error in the complement set $H_k \\setminus I_{\\text{target}}$. The maximum possible squared error for any single walker $i$ is $\\|\\Delta\\delta_{x,i}\\|^2 \\leq (2D_{\\text{valid}})^2 = 4D_{\\text{valid}}^2$. The total error is bounded by the size of the set times this maximum:\n\n$$\nE(H_k \\setminus I_{\\text{target}}) \\le |H_k \\setminus I_{\\text{target}}| \\cdot 4D_{\\mathrm{valid}}^2\n$$\n\nThe set $H_k \\setminus I_{\\text{target}}$ contains walkers that are in $H_k$ but not in the three-way intersection $I_{11} \\cap U_k \\cap H_k$. Crucially, from Chapter 7, we have N-uniform lower bounds on the fractional sizes of these sets relative to the $k$ alive walkers: $|H_k|/k \\geq f_H(\\epsilon)$ and $|I_{\\text{target}}|/k \\geq f_{UH}(\\epsilon)$. The size of the complement is $|H_k| - |I_{\\text{target}}|$. A simple and robust upper bound is to use the total number of alive walkers: $|H_k \\setminus I_{\\text{target}}| \\leq k$. Therefore:\n\n$$\nE(H_k \\setminus I_{\\text{target}}) \\le k \\cdot 4D_{\\mathrm{valid}}^2\n$$\n\n**Step 4: Final Assembly with Explicit Normalization.**\n\nWe assemble the final inequality for the **N-normalized** error in the target set. Starting from the un-normalized errors $E(\\cdot)$, we divide by $N$ to convert to Lyapunov normalization:\n\n$$\n\\frac{1}{N}E(I_{\\text{target}}) = \\frac{1}{N}E(H_k) - \\frac{1}{N}E(H_k \\setminus I_{\\text{target}})\n$$\n\n**Applying bounds from Steps 2-3:** Substitute the lower bound for the first term from $(*_2)$ and the upper bound for the second term from Step 3:\n\n$$\n\\ge \\left[ \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2 \\right] - \\frac{k \\cdot 4D_{\\mathrm{valid}}^2}{N}\n$$\n\n**N-uniformity:** Since $k \\leq N$ (number of alive walkers bounded by total slots), the ratio $k/N \\leq 1$ is state-dependent but uniformly bounded. We can weaken the inequality to achieve a clean, N-independent form by replacing $k/N$ with its worst case 1:\n\n$$\n\\ge \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2 - 4D_{\\mathrm{valid}}^2 = \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 5\\right) D_{\\mathrm{valid}}^2\n$$\n\nThis is the desired linear lower bound. We can now define the final, **explicitly N-uniform** constants:\n*   $c_{\\text{err}}(\\epsilon) := \\frac{c_H(\\epsilon)}{4}$\n*   $g_{\\text{err}}(\\epsilon) := \\left(\\frac{c_H(\\epsilon)}{2} + 5\\right) D_{\\mathrm{valid}}^2$\n\nSince $c_H(\\epsilon)$ is a positive N-uniform constant from our supporting lemma and $D_{\\text{valid}}$ is a fixed environmental parameter, both $c_{\\text{err}}(\\epsilon)$ and $g_{\\text{err}}(\\epsilon)$ are strictly N-uniform. This completes the proof.",
      "raw_directive": "5166: where $c_{err}(\\epsilon) > 0$ and $g_{err}(\\epsilon) \\ge 0$ are **strictly N-uniform constants**.\n5167: :::\n5168: :::{prf:proof}\n5169: :label: proof-lem-error-concentration-target-set\n5170: \n5171: **Proof.**\n5172: \n5173: The proof is constructive and proceeds in four steps. We first establish a linear relationship between the total system error $V_{\\text{struct}}$ and the internal variance of the high-variance swarm $k$. Second, we use this to find a linear lower bound on the error concentrated within the high-error set $H_k(\\epsilon)$. Third, we subtract the maximum possible error that can exist in the part of $H_k(\\epsilon)$ that is *not* our target set. Finally, we assemble these results to derive the N-uniform constants $c_{\\text{err}}$ and $g_{\\text{err}}$.\n5174: \n5175: **Notation and Scaling:** Let $k$ be the index of the high-variance swarm and $j$ be the index of the other swarm. Following {prf:ref}`def-variance-conversions`, we use:\n5176: - $S_k = \\sum_{i \\in \\mathcal{A}_k} \\|\\delta_{x,k,i}\\|^2$: Un-normalized sum (total variance)\n5177: - $V_{\\text{struct}}$: N-normalized structural error (Lyapunov component)\n5178: - $E(S) := \\sum_{i \\in S} \\|\\Delta\\delta_{x,i}\\|^2$: Un-normalized error in set $S$\n5179: \n5180: **Key conversions used in this proof:**\n5181: \n5182: $$\n5183: S_k = N \\cdot V_{\\text{Var},x}(S_k), \\quad \\frac{E(S)}{N} = \\text{(N-normalized error in set } S\\text{)}\n5184: $$\n5185: \n5186: **Step 1: From Total System Error to Internal Swarm Variance.**\n5187: \n5188: From the proof of {prf:ref}`lem-V_Varx-implies-variance`, we have the inequality on the total sums of squared deviations: $N \\cdot V_{\\text{struct}} \\leq 2(S_k + S_j)$. This gives a lower bound on $S_k$:\n5189: \n5190: $$\n5191: S_k \\ge \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - S_j\n5192: $$\n5193: \n5194: The positions of all walkers lie in the valid domain $\\mathcal{X}_{\\text{valid}}$ of diameter $D_{\\text{valid}}$. Thus, the maximum possible deviation from the mean for any walker is $D_{\\text{valid}}$. This provides a uniform upper bound on $S_j$: $S_j = \\sum_{i \\in \\mathcal{A}_j} \\|\\delta_{x,j,i}\\|^2 \\leq k_j \\cdot D_{\\text{valid}}^2 \\leq N \\cdot D_{\\text{valid}}^2$. Substituting this gives our first key inequality:\n5195: \n5196: $$\n5197: S_k \\ge \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - N \\cdot D_{\\mathrm{valid}}^2 \\quad (*_1)\n5198: $$\n5199: \n5200: **Step 2: From Internal Variance to Error in the High-Error Set $H_k$.**\n5201: \n5202: Using the vector inequality $\\|a-b\\|^2 \\geq (1/2)\\|a\\|^2 - \\|b\\|^2$, we have $\\|\\Delta\\delta_{x,i}\\|^2 \\geq (1/2)\\|\\delta_{x,k,i}\\|^2 - \\|\\delta_{x,j,i}\\|^2$. Summing over the indices $i \\in H_k(\\epsilon)$:\n5203: \n5204: $$\n5205: E(H_k) \\ge \\frac{1}{2}\\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,k,i}\\|^2 - \\sum_{i \\in H_k(\\epsilon)} \\|\\delta_{x,j,i}\\|^2\n5206: $$\n5207: \n5208: Using **{prf:ref}`lem-variance-concentration-Hk`** on the first term and uniformly bounding the second term gives:\n5209: \n5210: $$\n5211: E(H_k) \\ge \\frac{c_H}{2} S_k - |H_k(\\epsilon)| \\cdot D_{\\mathrm{valid}}^2 \\ge \\frac{c_H}{2} S_k - N \\cdot D_{\\mathrm{valid}}^2\n5212: $$\n5213: \n5214: Now, substitute the lower bound for $S_k$ from inequality $(*_1)$:\n5215: \n5216: $$\n5217: \\begin{aligned}\n5218: E(H_k) &\\ge \\frac{c_H}{2} \\left( \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - N \\cdot D_{\\mathrm{valid}}^2 \\right) - N \\cdot D_{\\mathrm{valid}}^2 \\\\\n5219: &= \\frac{c_H}{4} N \\cdot V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) N \\cdot D_{\\mathrm{valid}}^2\n5220: \\end{aligned}\n5221: $$\n5222: \n5223: Dividing by $N$ gives the per-walker average error in $H_k(\\epsilon)$:\n5224: \n5225: $$\n5226: \\frac{1}{N}E(H_k) \\ge \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2 \\quad (*_2)\n5227: $$\n5228: \n5229: This establishes that the error in $H_k$ is linearly bounded below by $V_{\\text{struct}}$.\n5230: \n5231: **Step 3: Bounding the Error Outside the Target Set.**\n5232: \n5233: The error in our target set is $E(I_{\\text{target}}) = E(H_k) - E(H_k \\setminus I_{\\text{target}})$. We need a uniform upper bound for the error in the complement set $H_k \\setminus I_{\\text{target}}$. The maximum possible squared error for any single walker $i$ is $\\|\\Delta\\delta_{x,i}\\|^2 \\leq (2D_{\\text{valid}})^2 = 4D_{\\text{valid}}^2$. The total error is bounded by the size of the set times this maximum:\n5234: \n5235: $$\n5236: E(H_k \\setminus I_{\\text{target}}) \\le |H_k \\setminus I_{\\text{target}}| \\cdot 4D_{\\mathrm{valid}}^2\n5237: $$\n5238: \n5239: The set $H_k \\setminus I_{\\text{target}}$ contains walkers that are in $H_k$ but not in the three-way intersection $I_{11} \\cap U_k \\cap H_k$. Crucially, from Chapter 7, we have N-uniform lower bounds on the fractional sizes of these sets relative to the $k$ alive walkers: $|H_k|/k \\geq f_H(\\epsilon)$ and $|I_{\\text{target}}|/k \\geq f_{UH}(\\epsilon)$. The size of the complement is $|H_k| - |I_{\\text{target}}|$. A simple and robust upper bound is to use the total number of alive walkers: $|H_k \\setminus I_{\\text{target}}| \\leq k$. Therefore:\n5240: \n5241: $$\n5242: E(H_k \\setminus I_{\\text{target}}) \\le k \\cdot 4D_{\\mathrm{valid}}^2\n5243: $$\n5244: \n5245: **Step 4: Final Assembly with Explicit Normalization.**\n5246: \n5247: We assemble the final inequality for the **N-normalized** error in the target set. Starting from the un-normalized errors $E(\\cdot)$, we divide by $N$ to convert to Lyapunov normalization:\n5248: \n5249: $$\n5250: \\frac{1}{N}E(I_{\\text{target}}) = \\frac{1}{N}E(H_k) - \\frac{1}{N}E(H_k \\setminus I_{\\text{target}})\n5251: $$\n5252: \n5253: **Applying bounds from Steps 2-3:** Substitute the lower bound for the first term from $(*_2)$ and the upper bound for the second term from Step 3:\n5254: \n5255: $$\n5256: \\ge \\left[ \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2 \\right] - \\frac{k \\cdot 4D_{\\mathrm{valid}}^2}{N}\n5257: $$\n5258: \n5259: **N-uniformity:** Since $k \\leq N$ (number of alive walkers bounded by total slots), the ratio $k/N \\leq 1$ is state-dependent but uniformly bounded. We can weaken the inequality to achieve a clean, N-independent form by replacing $k/N$ with its worst case 1:\n5260: \n5261: $$\n5262: \\ge \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2 - 4D_{\\mathrm{valid}}^2 = \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 5\\right) D_{\\mathrm{valid}}^2\n5263: $$\n5264: \n5265: This is the desired linear lower bound. We can now define the final, **explicitly N-uniform** constants:\n5266: *   $c_{\\text{err}}(\\epsilon) := \\frac{c_H(\\epsilon)}{4}$\n5267: *   $g_{\\text{err}}(\\epsilon) := \\left(\\frac{c_H(\\epsilon)}{2} + 5\\right) D_{\\mathrm{valid}}^2$\n5268: \n5269: Since $c_H(\\epsilon)$ is a positive N-uniform constant from our supporting lemma and $D_{\\text{valid}}$ is a fixed environmental parameter, both $c_{\\text{err}}(\\epsilon)$ and $g_{\\text{err}}(\\epsilon)$ are strictly N-uniform. This completes the proof.\n5270: ",
      "strategy_summary": "The proof constructs explicit N-uniform constants for error concentration in the target set by chaining linear bounds: relating total structural error to high-variance swarm variance, lower-bounding error in the high-error set, upper-bounding error outside the target, and assembling into a normalized linear inequality.",
      "conclusion": {
        "text": "The N-normalized error in the target set satisfies \\frac{1}{N} E(I_{\\text{target}}) \\ge c_{\\text{err}}(\\epsilon) V_{\\text{struct}} - g_{\\text{err}}(\\epsilon), where c_{\\text{err}}(\\epsilon) := \\frac{c_H(\\epsilon)}{4} > 0 and g_{\\text{err}}(\\epsilon) := \\left(\\frac{c_H(\\epsilon)}{2} + 5\\right) D_{\\text{valid}}^2 \\ge 0 are strictly N-uniform constants.",
        "latex": "\\frac{1}{N} E(I_{\\text{target}}) \\ge \\frac{c_H}{4} V_{\\text{struct}} - \\left(\\frac{c_H}{2} + 5\\right) D_{\\text{valid}}^2"
      },
      "assumptions": [
        {
          "text": "Positions of walkers lie in the valid domain \\mathcal{X}_{\\text{valid}} of finite diameter D_{\\text{valid}}.",
          "latex": null
        },
        {
          "text": "Supporting lemmas provide N-uniform constants c_H(\\epsilon) > 0 from variance concentration in H_k(\\epsilon).",
          "latex": null
        },
        {
          "text": "Fractional sizes |H_k|/k \\ge f_H(\\epsilon) and |I_{\\text{target}}|/k \\ge f_{UH}(\\epsilon) have N-uniform lower bounds.",
          "latex": null
        },
        {
          "text": "Number of alive walkers k \\le N.",
          "latex": null
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "bound-variance",
          "text": "Establish linear lower bound on high-variance swarm sum S_k from total error: S_k \\ge \\frac{N V_{\\text{struct}}}{2} - S_j \\ge \\frac{N V_{\\text{struct}}}{2} - N D_{\\text{valid}}^2.",
          "latex": "S_k \\ge \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - N \\cdot D_{\\mathrm{valid}}^2 \\quad (*_1)",
          "references": [
            "prf-lem-V_Varx-implies-variance"
          ],
          "derived_statement": "(*1)"
        },
        {
          "order": 2.0,
          "kind": "error-lower-bound",
          "text": "Lower bound error in high-error set H_k using squared norm inequality and variance concentration: \\frac{1}{N} E(H_k) \\ge \\frac{c_H}{4} V_{\\text{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\text{valid}}^2.",
          "latex": "\\frac{1}{N}E(H_k) \\ge \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2 \\quad (*_2)",
          "references": [
            "prf-lem-variance-concentration-Hk"
          ],
          "derived_statement": "(*2)"
        },
        {
          "order": 3.0,
          "kind": "complement-upper-bound",
          "text": "Upper bound error in H_k \\setminus I_{\\text{target}}: E(H_k \\setminus I_{\\text{target}}) \\le k \\cdot 4 D_{\\text{valid}}^2, so \\frac{1}{N} E(H_k \\setminus I_{\\text{target}}) \\le 4 D_{\\text{valid}}^2.",
          "latex": "E(H_k \\setminus I_{\\text{target}}) \\le |H_k \\setminus I_{\\text{target}}| \\cdot 4D_{\\mathrm{valid}}^2 \\le k \\cdot 4D_{\\mathrm{valid}}^2",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 4.0,
          "kind": "assembly",
          "text": "Combine bounds: \\frac{1}{N} E(I_{\\text{target}}) = \\frac{1}{N} E(H_k) - \\frac{1}{N} E(H_k \\setminus I_{\\text{target}}) \\ge \\frac{c_H}{4} V_{\\text{struct}} - \\left(\\frac{c_H}{2} + 5\\right) D_{\\text{valid}}^2, defining c_{\\text{err}} and g_{\\text{err}}.",
          "latex": null,
          "references": [],
          "derived_statement": null
        }
      ],
      "key_equations": [
        {
          "label": "eq-Sk-lower",
          "latex": "S_k \\ge \\frac{N \\cdot V_{\\mathrm{struct}}}{2} - N \\cdot D_{\\mathrm{valid}}^2",
          "role": "Lower bound on high-variance sum (*1)"
        },
        {
          "label": "eq-EHk-lower",
          "latex": "\\frac{1}{N}E(H_k) \\ge \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 1\\right) D_{\\mathrm{valid}}^2",
          "role": "Lower bound on normalized error in H_k (*2)"
        },
        {
          "label": "eq-complement-upper",
          "latex": "E(H_k \\setminus I_{\\text{target}}) \\le k \\cdot 4D_{\\mathrm{valid}}^2",
          "role": "Upper bound on error in complement"
        },
        {
          "label": "eq-final-bound",
          "latex": "\\frac{1}{N}E(I_{\\text{target}}) \\ge \\frac{c_H}{4} V_{\\mathrm{struct}} - \\left(\\frac{c_H}{2} + 5\\right) D_{\\mathrm{valid}}^2",
          "role": "Final N-normalized linear lower bound"
        }
      ],
      "references": [
        "def-variance-conversions",
        "lem-V_Varx-implies-variance",
        "lem-variance-concentration-Hk",
        "prf-lem-V_Varx-implies-variance",
        "prf-lem-variance-concentration-Hk"
      ],
      "math_tools": [
        {
          "toolName": "Squared norm inequality",
          "field": "Linear Algebra",
          "description": "The inequality \\|a - b\\|^2 \\geq (1/2)\\|a\\|^2 - \\|b\\|^2 for vectors a and b.",
          "roleInProof": "Relates squared errors \\|\\Delta\\delta_{x,i}\\|^2 to deviations \\|\\delta_{x,k,i}\\|^2 and \\|\\delta_{x,j,i}\\|^2, enabling lower bounds on error sums.",
          "levelOfAbstraction": "Technique",
          "relatedTools": []
        },
        {
          "toolName": "Uniform bounding",
          "field": "Analysis",
          "description": "Using domain diameter to bound maximum deviations and errors, e.g., \\|\\delta\\|^2 \\leq D^2.",
          "roleInProof": "Provides worst-case upper bounds on low-variance swarm variance and complement set errors to ensure N-uniformity.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Squared norm inequality"
          ]
        },
        {
          "toolName": "Variance concentration lemma",
          "field": "Probability",
          "description": "Lemma bounding the concentration of variance within high-error subsets like H_k(\\epsilon).",
          "roleInProof": "Lower-bounds the sum of deviations in H_k using c_H S_k.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": []
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "notation",
          "text": "Uses N-normalized structural error V_{\\text{struct}} and un-normalized sums like S_k = N V_{\\text{Var},x}(S_k)."
        },
        {
          "type": "N-uniformity",
          "text": "Bounds are made N-uniform by replacing k/N \\le 1 with 1 in worst case; relies on fixed D_{\\text{valid}} and c_H > 0."
        }
      ],
      "gaps": [],
      "tags": [
        "error-concentration",
        "N-uniform",
        "constructive",
        "variance-bounding",
        "Lyapunov-error",
        "target-set",
        "inequality-chaining"
      ],
      "document_id": "03_cloning",
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "span": {
        "start_line": 5166,
        "end_line": 5270,
        "content_start": 5169,
        "content_end": 5269,
        "header_lines": [
          5167
        ]
      },
      "metadata": {
        "label": "proof-lem-error-concentration-target-set"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 8,
        "chapter_file": "chapter_8.json",
        "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "error concentration",
      "target set",
      "structural error",
      "swarm state",
      "high-error regime",
      "variance",
      "inequality bound"
    ],
    "content_markdown": ":label: lem-error-concentration-target-set\n\nLet a swarm state $(S_1, S_2)$ be in the high-error regime, such that $V_{\\mathrm{struct}} > R^2_{\\mathrm{spread}}$. Let $k$ be the high-variance swarm and let $I_{\\text{target}} := I_{11} \\cap U_k \\cap H_k(\\epsilon)$ be the critical target set.\n\nThe positional structural error concentrated within this target set is bounded below by a linear function of the total structural error:\n\n$$\n\\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2 \\ge c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon)\n$$",
    "raw_directive": "5153: We can now prove the main lemma of this section.\n5154: \n5155: :::{prf:lemma} Error Concentration in the Target Set\n5156: :label: lem-error-concentration-target-set\n5157: \n5158: Let a swarm state $(S_1, S_2)$ be in the high-error regime, such that $V_{\\mathrm{struct}} > R^2_{\\mathrm{spread}}$. Let $k$ be the high-variance swarm and let $I_{\\text{target}} := I_{11} \\cap U_k \\cap H_k(\\epsilon)$ be the critical target set.\n5159: \n5160: The positional structural error concentrated within this target set is bounded below by a linear function of the total structural error:\n5161: \n5162: $$\n5163: \\frac{1}{N}\\sum_{i \\in I_{\\text{target}}} \\|\\Delta\\delta_{x,i}\\|^2 \\ge c_{err}(\\epsilon)V_{\\mathrm{struct}} - g_{err}(\\epsilon)\n5164: $$\n5165: ",
    "document_id": "03_cloning",
    "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
    "span": {
      "start_line": 5153,
      "end_line": 5165,
      "content_start": 5156,
      "content_end": 5164,
      "header_lines": [
        5154
      ]
    },
    "references": [
      "def-variance-conversions",
      "lem-V_Varx-implies-variance",
      "lem-variance-concentration-Hk",
      "prf-lem-V_Varx-implies-variance",
      "prf-lem-variance-concentration-Hk"
    ],
    "metadata": {
      "label": "lem-error-concentration-target-set"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 8,
      "chapter_file": "chapter_8.json",
      "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-dead-walker-clone-prob",
    "title": "Total Cloning Probability for Dead Walkers",
    "type": "lemma",
    "nl_statement": "Under the Axiom of Guaranteed Revival (\u03b5_clone \u00b7 p_max < \u03b7^{\u03b1+\u03b2}), any dead walker clones with probability 1.",
    "equations": [
      {
        "label": null,
        "latex": "i \\in \\mathcal{D}(S) \\implies p_i = 1"
      }
    ],
    "hypotheses": [
      {
        "text": "Axiom of Guaranteed Revival: \u03b5_clone \u00b7 p_max < \u03b7^{\u03b1+\u03b2}",
        "latex": "\\varepsilon_{\\text{clone}} \\cdot p_{\\max} < \\eta^{\\alpha + \\beta}"
      },
      {
        "text": "i is a dead walker in state S",
        "latex": "i \\in \\mathcal{D}(S)"
      }
    ],
    "conclusion": {
      "text": "The cloning probability for dead walker i is 1",
      "latex": "p_i = 1"
    },
    "variables": [
      {
        "symbol": "i",
        "name": "walker index",
        "description": "Index of a walker in the system",
        "constraints": [
          "i \\in \\mathcal{D}(S)"
        ],
        "tags": [
          "index",
          "dead"
        ]
      },
      {
        "symbol": "p_i",
        "name": "cloning probability",
        "description": "Cloning probability for walker i",
        "constraints": [],
        "tags": [
          "probability"
        ]
      },
      {
        "symbol": "\\varepsilon_{\\text{clone}}",
        "name": "cloning error",
        "description": "Error parameter in cloning process",
        "constraints": [],
        "tags": [
          "error"
        ]
      },
      {
        "symbol": "p_{\\max}",
        "name": "maximum probability",
        "description": "Upper bound on probabilities in the system",
        "constraints": [],
        "tags": [
          "maximum"
        ]
      },
      {
        "symbol": "\\eta",
        "name": "eta parameter",
        "description": "Base parameter in exponential bound",
        "constraints": [],
        "tags": [
          "base"
        ]
      },
      {
        "symbol": "\\alpha",
        "name": "alpha exponent",
        "description": "Exponent in the revival bound",
        "constraints": [],
        "tags": [
          "exponent"
        ]
      },
      {
        "symbol": "\\beta",
        "name": "beta exponent",
        "description": "Exponent in the revival bound",
        "constraints": [],
        "tags": [
          "exponent"
        ]
      },
      {
        "symbol": "\\mathcal{D}(S)",
        "name": "dead walkers set",
        "description": "Set of dead walkers in state S",
        "constraints": [],
        "tags": [
          "set",
          "dead"
        ]
      }
    ],
    "implicit_assumptions": [],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-dead-walker-clone-prob",
      "title": null,
      "type": "proof",
      "proves": "lem-dead-walker-clone-prob",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-lem-dead-walker-clone-prob\n\nFor a dead walker $i$, the fitness potential is $V_{\\text{fit},i} = 0$. Any alive companion $c_i$ has $V_{\\text{fit},c_i} \\geq \\eta^{\\alpha+\\beta}$ by {prf:ref}`lem-potential-bounds`.\n\nThe cloning score is:\n\n$$\nS_i = \\frac{V_{\\text{fit},c_i} - 0}{0 + \\varepsilon_{\\text{clone}}} = \\frac{V_{\\text{fit},c_i}}{\\varepsilon_{\\text{clone}}} \\geq \\frac{\\eta^{\\alpha+\\beta}}{\\varepsilon_{\\text{clone}}}\n$$\n\nBy the revival axiom: $\\frac{\\eta^{\\alpha+\\beta}}{\\varepsilon_{\\text{clone}}} > p_{\\max}$\n\nSince $T_i \\in [0, p_{\\max}]$, we have $S_i > T_i$ with probability 1.",
      "raw_directive": "6042: :::\n6043: \n6044: :::{prf:proof}\n6045: :label: proof-lem-dead-walker-clone-prob\n6046: \n6047: For a dead walker $i$, the fitness potential is $V_{\\text{fit},i} = 0$. Any alive companion $c_i$ has $V_{\\text{fit},c_i} \\geq \\eta^{\\alpha+\\beta}$ by {prf:ref}`lem-potential-bounds`.\n6048: \n6049: The cloning score is:\n6050: \n6051: $$\n6052: S_i = \\frac{V_{\\text{fit},c_i} - 0}{0 + \\varepsilon_{\\text{clone}}} = \\frac{V_{\\text{fit},c_i}}{\\varepsilon_{\\text{clone}}} \\geq \\frac{\\eta^{\\alpha+\\beta}}{\\varepsilon_{\\text{clone}}}\n6053: $$\n6054: \n6055: By the revival axiom: $\\frac{\\eta^{\\alpha+\\beta}}{\\varepsilon_{\\text{clone}}} > p_{\\max}$\n6056: \n6057: Since $T_i \\in [0, p_{\\max}]$, we have $S_i > T_i$ with probability 1.\n6058: ",
      "strategy_summary": "The proof directly calculates the cloning score for a dead walker using the given fitness potential bounds and the revival axiom, demonstrating that the score strictly exceeds the random threshold with probability 1.",
      "conclusion": {
        "text": "we have $S_i > T_i$ with probability 1.",
        "latex": "$S_i > T_i$ with probability 1"
      },
      "assumptions": [
        {
          "text": "Revival axiom: $\\frac{\\eta^{\\alpha+\\beta}}{\\varepsilon_{\\text{clone}}} > p_{\\max}$",
          "latex": null
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "setup",
          "text": "For a dead walker $i$, the fitness potential is $V_{\\text{fit},i} = 0$.",
          "latex": null,
          "references": [],
          "derived_statement": null
        },
        {
          "order": 2.0,
          "kind": "bound",
          "text": "Any alive companion $c_i$ has $V_{\\text{fit},c_i} \\geq \\eta^{\\alpha+\\beta}$ by lem-potential-bounds.",
          "latex": null,
          "references": [
            "lem-potential-bounds"
          ],
          "derived_statement": "$V_{\\text{fit},c_i} \\geq \\eta^{\\alpha+\\beta}$"
        },
        {
          "order": 3.0,
          "kind": "calculation",
          "text": "The cloning score is $S_i = \\frac{V_{\\text{fit},c_i} - 0}{0 + \\varepsilon_{\\text{clone}}} = \\frac{V_{\\text{fit},c_i}}{\\varepsilon_{\\text{clone}}} \\geq \\frac{\\eta^{\\alpha+\\beta}}{\\varepsilon_{\\text{clone}}}$",
          "latex": "$S_i = \\frac{V_{\\text{fit},c_i}}{\\varepsilon_{\\text{clone}}} \\geq \\frac{\\eta^{\\alpha+\\beta}}{\\varepsilon_{\\text{clone}}}$",
          "references": [],
          "derived_statement": "$S_i \\geq \\frac{\\eta^{\\alpha+\\beta}}{\\varepsilon_{\\text{clone}}}$"
        },
        {
          "order": 4.0,
          "kind": "application",
          "text": "By the revival axiom: $\\frac{\\eta^{\\alpha+\\beta}}{\\varepsilon_{\\text{clone}}} > p_{\\max}$",
          "latex": null,
          "references": [
            "revival-axiom"
          ],
          "derived_statement": "$S_i > p_{\\max}$"
        },
        {
          "order": 5.0,
          "kind": "conclusion",
          "text": "Since $T_i \\in [0, p_{\\max}]$, we have $S_i > T_i$ with probability 1.",
          "latex": null,
          "references": [],
          "derived_statement": "$S_i > T_i$ with probability 1"
        }
      ],
      "key_equations": [
        {
          "label": "eq-cloning-score",
          "latex": "S_i = \\frac{V_{\\text{fit},c_i} - 0}{0 + \\varepsilon_{\\text{clone}}} = \\frac{V_{\\text{fit},c_i}}{\\varepsilon_{\\text{clone}}} \\geq \\frac{\\eta^{\\alpha+\\beta}}{\\varepsilon_{\\text{clone}}}",
          "role": "Defines and bounds the cloning score for a dead walker"
        }
      ],
      "references": [
        "lem-potential-bounds"
      ],
      "math_tools": [],
      "cases": [],
      "remarks": [],
      "gaps": [],
      "tags": [
        "dead-walker",
        "cloning-score",
        "fitness-potential",
        "probability",
        "revival-axiom"
      ],
      "document_id": "03_cloning",
      "section": "## 9.3. Decomposition into Sub-Operators",
      "span": {
        "start_line": 6042,
        "end_line": 6058,
        "content_start": 6045,
        "content_end": 6057,
        "header_lines": [
          6043
        ]
      },
      "metadata": {
        "label": "proof-lem-dead-walker-clone-prob"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 11,
        "chapter_file": "chapter_11.json",
        "section_id": "## 9.3. Decomposition into Sub-Operators"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "dead-walker",
      "cloning",
      "probability",
      "guaranteed-revival",
      "axiom"
    ],
    "content_markdown": ":label: lem-dead-walker-clone-prob\n\nUnder the Axiom of Guaranteed Revival ($\\varepsilon_{\\text{clone}} \\cdot p_{\\max} < \\eta^{\\alpha+\\beta}$), any dead walker clones with probability 1:\n\n$$\ni \\in \\mathcal{D}(S) \\implies p_i = 1",
    "raw_directive": "6031: :::\n6032: \n6033: :::{prf:lemma} Total Cloning Probability for Dead Walkers\n6034: :label: lem-dead-walker-clone-prob\n6035: \n6036: Under the Axiom of Guaranteed Revival ($\\varepsilon_{\\text{clone}} \\cdot p_{\\max} < \\eta^{\\alpha+\\beta}$), any dead walker clones with probability 1:\n6037: \n6038: $$\n6039: i \\in \\mathcal{D}(S) \\implies p_i = 1\n6040: $$",
    "document_id": "03_cloning",
    "section": "## 9.3. Decomposition into Sub-Operators",
    "span": {
      "start_line": 6031,
      "end_line": 6040,
      "content_start": 6034,
      "content_end": 6039,
      "header_lines": [
        6032
      ]
    },
    "references": [
      "lem-potential-bounds"
    ],
    "metadata": {
      "label": "lem-dead-walker-clone-prob"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 11,
      "chapter_file": "chapter_11.json",
      "section_id": "## 9.3. Decomposition into Sub-Operators"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-variance-change-decomposition",
    "title": "Variance Change Decomposition",
    "type": "lemma",
    "nl_statement": "The total change in positional variance decomposes into sums over alive walker contributions and status change contributions for k from 1 to 2.",
    "equations": [
      {
        "label": null,
        "latex": "\\Delta V_{\\text{Var},x} = \\sum_{k=1}^{2} \\left[\\Delta V_{\\text{Var},x}^{(k,\\text{alive})} + \\Delta V_{\\text{Var},x}^{(k,\\text{status})}\\right]"
      },
      {
        "label": null,
        "latex": "\\Delta V_{\\text{Var},x}^{(k,\\text{alive})} = \\frac{1}{N}\\sum_{i \\in \\mathcal{A}(S_k)} \\left[\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right]"
      },
      {
        "label": null,
        "latex": "\\Delta V_{\\text{Var},x}^{(k,\\text{status})} = \\frac{1}{N}\\sum_{i \\in \\mathcal{D}(S_k)} \\|\\delta'_{x,k,i}\\|^2"
      }
    ],
    "hypotheses": [],
    "conclusion": {
      "text": "The total change in positional variance can be decomposed as \\Delta V_{\\text{Var},x} = \\sum_{k=1}^{2} \\left[\\Delta V_{\\text{Var},x}^{(k,\\text{alive})} + \\Delta V_{\\text{Var},x}^{(k,\\text{status})}\\right], with the specified contributions from alive walkers and status changes.",
      "latex": "\\Delta V_{\\text{Var},x} = \\sum_{k=1}^{2} \\left[\\Delta V_{\\text{Var},x}^{(k,\\text{alive})} + \\Delta V_{\\text{Var},x}^{(k,\\text{status})}\\right]"
    },
    "variables": [
      {
        "symbol": "\\Delta V_{\\text{Var},x}",
        "name": "total-variance-change",
        "description": "Total change in positional variance.",
        "constraints": [],
        "tags": [
          "variance"
        ]
      },
      {
        "symbol": "N",
        "name": "total-walkers",
        "description": "Total number of walkers.",
        "constraints": [
          "positive-integer"
        ],
        "tags": [
          "count"
        ]
      },
      {
        "symbol": "k",
        "name": "dimension-index",
        "description": "Index for dimensions, from 1 to 2.",
        "constraints": [
          "k \\in {1,2}"
        ],
        "tags": [
          "index"
        ]
      },
      {
        "symbol": "\\mathcal{A}(S_k)",
        "name": "alive-set",
        "description": "Set of alive walkers in state S_k.",
        "constraints": [],
        "tags": [
          "alive"
        ]
      },
      {
        "symbol": "\\mathcal{D}(S_k)",
        "name": "dead-set",
        "description": "Set of dead or status-changed walkers in state S_k.",
        "constraints": [],
        "tags": [
          "status-change"
        ]
      },
      {
        "symbol": "\\delta_{x,k,i}",
        "name": "centered-position",
        "description": "Centered position of walker i in dimension k before update.",
        "constraints": [],
        "tags": [
          "position"
        ]
      },
      {
        "symbol": "\\delta'_{x,k,i}",
        "name": "updated-centered-position",
        "description": "Centered position of walker i in dimension k after cloning or update.",
        "constraints": [],
        "tags": [
          "position"
        ]
      },
      {
        "symbol": "S_k",
        "name": "state-k",
        "description": "State associated with dimension k.",
        "constraints": [],
        "tags": [
          "state"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "N is the fixed total number of walkers.",
        "confidence": 1.0
      },
      {
        "text": "Positions are in a vector space with Euclidean norm.",
        "confidence": 0.9
      },
      {
        "text": "\\delta_{x,k,i} represents centered deviations from the mean position.",
        "confidence": 1.0
      },
      {
        "text": "The decomposition applies in the context of a walker simulation with cloning and status updates.",
        "confidence": 0.8
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-variance-change-decomposition",
      "title": null,
      "type": "proof",
      "proves": "lem-variance-change-decomposition",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":::{prf:proof}\n:label: proof-lem-variance-change-decomposition\n**Proof.**\n\nFollowing {prf:ref}`def-variance-conversions`, recall that $V_{\\text{Var},x}$ is **$N$-normalized** (per walker slot):\n\n$$\nV_{\\text{Var},x}(S_k) = \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2\n$$\n\nAfter cloning, all walkers are alive (dead walkers are revived), so:\n\n$$\nV_{\\text{Var},x}(S'_k) = \\frac{1}{N} \\sum_{i=1}^{N} \\|\\delta'_{x,k,i}\\|^2\n$$\n\nThe change is (keeping $\\frac{1}{N}$ normalization throughout):\n\n$$\n\\Delta V_{\\text{Var},x}^{(k)} = \\frac{1}{N} \\sum_{i=1}^{N} \\|\\delta'_{x,k,i}\\|^2 - \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2\n$$\n\nSplit the first sum into alive and dead walkers in the input state:\n\n$$\n\\Delta V_{\\text{Var},x}^{(k)} = \\frac{1}{N}\\sum_{i \\in \\mathcal{A}(S_k)} \\left[\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right] + \\frac{1}{N}\\sum_{i \\in \\mathcal{D}(S_k)} \\|\\delta'_{x,k,i}\\|^2\n$$\n\nThis decomposition preserves the N-normalization, ensuring all subsequent bounds are N-uniform.",
      "raw_directive": "6423: :::\n6424: \n6425: :::{prf:proof}\n6426: :label: proof-lem-variance-change-decomposition\n6427: **Proof.**\n6428: \n6429: Following {prf:ref}`def-variance-conversions`, recall that $V_{\\text{Var},x}$ is **$N$-normalized** (per walker slot):\n6430: \n6431: $$\n6432: V_{\\text{Var},x}(S_k) = \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2\n6433: $$\n6434: \n6435: After cloning, all walkers are alive (dead walkers are revived), so:\n6436: \n6437: $$\n6438: V_{\\text{Var},x}(S'_k) = \\frac{1}{N} \\sum_{i=1}^{N} \\|\\delta'_{x,k,i}\\|^2\n6439: $$\n6440: \n6441: The change is (keeping $\\frac{1}{N}$ normalization throughout):\n6442: \n6443: $$\n6444: \\Delta V_{\\text{Var},x}^{(k)} = \\frac{1}{N} \\sum_{i=1}^{N} \\|\\delta'_{x,k,i}\\|^2 - \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2\n6445: $$\n6446: \n6447: Split the first sum into alive and dead walkers in the input state:\n6448: \n6449: $$\n6450: \\Delta V_{\\text{Var},x}^{(k)} = \\frac{1}{N}\\sum_{i \\in \\mathcal{A}(S_k)} \\left[\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right] + \\frac{1}{N}\\sum_{i \\in \\mathcal{D}(S_k)} \\|\\delta'_{x,k,i}\\|^2\n6451: $$\n6452: \n6453: This decomposition preserves the N-normalization, ensuring all subsequent bounds are N-uniform.\n6454: ",
      "strategy_summary": "The proof recalls the N-normalized variance definition and decomposes the change in variance after cloning into separate contributions from modifications to alive walkers and the addition of revived dead walkers, preserving uniform N-normalization.",
      "conclusion": {
        "text": "This decomposition preserves the N-normalization, ensuring all subsequent bounds are N-uniform.",
        "latex": null
      },
      "assumptions": [
        {
          "text": "V_{\\text{Var},x} is N-normalized per walker slot as per def-variance-conversions.",
          "latex": null
        },
        {
          "text": "After cloning, all walkers are alive (dead walkers revived).",
          "latex": null
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "recall",
          "text": "Recall that V_{\\text{Var},x}(S_k) = \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2.",
          "latex": "V_{\\text{Var},x}(S_k) = \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2",
          "references": [
            "def-variance-conversions"
          ],
          "derived_statement": null
        },
        {
          "order": 2.0,
          "kind": "definition",
          "text": "After cloning, all walkers are alive, so V_{\\text{Var},x}(S'_k) = \\frac{1}{N} \\sum_{i=1}^{N} \\|\\delta'_{x,k,i}\\|^2.",
          "latex": "V_{\\text{Var},x}(S'_k) = \\frac{1}{N} \\sum_{i=1}^{N} \\|\\delta'_{x,k,i}\\|^2",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 3.0,
          "kind": "computation",
          "text": "The change is \\Delta V_{\\text{Var},x}^{(k)} = \\frac{1}{N} \\sum_{i=1}^{N} \\|\\delta'_{x,k,i}\\|^2 - \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2.",
          "latex": "\\Delta V_{\\text{Var},x}^{(k)} = \\frac{1}{N} \\sum_{i=1}^{N} \\|\\delta'_{x,k,i}\\|^2 - \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 4.0,
          "kind": "decomposition",
          "text": "Split the first sum into alive and dead walkers: \\Delta V_{\\text{Var},x}^{(k)} = \\frac{1}{N}\\sum_{i \\in \\mathcal{A}(S_k)} \\left[\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right] + \\frac{1}{N}\\sum_{i \\in \\mathcal{D}(S_k)} \\|\\delta'_{x,k,i}\\|^2.",
          "latex": "\\Delta V_{\\text{Var},x}^{(k)} = \\frac{1}{N}\\sum_{i \\in \\mathcal{A}(S_k)} \\left[\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right] + \\frac{1}{N}\\sum_{i \\in \\mathcal{D}(S_k)} \\|\\delta'_{x,k,i}\\|^2",
          "references": [],
          "derived_statement": "Decomposition of variance change."
        }
      ],
      "key_equations": [
        {
          "label": "eq-v-var-sk",
          "latex": "V_{\\text{Var},x}(S_k) = \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2",
          "role": "Pre-cloning variance"
        },
        {
          "label": "eq-v-var-sk-prime",
          "latex": "V_{\\text{Var},x}(S'_k) = \\frac{1}{N} \\sum_{i=1}^{N} \\|\\delta'_{x,k,i}\\|^2",
          "role": "Post-cloning variance"
        },
        {
          "label": "eq-delta-v",
          "latex": "\\Delta V_{\\text{Var},x}^{(k)} = \\frac{1}{N} \\sum_{i=1}^{N} \\|\\delta'_{x,k,i}\\|^2 - \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|\\delta_{x,k,i}\\|^2",
          "role": "Variance change"
        },
        {
          "label": "eq-decomp-delta",
          "latex": "\\Delta V_{\\text{Var},x}^{(k)} = \\frac{1}{N}\\sum_{i \\in \\mathcal{A}(S_k)} \\left[\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right] + \\frac{1}{N}\\sum_{i \\in \\mathcal{D}(S_k)} \\|\\delta'_{x,k,i}\\|^2",
          "role": "Decomposed variance change"
        }
      ],
      "references": [
        "def-variance-conversions"
      ],
      "math_tools": [
        {
          "toolName": "Euclidean norm",
          "field": "Linear Algebra",
          "description": "The squared Euclidean norm measures the displacement of walker positions.",
          "roleInProof": "Used to compute variance as average squared deviations.",
          "levelOfAbstraction": "Notation",
          "relatedTools": []
        },
        {
          "toolName": "Summation decomposition",
          "field": "Analysis",
          "description": "Splitting a sum over a full set into disjoint subsets.",
          "roleInProof": "Separates the variance change into alive and dead walker contributions.",
          "levelOfAbstraction": "Technique",
          "relatedTools": []
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "note",
          "text": "This decomposition preserves the N-normalization, ensuring all subsequent bounds are N-uniform."
        }
      ],
      "gaps": [],
      "tags": [
        "variance",
        "decomposition",
        "cloning",
        "N-normalization",
        "walkers",
        "alive-dead"
      ],
      "document_id": "03_cloning",
      "section": "## 10.3. Positional Variance Contraction",
      "span": {
        "start_line": 6423,
        "end_line": 6454,
        "content_start": 6425,
        "content_end": 6453,
        "header_lines": [
          6424
        ]
      },
      "metadata": {
        "label": "proof-lem-variance-change-decomposition"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 17,
        "chapter_file": "chapter_17.json",
        "section_id": "## 10.3. Positional Variance Contraction"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "variance-decomposition",
      "positional-variance",
      "alive-walkers",
      "status-changes",
      "cloning",
      "centered-positions"
    ],
    "content_markdown": ":label: lem-variance-change-decomposition\n\nThe total change in positional variance can be decomposed as:\n\n$$\n\\Delta V_{\\text{Var},x} = \\sum_{k=1}^{2} \\left[\\underbrace{\\Delta V_{\\text{Var},x}^{(k,\\text{alive})}}_{\\text{alive walkers}} + \\underbrace{\\Delta V_{\\text{Var},x}^{(k,\\text{status})}}_{\\text{status changes}}\\right]\n$$\n\nwhere:\n\n1. **Alive walker contribution:**\n\n\n$$\n\\Delta V_{\\text{Var},x}^{(k,\\text{alive})} = \\frac{1}{N}\\sum_{i \\in \\mathcal{A}(S_k)} \\left[\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right]\n$$\n\n   where $\\delta'_{x,k,i}$ is the centered position after cloning.\n\n2. **Status change contribution:**\n\n\n$$\n\\Delta V_{\\text{Var},x}^{(k,\\text{status})} = \\frac{1}{N}\\sum_{i \\in \\mathcal{D}(S_k)} \\|\\delta'_{x,k,i}\\|^2\n$$",
    "raw_directive": "6393: ### 10.3.3. Variance Decomposition\n6394: \n6395: :::{prf:lemma} Variance Change Decomposition\n6396: :label: lem-variance-change-decomposition\n6397: \n6398: The total change in positional variance can be decomposed as:\n6399: \n6400: $$\n6401: \\Delta V_{\\text{Var},x} = \\sum_{k=1}^{2} \\left[\\underbrace{\\Delta V_{\\text{Var},x}^{(k,\\text{alive})}}_{\\text{alive walkers}} + \\underbrace{\\Delta V_{\\text{Var},x}^{(k,\\text{status})}}_{\\text{status changes}}\\right]\n6402: $$\n6403: \n6404: where:\n6405: \n6406: 1. **Alive walker contribution:**\n6407: \n6408: \n6409: $$\n6410: \\Delta V_{\\text{Var},x}^{(k,\\text{alive})} = \\frac{1}{N}\\sum_{i \\in \\mathcal{A}(S_k)} \\left[\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right]\n6411: $$\n6412: \n6413:    where $\\delta'_{x,k,i}$ is the centered position after cloning.\n6414: \n6415: 2. **Status change contribution:**\n6416: \n6417: \n6418: $$\n6419: \\Delta V_{\\text{Var},x}^{(k,\\text{status})} = \\frac{1}{N}\\sum_{i \\in \\mathcal{D}(S_k)} \\|\\delta'_{x,k,i}\\|^2\n6420: $$\n6421: ",
    "document_id": "03_cloning",
    "section": "## 10.3. Positional Variance Contraction",
    "span": {
      "start_line": 6393,
      "end_line": 6421,
      "content_start": 6396,
      "content_end": 6420,
      "header_lines": [
        6394
      ]
    },
    "references": [
      "def-variance-conversions"
    ],
    "metadata": {
      "label": "lem-variance-change-decomposition"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 17,
      "chapter_file": "chapter_17.json",
      "section_id": "## 10.3. Positional Variance Contraction"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-keystone-contraction-alive",
    "title": "Keystone-Driven Contraction for Stably Alive Walkers",
    "type": "lemma",
    "nl_statement": "For walkers in the stably alive set $I_{11}$, the expected change in their contribution to variance satisfies $\\mathbb{E}_{\\text{clone}}\\left[\\sum_{i \\in I_{11}} \\sum_{k=1,2} \\left(\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right)\\right] \\leq -\\frac{\\chi(\\epsilon)}{2N} \\cdot V_{\\text{struct}} + \\frac{g_{\\max}(\\epsilon)}{N} + C_{\\text{pers}}$, where $\\chi(\\epsilon) > 0$ and $g_{\\max}(\\epsilon)$ are the Keystone constants, and $C_{\\text{pers}}$ accounts for persisting walkers.",
    "equations": [
      {
        "label": null,
        "latex": "\\mathbb{E}_{\\text{clone}}\\left[\\sum_{i \\in I_{11}} \\sum_{k=1,2} \\left(\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right)\\right] \\leq -\\frac{\\chi(\\epsilon)}{2N} \\cdot V_{\\text{struct}} + \\frac{g_{\\max}(\\epsilon)}{N} + C_{\\text{pers}}"
      }
    ],
    "hypotheses": [
      {
        "text": "Walkers in the stably alive set $I_{11}$",
        "latex": null
      }
    ],
    "conclusion": {
      "text": null,
      "latex": "\\mathbb{E}_{\\text{clone}}\\left[\\sum_{i \\in I_{11}} \\sum_{k=1,2} \\left(\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right)\\right] \\leq -\\frac{\\chi(\\epsilon)}{2N} \\cdot V_{\\text{struct}} + \\frac{g_{\\max}(\\epsilon)}{N} + C_{\\text{pers}}"
    },
    "variables": [
      {
        "symbol": "I_{11}",
        "name": "stably alive set",
        "description": "Set of walkers alive in both swarms",
        "constraints": [],
        "tags": [
          "set",
          "walkers"
        ]
      },
      {
        "symbol": "\\chi(\\epsilon)",
        "name": "chi epsilon",
        "description": "Positive Keystone constant",
        "constraints": [
          "\\chi(\\epsilon) > 0"
        ],
        "tags": [
          "constant",
          "keystone"
        ]
      },
      {
        "symbol": "g_{\\max}(\\epsilon)",
        "name": "g max epsilon",
        "description": "Maximum Keystone constant",
        "constraints": [],
        "tags": [
          "constant",
          "keystone"
        ]
      },
      {
        "symbol": "V_{\\text{struct}}",
        "name": "structural variance",
        "description": "Structural component of variance",
        "constraints": [],
        "tags": [
          "variance"
        ]
      },
      {
        "symbol": "C_{\\text{pers}}",
        "name": "persistence constant",
        "description": "Term accounting for persisting walkers",
        "constraints": [],
        "tags": [
          "constant",
          "persistence"
        ]
      },
      {
        "symbol": "N",
        "name": "number of walkers",
        "description": "Total number of walkers per swarm",
        "constraints": [],
        "tags": [
          "parameter",
          "scale"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Definitions of delta vectors and cloning process from context",
        "confidence": 0.9
      },
      {
        "text": "Keystone constants defined in referenced lemma",
        "confidence": 1.0
      }
    ],
    "local_refs": [
      "lem-quantitative-keystone"
    ],
    "proof": {
      "label": "proof-lem-keystone-contraction-alive",
      "title": null,
      "type": "proof",
      "proves": "lem-keystone-contraction-alive",
      "proof_type": "direct",
      "proof_status": "sketch",
      "content_markdown": ":::{prf:proof}\n:label: proof-lem-keystone-contraction-alive\n**Proof.**\n\nWe analyze the variance change for each walker $i \\in I_{11}$ by conditioning on its cloning action.\n\n**Case 1: Walker $i$ clones in at least one swarm**\n\nWhen walker $i$ clones in swarm $k$, its centered position changes as:\n\n$$\n\\delta'_{x,k,i} = x'_{k,i} - \\mu'_{x,k}\n$$\n\nwhere $x'_{k,i} = x_{k,c_i} + \\sigma_x \\zeta_i^x$ (companion position plus jitter).\n\nThe key insight from the Keystone Lemma is that walkers with large centered position errors $\\|\\Delta\\delta_{x,i}\\| = \\|\\delta_{x,1,i} - \\delta_{x,2,i}\\|$ have high cloning probability. When they clone, their positions are reset, causing:\n\n$$\n\\mathbb{E}[\\|\\delta'_{x,k,i}\\|^2 \\mid \\text{clone}] \\ll \\|\\delta_{x,k,i}\\|^2 \\quad \\text{when } \\|\\delta_{x,k,i}\\|^2 \\text{ is large}\n$$\n\n**Quantitative bound from Keystone Lemma:**\n\nThe Keystone Lemma ({prf:ref}`lem-quantitative-keystone`) states:\n\n$$\n\\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\geq \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\n$$\n\nWhen walker $i$ clones with probability $p_{k,i}$, its centered position is reset. Using the triangle inequality and the fact that the new position $x'_{k,i}$ is drawn from near the companion's position:\n\n$$\n\\mathbb{E}[\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2 \\mid i \\in I_{11}] \\leq -p_{k,i} \\cdot \\frac{1}{4}\\|\\Delta\\delta_{x,i}\\|^2 + p_{k,i} \\cdot C_{\\text{jitter}}\n$$\n\nwhere $C_{\\text{jitter}} = O(\\sigma_x^2)$ accounts for the Gaussian position jitter and barycenter shifts.\n\nSumming over all stably alive walkers and both swarms:\n\n$$\n\\mathbb{E}\\left[\\sum_{i \\in I_{11}} \\sum_{k=1,2} \\left(\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right)\\right] \\leq -\\frac{1}{4}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 + C_{\\text{jitter}} \\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\n$$\n\n**Applying the Keystone Lemma with explicit normalization:**\n\nThe Keystone Lemma (8.1.1) states:\n\n$$\n\\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\geq \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\n$$\n\nMultiplying both sides by $N$ to convert from N-normalized to un-normalized form:\n\n$$\n\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\geq N \\left[\\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\\right]\n$$\n\nSubstituting this into the first term above (with factor $-\\frac{1}{4}$):\n\n$$\n\\leq -\\frac{1}{4} \\cdot N \\left[\\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\\right] + C_{\\text{jitter}} \\cdot N = -\\frac{N\\chi(\\epsilon)}{4} V_{\\text{struct}} + \\frac{Ng_{\\max}(\\epsilon)}{4} + C_{\\text{jitter}} N\n$$\n\nFactoring out $N$ for clarity:\n\n$$\n\\leq N \\left[-\\frac{\\chi(\\epsilon)}{4} V_{\\text{struct}} + \\frac{g_{\\max}(\\epsilon)}{4} + C_{\\text{jitter}}\\right]\n$$\n\n**Case 2: Walker persists in both swarms**\n\nFor walkers that persist in both swarms, their centered positions change only due to barycenter shifts:\n\n$$\n\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2 = O(\\|\\mu'_{x,k} - \\mu_{x,k}\\|^2)\n$$\n\nThe barycenter shift is bounded by the number of cloning events, yielding a bounded contribution $C_{\\text{pers}}$.\n\nCombining both cases yields the stated bound.",
      "raw_directive": "6474: :::\n6475: \n6476: :::{prf:proof}\n6477: :label: proof-lem-keystone-contraction-alive\n6478: **Proof.**\n6479: \n6480: We analyze the variance change for each walker $i \\in I_{11}$ by conditioning on its cloning action.\n6481: \n6482: **Case 1: Walker $i$ clones in at least one swarm**\n6483: \n6484: When walker $i$ clones in swarm $k$, its centered position changes as:\n6485: \n6486: $$\n6487: \\delta'_{x,k,i} = x'_{k,i} - \\mu'_{x,k}\n6488: $$\n6489: \n6490: where $x'_{k,i} = x_{k,c_i} + \\sigma_x \\zeta_i^x$ (companion position plus jitter).\n6491: \n6492: The key insight from the Keystone Lemma is that walkers with large centered position errors $\\|\\Delta\\delta_{x,i}\\| = \\|\\delta_{x,1,i} - \\delta_{x,2,i}\\|$ have high cloning probability. When they clone, their positions are reset, causing:\n6493: \n6494: $$\n6495: \\mathbb{E}[\\|\\delta'_{x,k,i}\\|^2 \\mid \\text{clone}] \\ll \\|\\delta_{x,k,i}\\|^2 \\quad \\text{when } \\|\\delta_{x,k,i}\\|^2 \\text{ is large}\n6496: $$\n6497: \n6498: **Quantitative bound from Keystone Lemma:**\n6499: \n6500: The Keystone Lemma ({prf:ref}`lem-quantitative-keystone`) states:\n6501: \n6502: $$\n6503: \\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\geq \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\n6504: $$\n6505: \n6506: When walker $i$ clones with probability $p_{k,i}$, its centered position is reset. Using the triangle inequality and the fact that the new position $x'_{k,i}$ is drawn from near the companion's position:\n6507: \n6508: $$\n6509: \\mathbb{E}[\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2 \\mid i \\in I_{11}] \\leq -p_{k,i} \\cdot \\frac{1}{4}\\|\\Delta\\delta_{x,i}\\|^2 + p_{k,i} \\cdot C_{\\text{jitter}}\n6510: $$\n6511: \n6512: where $C_{\\text{jitter}} = O(\\sigma_x^2)$ accounts for the Gaussian position jitter and barycenter shifts.\n6513: \n6514: Summing over all stably alive walkers and both swarms:\n6515: \n6516: $$\n6517: \\mathbb{E}\\left[\\sum_{i \\in I_{11}} \\sum_{k=1,2} \\left(\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right)\\right] \\leq -\\frac{1}{4}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 + C_{\\text{jitter}} \\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\n6518: $$\n6519: \n6520: **Applying the Keystone Lemma with explicit normalization:**\n6521: \n6522: The Keystone Lemma (8.1.1) states:\n6523: \n6524: $$\n6525: \\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\geq \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\n6526: $$\n6527: \n6528: Multiplying both sides by $N$ to convert from N-normalized to un-normalized form:\n6529: \n6530: $$\n6531: \\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\geq N \\left[\\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\\right]\n6532: $$\n6533: \n6534: Substituting this into the first term above (with factor $-\\frac{1}{4}$):\n6535: \n6536: $$\n6537: \\leq -\\frac{1}{4} \\cdot N \\left[\\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\\right] + C_{\\text{jitter}} \\cdot N = -\\frac{N\\chi(\\epsilon)}{4} V_{\\text{struct}} + \\frac{Ng_{\\max}(\\epsilon)}{4} + C_{\\text{jitter}} N\n6538: $$\n6539: \n6540: Factoring out $N$ for clarity:\n6541: \n6542: $$\n6543: \\leq N \\left[-\\frac{\\chi(\\epsilon)}{4} V_{\\text{struct}} + \\frac{g_{\\max}(\\epsilon)}{4} + C_{\\text{jitter}}\\right]\n6544: $$\n6545: \n6546: **Case 2: Walker persists in both swarms**\n6547: \n6548: For walkers that persist in both swarms, their centered positions change only due to barycenter shifts:\n6549: \n6550: $$\n6551: \\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2 = O(\\|\\mu'_{x,k} - \\mu_{x,k}\\|^2)\n6552: $$\n6553: \n6554: The barycenter shift is bounded by the number of cloning events, yielding a bounded contribution $C_{\\text{pers}}$.\n6555: \n6556: Combining both cases yields the stated bound.\n6557: ",
      "strategy_summary": "The proof conditions on whether stably alive walkers clone or persist in swarms, deriving a contraction in variance from cloning resets via the Keystone Lemma's bound on cloning probabilities for erroneous walkers, while bounding perturbation terms from jitter and barycenter shifts.",
      "conclusion": {
        "text": "The expected total change in variance for stably alive walkers is bounded above by a negative term proportional to the structural variance from the Keystone Lemma, plus controlled error terms from jitter and persistence shifts.",
        "latex": null
      },
      "assumptions": [
        {
          "text": "Walkers i belong to I_{11}, the set of stably alive walkers in both swarms.",
          "latex": null
        },
        {
          "text": "Cloning probabilities p_{k,i} are defined based on position errors, with parameters epsilon, sigma_x, N, and functions chi(epsilon), g_max(epsilon).",
          "latex": null
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "case-introduction",
          "text": "Consider Case 1: Walker i clones in at least one swarm k.",
          "latex": null,
          "references": [],
          "derived_statement": null
        },
        {
          "order": 2.0,
          "kind": "definition",
          "text": "The updated centered position after cloning is delta'_{x,k,i} = x'_{k,i} - mu'_{x,k}, where x'_{k,i} = x_{k,c_i} + sigma_x zeta_i^x.",
          "latex": "\\delta'_{x,k,i} = x'_{k,i} - \\mu'_{x,k} \\quad x'_{k,i} = x_{k,c_i} + \\sigma_x \\zeta_i^x",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 3.0,
          "kind": "insight",
          "text": "From the Keystone Lemma, walkers with large ||Delta delta_{x,i}|| have high cloning probability, resetting positions to reduce large errors: E[||delta'_{x,k,i}||^2 | clone] << ||delta_{x,k,i}||^2 when ||delta_{x,k,i}||^2 is large.",
          "latex": "\\mathbb{E}[\\|\\delta'_{x,k,i}\\|^2 \\mid \\text{clone}] \\ll \\|\\delta_{x,k,i}\\|^2",
          "references": [
            "lem-quantitative-keystone"
          ],
          "derived_statement": null
        },
        {
          "order": 4.0,
          "kind": "lemma-application",
          "text": "Apply the Keystone Lemma: (1/N) sum_{i in I_{11}} (p_{1,i} + p_{2,i}) ||Delta delta_{x,i}||^2 >= chi(epsilon) V_struct - g_max(epsilon).",
          "latex": "\\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\geq \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)",
          "references": [
            "lem-quantitative-keystone"
          ],
          "derived_statement": null
        },
        {
          "order": 5.0,
          "kind": "bound-derivation",
          "text": "The expected change conditional on i in I_{11} is E[||delta'_{x,k,i}||^2 - ||delta_{x,k,i}||^2 | i in I_{11}] <= -p_{k,i} * (1/4) ||Delta delta_{x,i}||^2 + p_{k,i} * C_{jitter}, where C_{jitter} = O(sigma_x^2).",
          "latex": "\\mathbb{E}[\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2 \\mid i \\in I_{11}] \\leq -p_{k,i} \\cdot \\frac{1}{4}\\|\\Delta\\delta_{x,i}\\|^2 + p_{k,i} \\cdot C_{\\text{jitter}}",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 6.0,
          "kind": "summation",
          "text": "Summing over i in I_{11} and k=1,2: E[sum_{i,k} (||delta'_{x,k,i}||^2 - ||delta_{x,k,i}||^2)] <= -(1/4) sum_i (p_{1,i} + p_{2,i}) ||Delta delta_{x,i}||^2 + C_{jitter} sum_i (p_{1,i} + p_{2,i}).",
          "latex": "\\mathbb{E}\\left[\\sum_{i \\in I_{11}} \\sum_{k=1,2} \\left(\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right)\\right] \\leq -\\frac{1}{4}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 + C_{\\text{jitter}} \\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 7.0,
          "kind": "normalization",
          "text": "Multiply the Keystone Lemma by N: sum_i (p_{1,i} + p_{2,i}) ||Delta delta_{x,i}||^2 >= N [chi(epsilon) V_struct - g_max(epsilon)].",
          "latex": "\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\geq N \\left[\\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\\right]",
          "references": [
            "lem-quantitative-keystone"
          ],
          "derived_statement": null
        },
        {
          "order": 8.0,
          "kind": "substitution",
          "text": "Substitute to get <= - (N chi(epsilon)/4) V_struct + (N g_max(epsilon)/4) + C_{jitter} N = N [ -chi(epsilon)/4 V_struct + g_max(epsilon)/4 + C_{jitter} ].",
          "latex": "\\leq N \\left[-\\frac{\\chi(\\epsilon)}{4} V_{\\text{struct}} + \\frac{g_{\\max}(\\epsilon)}{4} + C_{\\text{jitter}}\\right]",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 9.0,
          "kind": "case-introduction",
          "text": "Case 2: Walker persists in both swarms; position changes only due to barycenter shifts: ||delta'_{x,k,i}||^2 - ||delta_{x,k,i}||^2 = O(||mu'_{x,k} - mu_{x,k}||^2), bounded by C_pers.",
          "latex": "\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2 = O(\\||\\mu'_{x,k} - \\mu_{x,k}\\|^2)",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 10.0,
          "kind": "combination",
          "text": "Combining both cases yields the overall bound on variance change.",
          "latex": null,
          "references": [],
          "derived_statement": null
        }
      ],
      "key_equations": [
        {
          "label": "eq-clone-position",
          "latex": "\\delta'_{x,k,i} = x'_{k,i} - \\mu'_{x,k} \\quad x'_{k,i} = x_{k,c_i} + \\sigma_x \\zeta_i^x",
          "role": "Defines updated centered position after cloning"
        },
        {
          "label": "eq-keystone-lemma",
          "latex": "\\frac{1}{N}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\geq \\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)",
          "role": "Quantitative bound from Keystone Lemma on cloning-weighted errors"
        },
        {
          "label": "eq-expectation-change",
          "latex": "\\mathbb{E}[\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2 \\mid i \\in I_{11}] \\leq -p_{k,i} \\cdot \\frac{1}{4}\\|\\Delta\\delta_{x,i}\\|^2 + p_{k,i} \\cdot C_{\\text{jitter}}",
          "role": "Bound on variance change conditional on cloning probability"
        },
        {
          "label": "eq-summed-change",
          "latex": "\\mathbb{E}\\left[\\sum_{i \\in I_{11}} \\sum_{k=1,2} \\left(\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right)\\right] \\leq -\\frac{1}{4}\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 + C_{\\text{jitter}} \\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})",
          "role": "Aggregated expectation over all stably alive walkers and swarms"
        },
        {
          "label": "eq-normalized-keystone",
          "latex": "\\sum_{i \\in I_{11}} (p_{1,i} + p_{2,i})\\|\\Delta\\delta_{x,i}\\|^2 \\geq N \\left[\\chi(\\epsilon) V_{\\text{struct}} - g_{\\max}(\\epsilon)\\right]",
          "role": "Un-normalized form of Keystone Lemma for substitution"
        },
        {
          "label": "eq-final-contraction",
          "latex": "N \\left[-\\frac{\\chi(\\epsilon)}{4} V_{\\text{struct}} + \\frac{g_{\\max}(\\epsilon)}{4} + C_{\\text{jitter}}\\right]",
          "role": "Final bound for Case 1 after substitution"
        },
        {
          "label": "eq-persistence-shift",
          "latex": "\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2 = O(\\||\\mu'_{x,k} - \\mu_{x,k}\\|^2)",
          "role": "Bound for variance change in persistence case"
        }
      ],
      "references": [
        "lem-quantitative-keystone"
      ],
      "math_tools": [
        {
          "toolName": "Conditional Expectation",
          "field": "Probability",
          "description": "Computing expectations given specific events or conditions, such as cloning or persistence.",
          "roleInProof": "Used to bound the change in squared centered position norms conditional on cloning events and to aggregate over walkers.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Triangle Inequality"
          ]
        },
        {
          "toolName": "Triangle Inequality",
          "field": "Metric Geometry",
          "description": "A fundamental inequality for bounding distances or norms in vector spaces.",
          "roleInProof": "Applied to derive the negative contraction term in the expected variance change after position resets during cloning.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Conditional Expectation"
          ]
        },
        {
          "toolName": "Gaussian Jitter",
          "field": "Stochastic Processes",
          "description": "Addition of Gaussian noise to positions, modeling small random perturbations.",
          "roleInProof": "Accounts for the bounded variance increase due to position jitter in cloned walkers.",
          "levelOfAbstraction": "Concept",
          "relatedTools": []
        }
      ],
      "cases": [
        {
          "name": "Case 1: Walker clones in at least one swarm",
          "condition": "i clones in swarm k=1 or 2",
          "summary": "Cloning resets positions of erroneous walkers, leading to contraction bounded by Keystone Lemma minus jitter."
        },
        {
          "name": "Case 2: Walker persists in both swarms",
          "condition": "i persists without cloning in k=1 and 2",
          "summary": "Variance change bounded by barycenter shifts, yielding controlled contribution C_pers."
        }
      ],
      "remarks": [
        {
          "type": "insight",
          "text": "The key mechanism is that cloning preferentially targets walkers with large inter-swarm position discrepancies, as quantified by the Keystone Lemma."
        },
        {
          "type": "quantitative",
          "text": "Constants like 1/4 and C_jitter are derived from triangle inequality and Gaussian variance; precise values depend on model parameters."
        }
      ],
      "gaps": [
        {
          "description": "The inequality E[||delta'||^2 | clone] << ||delta||^2 is qualitative; a precise constant bound is not provided.",
          "severity": "minor",
          "location_hint": "Early in Case 1"
        },
        {
          "description": "The persistence contribution C_pers is stated as bounded but not explicitly computed or bounded in terms of parameters.",
          "severity": "minor",
          "location_hint": "Case 2"
        },
        {
          "description": "The overall combined bound is referenced as 'the stated bound' but not explicitly written in the proof text.",
          "severity": "moderate",
          "location_hint": "Conclusion"
        }
      ],
      "tags": [
        "variance analysis",
        "cloning probability",
        "centered positions",
        "Keystone Lemma",
        "barycenter shifts",
        "persistence",
        "expectation bounds"
      ],
      "document_id": "03_cloning",
      "section": "## 10.3. Positional Variance Contraction",
      "span": {
        "start_line": 6474,
        "end_line": 6557,
        "content_start": 6476,
        "content_end": 6556,
        "header_lines": [
          6475
        ]
      },
      "metadata": {
        "label": "proof-lem-keystone-contraction-alive"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 17,
        "chapter_file": "chapter_17.json",
        "section_id": "## 10.3. Positional Variance Contraction"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "keystone",
      "contraction",
      "stably-alive",
      "variance",
      "walkers",
      "expected-change"
    ],
    "content_markdown": ":label: lem-keystone-contraction-alive\n\nFor walkers in the stably alive set $I_{11}$, the expected change in their contribution to variance satisfies:\n\n$$\n\\mathbb{E}_{\\text{clone}}\\left[\\sum_{i \\in I_{11}} \\sum_{k=1,2} \\left(\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right)\\right] \\leq -\\frac{\\chi(\\epsilon)}{2N} \\cdot V_{\\text{struct}} + \\frac{g_{\\max}(\\epsilon)}{N} + C_{\\text{pers}}\n$$\n\nwhere $\\chi(\\epsilon) > 0$ and $g_{\\max}(\\epsilon)$ are the Keystone constants ({prf:ref}`lem-quantitative-keystone`), and $C_{\\text{pers}}$ accounts for persisting walkers.",
    "raw_directive": "6460: We now bound the contribution from walkers that are alive in both swarms (the stably alive set $I_{11}$).\n6461: \n6462: :::{prf:lemma} Keystone-Driven Contraction for Stably Alive Walkers\n6463: :label: lem-keystone-contraction-alive\n6464: \n6465: For walkers in the stably alive set $I_{11}$, the expected change in their contribution to variance satisfies:\n6466: \n6467: $$\n6468: \\mathbb{E}_{\\text{clone}}\\left[\\sum_{i \\in I_{11}} \\sum_{k=1,2} \\left(\\|\\delta'_{x,k,i}\\|^2 - \\|\\delta_{x,k,i}\\|^2\\right)\\right] \\leq -\\frac{\\chi(\\epsilon)}{2N} \\cdot V_{\\text{struct}} + \\frac{g_{\\max}(\\epsilon)}{N} + C_{\\text{pers}}\n6469: $$\n6470: \n6471: where $\\chi(\\epsilon) > 0$ and $g_{\\max}(\\epsilon)$ are the Keystone constants ({prf:ref}`lem-quantitative-keystone`), and $C_{\\text{pers}}$ accounts for persisting walkers.\n6472: ",
    "document_id": "03_cloning",
    "section": "## 10.3. Positional Variance Contraction",
    "span": {
      "start_line": 6460,
      "end_line": 6472,
      "content_start": 6463,
      "content_end": 6471,
      "header_lines": [
        6461
      ]
    },
    "references": [
      "lem-quantitative-keystone"
    ],
    "metadata": {
      "label": "lem-keystone-contraction-alive"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 17,
      "chapter_file": "chapter_17.json",
      "section_id": "## 10.3. Positional Variance Contraction"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-dead-walker-revival-bounded",
    "title": "Bounded Contribution from Dead Walker Revival",
    "type": "lemma",
    "nl_statement": "The contribution to variance from revived dead walkers is bounded.",
    "equations": [
      {
        "label": null,
        "latex": "\\mathbb{E}_{\\text{clone}}\\left[\\sum_{k=1,2} \\Delta V_{\\text{Var},x}^{(k,\\text{status})}\\right] \\leq \\frac{2}{N} \\sum_{k=1,2} |\\mathcal{D}(S_k)| \\cdot D_{\\text{valid}}^2"
      }
    ],
    "hypotheses": [],
    "conclusion": {
      "text": "The contribution to variance from revived dead walkers is bounded.",
      "latex": "\\mathbb{E}_{\\text{clone}}\\left[\\sum_{k=1,2} \\Delta V_{\\text{Var},x}^{(k,\\text{status})}\\right] \\leq \\frac{2}{N} \\sum_{k=1,2} |\\mathcal{D}(S_k)| \\cdot D_{\\text{valid}}^2"
    },
    "variables": [
      {
        "symbol": "N",
        "name": "N",
        "description": "Number of clones or particles.",
        "constraints": [],
        "tags": [
          "cloning"
        ]
      },
      {
        "symbol": "\\mathcal{D}(S_k)",
        "name": "\\mathcal{D}(S_k)",
        "description": "Size of the dead set for state S_k.",
        "constraints": [
          "k=1,2"
        ],
        "tags": [
          "dead set"
        ]
      },
      {
        "symbol": "D_{\\text{valid}}",
        "name": "D_valid",
        "description": "Bound on valid displacements or similar.",
        "constraints": [],
        "tags": [
          "displacement"
        ]
      },
      {
        "symbol": "\\Delta V_{\\text{Var},x}^{(k,\\text{status})}",
        "name": "\\Delta V_{Var,x}^{(k,status)}",
        "description": "Change in variance due to status update for walker k.",
        "constraints": [
          "k=1,2"
        ],
        "tags": [
          "variance",
          "status"
        ]
      },
      {
        "symbol": "\\mathbb{E}_{\\text{clone}}",
        "name": "E_clone",
        "description": "Expectation over cloning process.",
        "constraints": [],
        "tags": [
          "expectation",
          "cloning"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The process involves two walkers (k=1,2) with dead sets \\mathcal{D}(S_k).",
        "confidence": 0.9
      },
      {
        "text": "N is a positive integer representing the number of clones.",
        "confidence": 0.8
      },
      {
        "text": "D_valid is a non-negative bound on relevant quantities.",
        "confidence": 0.8
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-dead-walker-revival-bounded",
      "title": null,
      "type": "proof",
      "proves": "lem-dead-walker-revival-bounded",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":::{prf:proof}\n:label: proof-lem-dead-walker-revival-bounded\n**Proof.**\n\nThe proof establishes an upper bound on the variance contribution from dead walker revival by carefully analyzing the geometry of centered positions after cloning.\n\n**Step 1: Cloning behavior of dead walkers.**\n\nBy {prf:ref}`lem-dead-walker-clone-prob`, every dead walker has zero fitness potential and therefore receives the maximum cloning score. Consequently, every dead walker clones with probability 1 under the cloning decision rule.\n\nWhen a dead walker $i \\in \\mathcal{D}(S_k)$ clones, it selects a companion $c_i \\in \\mathcal{A}(S_k)$ from the alive set and receives a new position:\n\n$$\nx'_{k,i} = x_{k,c_i} + \\sigma_x \\zeta_i^x\n$$\n\nwhere $\\zeta_i^x \\sim \\mathcal{N}(0, I_d)$ is the standard Gaussian jitter and $\\sigma_x > 0$ is the position jitter scale.\n\n**Step 2: Bounding the centered position after revival.**\n\nAfter cloning, all walkers are alive, and the swarm has a new barycenter $\\mu'_{x,k}$ computed over all $N$ walkers. The centered position of the revived walker $i$ is:\n\n$$\n\\delta'_{x,k,i} = x'_{k,i} - \\mu'_{x,k}\n$$\n\nTo bound $\\|\\delta'_{x,k,i}\\|^2$, we use the triangle inequality:\n\n$$\n\\begin{aligned}\n\\|\\delta'_{x,k,i}\\| &= \\|x'_{k,i} - \\mu'_{x,k}\\| \\\\\n&\\leq \\|x'_{k,i}\\| + \\|\\mu'_{x,k}\\|\n\\end{aligned}\n$$\n\n**Step 2.1: Bounding the new position $\\|x'_{k,i}\\|$.**\n\nThe new position is:\n\n$$\nx'_{k,i} = x_{k,c_i} + \\sigma_x \\zeta_i^x\n$$\n\nSince $c_i \\in \\mathcal{A}(S_k)$, we have $x_{k,c_i} \\in \\mathcal{X}_{\\text{valid}}$. The position jitter $\\sigma_x \\zeta_i^x$ is typically small (bounded in expectation), and the cloning mechanism includes an implicit or explicit check to ensure $x'_{k,i} \\in \\mathcal{X}_{\\text{valid}}$ (either through rejection sampling or projection).\n\nTherefore, $x'_{k,i} \\in \\mathcal{X}_{\\text{valid}}$, which implies:\n\n$$\n\\|x'_{k,i}\\| \\leq \\sup_{x \\in \\mathcal{X}_{\\text{valid}}} \\|x\\| \\leq D_{\\text{valid}}\n$$\n\nwhere $D_{\\text{valid}} := \\text{diam}(\\mathcal{X}_{\\text{valid}})$ is the spatial diameter of the valid domain (assuming the origin is chosen appropriately, or using a more careful bound relative to a fixed reference point).\n\n**Step 2.2: Bounding the new barycenter $\\|\\mu'_{x,k}\\|$.**\n\nThe new barycenter is:\n\n$$\n\\mu'_{x,k} = \\frac{1}{N} \\sum_{j=1}^{N} x'_{k,j}\n$$\n\nSince all post-cloning positions satisfy $x'_{k,j} \\in \\mathcal{X}_{\\text{valid}}$, and $\\mathcal{X}_{\\text{valid}}$ is convex (a standard assumption), the barycenter as a convex combination also satisfies $\\mu'_{x,k} \\in \\mathcal{X}_{\\text{valid}}$. Therefore:\n\n$$\n\\|\\mu'_{x,k}\\| \\leq D_{\\text{valid}}\n$$\n\n**Step 2.3: Combining bounds via triangle inequality.**\n\nSubstituting the bounds from Steps 2.1 and 2.2:\n\n$$\n\\|\\delta'_{x,k,i}\\| \\leq \\|x'_{k,i}\\| + \\|\\mu'_{x,k}\\| \\leq D_{\\text{valid}} + D_{\\text{valid}} = 2D_{\\text{valid}}\n$$\n\nSquaring both sides:\n\n$$\n\\|\\delta'_{x,k,i}\\|^2 \\leq (2D_{\\text{valid}})^2 = 4D_{\\text{valid}}^2\n$$\n\nThis bound holds for every revived dead walker.\n\n**Step 3: Summing over all dead walkers in swarm $k$.**\n\nThe total contribution to variance from dead walkers in swarm $k$ is:\n\n$$\n\\Delta V_{\\text{Var},x}^{(k,\\text{status})} = \\frac{1}{N} \\sum_{i \\in \\mathcal{D}(S_k)} \\|\\delta'_{x,k,i}\\|^2\n$$\n\nUsing the bound from Step 2.3 for each term:\n\n$$\n\\Delta V_{\\text{Var},x}^{(k,\\text{status})} \\leq \\frac{1}{N} \\sum_{i \\in \\mathcal{D}(S_k)} 4D_{\\text{valid}}^2 = \\frac{4|\\mathcal{D}(S_k)|}{N} D_{\\text{valid}}^2\n$$\n\n**Step 4: Summing over both swarms and taking expectation.**\n\nThe total status change contribution across both swarms is:\n\n$$\n\\sum_{k=1,2} \\Delta V_{\\text{Var},x}^{(k,\\text{status})} \\leq \\frac{4D_{\\text{valid}}^2}{N} \\sum_{k=1,2} |\\mathcal{D}(S_k)|\n$$\n\nSince this bound is deterministic (it holds for any realization of the cloning process), it also holds in expectation:\n\n$$\n\\mathbb{E}_{\\text{clone}}\\left[\\sum_{k=1,2} \\Delta V_{\\text{Var},x}^{(k,\\text{status})}\\right] \\leq \\frac{4D_{\\text{valid}}^2}{N} \\sum_{k=1,2} |\\mathcal{D}(S_k)|\n$$\n\nRewriting with the factor of 2:\n\n$$\n= \\frac{2}{N} \\sum_{k=1,2} |\\mathcal{D}(S_k)| \\cdot 2D_{\\text{valid}}^2 \\leq \\frac{2}{N} \\sum_{k=1,2} |\\mathcal{D}(S_k)| \\cdot 4D_{\\text{valid}}^2\n$$\n\nActually, the original bound stated $2/N \\cdot \\ldots \\cdot D_{\\text{valid}}^2$, which would require a bound of $2D_{\\text{valid}}^2$ per walker. Our derivation gives $4D_{\\text{valid}}^2$, which is a factor of 2 larger but still correct as an upper bound.\n\nThe stated lemma uses a slightly tighter constant, which can be justified by a more careful analysis of the centered position geometry. The key point is that the bound is $O(|\\mathcal{D}(S_k)|/N)$, which is the essential scaling for the drift analysis.\n\n**Conclusion:**\n\nThe contribution from dead walker revival is bounded by a term proportional to the number of dead walkers divided by $N$, multiplied by the square of the domain diameter. This is a deterministic upper bound that holds for all states.",
      "raw_directive": "6573: :::\n6574: \n6575: :::{prf:proof}\n6576: :label: proof-lem-dead-walker-revival-bounded\n6577: **Proof.**\n6578: \n6579: The proof establishes an upper bound on the variance contribution from dead walker revival by carefully analyzing the geometry of centered positions after cloning.\n6580: \n6581: **Step 1: Cloning behavior of dead walkers.**\n6582: \n6583: By {prf:ref}`lem-dead-walker-clone-prob`, every dead walker has zero fitness potential and therefore receives the maximum cloning score. Consequently, every dead walker clones with probability 1 under the cloning decision rule.\n6584: \n6585: When a dead walker $i \\in \\mathcal{D}(S_k)$ clones, it selects a companion $c_i \\in \\mathcal{A}(S_k)$ from the alive set and receives a new position:\n6586: \n6587: $$\n6588: x'_{k,i} = x_{k,c_i} + \\sigma_x \\zeta_i^x\n6589: $$\n6590: \n6591: where $\\zeta_i^x \\sim \\mathcal{N}(0, I_d)$ is the standard Gaussian jitter and $\\sigma_x > 0$ is the position jitter scale.\n6592: \n6593: **Step 2: Bounding the centered position after revival.**\n6594: \n6595: After cloning, all walkers are alive, and the swarm has a new barycenter $\\mu'_{x,k}$ computed over all $N$ walkers. The centered position of the revived walker $i$ is:\n6596: \n6597: $$\n6598: \\delta'_{x,k,i} = x'_{k,i} - \\mu'_{x,k}\n6599: $$\n6600: \n6601: To bound $\\|\\delta'_{x,k,i}\\|^2$, we use the triangle inequality:\n6602: \n6603: $$\n6604: \\begin{aligned}\n6605: \\|\\delta'_{x,k,i}\\| &= \\|x'_{k,i} - \\mu'_{x,k}\\| \\\\\n6606: &\\leq \\|x'_{k,i}\\| + \\|\\mu'_{x,k}\\|\n6607: \\end{aligned}\n6608: $$\n6609: \n6610: **Step 2.1: Bounding the new position $\\|x'_{k,i}\\|$.**\n6611: \n6612: The new position is:\n6613: \n6614: $$\n6615: x'_{k,i} = x_{k,c_i} + \\sigma_x \\zeta_i^x\n6616: $$\n6617: \n6618: Since $c_i \\in \\mathcal{A}(S_k)$, we have $x_{k,c_i} \\in \\mathcal{X}_{\\text{valid}}$. The position jitter $\\sigma_x \\zeta_i^x$ is typically small (bounded in expectation), and the cloning mechanism includes an implicit or explicit check to ensure $x'_{k,i} \\in \\mathcal{X}_{\\text{valid}}$ (either through rejection sampling or projection).\n6619: \n6620: Therefore, $x'_{k,i} \\in \\mathcal{X}_{\\text{valid}}$, which implies:\n6621: \n6622: $$\n6623: \\|x'_{k,i}\\| \\leq \\sup_{x \\in \\mathcal{X}_{\\text{valid}}} \\|x\\| \\leq D_{\\text{valid}}\n6624: $$\n6625: \n6626: where $D_{\\text{valid}} := \\text{diam}(\\mathcal{X}_{\\text{valid}})$ is the spatial diameter of the valid domain (assuming the origin is chosen appropriately, or using a more careful bound relative to a fixed reference point).\n6627: \n6628: **Step 2.2: Bounding the new barycenter $\\|\\mu'_{x,k}\\|$.**\n6629: \n6630: The new barycenter is:\n6631: \n6632: $$\n6633: \\mu'_{x,k} = \\frac{1}{N} \\sum_{j=1}^{N} x'_{k,j}\n6634: $$\n6635: \n6636: Since all post-cloning positions satisfy $x'_{k,j} \\in \\mathcal{X}_{\\text{valid}}$, and $\\mathcal{X}_{\\text{valid}}$ is convex (a standard assumption), the barycenter as a convex combination also satisfies $\\mu'_{x,k} \\in \\mathcal{X}_{\\text{valid}}$. Therefore:\n6637: \n6638: $$\n6639: \\|\\mu'_{x,k}\\| \\leq D_{\\text{valid}}\n6640: $$\n6641: \n6642: **Step 2.3: Combining bounds via triangle inequality.**\n6643: \n6644: Substituting the bounds from Steps 2.1 and 2.2:\n6645: \n6646: $$\n6647: \\|\\delta'_{x,k,i}\\| \\leq \\|x'_{k,i}\\| + \\|\\mu'_{x,k}\\| \\leq D_{\\text{valid}} + D_{\\text{valid}} = 2D_{\\text{valid}}\n6648: $$\n6649: \n6650: Squaring both sides:\n6651: \n6652: $$\n6653: \\|\\delta'_{x,k,i}\\|^2 \\leq (2D_{\\text{valid}})^2 = 4D_{\\text{valid}}^2\n6654: $$\n6655: \n6656: This bound holds for every revived dead walker.\n6657: \n6658: **Step 3: Summing over all dead walkers in swarm $k$.**\n6659: \n6660: The total contribution to variance from dead walkers in swarm $k$ is:\n6661: \n6662: $$\n6663: \\Delta V_{\\text{Var},x}^{(k,\\text{status})} = \\frac{1}{N} \\sum_{i \\in \\mathcal{D}(S_k)} \\|\\delta'_{x,k,i}\\|^2\n6664: $$\n6665: \n6666: Using the bound from Step 2.3 for each term:\n6667: \n6668: $$\n6669: \\Delta V_{\\text{Var},x}^{(k,\\text{status})} \\leq \\frac{1}{N} \\sum_{i \\in \\mathcal{D}(S_k)} 4D_{\\text{valid}}^2 = \\frac{4|\\mathcal{D}(S_k)|}{N} D_{\\text{valid}}^2\n6670: $$\n6671: \n6672: **Step 4: Summing over both swarms and taking expectation.**\n6673: \n6674: The total status change contribution across both swarms is:\n6675: \n6676: $$\n6677: \\sum_{k=1,2} \\Delta V_{\\text{Var},x}^{(k,\\text{status})} \\leq \\frac{4D_{\\text{valid}}^2}{N} \\sum_{k=1,2} |\\mathcal{D}(S_k)|\n6678: $$\n6679: \n6680: Since this bound is deterministic (it holds for any realization of the cloning process), it also holds in expectation:\n6681: \n6682: $$\n6683: \\mathbb{E}_{\\text{clone}}\\left[\\sum_{k=1,2} \\Delta V_{\\text{Var},x}^{(k,\\text{status})}\\right] \\leq \\frac{4D_{\\text{valid}}^2}{N} \\sum_{k=1,2} |\\mathcal{D}(S_k)|\n6684: $$\n6685: \n6686: Rewriting with the factor of 2:\n6687: \n6688: $$\n6689: = \\frac{2}{N} \\sum_{k=1,2} |\\mathcal{D}(S_k)| \\cdot 2D_{\\text{valid}}^2 \\leq \\frac{2}{N} \\sum_{k=1,2} |\\mathcal{D}(S_k)| \\cdot 4D_{\\text{valid}}^2\n6690: $$\n6691: \n6692: Actually, the original bound stated $2/N \\cdot \\ldots \\cdot D_{\\text{valid}}^2$, which would require a bound of $2D_{\\text{valid}}^2$ per walker. Our derivation gives $4D_{\\text{valid}}^2$, which is a factor of 2 larger but still correct as an upper bound.\n6693: \n6694: The stated lemma uses a slightly tighter constant, which can be justified by a more careful analysis of the centered position geometry. The key point is that the bound is $O(|\\mathcal{D}(S_k)|/N)$, which is the essential scaling for the drift analysis.\n6695: \n6696: **Conclusion:**\n6697: \n6698: The contribution from dead walker revival is bounded by a term proportional to the number of dead walkers divided by $N$, multiplied by the square of the domain diameter. This is a deterministic upper bound that holds for all states.\n6699: ",
      "strategy_summary": "The proof directly bounds the squared norm of centered positions for revived dead walkers using the triangle inequality on distances within the bounded convex valid domain, shows the barycenter remains bounded, and aggregates these to obtain a deterministic upper bound on the variance contribution proportional to the fraction of dead walkers times the domain diameter squared.",
      "conclusion": {
        "text": "The contribution from dead walker revival is bounded by a term proportional to the number of dead walkers divided by $N$, multiplied by the square of the domain diameter. This is a deterministic upper bound that holds for all states.",
        "latex": null
      },
      "assumptions": [
        {
          "text": "The valid domain $\\mathcal{X}_{\\text{valid}}$ is convex and bounded with finite diameter $D_{\\text{valid}}$.",
          "latex": null
        },
        {
          "text": "The cloning process ensures all new positions $x'_{k,i}$ lie in $\\mathcal{X}_{\\text{valid}}$, e.g., via rejection or projection.",
          "latex": null
        },
        {
          "text": "Dead walkers have zero fitness potential and clone with probability 1, as per referenced lemma.",
          "latex": null
        }
      ],
      "steps": [],
      "key_equations": [
        {
          "label": "eq-new-position",
          "latex": "x'_{k,i} = x_{k,c_i} + \\sigma_x \\zeta_i^x",
          "role": "Defines the position of a revived dead walker after cloning from an alive companion with Gaussian jitter."
        },
        {
          "label": "eq-centered-position",
          "latex": "\\delta'_{x,k,i} = x'_{k,i} - \\mu'_{x,k}",
          "role": "Centered position of the revived walker relative to the new barycenter."
        },
        {
          "label": "eq-barycenter",
          "latex": "\\mu'_{x,k} = \\frac{1}{N} \\sum_{j=1}^{N} x'_{k,j}",
          "role": "New barycenter after all cloning and revival."
        },
        {
          "label": "eq-triangle-bound",
          "latex": "\\|\\delta'_{x,k,i}\\| \\leq \\|x'_{k,i}\\| + \\|\\mu'_{x,k}\\|",
          "role": "Triangle inequality application for centering bound."
        },
        {
          "label": "eq-variance-contrib",
          "latex": "\\Delta V_{\\text{Var},x}^{(k,\\text{status})} = \\frac{1}{N} \\sum_{i \\in \\mathcal{D}(S_k)} \\|\\delta'_{x,k,i}\\|^2",
          "role": "Variance contribution from dead walkers in one swarm."
        }
      ],
      "references": [
        "lem-dead-walker-clone-prob"
      ],
      "math_tools": [
        {
          "toolName": "Triangle Inequality",
          "field": "Metric Spaces",
          "description": "For any vectors a and b in a normed space, ||a - b|| \u2264 ||a|| + ||b||.",
          "roleInProof": "Applied to bound the norm of the centered revived position \u03b4'_{x,k,i} by the sum of bounds on the new position and barycenter norms.",
          "levelOfAbstraction": "Technique",
          "relatedTools": []
        },
        {
          "toolName": "Convexity of Averages",
          "field": "Convex Analysis",
          "description": "The convex combination (e.g., barycenter) of points in a convex set remains within the set.",
          "roleInProof": "Ensures the new barycenter \u03bc'_{x,k} lies in the valid domain X_valid, inheriting its diameter bound.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Triangle Inequality"
          ]
        },
        {
          "toolName": "Expectation Linearity",
          "field": "Probability Theory",
          "description": "The expectation of a sum is the sum of expectations, applicable to bounded random variables.",
          "roleInProof": "Justifies that the deterministic position bounds imply the same bound in expectation over the cloning process.",
          "levelOfAbstraction": "Technique",
          "relatedTools": []
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "note",
          "text": "The derived bound uses 4 D_valid^2 per walker, yielding a factor of 4/N |D| D_valid^2 overall, but the lemma likely employs a tighter constant of 2 via more precise centering analysis; the O(|D|/N D_valid^2) scaling remains intact."
        }
      ],
      "gaps": [
        {
          "description": "The proof sketches a bound with constant 4 but notes the lemma uses a tighter constant (possibly 2); a detailed geometric argument for the tighter bound is omitted.",
          "severity": "minor",
          "location_hint": "Step 4 (summing and expectation)"
        }
      ],
      "tags": [
        "dead-walkers",
        "cloning",
        "variance-bound",
        "triangle-inequality",
        "barycenter",
        "convex-set",
        "domain-diameter",
        "expected-value"
      ],
      "document_id": "03_cloning",
      "section": "## 10.3. Positional Variance Contraction",
      "span": {
        "start_line": 6573,
        "end_line": 6699,
        "content_start": 6575,
        "content_end": 6698,
        "header_lines": [
          6574
        ]
      },
      "metadata": {
        "label": "proof-lem-dead-walker-revival-bounded"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 17,
        "chapter_file": "chapter_17.json",
        "section_id": "## 10.3. Positional Variance Contraction"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "variance",
      "dead walkers",
      "revival",
      "bounding",
      "cloning",
      "status change"
    ],
    "content_markdown": ":label: lem-dead-walker-revival-bounded\n\nThe contribution to variance from revived dead walkers is bounded:\n\n$$\n\\mathbb{E}_{\\text{clone}}\\left[\\sum_{k=1,2} \\Delta V_{\\text{Var},x}^{(k,\\text{status})}\\right] \\leq \\frac{2}{N} \\sum_{k=1,2} |\\mathcal{D}(S_k)| \\cdot D_{\\text{valid}}^2\n$$",
    "raw_directive": "6561: ### 10.3.5. Bounding Status Change Contributions\n6562: \n6563: :::{prf:lemma} Bounded Contribution from Dead Walker Revival\n6564: :label: lem-dead-walker-revival-bounded\n6565: \n6566: The contribution to variance from revived dead walkers is bounded:\n6567: \n6568: $$\n6569: \\mathbb{E}_{\\text{clone}}\\left[\\sum_{k=1,2} \\Delta V_{\\text{Var},x}^{(k,\\text{status})}\\right] \\leq \\frac{2}{N} \\sum_{k=1,2} |\\mathcal{D}(S_k)| \\cdot D_{\\text{valid}}^2\n6570: $$\n6571: ",
    "document_id": "03_cloning",
    "section": "## 10.3. Positional Variance Contraction",
    "span": {
      "start_line": 6561,
      "end_line": 6571,
      "content_start": 6564,
      "content_end": 6570,
      "header_lines": [
        6562
      ]
    },
    "references": [
      "lem-dead-walker-clone-prob"
    ],
    "metadata": {
      "label": "lem-dead-walker-revival-bounded"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 17,
      "chapter_file": "chapter_17.json",
      "section_id": "## 10.3. Positional Variance Contraction"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-fitness-gradient-boundary",
    "title": "Fitness Gradient from Boundary Proximity",
    "type": "lemma",
    "nl_statement": "Walkers closer to the boundary have systematically lower fitness potential than similar walkers farther from it, under the fitness evaluation pipeline.",
    "equations": [
      {
        "label": null,
        "latex": "\\varphi_{\\text{barrier}}(x_i) - \\varphi_{\\text{barrier}}(x_j) = \\Delta_{\\text{barrier}} > 0"
      },
      {
        "label": null,
        "latex": "V_{\\text{fit},i} \\leq V_{\\text{fit},j} - f(\\Delta_{\\text{barrier}})"
      }
    ],
    "hypotheses": [
      {
        "text": "Two walkers i and j with similar positions and velocities, except that walker i is closer to the boundary than walker j.",
        "latex": null
      },
      {
        "text": "\\varphi_{\\text{barrier}}(x_i) - \\varphi_{\\text{barrier}}(x_j) = \\Delta_{\\text{barrier}} > 0",
        "latex": "\\varphi_{\\text{barrier}}(x_i) - \\varphi_{\\text{barrier}}(x_j) = \\Delta_{\\text{barrier}} > 0"
      }
    ],
    "conclusion": {
      "text": "Under the fitness evaluation pipeline, walker i has systematically lower fitness potential",
      "latex": "V_{\\text{fit},i} \\leq V_{\\text{fit},j} - f(\\Delta_{\\text{barrier}})"
    },
    "variables": [
      {
        "symbol": "i",
        "name": "walker i",
        "description": "Walker closer to the boundary",
        "constraints": [
          "closer to boundary"
        ],
        "tags": [
          "walker"
        ]
      },
      {
        "symbol": "j",
        "name": "walker j",
        "description": "Walker farther from the boundary",
        "constraints": [
          "farther from boundary"
        ],
        "tags": [
          "walker"
        ]
      },
      {
        "symbol": "x_i",
        "name": "position i",
        "description": "Position of walker i",
        "constraints": [],
        "tags": [
          "position"
        ]
      },
      {
        "symbol": "x_j",
        "name": "position j",
        "description": "Position of walker j",
        "constraints": [],
        "tags": [
          "position"
        ]
      },
      {
        "symbol": "\\varphi_{\\text{barrier}}",
        "name": "barrier potential",
        "description": "Potential function measuring proximity to boundary",
        "constraints": [],
        "tags": [
          "potential",
          "barrier"
        ]
      },
      {
        "symbol": "\\Delta_{\\text{barrier}}",
        "name": "barrier difference",
        "description": "Difference in barrier potential between i and j",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "difference",
          "barrier"
        ]
      },
      {
        "symbol": "V_{\\text{fit},i}",
        "name": "fitness potential i",
        "description": "Fitness potential for walker i",
        "constraints": [],
        "tags": [
          "fitness"
        ]
      },
      {
        "symbol": "V_{\\text{fit},j}",
        "name": "fitness potential j",
        "description": "Fitness potential for walker j",
        "constraints": [],
        "tags": [
          "fitness"
        ]
      },
      {
        "symbol": "f",
        "name": "penalty function",
        "description": "Function applying penalty based on barrier difference",
        "constraints": [],
        "tags": [
          "function",
          "penalty"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Walkers i and j have similar positions and velocities apart from boundary proximity",
        "confidence": 1.0
      },
      {
        "text": "Fitness evaluation pipeline integrates barrier proximity into fitness calculation as per Chapter 5",
        "confidence": 0.9
      },
      {
        "text": "f is a positive decreasing function of \\Delta_{\\text{barrier}}",
        "confidence": 0.8
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-fitness-gradient-boundary",
      "title": null,
      "type": "proof",
      "proves": "lem-fitness-gradient-boundary",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":::{prf:proof}\n:label: proof-lem-fitness-gradient-boundary\n**Proof.**\n\nThe proof traces the boundary-induced fitness difference through each stage of the measurement and fitness evaluation pipeline defined in Chapter 5, demonstrating that the ordering is preserved and quantifying the resulting gap.\n\n**Step 1: Raw reward difference.**\n\nThe raw reward for walker $i$ is defined as (from Section 5.6):\n\n$$\nr_i = R_{\\text{pos}}(x_i) - c_{v\\_reg} \\|v_i\\|^2 - \\varphi_{\\text{barrier}}(x_i)\n$$\n\nwhere $R_{\\text{pos}}(x_i)$ is the positional reward component and $\\varphi_{\\text{barrier}}(x_i)$ is the boundary barrier penalty.\n\nFor walkers $i$ and $j$ with similar positions and velocities (so $R_{\\text{pos}}(x_i) \\approx R_{\\text{pos}}(x_j)$ and $\\|v_i\\| \\approx \\|v_j\\|$), but with walker $i$ closer to the boundary ($\\varphi_{\\text{barrier}}(x_i) > \\varphi_{\\text{barrier}}(x_j)$), the raw reward difference is:\n\n$$\n\\begin{aligned}\nr_i - r_j &= \\left[R_{\\text{pos}}(x_i) - c_{v\\_reg} \\|v_i\\|^2 - \\varphi_{\\text{barrier}}(x_i)\\right] - \\left[R_{\\text{pos}}(x_j) - c_{v\\_reg} \\|v_j\\|^2 - \\varphi_{\\text{barrier}}(x_j)\\right] \\\\\n&\\approx -\\left[\\varphi_{\\text{barrier}}(x_i) - \\varphi_{\\text{barrier}}(x_j)\\right] \\\\\n&= -\\Delta_{\\text{barrier}} < 0\n\\end{aligned}\n$$\n\nThus, walker $i$ has strictly lower raw reward than walker $j$.\n\n**Step 2: Floor addition preserves ordering.**\n\nThe fitness potential construction adds a positive floor $\\eta > 0$ to ensure all values are positive:\n\n$$\n\\tilde{r}_i = r_i + \\eta, \\quad \\tilde{r}_j = r_j + \\eta\n$$\n\nSince adding a constant preserves ordering:\n\n$$\n\\tilde{r}_i - \\tilde{r}_j = (r_i + \\eta) - (r_j + \\eta) = r_i - r_j = -\\Delta_{\\text{barrier}} < 0\n$$\n\nTherefore, $\\tilde{r}_i < \\tilde{r}_j$.\n\n**Step 3: Z-score standardization preserves ordering.**\n\nThe standardized Z-scores are computed as (from Section 5.3):\n\n$$\nz_{r,i} = \\frac{\\tilde{r}_i - \\mu_{\\tilde{r}}}{\\sigma'_{\\tilde{r}}}, \\quad z_{r,j} = \\frac{\\tilde{r}_j - \\mu_{\\tilde{r}}}{\\sigma'_{\\tilde{r}}}\n$$\n\nwhere $\\mu_{\\tilde{r}}$ is the mean and $\\sigma'_{\\tilde{r}} > 0$ is the patched standard deviation (see {prf:ref}`def-patched-std-dev-function`) (strictly positive by definition).\n\nSince $\\tilde{r}_i < \\tilde{r}_j$ and we're dividing by the same positive quantity:\n\n$$\nz_{r,i} = \\frac{\\tilde{r}_i - \\mu_{\\tilde{r}}}{\\sigma'_{\\tilde{r}}} < \\frac{\\tilde{r}_j - \\mu_{\\tilde{r}}}{\\sigma'_{\\tilde{r}}} = z_{r,j}\n$$\n\nThe ordering is preserved: $z_{r,i} < z_{r,j}$.\n\n**Step 4: Fitness potential computation.**\n\nThe fitness potential is computed as (from Section 5.7):\n\n$$\nV_{\\text{fit},i} = (\\alpha z_{d,i} + \\beta z_{r,i} + \\eta)^{\\alpha+\\beta}\n$$\n\nwhere $\\alpha, \\beta > 0$ are the weighting exponents and $\\eta > 0$ ensures positivity of the argument.\n\nAssuming walkers $i$ and $j$ have similar diversity measurements (i.e., $z_{d,i} \\approx z_{d,j}$), the fitness potentials satisfy:\n\n$$\n\\begin{aligned}\nV_{\\text{fit},i} &= (\\alpha z_{d,i} + \\beta z_{r,i} + \\eta)^{\\alpha+\\beta} \\\\\nV_{\\text{fit},j} &= (\\alpha z_{d,j} + \\beta z_{r,j} + \\eta)^{\\alpha+\\beta}\n\\end{aligned}\n$$\n\nSince $z_{r,i} < z_{r,j}$ and $z_{d,i} \\approx z_{d,j}$:\n\n$$\n\\alpha z_{d,i} + \\beta z_{r,i} + \\eta < \\alpha z_{d,j} + \\beta z_{r,j} + \\eta\n$$\n\nThe function $f(x) = x^{\\alpha+\\beta}$ is strictly increasing for $x > 0$ and $\\alpha + \\beta > 0$. Therefore:\n\n$$\nV_{\\text{fit},i} < V_{\\text{fit},j}\n$$\n\n**Step 5: Quantifying the fitness gap $f(\\Delta_{\\text{barrier}})$.**\n\nTo obtain an explicit lower bound on the fitness gap, we use the mean value theorem. Let:\n\n$$\nu_i := \\alpha z_{d,i} + \\beta z_{r,i} + \\eta, \\quad u_j := \\alpha z_{d,j} + \\beta z_{r,j} + \\eta\n$$\n\nBy the mean value theorem, there exists $\\xi \\in (u_i, u_j)$ such that:\n\n$$\nV_{\\text{fit},j} - V_{\\text{fit},i} = (u_j - u_i) \\cdot (\\alpha + \\beta) \\xi^{\\alpha + \\beta - 1}\n$$\n\nThe difference in arguments is:\n\n$$\nu_j - u_i = \\beta(z_{r,j} - z_{r,i}) + \\alpha(z_{d,j} - z_{d,i}) \\approx \\beta(z_{r,j} - z_{r,i}) = \\frac{\\beta}{\\sigma'_{\\tilde{r}}}(\\tilde{r}_j - \\tilde{r}_i) = \\frac{\\beta \\Delta_{\\text{barrier}}}{\\sigma'_{\\tilde{r}}}\n$$\n\nSince $u_i, u_j > \\eta > 0$ (by construction), we have $\\xi > \\eta$. The derivative term is bounded below:\n\n$$\n(\\alpha + \\beta) \\xi^{\\alpha + \\beta - 1} \\geq (\\alpha + \\beta) \\eta^{\\alpha + \\beta - 1} > 0\n$$\n\nTherefore, the fitness gap satisfies:\n\n$$\nV_{\\text{fit},j} - V_{\\text{fit},i} \\geq \\frac{\\beta \\Delta_{\\text{barrier}}}{\\sigma'_{\\tilde{r}}} \\cdot (\\alpha + \\beta) \\eta^{\\alpha + \\beta - 1} =: f(\\Delta_{\\text{barrier}})\n$$\n\n**Step 6: N-uniformity of the fitness gap function.**\n\nThe function $f(\\Delta) = c_{\\beta} \\Delta$ where:\n\n$$\nc_{\\beta} := \\frac{\\beta (\\alpha + \\beta)}{\\sigma'_{\\max}} \\eta^{\\alpha + \\beta - 1}\n$$\n\nis a positive constant independent of $N$. Here $\\sigma'_{\\max}$ is an upper bound on the patched standard deviation (which exists by the boundedness axioms). The fitness gap scales linearly with the barrier difference, with a proportionality constant determined by the algorithm's reward sensitivity parameter $\\beta$ and the standardization scaling.\n\n**Conclusion:**\n\nThe boundary proximity creates a systematic fitness deficit: walker $i$ (closer to boundary) has fitness potential at least $f(\\Delta_{\\text{barrier}})$ lower than walker $j$ (farther from boundary), where $f$ is a positive, N-uniform, monotonically increasing function of the barrier difference.",
      "raw_directive": "7113: :::\n7114: \n7115: :::{prf:proof}\n7116: :label: proof-lem-fitness-gradient-boundary\n7117: **Proof.**\n7118: \n7119: The proof traces the boundary-induced fitness difference through each stage of the measurement and fitness evaluation pipeline defined in Chapter 5, demonstrating that the ordering is preserved and quantifying the resulting gap.\n7120: \n7121: **Step 1: Raw reward difference.**\n7122: \n7123: The raw reward for walker $i$ is defined as (from Section 5.6):\n7124: \n7125: $$\n7126: r_i = R_{\\text{pos}}(x_i) - c_{v\\_reg} \\|v_i\\|^2 - \\varphi_{\\text{barrier}}(x_i)\n7127: $$\n7128: \n7129: where $R_{\\text{pos}}(x_i)$ is the positional reward component and $\\varphi_{\\text{barrier}}(x_i)$ is the boundary barrier penalty.\n7130: \n7131: For walkers $i$ and $j$ with similar positions and velocities (so $R_{\\text{pos}}(x_i) \\approx R_{\\text{pos}}(x_j)$ and $\\|v_i\\| \\approx \\|v_j\\|$), but with walker $i$ closer to the boundary ($\\varphi_{\\text{barrier}}(x_i) > \\varphi_{\\text{barrier}}(x_j)$), the raw reward difference is:\n7132: \n7133: $$\n7134: \\begin{aligned}\n7135: r_i - r_j &= \\left[R_{\\text{pos}}(x_i) - c_{v\\_reg} \\|v_i\\|^2 - \\varphi_{\\text{barrier}}(x_i)\\right] - \\left[R_{\\text{pos}}(x_j) - c_{v\\_reg} \\|v_j\\|^2 - \\varphi_{\\text{barrier}}(x_j)\\right] \\\\\n7136: &\\approx -\\left[\\varphi_{\\text{barrier}}(x_i) - \\varphi_{\\text{barrier}}(x_j)\\right] \\\\\n7137: &= -\\Delta_{\\text{barrier}} < 0\n7138: \\end{aligned}\n7139: $$\n7140: \n7141: Thus, walker $i$ has strictly lower raw reward than walker $j$.\n7142: \n7143: **Step 2: Floor addition preserves ordering.**\n7144: \n7145: The fitness potential construction adds a positive floor $\\eta > 0$ to ensure all values are positive:\n7146: \n7147: $$\n7148: \\tilde{r}_i = r_i + \\eta, \\quad \\tilde{r}_j = r_j + \\eta\n7149: $$\n7150: \n7151: Since adding a constant preserves ordering:\n7152: \n7153: $$\n7154: \\tilde{r}_i - \\tilde{r}_j = (r_i + \\eta) - (r_j + \\eta) = r_i - r_j = -\\Delta_{\\text{barrier}} < 0\n7155: $$\n7156: \n7157: Therefore, $\\tilde{r}_i < \\tilde{r}_j$.\n7158: \n7159: **Step 3: Z-score standardization preserves ordering.**\n7160: \n7161: The standardized Z-scores are computed as (from Section 5.3):\n7162: \n7163: $$\n7164: z_{r,i} = \\frac{\\tilde{r}_i - \\mu_{\\tilde{r}}}{\\sigma'_{\\tilde{r}}}, \\quad z_{r,j} = \\frac{\\tilde{r}_j - \\mu_{\\tilde{r}}}{\\sigma'_{\\tilde{r}}}\n7165: $$\n7166: \n7167: where $\\mu_{\\tilde{r}}$ is the mean and $\\sigma'_{\\tilde{r}} > 0$ is the patched standard deviation (see {prf:ref}`def-patched-std-dev-function`) (strictly positive by definition).\n7168: \n7169: Since $\\tilde{r}_i < \\tilde{r}_j$ and we're dividing by the same positive quantity:\n7170: \n7171: $$\n7172: z_{r,i} = \\frac{\\tilde{r}_i - \\mu_{\\tilde{r}}}{\\sigma'_{\\tilde{r}}} < \\frac{\\tilde{r}_j - \\mu_{\\tilde{r}}}{\\sigma'_{\\tilde{r}}} = z_{r,j}\n7173: $$\n7174: \n7175: The ordering is preserved: $z_{r,i} < z_{r,j}$.\n7176: \n7177: **Step 4: Fitness potential computation.**\n7178: \n7179: The fitness potential is computed as (from Section 5.7):\n7180: \n7181: $$\n7182: V_{\\text{fit},i} = (\\alpha z_{d,i} + \\beta z_{r,i} + \\eta)^{\\alpha+\\beta}\n7183: $$\n7184: \n7185: where $\\alpha, \\beta > 0$ are the weighting exponents and $\\eta > 0$ ensures positivity of the argument.\n7186: \n7187: Assuming walkers $i$ and $j$ have similar diversity measurements (i.e., $z_{d,i} \\approx z_{d,j}$), the fitness potentials satisfy:\n7188: \n7189: $$\n7190: \\begin{aligned}\n7191: V_{\\text{fit},i} &= (\\alpha z_{d,i} + \\beta z_{r,i} + \\eta)^{\\alpha+\\beta} \\\\\n7192: V_{\\text{fit},j} &= (\\alpha z_{d,j} + \\beta z_{r,j} + \\eta)^{\\alpha+\\beta}\n7193: \\end{aligned}\n7194: $$\n7195: \n7196: Since $z_{r,i} < z_{r,j}$ and $z_{d,i} \\approx z_{d,j}$:\n7197: \n7198: $$\n7199: \\alpha z_{d,i} + \\beta z_{r,i} + \\eta < \\alpha z_{d,j} + \\beta z_{r,j} + \\eta\n7200: $$\n7201: \n7202: The function $f(x) = x^{\\alpha+\\beta}$ is strictly increasing for $x > 0$ and $\\alpha + \\beta > 0$. Therefore:\n7203: \n7204: $$\n7205: V_{\\text{fit},i} < V_{\\text{fit},j}\n7206: $$\n7207: \n7208: **Step 5: Quantifying the fitness gap $f(\\Delta_{\\text{barrier}})$.**\n7209: \n7210: To obtain an explicit lower bound on the fitness gap, we use the mean value theorem. Let:\n7211: \n7212: $$\n7213: u_i := \\alpha z_{d,i} + \\beta z_{r,i} + \\eta, \\quad u_j := \\alpha z_{d,j} + \\beta z_{r,j} + \\eta\n7214: $$\n7215: \n7216: By the mean value theorem, there exists $\\xi \\in (u_i, u_j)$ such that:\n7217: \n7218: $$\n7219: V_{\\text{fit},j} - V_{\\text{fit},i} = (u_j - u_i) \\cdot (\\alpha + \\beta) \\xi^{\\alpha + \\beta - 1}\n7220: $$\n7221: \n7222: The difference in arguments is:\n7223: \n7224: $$\n7225: u_j - u_i = \\beta(z_{r,j} - z_{r,i}) + \\alpha(z_{d,j} - z_{d,i}) \\approx \\beta(z_{r,j} - z_{r,i}) = \\frac{\\beta}{\\sigma'_{\\tilde{r}}}(\\tilde{r}_j - \\tilde{r}_i) = \\frac{\\beta \\Delta_{\\text{barrier}}}{\\sigma'_{\\tilde{r}}}\n7226: $$\n7227: \n7228: Since $u_i, u_j > \\eta > 0$ (by construction), we have $\\xi > \\eta$. The derivative term is bounded below:\n7229: \n7230: $$\n7231: (\\alpha + \\beta) \\xi^{\\alpha + \\beta - 1} \\geq (\\alpha + \\beta) \\eta^{\\alpha + \\beta - 1} > 0\n7232: $$\n7233: \n7234: Therefore, the fitness gap satisfies:\n7235: \n7236: $$\n7237: V_{\\text{fit},j} - V_{\\text{fit},i} \\geq \\frac{\\beta \\Delta_{\\text{barrier}}}{\\sigma'_{\\tilde{r}}} \\cdot (\\alpha + \\beta) \\eta^{\\alpha + \\beta - 1} =: f(\\Delta_{\\text{barrier}})\n7238: $$\n7239: \n7240: **Step 6: N-uniformity of the fitness gap function.**\n7241: \n7242: The function $f(\\Delta) = c_{\\beta} \\Delta$ where:\n7243: \n7244: $$\n7245: c_{\\beta} := \\frac{\\beta (\\alpha + \\beta)}{\\sigma'_{\\max}} \\eta^{\\alpha + \\beta - 1}\n7246: $$\n7247: \n7248: is a positive constant independent of $N$. Here $\\sigma'_{\\max}$ is an upper bound on the patched standard deviation (which exists by the boundedness axioms). The fitness gap scales linearly with the barrier difference, with a proportionality constant determined by the algorithm's reward sensitivity parameter $\\beta$ and the standardization scaling.\n7249: \n7250: **Conclusion:**\n7251: \n7252: The boundary proximity creates a systematic fitness deficit: walker $i$ (closer to boundary) has fitness potential at least $f(\\Delta_{\\text{barrier}})$ lower than walker $j$ (farther from boundary), where $f$ is a positive, N-uniform, monotonically increasing function of the barrier difference.\n7253: ",
      "strategy_summary": "The proof proceeds by tracing the impact of boundary proximity on the raw reward difference through subsequent transformations including floor addition, Z-score standardization, and fitness potential computation, demonstrating preservation of ordering at each step and deriving a positive lower bound on the fitness gap using the mean value theorem.",
      "conclusion": {
        "text": "The boundary proximity creates a systematic fitness deficit: walker i (closer to boundary) has fitness potential at least f(\u0394_barrier) lower than walker j (farther from boundary), where f is a positive, N-uniform, monotonically increasing function of the barrier difference.",
        "latex": null
      },
      "assumptions": [
        {
          "text": "Walkers i and j have similar positions and velocities, implying R_pos(x_i) \u2248 R_pos(x_j) and ||v_i|| \u2248 ||v_j||.",
          "latex": null
        },
        {
          "text": "Walkers i and j have similar diversity measurements, i.e., z_{d,i} \u2248 z_{d,j}.",
          "latex": null
        },
        {
          "text": "The patched standard deviation \u03c3'_\tilde{r} > 0 and is bounded above by some \u03c3'_max due to boundedness axioms.",
          "latex": null
        },
        {
          "text": "Parameters \u03b1, \u03b2 > 0, \u03b7 > 0, ensuring positivity and strict increase of the power function.",
          "latex": null
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "definition",
          "text": "The raw reward for walker i is defined as r_i = R_pos(x_i) - c_v_reg ||v_i||^2 - \u03c6_barrier(x_i). For walkers i and j with similar positions and velocities but i closer to the boundary (\u03c6_barrier(x_i) > \u03c6_barrier(x_j)), the raw reward difference is approximately -\u0394_barrier < 0, so r_i < r_j.",
          "latex": "r_i = R_{\\text{pos}}(x_i) - c_{v_{\\text{reg}}} \\|v_i\\|^2 - \\varphi_{\\text{barrier}}(x_i) \\[ r_i - r_j \\approx -\\Delta_{\\text{barrier}} < 0 \\]",
          "references": [],
          "derived_statement": "r_i < r_j"
        },
        {
          "order": 2.0,
          "kind": "transformation",
          "text": "Adding a positive floor \u03b7 > 0 to ensure positivity: \tilde{r}_i = r_i + \u03b7, \tilde{r}_j = r_j + \u03b7. Since a constant is added, the ordering is preserved: \tilde{r}_i < \tilde{r}_j.",
          "latex": "\\tilde{r}_i = r_i + \\eta, \\quad \\tilde{r}_j = r_j + \\eta \\[ \\tilde{r}_i - \\tilde{r}_j = r_i - r_j = -\\Delta_{\\text{barrier}} < 0 \\]",
          "references": [],
          "derived_statement": "\\tilde{r}_i < \\tilde{r}_j"
        },
        {
          "order": 3.0,
          "kind": "standardization",
          "text": "Z-scores are computed as z_{r,i} = (\tilde{r}_i - \u03bc_\tilde{r}) / \u03c3'_\tilde{r}, similarly for j. Since \tilde{r}_i < \tilde{r}_j and \u03c3'_\tilde{r} > 0, the ordering is preserved: z_{r,i} < z_{r,j}.",
          "latex": "z_{r,i} = \\frac{\\tilde{r}_i - \\mu_{\\tilde{r}}}{\\sigma'_{\\tilde{r}}}, \\quad z_{r,j} = \\frac{\\tilde{r}_j - \\mu_{\\tilde{r}}}{\\sigma'_{\\tilde{r}}} \\[ z_{r,i} < z_{r,j} \\]",
          "references": [],
          "derived_statement": "z_{r,i} < z_{r,j}"
        },
        {
          "order": 4.0,
          "kind": "computation",
          "text": "Fitness potential V_{fit,i} = (\u03b1 z_{d,i} + \u03b2 z_{r,i} + \u03b7)^{\u03b1+\u03b2}, similarly for j. With z_{d,i} \u2248 z_{d,j} and z_{r,i} < z_{r,j}, the argument is smaller for i, and since x^{\u03b1+\u03b2} is strictly increasing for x > 0, V_{fit,i} < V_{fit,j}.",
          "latex": "V_{\\text{fit},i} = (\\alpha z_{d,i} + \\beta z_{r,i} + \\eta)^{\\alpha+\\beta} \\[ \\alpha z_{d,i} + \\beta z_{r,i} + \\eta < \\alpha z_{d,j} + \\beta z_{r,j} + \\eta \\] V_{\\text{fit},i} < V_{\\text{fit},j}",
          "references": [],
          "derived_statement": "V_{\\text{fit},i} < V_{\\text{fit},j}"
        },
        {
          "order": 5.0,
          "kind": "bounding",
          "text": "Using the mean value theorem on the power function, V_{fit,j} - V_{fit,i} = (u_j - u_i) \u00b7 (\u03b1 + \u03b2) \u03be^{\u03b1 + \u03b2 - 1} \u2265 (\u03b2 \u0394_barrier / \u03c3'_\tilde{r}) \u00b7 (\u03b1 + \u03b2) \u03b7^{\u03b1 + \u03b2 - 1} = f(\u0394_barrier), where u_i, u_j are the arguments and \u03be > \u03b7.",
          "latex": "u_i := \\alpha z_{d,i} + \\beta z_{r,i} + \\eta, \\quad u_j := \\alpha z_{d,j} + \\beta z_{r,j} + \\eta \\[ V_{\\text{fit},j} - V_{\\text{fit},i} = (u_j - u_i) \\cdot (\\alpha + \\beta) \\xi^{\\alpha + \\beta - 1} \\geq \\frac{\\beta \\Delta_{\\text{barrier}}}{\\sigma'_{\\tilde{r}}} \\cdot (\\alpha + \\beta) \\eta^{\\alpha + \\beta - 1} =: f(\\Delta_{\\text{barrier}}) \\]",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 6.0,
          "kind": "analysis",
          "text": "The function f(\u0394) = c_\u03b2 \u0394, with c_\u03b2 a positive constant independent of N (using \u03c3'_max), is linear in the barrier difference and N-uniform.",
          "latex": "f(\\Delta) = c_{\\beta} \\Delta \\quad c_{\\beta} := \\frac{\\beta (\\alpha + \\beta)}{\\sigma'_{\\max}} \\eta^{\\alpha + \\beta - 1}",
          "references": [],
          "derived_statement": "f is positive, N-uniform, monotonically increasing"
        }
      ],
      "key_equations": [
        {
          "label": "eq-raw-reward",
          "latex": "r_i = R_{\\text{pos}}(x_i) - c_{v_{\\text{reg}}} \\|v_i\\|^2 - \\varphi_{\\text{barrier}}(x_i)",
          "role": "Definition of raw reward for walker i"
        },
        {
          "label": "eq-reward-diff",
          "latex": "r_i - r_j \\approx -[\\varphi_{\\text{barrier}}(x_i) - \\varphi_{\\text{barrier}}(x_j)] = -\\Delta_{\\text{barrier}} < 0",
          "role": "Raw reward difference due to boundary penalty"
        },
        {
          "label": "eq-floored-reward",
          "latex": "\\tilde{r}_i = r_i + \\eta, \\quad \\tilde{r}_j = r_j + \\eta",
          "role": "Addition of floor to ensure positivity, preserves ordering"
        },
        {
          "label": "eq-z-score",
          "latex": "z_{r,i} = \\frac{\\tilde{r}_i - \\mu_{\\tilde{r}}}{\\sigma'_{\\tilde{r}}}",
          "role": "Standardization of floored rewards, preserves ordering"
        },
        {
          "label": "eq-fitness-potential",
          "latex": "V_{\\text{fit},i} = (\\alpha z_{d,i} + \\beta z_{r,i} + \\eta)^{\\alpha+\\beta}",
          "role": "Computation of fitness potential, preserves ordering via monotonicity"
        },
        {
          "label": "eq-mvt-bound",
          "latex": "V_{\\text{fit},j} - V_{\\text{fit},i} \\geq \\frac{\\beta \\Delta_{\\text{barrier}}}{\\sigma'_{\\tilde{r}}} \\cdot (\\alpha + \\beta) \\eta^{\\alpha + \\beta - 1} =: f(\\Delta_{\\text{barrier}})",
          "role": "Lower bound on fitness gap using mean value theorem"
        }
      ],
      "references": [
        "def-patched-std-dev-function"
      ],
      "math_tools": [
        {
          "toolName": "Z-score Standardization",
          "field": "Statistics",
          "description": "A normalization technique that transforms data to have mean zero and standard deviation one by subtracting the mean and dividing by the standard deviation.",
          "roleInProof": "Normalizes the floored rewards while preserving their relative ordering due to subtraction of the same mean and division by a positive standard deviation.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Standard Deviation"
          ]
        },
        {
          "toolName": "Mean Value Theorem",
          "field": "Real Analysis",
          "description": "For a function continuous on [a, b] and differentiable on (a, b), there exists some c in (a, b) such that f'(c) = (f(b) - f(a)) / (b - a).",
          "roleInProof": "Applied to the power function in the fitness potential to obtain a lower bound on the difference between fitness potentials of two walkers.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Rolle's Theorem"
          ]
        },
        {
          "toolName": "Monotonicity of Increasing Functions",
          "field": "Calculus",
          "description": "A function f is strictly increasing if for all x < y, f(x) < f(y), preserving inequalities under composition.",
          "roleInProof": "Ensures that inequalities in rewards and intermediate arguments propagate through additions, scalings, and the power function to the final fitness potentials.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Power Function"
          ]
        }
      ],
      "cases": [
        {
          "name": null,
          "condition": null,
          "summary": null
        }
      ],
      "remarks": [
        {
          "type": "N-uniformity",
          "text": "The proportionality constant c_\u03b2 is independent of the number of walkers N, ensuring the bound holds uniformly."
        },
        {
          "type": null,
          "text": null
        }
      ],
      "gaps": [],
      "tags": [
        "fitness-potential",
        "boundary-effect",
        "reward-difference",
        "z-score-standardization",
        "mean-value-theorem",
        "monotonicity",
        "linear-bound"
      ],
      "document_id": "03_cloning",
      "section": "## 11.2. The Boundary Barrier and Fitness Gradient",
      "span": {
        "start_line": 7113,
        "end_line": 7253,
        "content_start": 7115,
        "content_end": 7252,
        "header_lines": [
          7114
        ]
      },
      "metadata": {
        "label": "proof-lem-fitness-gradient-boundary"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 23,
        "chapter_file": "chapter_23.json",
        "section_id": "## 11.2. The Boundary Barrier and Fitness Gradient"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "fitness",
      "boundary",
      "gradient",
      "walkers",
      "proximity",
      "potential"
    ],
    "content_markdown": ":label: lem-fitness-gradient-boundary\n\nConsider two walkers $i$ and $j$ with similar positions and velocities, except that walker $i$ is closer to the boundary than walker $j$. Specifically, let:\n\n$$\n\\varphi_{\\text{barrier}}(x_i) - \\varphi_{\\text{barrier}}(x_j) = \\Delta_{\\text{barrier}} > 0\n$$\n\nThen, under the fitness evaluation pipeline (Chapter 5), walker $i$ has systematically lower fitness potential:\n\n$$\nV_{\\text{fit},i} \\leq V_{\\text{fit},j} - f(\\Delta_{\\text{barrier}})\n$$",
    "raw_directive": "7095: This raw reward feeds into the fitness potential calculation, so walkers near the boundary have systematically lower fitness.\n7096: \n7097: :::{prf:lemma} Fitness Gradient from Boundary Proximity\n7098: :label: lem-fitness-gradient-boundary\n7099: \n7100: Consider two walkers $i$ and $j$ with similar positions and velocities, except that walker $i$ is closer to the boundary than walker $j$. Specifically, let:\n7101: \n7102: $$\n7103: \\varphi_{\\text{barrier}}(x_i) - \\varphi_{\\text{barrier}}(x_j) = \\Delta_{\\text{barrier}} > 0\n7104: $$\n7105: \n7106: Then, under the fitness evaluation pipeline (Chapter 5), walker $i$ has systematically lower fitness potential:\n7107: \n7108: $$\n7109: V_{\\text{fit},i} \\leq V_{\\text{fit},j} - f(\\Delta_{\\text{barrier}})\n7110: $$\n7111: ",
    "document_id": "03_cloning",
    "section": "## 11.2. The Boundary Barrier and Fitness Gradient",
    "span": {
      "start_line": 7095,
      "end_line": 7111,
      "content_start": 7098,
      "content_end": 7110,
      "header_lines": [
        7096
      ]
    },
    "references": [
      "def-patched-std-dev-function"
    ],
    "metadata": {
      "label": "lem-fitness-gradient-boundary"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 23,
      "chapter_file": "chapter_23.json",
      "section_id": "## 11.2. The Boundary Barrier and Fitness Gradient"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-boundary-enhanced-cloning",
    "title": "Enhanced Cloning Probability Near Boundary",
    "type": "lemma",
    "nl_statement": "For any walker i in the boundary-exposed set \\mathcal{E}_{\\text{boundary}}(S), its cloning probability p_i satisfies p_i \\geq p_{\\text{boundary}}(\\phi_{\\text{thresh}}) > 0.",
    "equations": [
      {
        "label": null,
        "latex": "p_i \\geq p_{\\text{boundary}}(\\phi_{\\text{thresh}}) > 0"
      }
    ],
    "hypotheses": [],
    "conclusion": {
      "text": null,
      "latex": null
    },
    "variables": [
      {
        "symbol": "i",
        "name": "walker index",
        "description": "Index of a walker in the boundary-exposed set",
        "constraints": [
          "i \\in \\mathcal{E}_{\\text{boundary}}(S)"
        ],
        "tags": [
          "walker",
          "index"
        ]
      },
      {
        "symbol": "S",
        "name": "system configuration",
        "description": "The system or set defining the boundary-exposed walkers",
        "constraints": [],
        "tags": [
          "system",
          "configuration"
        ]
      },
      {
        "symbol": "\\mathcal{E}_{\\text{boundary}}(S)",
        "name": "boundary-exposed set",
        "description": "Set of walkers exposed to the boundary in system S",
        "constraints": [],
        "tags": [
          "set",
          "boundary",
          "exposed"
        ]
      },
      {
        "symbol": "p_i",
        "name": "cloning probability",
        "description": "Cloning probability for walker i",
        "constraints": [
          "p_i \\geq 0"
        ],
        "tags": [
          "probability",
          "cloning"
        ]
      },
      {
        "symbol": "p_{\\text{boundary}}",
        "name": "boundary cloning probability function",
        "description": "Function giving minimum cloning probability near boundary",
        "constraints": [],
        "tags": [
          "function",
          "boundary",
          "probability"
        ]
      },
      {
        "symbol": "\\phi_{\\text{thresh}}",
        "name": "threshold phase",
        "description": "Threshold value for the phase or angle parameter",
        "constraints": [],
        "tags": [
          "threshold",
          "phase"
        ]
      }
    ],
    "implicit_assumptions": [],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-boundary-enhanced-cloning",
      "title": null,
      "type": "proof",
      "proves": "lem-boundary-enhanced-cloning",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":::{prf:proof}\n:label: proof-lem-boundary-enhanced-cloning\n**Proof.**\n\nLet $i \\in \\mathcal{E}_{\\text{boundary}}(S)$ be a boundary-exposed walker. By definition, $\\varphi_{\\text{barrier}}(x_i) > \\phi_{\\text{thresh}}$.\n\n**Step 1: Fitness penalty from barrier**\n\nBy {prf:ref}`lem-fitness-gradient-boundary`, walker $i$ has lower fitness than interior walkers. Specifically, there exists at least one interior walker $j$ (in the safe region where $\\varphi_{\\text{barrier}}(x_j) = 0$) such that:\n\n$$\nV_{\\text{fit},i} < V_{\\text{fit},j} - f(\\phi_{\\text{thresh}})\n$$\n\n**Step 2: Companion selection probability**\n\nIn the companion selection operator (see {prf:ref}`def-cloning-companion-operator`), the probability that walker $i$ selects an interior walker as its companion is bounded below. Even if the selection is spatially weighted, there exists a non-zero probability mass on interior walkers:\n\n$$\nP(c_i \\in \\mathcal{I}_{\\text{safe}}) \\geq p_{\\text{interior}} > 0\n$$\n\nwhere $\\mathcal{I}_{\\text{safe}} = \\{j : \\varphi_{\\text{barrier}}(x_j) = 0\\}$ is the set of safe interior walkers.\n\n**Step 3: Cloning score lower bound**\n\nConditioning on selecting an interior companion $j$:\n\n$$\nS_i = \\frac{V_{\\text{fit},j} - V_{\\text{fit},i}}{V_{\\text{fit},i} + \\varepsilon_{\\text{clone}}} \\geq \\frac{f(\\phi_{\\text{thresh}})}{V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}} =: s_{\\text{min}}(\\phi_{\\text{thresh}})\n$$\n\n**Step 4: Cloning probability lower bound**\n\nThe probability of cloning is:\n\n$$\np_i = P(S_i > T_i) \\cdot P(c_i \\in \\mathcal{I}_{\\text{safe}}) \\geq P(s_{\\text{min}} > T_i) \\cdot p_{\\text{interior}}\n$$\n\nSince $T_i \\sim \\text{Uniform}(0, p_{\\max})$:\n\n$$\nP(s_{\\text{min}} > T_i) = \\min\\left(1, \\frac{s_{\\text{min}}}{p_{\\max}}\\right)\n$$\n\nTherefore:\n\n$$\np_i \\geq \\min\\left(1, \\frac{s_{\\text{min}}(\\phi_{\\text{thresh}})}{p_{\\max}}\\right) \\cdot p_{\\text{interior}} =: p_{\\text{boundary}}(\\phi_{\\text{thresh}}) > 0\n$$\n\nThis bound is independent of $N$ and depends only on $\\phi_{\\text{thresh}}$ and the algorithmic parameters.",
      "raw_directive": "7342: :::\n7343: \n7344: :::{prf:proof}\n7345: :label: proof-lem-boundary-enhanced-cloning\n7346: **Proof.**\n7347: \n7348: Let $i \\in \\mathcal{E}_{\\text{boundary}}(S)$ be a boundary-exposed walker. By definition, $\\varphi_{\\text{barrier}}(x_i) > \\phi_{\\text{thresh}}$.\n7349: \n7350: **Step 1: Fitness penalty from barrier**\n7351: \n7352: By {prf:ref}`lem-fitness-gradient-boundary`, walker $i$ has lower fitness than interior walkers. Specifically, there exists at least one interior walker $j$ (in the safe region where $\\varphi_{\\text{barrier}}(x_j) = 0$) such that:\n7353: \n7354: $$\n7355: V_{\\text{fit},i} < V_{\\text{fit},j} - f(\\phi_{\\text{thresh}})\n7356: $$\n7357: \n7358: **Step 2: Companion selection probability**\n7359: \n7360: In the companion selection operator (see {prf:ref}`def-cloning-companion-operator`), the probability that walker $i$ selects an interior walker as its companion is bounded below. Even if the selection is spatially weighted, there exists a non-zero probability mass on interior walkers:\n7361: \n7362: $$\n7363: P(c_i \\in \\mathcal{I}_{\\text{safe}}) \\geq p_{\\text{interior}} > 0\n7364: $$\n7365: \n7366: where $\\mathcal{I}_{\\text{safe}} = \\{j : \\varphi_{\\text{barrier}}(x_j) = 0\\}$ is the set of safe interior walkers.\n7367: \n7368: **Step 3: Cloning score lower bound**\n7369: \n7370: Conditioning on selecting an interior companion $j$:\n7371: \n7372: $$\n7373: S_i = \\frac{V_{\\text{fit},j} - V_{\\text{fit},i}}{V_{\\text{fit},i} + \\varepsilon_{\\text{clone}}} \\geq \\frac{f(\\phi_{\\text{thresh}})}{V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}} =: s_{\\text{min}}(\\phi_{\\text{thresh}})\n7374: $$\n7375: \n7376: **Step 4: Cloning probability lower bound**\n7377: \n7378: The probability of cloning is:\n7379: \n7380: $$\n7381: p_i = P(S_i > T_i) \\cdot P(c_i \\in \\mathcal{I}_{\\text{safe}}) \\geq P(s_{\\text{min}} > T_i) \\cdot p_{\\text{interior}}\n7382: $$\n7383: \n7384: Since $T_i \\sim \\text{Uniform}(0, p_{\\max})$:\n7385: \n7386: $$\n7387: P(s_{\\text{min}} > T_i) = \\min\\left(1, \\frac{s_{\\text{min}}}{p_{\\max}}\\right)\n7388: $$\n7389: \n7390: Therefore:\n7391: \n7392: $$\n7393: p_i \\geq \\min\\left(1, \\frac{s_{\\text{min}}(\\phi_{\\text{thresh}})}{p_{\\max}}\\right) \\cdot p_{\\text{interior}} =: p_{\\text{boundary}}(\\phi_{\\text{thresh}}) > 0\n7394: $$\n7395: \n7396: This bound is independent of $N$ and depends only on $\\phi_{\\text{thresh}}$ and the algorithmic parameters.\n7397: ",
      "strategy_summary": "The proof establishes a positive lower bound on the cloning probability for boundary-exposed walkers by sequentially bounding the fitness penalty relative to interior walkers, the probability of selecting a safe interior companion, the resulting cloning score, and the threshold comparison, ensuring the bound holds independently of the population size.",
      "conclusion": {
        "text": "The cloning probability p_i for a boundary-exposed walker satisfies p_i >= p_boundary(phi_thresh) > 0, independent of population size N and depending only on phi_thresh and algorithmic parameters.",
        "latex": "$p_i \\geq p_{\\text{boundary}}(\\phi_{\\text{thresh}}) > 0$"
      },
      "assumptions": [
        {
          "text": "i is a boundary-exposed walker, so varphi_barrier(x_i) > phi_thresh.",
          "latex": "$i \\in \\mathcal{E}_{\\text{boundary}}(S)$, $\\varphi_{\\text{barrier}}(x_i) > \\phi_{\\text{thresh}}$"
        },
        {
          "text": "Interior walkers j satisfy varphi_barrier(x_j) = 0.",
          "latex": "$\\varphi_{\\text{barrier}}(x_j) = 0$ for $j \\in \\mathcal{I}_{\\text{safe}}$"
        },
        {
          "text": "Threshold T_i ~ Uniform(0, p_max).",
          "latex": "$T_i \\sim \\text{Uniform}(0, p_{\\max})$"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "fitness-penalty",
          "text": "By lem-fitness-gradient-boundary, boundary walker i has lower fitness than some interior walker j: V_fit,i < V_fit,j - f(phi_thresh).",
          "latex": "$V_{\\text{fit},i} < V_{\\text{fit},j} - f(\\phi_{\\text{thresh}})$",
          "references": [
            "lem-fitness-gradient-boundary"
          ],
          "derived_statement": "Lower fitness difference established."
        },
        {
          "order": 2.0,
          "kind": "selection-probability",
          "text": "In the companion selection operator (def-decision-operator), P(c_i in I_safe) >= p_interior > 0, where I_safe = {j : varphi_barrier(x_j) = 0}.",
          "latex": "$P(c_i \\in \\mathcal{I}_{\\text{safe}}) \\geq p_{\\text{interior}} > 0$",
          "references": [
            "def-decision-operator"
          ],
          "derived_statement": "Non-zero probability of selecting safe interior companion."
        },
        {
          "order": 3.0,
          "kind": "cloning-score",
          "text": "Conditioning on interior companion j, S_i = (V_fit,j - V_fit,i)/(V_fit,i + epsilon_clone) >= f(phi_thresh)/(V_pot,max + epsilon_clone) =: s_min(phi_thresh).",
          "latex": "$S_i = \\frac{V_{\\text{fit},j} - V_{\\text{fit},i}}{V_{\\text{fit},i} + \\varepsilon_{\\text{clone}}} \\geq \\frac{f(\\phi_{\\text{thresh}})}{V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}} =: s_{\\text{min}}(\\phi_{\\text{thresh}})$",
          "references": [],
          "derived_statement": "Lower bound on cloning score."
        },
        {
          "order": 4.0,
          "kind": "cloning-probability",
          "text": "p_i = P(S_i > T_i) * P(c_i in I_safe) >= P(s_min > T_i) * p_interior, and since T_i ~ Uniform(0, p_max), P(s_min > T_i) = min(1, s_min / p_max), so p_i >= min(1, s_min(phi_thresh)/p_max) * p_interior =: p_boundary(phi_thresh) > 0.",
          "latex": "$p_i = P(S_i > T_i) \\cdot P(c_i \\in \\mathcal{I}_{\\text{safe}}) \\geq P(s_{\\text{min}} > T_i) \\cdot p_{\\text{interior}} = \\min\\left(1, \\frac{s_{\\text{min}}(\\phi_{\\text{thresh}})}{p_{\\max}}\\right) \\cdot p_{\\text{interior}} =: p_{\\text{boundary}}(\\phi_{\\text{thresh}}) > 0$",
          "references": [],
          "derived_statement": "Positive lower bound on cloning probability independent of N."
        }
      ],
      "key_equations": [
        {
          "label": "eq-fitness-ineq",
          "latex": "$V_{\\text{fit},i} < V_{\\text{fit},j} - f(\\phi_{\\text{thresh}})$",
          "role": "Fitness penalty bound"
        },
        {
          "label": "eq-selection-prob",
          "latex": "$P(c_i \\in \\mathcal{I}_{\\text{safe}}) \\geq p_{\\text{interior}} > 0$",
          "role": "Companion selection lower bound"
        },
        {
          "label": "eq-cloning-score",
          "latex": "$S_i \\geq \\frac{f(\\phi_{\\text{thresh}})}{V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}}} =: s_{\\text{min}}(\\phi_{\\text{thresh}})$",
          "role": "Cloning score lower bound"
        },
        {
          "label": "eq-cloning-prob",
          "latex": "$p_i \\geq \\min\\left(1, \\frac{s_{\\text{min}}(\\phi_{\\text{thresh}})}{p_{\\max}}\\right) \\cdot p_{\\text{interior}} =: p_{\\text{boundary}}(\\phi_{\\text{thresh}}) > 0$",
          "role": "Final cloning probability lower bound"
        }
      ],
      "references": [
        "lem-fitness-gradient-boundary",
        "def-cloning-companion-operator",
        "def-decision-operator"
      ],
      "math_tools": [
        {
          "toolName": "Uniform Distribution",
          "field": "Probability",
          "description": "A continuous probability distribution where all values in an interval are equally likely.",
          "roleInProof": "Used to model the random threshold T_i, enabling computation of the probability P(s_min > T_i).",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Probability Bounds"
          ]
        },
        {
          "toolName": "Lower Bound Inequality",
          "field": "Analysis",
          "description": "A technique to establish a minimum value for a quantity using inequalities.",
          "roleInProof": "Applied throughout to derive successive lower bounds on fitness differences, selection probabilities, scores, and final cloning probability.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Uniform Distribution"
          ]
        },
        {
          "toolName": "Conditional Probability",
          "field": "Probability",
          "description": "Probability of an event given that another event has occurred.",
          "roleInProof": "Used to condition the cloning score on selecting an interior companion, leading to the score lower bound.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Lower Bound Inequality"
          ]
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "independence",
          "text": "This bound is independent of N and depends only on phi_thresh and the algorithmic parameters."
        }
      ],
      "gaps": [],
      "tags": [
        "boundary-exposed",
        "fitness-penalty",
        "companion-selection",
        "cloning-probability",
        "uniform-threshold",
        "lower-bound"
      ],
      "document_id": "03_cloning",
      "section": "## 11.4. Proof of Boundary Potential Contraction",
      "span": {
        "start_line": 7342,
        "end_line": 7397,
        "content_start": 7344,
        "content_end": 7396,
        "header_lines": [
          7343
        ]
      },
      "metadata": {
        "label": "proof-lem-boundary-enhanced-cloning"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 25,
        "chapter_file": "chapter_25.json",
        "section_id": "## 11.4. Proof of Boundary Potential Contraction"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "cloning probability",
      "boundary-exposed",
      "walkers",
      "enhanced",
      "inequality"
    ],
    "content_markdown": ":label: lem-boundary-enhanced-cloning\n\nFor any walker $i$ in the boundary-exposed set $\\mathcal{E}_{\\text{boundary}}(S)$, its cloning probability satisfies:\n\n$$\np_i \\geq p_{\\text{boundary}}(\\phi_{\\text{thresh}}) > 0\n$$",
    "raw_directive": "7330: ### 11.4.1. Cloning Probability for Boundary-Exposed Walkers\n7331: \n7332: :::{prf:lemma} Enhanced Cloning Probability Near Boundary\n7333: :label: lem-boundary-enhanced-cloning\n7334: \n7335: For any walker $i$ in the boundary-exposed set $\\mathcal{E}_{\\text{boundary}}(S)$, its cloning probability satisfies:\n7336: \n7337: $$\n7338: p_i \\geq p_{\\text{boundary}}(\\phi_{\\text{thresh}}) > 0\n7339: $$\n7340: ",
    "document_id": "03_cloning",
    "section": "## 11.4. Proof of Boundary Potential Contraction",
    "span": {
      "start_line": 7330,
      "end_line": 7340,
      "content_start": 7333,
      "content_end": 7339,
      "header_lines": [
        7331
      ]
    },
    "references": [
      "lem-fitness-gradient-boundary",
      "def-cloning-companion-operator",
      "def-decision-operator"
    ],
    "metadata": {
      "label": "lem-boundary-enhanced-cloning"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 25,
      "chapter_file": "chapter_25.json",
      "section_id": "## 11.4. Proof of Boundary Potential Contraction"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  },
  {
    "label": "lem-barrier-reduction-cloning",
    "title": "Expected Barrier Reduction for Cloned Walker",
    "type": "lemma",
    "nl_statement": "When a boundary-exposed walker $i$ clones, the expected barrier penalty after cloning is at most the expected barrier penalty of its companion plus $C_{\\text{jitter}} = O(\\sigma_x^2)$. Furthermore, if the companion is from the safe interior, the expected barrier penalty is at most $C_{\\text{jitter}}$.",
    "equations": [
      {
        "label": null,
        "latex": "\\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i) \\mid i \\text{ clones}] \\leq \\mathbb{E}[\\varphi_{\\text{barrier}}(x_{c_i})] + C_{\\text{jitter}}"
      },
      {
        "label": null,
        "latex": "C_{\\text{jitter}} = O(\\sigma_x^2)"
      },
      {
        "label": null,
        "latex": "\\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i) \\mid c_i \\in \\mathcal{I}_{\\text{safe}}] \\leq C_{\\text{jitter}}"
      }
    ],
    "hypotheses": [
      {
        "text": "a boundary-exposed walker $i$ clones",
        "latex": null
      },
      {
        "text": "the companion $c_i$ is from the safe interior $\\mathcal{I}_{\\text{safe}}$ (for the second bound)",
        "latex": null
      }
    ],
    "conclusion": {
      "text": null,
      "latex": "\\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i) \\mid i \\text{ clones}] \\leq \\mathbb{E}[\\varphi_{\\text{barrier}}(x_{c_i})] + C_{\\text{jitter}} \\quad \\text{and} \\quad \\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i) \\mid c_i \\in \\mathcal{I}_{\\text{safe}}] \\leq C_{\\text{jitter}}"
    },
    "variables": [
      {
        "symbol": "i",
        "name": "walker index",
        "description": "Index of the boundary-exposed walker that clones",
        "constraints": [
          "boundary-exposed"
        ],
        "tags": [
          "walker"
        ]
      },
      {
        "symbol": "x'_i",
        "name": "cloned walker position",
        "description": "Position of the cloned walker after cloning",
        "constraints": [],
        "tags": [
          "position"
        ]
      },
      {
        "symbol": "\\varphi_{\\text{barrier}}",
        "name": "barrier penalty function",
        "description": "Function measuring the barrier penalty at a position",
        "constraints": [],
        "tags": [
          "barrier",
          "function"
        ]
      },
      {
        "symbol": "x_{c_i}",
        "name": "companion position",
        "description": "Position of the companion walker used in cloning",
        "constraints": [],
        "tags": [
          "position",
          "companion"
        ]
      },
      {
        "symbol": "c_i",
        "name": "companion index",
        "description": "Index of the companion walker",
        "constraints": [],
        "tags": [
          "companion"
        ]
      },
      {
        "symbol": "C_{\\text{jitter}}",
        "name": "jitter constant",
        "description": "Constant bounding the jitter effect, equal to O(\\sigma_x^2)",
        "constraints": [
          "O(\\sigma_x^2)"
        ],
        "tags": [
          "jitter",
          "constant"
        ]
      },
      {
        "symbol": "\\sigma_x",
        "name": "position jitter standard deviation",
        "description": "Standard deviation of the position jitter in cloning",
        "constraints": [],
        "tags": [
          "jitter",
          "variance"
        ]
      },
      {
        "symbol": "\\mathcal{I}_{\\text{safe}}",
        "name": "safe interior set",
        "description": "Set of safe interior positions or walkers",
        "constraints": [
          "safe",
          "interior"
        ],
        "tags": [
          "safe",
          "interior"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Cloning involves selecting a companion walker and adding position jitter.",
        "confidence": 0.9
      },
      {
        "text": "The barrier function \\varphi_{\\text{barrier}} is non-negative and penalizes boundary proximity.",
        "confidence": 0.8
      },
      {
        "text": "Walkers are positioned in a space with defined boundaries and safe interior.",
        "confidence": 1.0
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-lem-barrier-reduction-cloning",
      "title": null,
      "type": "proof",
      "proves": "lem-barrier-reduction-cloning",
      "proof_type": "probabilistic",
      "proof_status": "sketch",
      "content_markdown": ":::{prf:proof}\n:label: proof-lem-barrier-reduction-cloning\n**Proof.**\n\nWhen walker $i$ clones from companion $c_i$, its new position is:\n\n$$\nx'_i = x_{c_i} + \\sigma_x \\zeta_i^x \\quad \\text{where } \\zeta_i^x \\sim \\mathcal{N}(0, I_d)\n$$\n\n**Case 1: Companion in safe interior**\n\nIf $c_i \\in \\mathcal{I}_{\\text{safe}}$, then $\\varphi_{\\text{barrier}}(x_{c_i}) = 0$ by definition of the safe region.\n\nThe Gaussian jitter has variance $\\sigma_x^2$. If $\\sigma_x$ is chosen small enough (specifically, $\\sigma_x < \\delta_{\\text{safe}}$ where $\\delta_{\\text{safe}}$ is the width of the safe interior region), then with high probability, $x'_i$ remains in the safe region:\n\n$$\nP(\\varphi_{\\text{barrier}}(x'_i) = 0) \\geq 1 - \\epsilon_{\\text{jitter}}\n$$\n\nIn the worst case (jittering into the boundary region):\n\n$$\n\\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i)] \\leq \\epsilon_{\\text{jitter}} \\cdot \\varphi_{\\text{barrier,max}} =: C_{\\text{jitter}}\n$$\n\n**Case 2: General companion**\n\nFor a general companion, the barrier penalty of $x'_i$ is centered around $\\varphi_{\\text{barrier}}(x_{c_i})$:\n\n$$\n\\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i)] \\approx \\varphi_{\\text{barrier}}(x_{c_i}) + O(\\sigma_x^2 \\|\\nabla \\varphi_{\\text{barrier}}(x_{c_i})\\|^2)\n$$\n\nBy smoothness of $\\varphi_{\\text{barrier}}$ in the interior, the second term is bounded.",
      "raw_directive": "7420: :::\n7421: \n7422: :::{prf:proof}\n7423: :label: proof-lem-barrier-reduction-cloning\n7424: **Proof.**\n7425: \n7426: When walker $i$ clones from companion $c_i$, its new position is:\n7427: \n7428: $$\n7429: x'_i = x_{c_i} + \\sigma_x \\zeta_i^x \\quad \\text{where } \\zeta_i^x \\sim \\mathcal{N}(0, I_d)\n7430: $$\n7431: \n7432: **Case 1: Companion in safe interior**\n7433: \n7434: If $c_i \\in \\mathcal{I}_{\\text{safe}}$, then $\\varphi_{\\text{barrier}}(x_{c_i}) = 0$ by definition of the safe region.\n7435: \n7436: The Gaussian jitter has variance $\\sigma_x^2$. If $\\sigma_x$ is chosen small enough (specifically, $\\sigma_x < \\delta_{\\text{safe}}$ where $\\delta_{\\text{safe}}$ is the width of the safe interior region), then with high probability, $x'_i$ remains in the safe region:\n7437: \n7438: $$\n7439: P(\\varphi_{\\text{barrier}}(x'_i) = 0) \\geq 1 - \\epsilon_{\\text{jitter}}\n7440: $$\n7441: \n7442: In the worst case (jittering into the boundary region):\n7443: \n7444: $$\n7445: \\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i)] \\leq \\epsilon_{\\text{jitter}} \\cdot \\varphi_{\\text{barrier,max}} =: C_{\\text{jitter}}\n7446: $$\n7447: \n7448: **Case 2: General companion**\n7449: \n7450: For a general companion, the barrier penalty of $x'_i$ is centered around $\\varphi_{\\text{barrier}}(x_{c_i})$:\n7451: \n7452: $$\n7453: \\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i)] \\approx \\varphi_{\\text{barrier}}(x_{c_i}) + O(\\sigma_x^2 \\|\\nabla \\varphi_{\\text{barrier}}(x_{c_i})\\|^2)\n7454: $$\n7455: \n7456: By smoothness of $\\varphi_{\\text{barrier}}$ in the interior, the second term is bounded.\n7457: ",
      "strategy_summary": "The proof analyzes the expected barrier penalty after cloning in two cases: when the companion is in the safe interior, where small Gaussian jitter keeps the new position safe with high probability, bounding the expectation by a small constant; and in the general case, where smoothness allows a Taylor approximation to show the expected penalty remains close to that of the companion.",
      "conclusion": {
        "text": "The expected barrier penalty after cloning is bounded by a small constant in the safe case and approximates the companion's penalty plus a controlled error in the general case, ensuring reduction under appropriate conditions.",
        "latex": null
      },
      "assumptions": [
        {
          "text": "The barrier function \\(\\varphi_{\\text{barrier}}\\) is smooth (twice differentiable with bounded Hessian) in the interior.",
          "latex": "\\varphi_{\\text{barrier}} smooth"
        },
        {
          "text": "The jitter variance \\(\\sigma_x\\) is sufficiently small (\\(\\sigma_x < \\delta_{\\text{safe}}\\)) to control probabilistic escape from the safe region.",
          "latex": "\\sigma_x < \\delta_{\\text{safe}}"
        },
        {
          "text": "The safe interior \\(\\mathcal{I}_{\\text{safe}}\\) has positive width \\(\\delta_{\\text{safe}}\\) where \\(\\varphi_{\\text{barrier}} = 0\\).",
          "latex": "\\mathcal{I}_{\\text{safe}} has width \\delta_{\\text{safe}} > 0"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "definition",
          "text": "Define the cloned position of walker i from companion c_i as x'_i = x_{c_i} + \\sigma_x \\zeta_i^x where \\zeta_i^x ~ \\mathcal{N}(0, I_d).",
          "latex": "x'_i = x_{c_i} + \\sigma_x \\zeta_i^x \\quad \\zeta_i^x \\sim \\mathcal{N}(0, I_d)",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 2.0,
          "kind": "case-start",
          "text": "Case 1: Companion in safe interior (c_i \\in \\mathcal{I}_{\\text{safe}}).",
          "latex": null,
          "references": [],
          "derived_statement": null
        },
        {
          "order": 3.0,
          "kind": "claim",
          "text": "\\varphi_{\\text{barrier}}(x_{c_i}) = 0 by definition.",
          "latex": "\\varphi_{\\text{barrier}}(x_{c_i}) = 0",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 4.0,
          "kind": "calculation",
          "text": "With small \\sigma_x < \\delta_{\\text{safe}}, the probability that x'_i stays in safe region is high: P(\\varphi_{\\text{barrier}}(x'_i) = 0) \\geq 1 - \\epsilon_{\\text{jitter}}.",
          "latex": "P(\\varphi_{\\text{barrier}}(x'_i) = 0) \\geq 1 - \\epsilon_{\\text{jitter}}",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 5.0,
          "kind": "bound",
          "text": "In worst case, \\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i)] \\leq \\epsilon_{\\text{jitter}} \\cdot \\varphi_{\\text{barrier,max}} =: C_{\\text{jitter}}.",
          "latex": "\\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i)] \\leq \\epsilon_{\\text{jitter}} \\cdot \\varphi_{\\text{barrier,max}} =: C_{\\text{jitter}}",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 6.0,
          "kind": "case-start",
          "text": "Case 2: General companion position.",
          "latex": null,
          "references": [],
          "derived_statement": null
        },
        {
          "order": 7.0,
          "kind": "approximation",
          "text": "The expected barrier is centered around the companion's: \\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i)] \\approx \\varphi_{\\text{barrier}}(x_{c_i}) + O(\\sigma_x^2 \\|\\nabla \\varphi_{\\text{barrier}}(x_{c_i})\\|^2).",
          "latex": "\\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i)] \\approx \\varphi_{\\text{barrier}}(x_{c_i}) + O(\\sigma_x^2 \\|\\nabla \\varphi_{\\text{barrier}}(x_{c_i})\\|^2)",
          "references": [],
          "derived_statement": null
        },
        {
          "order": 8.0,
          "kind": "justification",
          "text": "The second-order term is bounded by smoothness of \\varphi_{\\text{barrier}} in the interior.",
          "latex": null,
          "references": [],
          "derived_statement": null
        }
      ],
      "key_equations": [
        {
          "label": "eq-cloning-position",
          "latex": "x'_i = x_{c_i} + \\sigma_x \\zeta_i^x \\quad \\zeta_i^x \\sim \\mathcal{N}(0, I_d)",
          "role": "Defines the random cloning mechanism"
        },
        {
          "label": "eq-safe-prob",
          "latex": "P(\\varphi_{\\text{barrier}}(x'_i) = 0) \\geq 1 - \\epsilon_{\\text{jitter}}",
          "role": "Probabilistic guarantee for safe cloning"
        },
        {
          "label": "eq-jitter-bound",
          "latex": "\\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i)] \\leq \\epsilon_{\\text{jitter}} \\cdot \\varphi_{\\text{barrier,max}} =: C_{\\text{jitter}}",
          "role": "Bounds expectation in safe case"
        },
        {
          "label": "eq-general-approx",
          "latex": "\\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i)] \\approx \\varphi_{\\text{barrier}}(x_{c_i}) + O(\\sigma_x^2 \\|\\nabla \\varphi_{\\text{barrier}}(x_{c_i})\\|^2)",
          "role": "Approximation for general case"
        }
      ],
      "references": [],
      "math_tools": [
        {
          "toolName": "Gaussian Distribution",
          "field": "Probability",
          "description": "The multivariate normal distribution used to model random perturbations.",
          "roleInProof": "Generates the jitter in cloning to place the new walker position probabilistically near the companion.",
          "levelOfAbstraction": "Concept",
          "relatedTools": []
        },
        {
          "toolName": "Taylor Expansion",
          "field": "Analysis",
          "description": "Second-order approximation for smooth functions to estimate changes under small perturbations.",
          "roleInProof": "Approximates the barrier function at the perturbed position to bound its expectation in the general case.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "smoothness"
          ]
        },
        {
          "toolName": "Smoothness Assumption",
          "field": "Analysis",
          "description": "Assumption that the function is twice differentiable with bounded Hessian, ensuring local quadratic behavior.",
          "roleInProof": "Justifies the bounded second-order term in the approximation for the barrier penalty.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Taylor Expansion"
          ]
        }
      ],
      "cases": [
        {
          "name": "Companion in safe interior",
          "condition": "c_i \\in \\mathcal{I}_{\\text{safe}}",
          "summary": "Jitter keeps x'_i safe with high probability, bounding \\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i)] \\leq C_{\\text{jitter}} small."
        },
        {
          "name": "General companion",
          "condition": "Arbitrary c_i",
          "summary": "Smoothness yields \\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i)] close to \\varphi_{\\text{barrier}}(x_{c_i}), with bounded perturbation error."
        }
      ],
      "remarks": [
        {
          "type": "approximation",
          "text": "The analysis relies on a second-order Taylor expansion, assuming sufficient smoothness to control higher-order terms."
        },
        {
          "type": "probabilistic",
          "text": "High-probability statements use concentration properties of the Gaussian, though exact constants like \\epsilon_{\\text{jitter}} depend on the barrier geometry."
        }
      ],
      "gaps": [
        {
          "description": "The proof sketches bounds but does not explicitly show how this leads to overall barrier reduction for the lemma, e.g., relating to the companion's penalty or aggregating over walkers.",
          "severity": "major",
          "location_hint": "Transition between cases and conclusion"
        },
        {
          "description": "The O(\\sigma_x^2 \\|\\nabla \\|^2) term is stated as bounded but without deriving the explicit constant from smoothness assumptions.",
          "severity": "minor",
          "location_hint": "End of Case 2"
        }
      ],
      "tags": [
        "barrier-penalty",
        "cloning",
        "gaussian-jitter",
        "safe-region",
        "smoothness",
        "expectation-bound",
        "taylor-approximation"
      ],
      "document_id": "03_cloning",
      "section": "## 11.4. Proof of Boundary Potential Contraction",
      "span": {
        "start_line": 7420,
        "end_line": 7457,
        "content_start": 7422,
        "content_end": 7456,
        "header_lines": [
          7421
        ]
      },
      "metadata": {
        "label": "proof-lem-barrier-reduction-cloning"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 25,
        "chapter_file": "chapter_25.json",
        "section_id": "## 11.4. Proof of Boundary Potential Contraction"
      },
      "generated_at": "2025-11-10T12:23:04.016510+00:00",
      "alt_labels": []
    },
    "tags": [
      "barrier",
      "cloning",
      "walker",
      "expected-value",
      "jitter",
      "variance"
    ],
    "content_markdown": ":label: lem-barrier-reduction-cloning\n\nWhen a boundary-exposed walker $i$ clones, its expected barrier penalty after cloning satisfies:\n\n$$\n\\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i) \\mid i \\text{ clones}] \\leq \\mathbb{E}[\\varphi_{\\text{barrier}}(x_{c_i})] + C_{\\text{jitter}}\n$$\n\nwhere $C_{\\text{jitter}} = O(\\sigma_x^2)$ accounts for the position jitter variance.\n\nFurthermore, if the companion is from the safe interior:\n\n$$\n\\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i) \\mid c_i \\in \\mathcal{I}_{\\text{safe}}] \\leq C_{\\text{jitter}}",
    "raw_directive": "7401: ### 11.4.2. Barrier Reduction from Cloning\n7402: \n7403: :::{prf:lemma} Expected Barrier Reduction for Cloned Walker\n7404: :label: lem-barrier-reduction-cloning\n7405: \n7406: When a boundary-exposed walker $i$ clones, its expected barrier penalty after cloning satisfies:\n7407: \n7408: $$\n7409: \\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i) \\mid i \\text{ clones}] \\leq \\mathbb{E}[\\varphi_{\\text{barrier}}(x_{c_i})] + C_{\\text{jitter}}\n7410: $$\n7411: \n7412: where $C_{\\text{jitter}} = O(\\sigma_x^2)$ accounts for the position jitter variance.\n7413: \n7414: Furthermore, if the companion is from the safe interior:\n7415: \n7416: $$\n7417: \\mathbb{E}[\\varphi_{\\text{barrier}}(x'_i) \\mid c_i \\in \\mathcal{I}_{\\text{safe}}] \\leq C_{\\text{jitter}}\n7418: $$",
    "document_id": "03_cloning",
    "section": "## 11.4. Proof of Boundary Potential Contraction",
    "span": {
      "start_line": 7401,
      "end_line": 7418,
      "content_start": 7404,
      "content_end": 7417,
      "header_lines": [
        7402
      ]
    },
    "references": [],
    "metadata": {
      "label": "lem-barrier-reduction-cloning"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 25,
      "chapter_file": "chapter_25.json",
      "section_id": "## 11.4. Proof of Boundary Potential Contraction"
    },
    "generated_at": "2025-11-10T12:23:04.015269+00:00",
    "alt_labels": []
  }
]