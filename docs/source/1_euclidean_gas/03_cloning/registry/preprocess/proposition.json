[
  {
    "label": "prop-barrier-existence",
    "title": "Existence of a Global Smooth Barrier Function",
    "type": "proposition",
    "nl_statement": "Assuming a regular domain where \\mathcal{X}_{\\text{valid}} satisfies Axiom EG-0, there exists a C^\\infty-smooth, strictly positive function \\varphi on \\mathcal{X}_{\\text{valid}}.",
    "equations": [],
    "hypotheses": [
      {
        "text": "\u039e_{\text{valid}} satisfies the conditions of Axiom EG-0.",
        "latex": "\u039e_{\text{valid}} satisfies the conditions of Axiom EG-0."
      }
    ],
    "conclusion": {
      "text": null,
      "latex": null
    },
    "variables": [
      {
        "symbol": "\u039e_{\text{valid}}",
        "name": "valid set",
        "description": "The domain satisfying Axiom EG-0, assumed regular.",
        "constraints": [
          "regular domain"
        ],
        "tags": [
          "domain",
          "valid"
        ]
      },
      {
        "symbol": "\u03c6",
        "name": "barrier function",
        "description": "Global smooth positive function on the valid set.",
        "constraints": [
          "C^{\\infty}-smooth",
          "strictly positive"
        ],
        "tags": [
          "function",
          "barrier"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The domain is regular, as per the introductory assumption.",
        "confidence": 0.9
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-prop-barrier-existence",
      "title": null,
      "type": "proof",
      "proves": "prop-barrier-existence",
      "proof_type": "construction",
      "proof_status": "complete",
      "content_markdown": ":label: proof-prop-barrier-existence\n\n**Proof.**\n\nThe proof is constructive. We build the function $\\varphi(x)$ using two primary tools: the signed distance function to the boundary and a smooth cutoff function. The construction proceeds in three steps, followed by rigorous verification of all required properties.\n\n**Step 1: The Signed Distance Function.**\n\nSince $\\partial \\mathcal{X}_{\\text{valid}}$ is a $C^{\\infty}$ compact manifold without boundary embedded in $\\mathbb{R}^d$, the **Tubular Neighborhood Theorem** (see [Lee, 2013, Theorem 6.24]) guarantees the existence of an open tubular neighborhood $U \\supset \\partial \\mathcal{X}_{\\text{valid}}$ and a smooth retraction $\\pi: U \\to \\partial \\mathcal{X}_{\\text{valid}}$ such that the signed distance function\n\n$$\n\\rho(x) := \\begin{cases}\nd(x, \\partial \\mathcal{X}_{\\text{valid}}) & \\text{if } x \\in \\mathcal{X}_{\\text{valid}} \\\\\n-d(x, \\partial \\mathcal{X}_{\\text{valid}}) & \\text{if } x \\notin \\mathcal{X}_{\\text{valid}}\n\\end{cases}\n$$\n\nis $C^{\\infty}$-smooth on $U$. Here $d(\\cdot, \\cdot)$ denotes the Euclidean distance. For any $x \\in U \\cap \\mathcal{X}_{\\text{valid}}$, we have $\\rho(x) = \\|x - \\pi(x)\\| > 0$, and $\\nabla \\rho(x)$ is the outward-pointing unit normal vector at the closest boundary point.\n\n**Explicit construction of the tubular neighborhood width:** By compactness of $\\partial \\mathcal{X}_{\\text{valid}}$ and smoothness, there exists $\\delta_0 > 0$ such that $U := \\{x \\in \\mathbb{R}^d : d(x, \\partial \\mathcal{X}_{\\text{valid}}) < \\delta_0\\}$ is a smooth tubular neighborhood. We will use $\\delta < \\delta_0/3$ in the sequel to ensure all relevant regions lie within $U$.\n\n**Step 2: Construction of a Smooth Cutoff Function.**\n\nWe require a smooth cutoff function $\\psi: \\mathbb{R} \\to [0, 1]$ with the following properties:\n1. $\\psi \\in C^{\\infty}(\\mathbb{R})$\n2. $\\psi(t) = 1$ for all $t \\leq 1$\n3. $\\psi(t) = 0$ for all $t \\geq 2$\n4. $\\psi$ is non-increasing on $\\mathbb{R}$\n5. $\\psi'(t) < 0$ for all $t \\in (1, 2)$\n\n**Explicit construction:** A standard construction uses the mollifier function. Define\n\n$$\n\\eta(t) := \\begin{cases}\n\\exp\\left(-\\frac{1}{1-t^2}\\right) & \\text{if } |t| < 1 \\\\\n0 & \\text{if } |t| \\geq 1\n\\end{cases}\n$$\n\nwhich is $C^{\\infty}$ on $\\mathbb{R}$ (see [Rudin, 1987, Theorem 1.46]). Then set\n\n$$\n\\psi(t) := \\frac{\\int_{t}^{\\infty} \\eta(2s - 3) \\, ds}{\\int_{-\\infty}^{\\infty} \\eta(2s - 3) \\, ds}\n$$\n\nThis gives a smooth non-increasing function with $\\psi(t) = 1$ for $t \\leq 1$ and $\\psi(t) = 0$ for $t \\geq 2$.\n\n**Step 3: Construction of the Barrier Function.**\n\nFix $\\delta \\in (0, \\delta_0/3)$ where $\\delta_0$ is the tubular neighborhood width from Step 1. We define $\\varphi: \\mathcal{X}_{\\text{valid}} \\to (0, \\infty)$ by\n\n$$\n\\varphi(x) := \\frac{1}{\\delta} + \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)\\left( \\frac{1}{\\rho(x)} - \\frac{1}{\\delta} \\right)\n$$\n\n**Verification of Properties:**\n\n**Property 1: Smoothness.**\n\nWe verify $\\varphi \\in C^{\\infty}(\\mathcal{X}_{\\text{valid}})$ by analyzing the composition structure.\n\nFor any $x \\in \\mathcal{X}_{\\text{valid}}$ with $\\rho(x) < 3\\delta < \\delta_0$, we have $x \\in U$, so $\\rho(x)$ is $C^{\\infty}$ near $x$. Since $\\rho(x) > 0$ for all $x \\in \\mathcal{X}_{\\text{valid}}$, the function $1/\\rho(x)$ is $C^{\\infty}$ on all of $\\mathcal{X}_{\\text{valid}}$. The composition $\\psi(\\rho(x)/\\delta)$ is $C^{\\infty}$ since both $\\psi$ and $\\rho$ are $C^{\\infty}$.\n\nFor $x$ with $\\rho(x) \\geq 3\\delta$, we have $\\rho(x)/\\delta \\geq 3 > 2$, so $\\psi(\\rho(x)/\\delta) = 0$ identically in a neighborhood of $x$. Thus $\\varphi(x) = 1/\\delta$ (constant) in this region, which is trivially $C^{\\infty}$.\n\nThe matching at $\\rho(x) = 3\\delta$ is smooth because $\\psi$ and all its derivatives vanish for arguments $\\geq 2$.\n\nTherefore, $\\varphi \\in C^{\\infty}(\\mathcal{X}_{\\text{valid}})$.\n\n**Property 2: Boundary Divergence.**\n\nWe must show that for any sequence $(x_n) \\subset \\mathcal{X}_{\\text{valid}}$ with $x_n \\to x_{\\infty} \\in \\partial \\mathcal{X}_{\\text{valid}}$, we have $\\varphi(x_n) \\to \\infty$.\n\nSince $x_n \\to x_{\\infty} \\in \\partial \\mathcal{X}_{\\text{valid}}$ and $x_n \\in \\mathcal{X}_{\\text{valid}}$, by continuity of the distance function, $\\rho(x_n) = d(x_n, \\partial \\mathcal{X}_{\\text{valid}}) \\to 0^{+}$.\n\nFor sufficiently large $n$, we have $\\rho(x_n) < \\delta$, which implies $\\rho(x_n)/\\delta < 1$, hence $\\psi(\\rho(x_n)/\\delta) = 1$. In this regime:\n\n$$\n\\varphi(x_n) = \\frac{1}{\\delta} + 1 \\cdot \\left( \\frac{1}{\\rho(x_n)} - \\frac{1}{\\delta} \\right) = \\frac{1}{\\rho(x_n)}\n$$\n\nSince $\\rho(x_n) \\to 0^{+}$, we have $\\varphi(x_n) = 1/\\rho(x_n) \\to +\\infty$.\n\n**Property 3: Strict Positivity.**\n\nWe prove $\\varphi(x) > 0$ for all $x \\in \\mathcal{X}_{\\text{valid}}$ by case analysis.\n\n*Case 1: $0 < \\rho(x) \\leq \\delta$.*\nHere $\\rho(x)/\\delta \\leq 1$, so $\\psi(\\rho(x)/\\delta) = 1$. Thus:\n\n$$\n\\varphi(x) = \\frac{1}{\\delta} + 1 \\cdot \\left( \\frac{1}{\\rho(x)} - \\frac{1}{\\delta} \\right) = \\frac{1}{\\rho(x)} > 0\n$$\n\nsince $\\rho(x) > 0$.\n\n*Case 2: $\\rho(x) \\geq 2\\delta$.*\nHere $\\rho(x)/\\delta \\geq 2$, so $\\psi(\\rho(x)/\\delta) = 0$. Thus:\n\n$$\n\\varphi(x) = \\frac{1}{\\delta} + 0 \\cdot \\left( \\frac{1}{\\rho(x)} - \\frac{1}{\\delta} \\right) = \\frac{1}{\\delta} > 0\n$$\n\n*Case 3: $\\delta < \\rho(x) < 2\\delta$.*\nThis is the transition region. We have $1 < \\rho(x)/\\delta < 2$, so $\\psi(\\rho(x)/\\delta) \\in (0, 1)$.\n\nRewrite $\\varphi(x)$ by expanding:\n\n$$\n\\begin{aligned}\n\\varphi(x) &= \\frac{1}{\\delta} + \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)\\left( \\frac{1}{\\rho(x)} - \\frac{1}{\\delta} \\right) \\\\\n&= \\frac{1}{\\delta} + \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right) \\cdot \\frac{1}{\\rho(x)} - \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right) \\cdot \\frac{1}{\\delta} \\\\\n&= \\frac{1}{\\delta}\\left(1 - \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)\\right) + \\frac{1}{\\rho(x)} \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)\n\\end{aligned}\n$$\n\nSince $\\psi(\\rho(x)/\\delta) \\in (0,1)$, we have $1 - \\psi(\\rho(x)/\\delta) \\in (0, 1) \\subset (0, \\infty)$. Thus:\n\n$$\n\\varphi(x) = \\underbrace{\\frac{1}{\\delta}\\left(1 - \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)\\right)}_{> 0} + \\underbrace{\\frac{1}{\\rho(x)} \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)}_{> 0} > 0\n$$\n\nBoth terms are strictly positive since $\\delta > 0$, $\\rho(x) > 0$, $1 - \\psi > 0$, and $\\psi > 0$ in this regime.\n\n**Conclusion:**\n\nWe have constructed a function $\\varphi: \\mathcal{X}_{\\text{valid}} \\to (0, \\infty)$ satisfying all three properties: $\\varphi \\in C^{\\infty}(\\mathcal{X}_{\\text{valid}})$, $\\varphi(x) > 0$ everywhere, and $\\varphi(x) \\to \\infty$ as $x \\to \\partial \\mathcal{X}_{\\text{valid}}$.",
      "raw_directive": "216: 3.  **Boundary Divergence:** $\\varphi(x) \\to \\infty$ as $x \\to \\partial \\mathcal{X}_{\\text{valid}}$.\n217: :::\n218: :::{prf:proof}\n219: :label: proof-prop-barrier-existence\n220: \n221: **Proof.**\n222: \n223: The proof is constructive. We build the function $\\varphi(x)$ using two primary tools: the signed distance function to the boundary and a smooth cutoff function. The construction proceeds in three steps, followed by rigorous verification of all required properties.\n224: \n225: **Step 1: The Signed Distance Function.**\n226: \n227: Since $\\partial \\mathcal{X}_{\\text{valid}}$ is a $C^{\\infty}$ compact manifold without boundary embedded in $\\mathbb{R}^d$, the **Tubular Neighborhood Theorem** (see [Lee, 2013, Theorem 6.24]) guarantees the existence of an open tubular neighborhood $U \\supset \\partial \\mathcal{X}_{\\text{valid}}$ and a smooth retraction $\\pi: U \\to \\partial \\mathcal{X}_{\\text{valid}}$ such that the signed distance function\n228: \n229: $$\n230: \\rho(x) := \\begin{cases}\n231: d(x, \\partial \\mathcal{X}_{\\text{valid}}) & \\text{if } x \\in \\mathcal{X}_{\\text{valid}} \\\\\n232: -d(x, \\partial \\mathcal{X}_{\\text{valid}}) & \\text{if } x \\notin \\mathcal{X}_{\\text{valid}}\n233: \\end{cases}\n234: $$\n235: \n236: is $C^{\\infty}$-smooth on $U$. Here $d(\\cdot, \\cdot)$ denotes the Euclidean distance. For any $x \\in U \\cap \\mathcal{X}_{\\text{valid}}$, we have $\\rho(x) = \\|x - \\pi(x)\\| > 0$, and $\\nabla \\rho(x)$ is the outward-pointing unit normal vector at the closest boundary point.\n237: \n238: **Explicit construction of the tubular neighborhood width:** By compactness of $\\partial \\mathcal{X}_{\\text{valid}}$ and smoothness, there exists $\\delta_0 > 0$ such that $U := \\{x \\in \\mathbb{R}^d : d(x, \\partial \\mathcal{X}_{\\text{valid}}) < \\delta_0\\}$ is a smooth tubular neighborhood. We will use $\\delta < \\delta_0/3$ in the sequel to ensure all relevant regions lie within $U$.\n239: \n240: **Step 2: Construction of a Smooth Cutoff Function.**\n241: \n242: We require a smooth cutoff function $\\psi: \\mathbb{R} \\to [0, 1]$ with the following properties:\n243: 1. $\\psi \\in C^{\\infty}(\\mathbb{R})$\n244: 2. $\\psi(t) = 1$ for all $t \\leq 1$\n245: 3. $\\psi(t) = 0$ for all $t \\geq 2$\n246: 4. $\\psi$ is non-increasing on $\\mathbb{R}$\n247: 5. $\\psi'(t) < 0$ for all $t \\in (1, 2)$\n248: \n249: **Explicit construction:** A standard construction uses the mollifier function. Define\n250: \n251: $$\n252: \\eta(t) := \\begin{cases}\n253: \\exp\\left(-\\frac{1}{1-t^2}\\right) & \\text{if } |t| < 1 \\\\\n254: 0 & \\text{if } |t| \\geq 1\n255: \\end{cases}\n256: $$\n257: \n258: which is $C^{\\infty}$ on $\\mathbb{R}$ (see [Rudin, 1987, Theorem 1.46]). Then set\n259: \n260: $$\n261: \\psi(t) := \\frac{\\int_{t}^{\\infty} \\eta(2s - 3) \\, ds}{\\int_{-\\infty}^{\\infty} \\eta(2s - 3) \\, ds}\n262: $$\n263: \n264: This gives a smooth non-increasing function with $\\psi(t) = 1$ for $t \\leq 1$ and $\\psi(t) = 0$ for $t \\geq 2$.\n265: \n266: **Step 3: Construction of the Barrier Function.**\n267: \n268: Fix $\\delta \\in (0, \\delta_0/3)$ where $\\delta_0$ is the tubular neighborhood width from Step 1. We define $\\varphi: \\mathcal{X}_{\\text{valid}} \\to (0, \\infty)$ by\n269: \n270: $$\n271: \\varphi(x) := \\frac{1}{\\delta} + \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)\\left( \\frac{1}{\\rho(x)} - \\frac{1}{\\delta} \\right)\n272: $$\n273: \n274: **Verification of Properties:**\n275: \n276: **Property 1: Smoothness.**\n277: \n278: We verify $\\varphi \\in C^{\\infty}(\\mathcal{X}_{\\text{valid}})$ by analyzing the composition structure.\n279: \n280: For any $x \\in \\mathcal{X}_{\\text{valid}}$ with $\\rho(x) < 3\\delta < \\delta_0$, we have $x \\in U$, so $\\rho(x)$ is $C^{\\infty}$ near $x$. Since $\\rho(x) > 0$ for all $x \\in \\mathcal{X}_{\\text{valid}}$, the function $1/\\rho(x)$ is $C^{\\infty}$ on all of $\\mathcal{X}_{\\text{valid}}$. The composition $\\psi(\\rho(x)/\\delta)$ is $C^{\\infty}$ since both $\\psi$ and $\\rho$ are $C^{\\infty}$.\n281: \n282: For $x$ with $\\rho(x) \\geq 3\\delta$, we have $\\rho(x)/\\delta \\geq 3 > 2$, so $\\psi(\\rho(x)/\\delta) = 0$ identically in a neighborhood of $x$. Thus $\\varphi(x) = 1/\\delta$ (constant) in this region, which is trivially $C^{\\infty}$.\n283: \n284: The matching at $\\rho(x) = 3\\delta$ is smooth because $\\psi$ and all its derivatives vanish for arguments $\\geq 2$.\n285: \n286: Therefore, $\\varphi \\in C^{\\infty}(\\mathcal{X}_{\\text{valid}})$.\n287: \n288: **Property 2: Boundary Divergence.**\n289: \n290: We must show that for any sequence $(x_n) \\subset \\mathcal{X}_{\\text{valid}}$ with $x_n \\to x_{\\infty} \\in \\partial \\mathcal{X}_{\\text{valid}}$, we have $\\varphi(x_n) \\to \\infty$.\n291: \n292: Since $x_n \\to x_{\\infty} \\in \\partial \\mathcal{X}_{\\text{valid}}$ and $x_n \\in \\mathcal{X}_{\\text{valid}}$, by continuity of the distance function, $\\rho(x_n) = d(x_n, \\partial \\mathcal{X}_{\\text{valid}}) \\to 0^{+}$.\n293: \n294: For sufficiently large $n$, we have $\\rho(x_n) < \\delta$, which implies $\\rho(x_n)/\\delta < 1$, hence $\\psi(\\rho(x_n)/\\delta) = 1$. In this regime:\n295: \n296: $$\n297: \\varphi(x_n) = \\frac{1}{\\delta} + 1 \\cdot \\left( \\frac{1}{\\rho(x_n)} - \\frac{1}{\\delta} \\right) = \\frac{1}{\\rho(x_n)}\n298: $$\n299: \n300: Since $\\rho(x_n) \\to 0^{+}$, we have $\\varphi(x_n) = 1/\\rho(x_n) \\to +\\infty$.\n301: \n302: **Property 3: Strict Positivity.**\n303: \n304: We prove $\\varphi(x) > 0$ for all $x \\in \\mathcal{X}_{\\text{valid}}$ by case analysis.\n305: \n306: *Case 1: $0 < \\rho(x) \\leq \\delta$.*\n307: Here $\\rho(x)/\\delta \\leq 1$, so $\\psi(\\rho(x)/\\delta) = 1$. Thus:\n308: \n309: $$\n310: \\varphi(x) = \\frac{1}{\\delta} + 1 \\cdot \\left( \\frac{1}{\\rho(x)} - \\frac{1}{\\delta} \\right) = \\frac{1}{\\rho(x)} > 0\n311: $$\n312: \n313: since $\\rho(x) > 0$.\n314: \n315: *Case 2: $\\rho(x) \\geq 2\\delta$.*\n316: Here $\\rho(x)/\\delta \\geq 2$, so $\\psi(\\rho(x)/\\delta) = 0$. Thus:\n317: \n318: $$\n319: \\varphi(x) = \\frac{1}{\\delta} + 0 \\cdot \\left( \\frac{1}{\\rho(x)} - \\frac{1}{\\delta} \\right) = \\frac{1}{\\delta} > 0\n320: $$\n321: \n322: *Case 3: $\\delta < \\rho(x) < 2\\delta$.*\n323: This is the transition region. We have $1 < \\rho(x)/\\delta < 2$, so $\\psi(\\rho(x)/\\delta) \\in (0, 1)$.\n324: \n325: Rewrite $\\varphi(x)$ by expanding:\n326: \n327: $$\n328: \\begin{aligned}\n329: \\varphi(x) &= \\frac{1}{\\delta} + \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)\\left( \\frac{1}{\\rho(x)} - \\frac{1}{\\delta} \\right) \\\\\n330: &= \\frac{1}{\\delta} + \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right) \\cdot \\frac{1}{\\rho(x)} - \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right) \\cdot \\frac{1}{\\delta} \\\\\n331: &= \\frac{1}{\\delta}\\left(1 - \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)\\right) + \\frac{1}{\\rho(x)} \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)\n332: \\end{aligned}\n333: $$\n334: \n335: Since $\\psi(\\rho(x)/\\delta) \\in (0,1)$, we have $1 - \\psi(\\rho(x)/\\delta) \\in (0, 1) \\subset (0, \\infty)$. Thus:\n336: \n337: $$\n338: \\varphi(x) = \\underbrace{\\frac{1}{\\delta}\\left(1 - \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)\\right)}_{> 0} + \\underbrace{\\frac{1}{\\rho(x)} \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)}_{> 0} > 0\n339: $$\n340: \n341: Both terms are strictly positive since $\\delta > 0$, $\\rho(x) > 0$, $1 - \\psi > 0$, and $\\psi > 0$ in this regime.\n342: \n343: **Conclusion:**\n344: \n345: We have constructed a function $\\varphi: \\mathcal{X}_{\\text{valid}} \\to (0, \\infty)$ satisfying all three properties: $\\varphi \\in C^{\\infty}(\\mathcal{X}_{\\text{valid}})$, $\\varphi(x) > 0$ everywhere, and $\\varphi(x) \\to \\infty$ as $x \\to \\partial \\mathcal{X}_{\\text{valid}}$.\n346: ",
      "strategy_summary": "The proof constructs the barrier function \u03c6 explicitly using the signed distance function \u03c1 to the boundary, ensured smooth by the Tubular Neighborhood Theorem, and a smooth non-increasing cutoff function \u03c8 built from a mollifier; it then verifies smoothness, strict positivity via case analysis, and boundary divergence directly from the form of \u03c6 near the boundary.",
      "conclusion": {
        "text": "We have constructed a function \u03c6: X_valid \u2192 (0, \u221e) satisfying all three properties: \u03c6 \u2208 C^\u221e(X_valid), \u03c6(x) > 0 everywhere, and \u03c6(x) \u2192 \u221e as x \u2192 \u2202X_valid.",
        "latex": "We have constructed a function $\\varphi: \\mathcal{X}_{\\text{valid}} \\to (0, \\infty)$ satisfying all three properties: $\\varphi \\in C^{\\infty}(\\mathcal{X}_{\\text{valid}})$, $\\varphi(x) > 0$ everywhere, and $\\varphi(x) \\to \\infty$ as $x \\to \\partial \\mathcal{X}_{\\text{valid}}$."
      },
      "assumptions": [
        {
          "text": "The boundary \u2202X_valid is a C^\u221e compact manifold without boundary embedded in R^d.",
          "latex": "$\\partial \\mathcal{X}_{\\text{valid}}$ is a $C^{\\infty}$ compact manifold without boundary embedded in $\\mathbb{R}^d$."
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "construction",
          "text": "Define the signed distance function \u03c1 using the Tubular Neighborhood Theorem, which ensures a smooth tubular neighborhood U where \u03c1 is C^\u221e-smooth.",
          "latex": null,
          "references": [
            "thm-tubular-neighborhood"
          ],
          "derived_statement": "\u03c1(x) > 0 for x in X_valid \u2229 U, with \u2207\u03c1 as the outward unit normal."
        },
        {
          "order": 2.0,
          "kind": "construction",
          "text": "Construct the smooth non-increasing cutoff function \u03c8: R \u2192 [0,1] using a mollifier \u03b7, satisfying \u03c8(t)=1 for t\u22641, \u03c8(t)=0 for t\u22652, and \u03c8'(t)<0 on (1,2).",
          "latex": null,
          "references": [
            "thm-mollifier-rudin"
          ],
          "derived_statement": "\u03c8 is C^\u221e and non-increasing."
        },
        {
          "order": 3.0,
          "kind": "construction",
          "text": "Fix \u03b4 < \u03b4_0/3 and define the barrier \u03c6(x) = 1/\u03b4 + \u03c8(\u03c1(x)/\u03b4) (1/\u03c1(x) - 1/\u03b4) on X_valid.",
          "latex": null,
          "references": [],
          "derived_statement": "\u03c6: X_valid \u2192 (0,\u221e)."
        },
        {
          "order": 4.0,
          "kind": "verification",
          "text": "Verify smoothness: \u03c1 and 1/\u03c1 are C^\u221e on X_valid, \u03c8\u2218(\u03c1/\u03b4) is C^\u221e, and \u03c6 is constant (hence smooth) where \u03c1\u22653\u03b4, with smooth matching at the transition.",
          "latex": null,
          "references": [],
          "derived_statement": "\u03c6 \u2208 C^\u221e(X_valid)."
        },
        {
          "order": 5.0,
          "kind": "verification",
          "text": "Verify boundary divergence: As x\u2192\u2202X_valid, \u03c1(x)\u21920^+, so for \u03c1(x)<\u03b4, \u03c6(x)=1/\u03c1(x)\u2192\u221e.",
          "latex": null,
          "references": [],
          "derived_statement": "\u03c6(x)\u2192\u221e as x\u2192\u2202X_valid."
        },
        {
          "order": 6.0,
          "kind": "verification",
          "text": "Verify strict positivity by case analysis on \u03c1(x).",
          "latex": null,
          "references": [],
          "derived_statement": "\u03c6(x)>0 for all x in X_valid."
        }
      ],
      "key_equations": [
        {
          "label": "eq-signed-distance",
          "latex": "\\rho(x) := \\begin{cases} d(x, \\partial \\mathcal{X}_{\\text{valid}}) & \\text{if } x \\in \\mathcal{X}_{\\text{valid}} \\\\ -d(x, \\partial \\mathcal{X}_{\\text{valid}}) & \\text{if } x \\notin \\mathcal{X}_{\\text{valid}} \\end{cases}",
          "role": "Defines the signed distance function \u03c1."
        },
        {
          "label": "eq-mollifier",
          "latex": "\\eta(t) := \\begin{cases} \\exp\\left(-\\frac{1}{1-t^2}\\right) & \\text{if } |t| < 1 \\\\ 0 & \\text{if } |t| \\geq 1 \\end{cases}",
          "role": "Standard mollifier used to build the cutoff."
        },
        {
          "label": "eq-cutoff",
          "latex": "\\psi(t) := \\frac{\\int_{t}^{\\infty} \\eta(2s - 3) \\, ds}{\\int_{-\\infty}^{\\infty} \\eta(2s - 3) \\, ds}",
          "role": "Defines the smooth cutoff function \u03c8."
        },
        {
          "label": "eq-barrier",
          "latex": "\\varphi(x) := \\frac{1}{\\delta} + \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)\\left( \\frac{1}{\\rho(x)} - \\frac{1}{\\delta} \\right)",
          "role": "Explicit construction of the barrier function \u03c6."
        },
        {
          "label": "eq-positivity-rewrite",
          "latex": "\\varphi(x) = \\frac{1}{\\delta}\\left(1 - \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)\\right) + \\frac{1}{\\rho(x)} \\psi\\left(\\frac{\\rho(x)}{\\delta}\\right)",
          "role": "Rewritten form used to show positivity in the transition case."
        }
      ],
      "references": [],
      "math_tools": [
        {
          "toolName": "Signed Distance Function",
          "field": "Differential Geometry",
          "description": "Measures the signed Euclidean distance to a boundary manifold, positive inside and negative outside.",
          "roleInProof": "Serves as the base for defining the barrier's behavior near the boundary, capturing distance to \u2202X_valid.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Tubular Neighborhood Theorem"
          ]
        },
        {
          "toolName": "Tubular Neighborhood Theorem",
          "field": "Differential Geometry",
          "description": "Asserts the existence of a neighborhood around a smooth submanifold where a smooth projection retraction exists, enabling smooth distance functions.",
          "roleInProof": "Justifies the C^\u221e smoothness of the signed distance function \u03c1 in a tubular neighborhood of the boundary.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Signed Distance Function"
          ]
        },
        {
          "toolName": "Mollifier",
          "field": "Real Analysis",
          "description": "A smooth non-negative function with compact support used to construct smooth approximations and cutoffs.",
          "roleInProof": "Used to explicitly construct the smooth non-increasing cutoff function \u03c8 that transitions the barrier from 1/\u03c1 to a constant.",
          "levelOfAbstraction": "Technique",
          "relatedTools": []
        }
      ],
      "cases": [
        {
          "name": "Near boundary: 0 < \u03c1(x) \u2264 \u03b4",
          "condition": "0 < \\rho(x) \\leq \\delta",
          "summary": "\u03c8(\u03c1(x)/\u03b4) = 1, so \u03c6(x) = 1/\u03c1(x) > 0."
        },
        {
          "name": "Far interior: \u03c1(x) \u2265 2\u03b4",
          "condition": "\\rho(x) \\geq 2\\delta",
          "summary": "\u03c8(\u03c1(x)/\u03b4) = 0, so \u03c6(x) = 1/\u03b4 > 0."
        },
        {
          "name": "Transition: \u03b4 < \u03c1(x) < 2\u03b4",
          "condition": "\\delta < \\rho(x) < 2\\delta",
          "summary": "\u03c8(\u03c1(x)/\u03b4) \u2208 (0,1), and \u03c6(x) is a convex combination of positive terms 1/\u03b4 and 1/\u03c1(x), hence > 0."
        }
      ],
      "remarks": [
        {
          "type": "note",
          "text": "The choice of \u03b4 < \u03b4_0/3 ensures all constructions stay within the smooth tubular neighborhood U."
        },
        {
          "type": "reference",
          "text": "Relies on standard results from differential geometry and analysis for smoothness guarantees."
        }
      ],
      "gaps": [],
      "tags": [
        "barrier-function",
        "signed-distance",
        "smooth-cutoff",
        "tubular-neighborhood",
        "mollifier",
        "smoothness",
        "positivity",
        "boundary-divergence",
        "constructive"
      ],
      "document_id": "03_cloning",
      "section": "## 2. The Coupled State Space and State Differences",
      "span": {
        "start_line": 216,
        "end_line": 346,
        "content_start": 219,
        "content_end": 345,
        "header_lines": [
          217
        ]
      },
      "metadata": {
        "label": "proof-prop-barrier-existence"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 2,
        "chapter_file": "chapter_2.json",
        "section_id": "## 2. The Coupled State Space and State Differences"
      },
      "generated_at": "2025-11-10T13:26:08.165093+00:00",
      "alt_labels": []
    },
    "tags": [
      "barrier-function",
      "existence",
      "smoothness",
      "positivity",
      "global"
    ],
    "content_markdown": ":label: prop-barrier-existence\n\nLet $\\mathcal{X}_{\\text{valid}}$ satisfy the conditions of {prf:ref}`axiom-domain-regularity`. Then there exists a function $\\varphi: \\mathcal{X}_{\\text{valid}} \\to \\mathbb{R}$ with the following properties:\n1.  **Smoothness:** $\\varphi(x)$ is $C^{\\infty}$-smooth on $\\mathcal{X}_{\\text{valid}}$.",
    "raw_directive": "208: Under the assumption of a regular domain, we can state and prove the existence of our desired barrier function.\n209: \n210: :::{prf:proposition} Existence of a Global Smooth Barrier Function\n211: :label: prop-barrier-existence\n212: \n213: Let $\\mathcal{X}_{\\text{valid}}$ satisfy the conditions of {prf:ref}`axiom-domain-regularity`. Then there exists a function $\\varphi: \\mathcal{X}_{\\text{valid}} \\to \\mathbb{R}$ with the following properties:\n214: 1.  **Smoothness:** $\\varphi(x)$ is $C^{\\infty}$-smooth on $\\mathcal{X}_{\\text{valid}}$.\n215: 2.  **Positivity:** $\\varphi(x)$ is strictly positive for all $x \\in \\mathcal{X}_{\\text{valid}}$.",
    "document_id": "03_cloning",
    "section": "## 2. The Coupled State Space and State Differences",
    "span": {
      "start_line": 208,
      "end_line": 215,
      "content_start": 211,
      "content_end": 214,
      "header_lines": [
        209
      ]
    },
    "references": [
      "axiom-domain-regularity"
    ],
    "metadata": {
      "label": "prop-barrier-existence"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 2,
      "chapter_file": "chapter_2.json",
      "section_id": "## 2. The Coupled State Space and State Differences"
    },
    "generated_at": "2025-11-10T13:26:08.168931+00:00",
    "alt_labels": []
  },
  {
    "label": "prop-lyapunov-necessity",
    "title": "Necessity of the Augmented Lyapunov Structure",
    "type": "proposition",
    "nl_statement": "The augmented Lyapunov function V_total = W_h^2 + c_V V_Var + c_B W_b, with its weighted components measuring inter-swarm distance, intra-swarm dispersion, and boundary penalties, is mathematically necessary to prove convergence by balancing contractions and expansions from the cloning and kinetic operators, ensuring net negative drift.",
    "equations": [
      {
        "label": null,
        "latex": "\\mathbb{E}[V_{\\text{total}}(t+1) - V_{\\text{total}}(t)] = \\underbrace{\\mathbb{E}[\\Delta W_h^2]}_{\\Psi_{\\text{clone}}: +, \\ \\Psi_{\\text{kin}}: -} + c_V \\underbrace{\\mathbb{E}[\\Delta V_{\\text{Var}}]}_{\\Psi_{\\text{clone}}: -, \\ \\Psi_{\\text{kin}}: +} + c_B \\underbrace{\\mathbb{E}[\\Delta W_b]}_{\\text{both: } -}"
      }
    ],
    "hypotheses": [],
    "conclusion": {
      "text": "The weighted-sum structure of V_total ensures net negative drift: E[V_total(t+1) - V_total(t)] \u2264 -\u03ba V_total(t) + C for some \u03ba > 0, by balancing operator-induced expansions and contractions.",
      "latex": "\\mathbb{E}[V_{\\text{total}}(t+1) - V_{\\text{total}}(t)] \\leq -\\kappa V_{\\text{total}}(t) + C \\quad (\\kappa > 0)"
    },
    "variables": [
      {
        "symbol": "V_{\\text{total}}",
        "name": "total Lyapunov function",
        "description": "Augmented function combining inter-swarm, intra-swarm, and boundary errors",
        "constraints": [
          "positive definite"
        ],
        "tags": [
          "Lyapunov",
          "total"
        ]
      },
      {
        "symbol": "W_h^2",
        "name": "inter-swarm error",
        "description": "Squared Wasserstein distance between empirical measures \u03bc1 and \u03bc2",
        "constraints": [
          "non-negative"
        ],
        "tags": [
          "Wasserstein",
          "distribution"
        ]
      },
      {
        "symbol": "V_{\\text{Var}}",
        "name": "intra-swarm error",
        "description": "Sum of internal variances (position and velocity) for each swarm",
        "constraints": [
          "non-negative"
        ],
        "tags": [
          "variance",
          "dispersion"
        ]
      },
      {
        "symbol": "W_b",
        "name": "boundary term",
        "description": "Penalty for walkers near the domain boundary \u2202X_valid",
        "constraints": [
          "non-negative"
        ],
        "tags": [
          "boundary",
          "confinement"
        ]
      },
      {
        "symbol": "c_V",
        "name": "variance weight",
        "description": "Coupling constant balancing V_Var term",
        "constraints": [
          "positive"
        ],
        "tags": [
          "weight",
          "coupling"
        ]
      },
      {
        "symbol": "c_B",
        "name": "boundary weight",
        "description": "Coupling constant for W_b term",
        "constraints": [
          "positive"
        ],
        "tags": [
          "weight",
          "boundary"
        ]
      },
      {
        "symbol": "\\Psi_{\\text{clone}}",
        "name": "cloning operator",
        "description": "Stochastic operator reducing internal variance via fitness-based selection",
        "constraints": [],
        "tags": [
          "cloning",
          "stochastic"
        ]
      },
      {
        "symbol": "\\Psi_{\\text{kin}}",
        "name": "kinetic operator",
        "description": "Langevin dynamics with drift F(x) = -\u2207U(x) and diffusion",
        "constraints": [],
        "tags": [
          "kinetic",
          "Langevin"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The system involves two swarms with empirical measures \u03bc1 and \u03bc2 in phase space.",
        "confidence": 1.0
      },
      {
        "text": "Operators \u03a8_clone and \u03a8_kin are stochastic and applied alternately.",
        "confidence": 1.0
      },
      {
        "text": "The hypocoercive norm ||(\u03b4x, \u03b4v)||_h^2 captures both position and velocity errors.",
        "confidence": 0.9
      },
      {
        "text": "Cloning adds Gaussian velocity jitter N(0, \u03b4^2 I_d), and kinetic step includes diffusion \u03c3 dW.",
        "confidence": 1.0
      },
      {
        "text": "Constants c_V and c_B can be chosen to ensure contraction dominates expansion.",
        "confidence": 0.8
      }
    ],
    "local_refs": [],
    "proof": null,
    "tags": [
      "Lyapunov",
      "necessity",
      "swarm",
      "Wasserstein",
      "variance",
      "cloning",
      "kinetic",
      "convergence",
      "hypocoercive"
    ],
    "content_markdown": ":label: prop-lyapunov-necessity\n\nThe Lyapunov function $V_{\\text{total}} = W_h^2 + c_V V_{\\text{Var}} + c_B W_b$ with three distinct weighted components is mathematically necessary for the following reasons:\n\n**1. Complementary Information Content**\n\nThe two kinematic components measure fundamentally different aspects of swarm error:\n\n- **$W_h^2(\\mu_1, \\mu_2)$**: Measures how far apart the two swarms are **as distributions**. This is the squared Wasserstein distance between the full empirical measures $\\mu_1$ and $\\mu_2$. It quantifies the minimal transport cost to transform one swarm's distribution into the other's.\n\n- **$V_{\\text{Var}}(S_1, S_2)$**: Measures the **internal dispersion within each swarm**. This is the sum of the internal variances (positional and velocity) of each swarm's alive-walker population.\n\nThese quantities contain **non-redundant information**:\n- A system can have **small $W_h^2$ but large $V_{\\text{Var}}$**: Both swarms have similar empirical measures (so Wasserstein distance is small), but each swarm is internally highly dispersed (large variance).\n- A system can have **small $V_{\\text{Var}}$ but large $W_h^2$**: Both swarms are internally tight clusters (small variance), but the two tight clusters are far apart in phase space (large Wasserstein distance).\n\n**2. Operator-Specific Targeting**\n\nThe two stochastic operators act on fundamentally different error components:\n\n- **The Cloning Operator $\\Psi_{\\text{clone}}$**: Acts **within** each swarm independently. It selects walkers based on their fitness **relative to their own swarm's distribution**. The cloning mechanism directly targets $V_{\\text{Var}}$ by eliminating low-fitness walkers and duplicating high-fitness walkers, thereby reducing the internal spread of each swarm's distribution.\n\n- **The Kinetic Operator $\\Psi_{\\text{kin}}$**: Contains a drift term $F(x)$ (the negative gradient of a confining potential) that acts on walker positions. This drift causes walkers in both swarms to move toward regions of lower potential, thereby moving both swarms' barycenters toward the same equilibrium. This directly targets $W_h^2$ by reducing the distance between the swarms' centers of mass.\n\n**3. Synergistic Dissipation Necessity**\n\nNeither operator can contract the full hypocoercive norm $\\|\\!(\\delta x, \\delta v)\\!\\|_h^2 = \\|\\delta x\\|^2 + \\lambda_v \\|\\delta v\\|^2$ in both position and velocity simultaneously:\n\n- **Velocity Desynchronization from Cloning**: When the cloning operator duplicates a walker, it adds Gaussian jitter to the velocity: $v_{\\text{new}} = v_{\\text{parent}} + \\mathcal{N}(0, \\delta^2 I_d)$. This randomization **breaks velocity correlations** between swarms, causing the velocity component of the structural error to increase (expansion of the velocity-related parts of $W_h^2$). Additionally, the cloning mechanism creates a distribution of velocities within each swarm that may increase $V_{\\text{Var},v}$.\n\n- **Positional Diffusion from Kinetic Noise**: The Langevin equation for the kinetic step includes a diffusion term: $dx = (\\text{drift terms}) \\, dt + \\sigma \\, dW$. This stochastic noise **desynchronizes positions** between the two swarms' trajectories, causing positional components to expand. It also contributes to an increase in $V_{\\text{Var},x}$ within each swarm.\n\n**4. The Weighted Sum as a Solution**\n\nThe augmented Lyapunov function resolves this by allowing us to **balance expansions against contractions**:\n\n$$\n\\mathbb{E}[V_{\\text{total}}(t+1) - V_{\\text{total}}(t)] = \\underbrace{\\mathbb{E}[\\Delta W_h^2]}_{\\Psi_{\\text{clone}}: +, \\ \\Psi_{\\text{kin}}: -} + c_V \\underbrace{\\mathbb{E}[\\Delta V_{\\text{Var}}]}_{\\Psi_{\\text{clone}}: -, \\ \\Psi_{\\text{kin}}: +} + c_B \\underbrace{\\mathbb{E}[\\Delta W_b]}_{\\text{both: } -}\n$$\n\nBy choosing the coupling constant $c_V$ appropriately, we can ensure that:\n- The **strong contraction** of $V_{\\text{Var}}$ under $\\Psi_{\\text{clone}}$ (weighted by $c_V$) **dominates** the bounded expansion of $W_h^2$ under $\\Psi_{\\text{clone}}$.\n- The **strong contraction** of $W_h^2$ under $\\Psi_{\\text{kin}}$ **dominates** the bounded expansion of $c_V V_{\\text{Var}}$ under $\\Psi_{\\text{kin}}$.\n\nThis yields **net negative drift**: $\\mathbb{E}[V_{\\text{total}}(t+1) - V_{\\text{total}}(t)] \\leq -\\kappa V_{\\text{total}}(t) + C$ for some $\\kappa > 0$.\n\n**5. The Boundary Term $W_b$**\n\nThe term $c_B W_b$ ensures that walkers near the boundary $\\partial \\mathcal{X}_{\\text{valid}}$ are penalized. Both operators have mechanisms that contract this term:\n- **$\\Psi_{\\text{clone}}$**: Walkers near the boundary have lower survival probability and are thus eliminated and replaced by clones of interior walkers.\n- **$\\Psi_{\\text{kin}}$**: The confining potential $U(x)$ and force field $F(x) = -\\nabla U(x)$ (see {prf:ref}`axiom-lipschitz-fields`) push walkers away from the boundary.",
    "raw_directive": "918: The inclusion of both $W_h^2$ (inter-swarm error) and $V_{\\text{Var}}$ (intra-swarm error) in the Lyapunov function is not merely convenient but mathematically necessary. This subsection explains why the specific weighted-sum structure is required for proving convergence.\n919: \n920: :::{prf:proposition} Necessity of the Augmented Lyapunov Structure\n921: :label: prop-lyapunov-necessity\n922: \n923: The Lyapunov function $V_{\\text{total}} = W_h^2 + c_V V_{\\text{Var}} + c_B W_b$ with three distinct weighted components is mathematically necessary for the following reasons:\n924: \n925: **1. Complementary Information Content**\n926: \n927: The two kinematic components measure fundamentally different aspects of swarm error:\n928: \n929: - **$W_h^2(\\mu_1, \\mu_2)$**: Measures how far apart the two swarms are **as distributions**. This is the squared Wasserstein distance between the full empirical measures $\\mu_1$ and $\\mu_2$. It quantifies the minimal transport cost to transform one swarm's distribution into the other's.\n930: \n931: - **$V_{\\text{Var}}(S_1, S_2)$**: Measures the **internal dispersion within each swarm**. This is the sum of the internal variances (positional and velocity) of each swarm's alive-walker population.\n932: \n933: These quantities contain **non-redundant information**:\n934: - A system can have **small $W_h^2$ but large $V_{\\text{Var}}$**: Both swarms have similar empirical measures (so Wasserstein distance is small), but each swarm is internally highly dispersed (large variance).\n935: - A system can have **small $V_{\\text{Var}}$ but large $W_h^2$**: Both swarms are internally tight clusters (small variance), but the two tight clusters are far apart in phase space (large Wasserstein distance).\n936: \n937: **2. Operator-Specific Targeting**\n938: \n939: The two stochastic operators act on fundamentally different error components:\n940: \n941: - **The Cloning Operator $\\Psi_{\\text{clone}}$**: Acts **within** each swarm independently. It selects walkers based on their fitness **relative to their own swarm's distribution**. The cloning mechanism directly targets $V_{\\text{Var}}$ by eliminating low-fitness walkers and duplicating high-fitness walkers, thereby reducing the internal spread of each swarm's distribution.\n942: \n943: - **The Kinetic Operator $\\Psi_{\\text{kin}}$**: Contains a drift term $F(x)$ (the negative gradient of a confining potential) that acts on walker positions. This drift causes walkers in both swarms to move toward regions of lower potential, thereby moving both swarms' barycenters toward the same equilibrium. This directly targets $W_h^2$ by reducing the distance between the swarms' centers of mass.\n944: \n945: **3. Synergistic Dissipation Necessity**\n946: \n947: Neither operator can contract the full hypocoercive norm $\\|\\!(\\delta x, \\delta v)\\!\\|_h^2 = \\|\\delta x\\|^2 + \\lambda_v \\|\\delta v\\|^2$ in both position and velocity simultaneously:\n948: \n949: - **Velocity Desynchronization from Cloning**: When the cloning operator duplicates a walker, it adds Gaussian jitter to the velocity: $v_{\\text{new}} = v_{\\text{parent}} + \\mathcal{N}(0, \\delta^2 I_d)$. This randomization **breaks velocity correlations** between swarms, causing the velocity component of the structural error to increase (expansion of the velocity-related parts of $W_h^2$). Additionally, the cloning mechanism creates a distribution of velocities within each swarm that may increase $V_{\\text{Var},v}$.\n950: \n951: - **Positional Diffusion from Kinetic Noise**: The Langevin equation for the kinetic step includes a diffusion term: $dx = (\\text{drift terms}) \\, dt + \\sigma \\, dW$. This stochastic noise **desynchronizes positions** between the two swarms' trajectories, causing positional components to expand. It also contributes to an increase in $V_{\\text{Var},x}$ within each swarm.\n952: \n953: **4. The Weighted Sum as a Solution**\n954: \n955: The augmented Lyapunov function resolves this by allowing us to **balance expansions against contractions**:\n956: \n957: $$\n958: \\mathbb{E}[V_{\\text{total}}(t+1) - V_{\\text{total}}(t)] = \\underbrace{\\mathbb{E}[\\Delta W_h^2]}_{\\Psi_{\\text{clone}}: +, \\ \\Psi_{\\text{kin}}: -} + c_V \\underbrace{\\mathbb{E}[\\Delta V_{\\text{Var}}]}_{\\Psi_{\\text{clone}}: -, \\ \\Psi_{\\text{kin}}: +} + c_B \\underbrace{\\mathbb{E}[\\Delta W_b]}_{\\text{both: } -}\n959: $$\n960: \n961: By choosing the coupling constant $c_V$ appropriately, we can ensure that:\n962: - The **strong contraction** of $V_{\\text{Var}}$ under $\\Psi_{\\text{clone}}$ (weighted by $c_V$) **dominates** the bounded expansion of $W_h^2$ under $\\Psi_{\\text{clone}}$.\n963: - The **strong contraction** of $W_h^2$ under $\\Psi_{\\text{kin}}$ **dominates** the bounded expansion of $c_V V_{\\text{Var}}$ under $\\Psi_{\\text{kin}}$.\n964: \n965: This yields **net negative drift**: $\\mathbb{E}[V_{\\text{total}}(t+1) - V_{\\text{total}}(t)] \\leq -\\kappa V_{\\text{total}}(t) + C$ for some $\\kappa > 0$.\n966: \n967: **5. The Boundary Term $W_b$**\n968: \n969: The term $c_B W_b$ ensures that walkers near the boundary $\\partial \\mathcal{X}_{\\text{valid}}$ are penalized. Both operators have mechanisms that contract this term:\n970: - **$\\Psi_{\\text{clone}}$**: Walkers near the boundary have lower survival probability and are thus eliminated and replaced by clones of interior walkers.\n971: - **$\\Psi_{\\text{kin}}$**: The confining potential $U(x)$ and force field $F(x) = -\\nabla U(x)$ (see {prf:ref}`axiom-lipschitz-fields`) push walkers away from the boundary.\n972: ",
    "document_id": "03_cloning",
    "section": "## 3. The Augmented Hypocoercive Lyapunov Function",
    "span": {
      "start_line": 918,
      "end_line": 972,
      "content_start": 921,
      "content_end": 971,
      "header_lines": [
        919
      ]
    },
    "references": [
      "axiom-lipschitz-fields"
    ],
    "metadata": {
      "label": "prop-lyapunov-necessity"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 3,
      "chapter_file": "chapter_3.json",
      "section_id": "## 3. The Augmented Hypocoercive Lyapunov Function"
    },
    "generated_at": "2025-11-10T13:26:08.168931+00:00",
    "alt_labels": []
  },
  {
    "label": "prop-bounded-velocity-expansion",
    "title": "Bounded Velocity Variance Expansion from Cloning",
    "type": "proposition",
    "nl_statement": "For any cloning event where a fraction f_clone of walkers are cloned with restitution coefficient \u03b1_restitution, the change in internal velocity variance from velocity resets is bounded by \u0394V_{Var,v} \u2264 f_clone \u00b7 C_reset \u00b7 V_{max,KE}.",
    "equations": [
      {
        "label": null,
        "latex": "\\Delta V_{Var,v} \\leq f_{\\text{clone}} \\cdot C_{\\text{reset}} \\cdot V_{\\max,\\text{KE}}"
      }
    ],
    "hypotheses": [
      {
        "text": "Any cloning event where a fraction f_clone of walkers are cloned with restitution coefficient \u03b1_restitution",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "The change in internal velocity variance from the velocity resets is bounded",
      "latex": "\\Delta V_{Var,v} \\leq f_{\\text{clone}} \\cdot C_{\\text{reset}} \\cdot V_{\\max,\\text{KE}}"
    },
    "variables": [
      {
        "symbol": "f_{\\text{clone}}",
        "name": "f_clone",
        "description": "Fraction of walkers that are cloned in the event",
        "constraints": [
          "0 \\leq f_{\\text{clone}} \\leq 1"
        ],
        "tags": [
          "cloning",
          "fraction"
        ]
      },
      {
        "symbol": "\\alpha_{\\text{restitution}}",
        "name": "alpha_restitution",
        "description": "Restitution coefficient used in cloning",
        "constraints": [
          "0 \\leq \\alpha_{\\text{restitution}} \\leq 1"
        ],
        "tags": [
          "restitution",
          "coefficient"
        ]
      },
      {
        "symbol": "\\Delta V_{Var,v}",
        "name": "Delta_V_Var_v",
        "description": "Change in internal velocity variance due to resets",
        "constraints": [],
        "tags": [
          "variance",
          "change"
        ]
      },
      {
        "symbol": "C_{\\text{reset}}",
        "name": "C_reset",
        "description": "Constant bounding the reset effect",
        "constraints": [
          "C_{\\text{reset}} > 0"
        ],
        "tags": [
          "constant",
          "reset"
        ]
      },
      {
        "symbol": "V_{\\max,\\text{KE}}",
        "name": "V_max_KE",
        "description": "Maximum velocity related to kinetic energy",
        "constraints": [
          "V_{\\max,\\text{KE}} \\geq 0"
        ],
        "tags": [
          "velocity",
          "kinetic-energy"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Cloning events involve velocity resets that affect variance",
        "confidence": 0.95
      },
      {
        "text": "C_reset is a fixed positive constant independent of the event",
        "confidence": 1.0
      },
      {
        "text": "V_max,KE represents a uniform upper bound on velocities",
        "confidence": 0.9
      },
      {
        "text": "Walkers have velocities prior to cloning",
        "confidence": 0.85
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-prop-bounded-velocity-expansion",
      "title": null,
      "type": "proof",
      "proves": "prop-bounded-velocity-expansion",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":::{prf:proof}\n:label: proof-prop-bounded-velocity-expansion\n**Proof:**\n\nWe will prove that the one-step change in the velocity variance component $V_{Var,v}$ due to cloning is bounded by a state-independent constant. The proof proceeds in four parts: (1) establish the domain of possible velocities, (2) bound the per-walker variance change from velocity reset, (3) bound the total variance change across all cloned walkers, and (4) verify that all bounds are state-independent through the velocity regularization mechanism.\n\n**Part 1: The Velocity Domain and Its Diameter**\n\nBy the compactness of the valid position domain $\\mathcal{X}_{\\text{valid}}$ and the Lipschitz continuity of the drift field (see {prf:ref}`axiom-lipschitz-fields`), the velocity domain is implicitly bounded. Specifically:\n\n1. The kinetic operator includes a friction term $-\\gamma v$ and a bounded drift field $F(x)$ with $\\|F(x)\\| \\leq F_{\\max}$ for all $x \\in \\mathcal{X}_{\\text{valid}}$.\n\n2. The velocity regularization term in {prf:ref}`axiom-velocity-regularization` ensures walkers with $\\|v\\|^2 > V_{\\text{thresh}}^2$ have extremely low fitness and are preferentially cloned, where $V_{\\text{thresh}}$ is determined by the balance between the positional reward scale and the regularization coefficient $c_{v\\_reg}$.\n\n3. These mechanisms ensure that in any viable swarm state (where extinction probability is negligible), all walker velocities satisfy $\\|v_i\\| \\leq V_{\\max}$ for a finite constant:\n\n$$\nV_{\\max}^2 := \\max\\left\\{ \\frac{2F_{\\max}}{\\gamma}, V_{\\text{thresh}}^2 \\right\\}\n$$\n\nThis bound is **state-independent**, depending only on the domain geometry ($F_{\\max}$), algorithmic parameters ($\\gamma$, $c_{v\\_reg}$), and the reward scale.\n\n**Part 2: Bounding the Per-Walker Variance Change**\n\nConsider a single walker $i$ that is cloned at step $t$. Let $v_i^{\\text{old}}$ be its velocity before cloning and $v_i^{\\text{new}}$ be its velocity after the inelastic collision reset. Let $\\mu_v^{\\text{old}}$ and $\\mu_v^{\\text{new}}$ be the velocity barycentres before and after cloning.\n\nThe contribution of walker $i$ to the velocity variance changes as:\n\n$$\n\\Delta_i := \\|v_i^{\\text{new}} - \\mu_v^{\\text{new}}\\|^2 - \\|v_i^{\\text{old}} - \\mu_v^{\\text{old}}\\|^2\n$$\n\nWe bound this change using the triangle inequality and the velocity domain bounds. First, note that:\n\n$$\n\\|v_i^{\\text{new}} - \\mu_v^{\\text{new}}\\|^2 \\leq 2\\|v_i^{\\text{new}}\\|^2 + 2\\|\\mu_v^{\\text{new}}\\|^2 \\leq 2V_{\\max}^2 + 2V_{\\max}^2 = 4V_{\\max}^2\n$$\n\nSimilarly, $\\|v_i^{\\text{old}} - \\mu_v^{\\text{old}}\\|^2 \\geq 0$. Therefore:\n\n$$\n\\Delta_i \\leq 4V_{\\max}^2\n$$\n\nHowever, this is a worst-case bound. We can obtain a tighter bound by analyzing the inelastic collision mechanism directly.\n\n**Step 2a: The Inelastic Collision Model**\n\nWhen walker $i$ is cloned, it participates in an inelastic collision with $M$ companion walkers. Let $v_i^{\\text{old}}$ and $\\{v_j^{\\text{comp}}\\}_{j=1}^M$ be the velocities of the participants. The center-of-mass velocity is:\n\n$$\nV_{\\text{COM}} = \\frac{1}{M+1}\\left(v_i^{\\text{old}} + \\sum_{j=1}^M v_j^{\\text{comp}}\\right)\n$$\n\nThe new velocity is computed via:\n\n$$\nv_i^{\\text{new}} = V_{\\text{COM}} + \\alpha_{\\text{restitution}} \\cdot R(u_i)\n$$\n\nwhere $u_i = v_i^{\\text{old}} - V_{\\text{COM}}$ is the old relative velocity and $R$ is a random rotation. The magnitude change is bounded by:\n\n$$\n\\|v_i^{\\text{new}} - v_i^{\\text{old}}\\|^2 \\leq \\|v_i^{\\text{new}} - V_{\\text{COM}}\\|^2 + \\|V_{\\text{COM}} - v_i^{\\text{old}}\\|^2\n$$\n\nSince $\\|v_i^{\\text{new}} - V_{\\text{COM}}\\| = \\alpha_{\\text{restitution}} \\|u_i\\|$ and $\\|V_{\\text{COM}} - v_i^{\\text{old}}\\| = \\|u_i\\|$:\n\n$$\n\\|v_i^{\\text{new}} - v_i^{\\text{old}}\\|^2 \\leq (\\alpha_{\\text{restitution}}^2 + 1) \\|u_i\\|^2\n$$\n\nThe relative velocity magnitude is bounded by:\n\n$$\n\\|u_i\\| = \\|v_i^{\\text{old}} - V_{\\text{COM}}\\| \\leq \\|v_i^{\\text{old}}\\| + \\|V_{\\text{COM}}\\| \\leq V_{\\max} + V_{\\max} = 2V_{\\max}\n$$\n\nTherefore:\n\n$$\n\\|v_i^{\\text{new}} - v_i^{\\text{old}}\\|^2 \\leq 4(\\alpha_{\\text{restitution}}^2 + 1) V_{\\max}^2\n$$\n\n**Part 3: Total Variance Change from All Cloned Walkers**\n\nThe velocity variance component of the Lyapunov function is defined (with $N$-normalization) as:\n\n$$\nV_{Var,v}(S_k) = \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|v_i - \\mu_v\\|^2\n$$\n\nWhen a cloning event occurs, let $\\mathcal{C} \\subset \\mathcal{A}(S_k)$ be the set of walkers that are cloned, with $|\\mathcal{C}| = n_{\\text{clone}}$. The change in $V_{Var,v}$ can be decomposed into three contributions:\n\n1. **Direct variance change from velocity resets** (cloned walkers)\n2. **Barycentre shift effect** (changes $\\mu_v$, affecting all walkers)\n3. **Status changes** (deaths and revivals)\n\nWe bound each contribution separately.\n\n**Contribution 1 (Direct Reset):** For each cloned walker $i \\in \\mathcal{C}$, the velocity changes from $v_i^{\\text{old}}$ to $v_i^{\\text{new}}$. Using the squared-norm expansion:\n\n$$\n\\begin{aligned}\n&\\|v_i^{\\text{new}} - \\mu_v^{\\text{new}}\\|^2 - \\|v_i^{\\text{old}} - \\mu_v^{\\text{old}}\\|^2 \\\\\n&= \\|v_i^{\\text{new}}\\|^2 - 2\\langle v_i^{\\text{new}}, \\mu_v^{\\text{new}}\\rangle + \\|\\mu_v^{\\text{new}}\\|^2 - \\|v_i^{\\text{old}}\\|^2 + 2\\langle v_i^{\\text{old}}, \\mu_v^{\\text{old}}\\rangle - \\|\\mu_v^{\\text{old}}\\|^2\n\\end{aligned}\n$$\n\nThis can be bounded using the fact that $\\|v_i^{\\text{new}} - v_i^{\\text{old}}\\|^2 \\leq 4(\\alpha_{\\text{restitution}}^2 + 1) V_{\\max}^2$ and $\\|\\mu_v^{\\text{new}} - \\mu_v^{\\text{old}}\\|^2$ is also bounded by a similar expression (since the barycentre is an average of velocities, all bounded by $V_{\\max}$).\n\nThrough careful algebraic expansion (using $\\|a - b\\|^2 = \\|a\\|^2 - 2\\langle a, b\\rangle + \\|b\\|^2$) and the triangle inequality:\n\n$$\n\\left|\\|v_i^{\\text{new}} - \\mu_v^{\\text{new}}\\|^2 - \\|v_i^{\\text{old}} - \\mu_v^{\\text{old}}\\|^2\\right| \\leq 8(\\alpha_{\\text{restitution}}^2 + 1) V_{\\max}^2 + 8V_{\\max}^2 = 8(\\alpha_{\\text{restitution}}^2 + 2) V_{\\max}^2\n$$\n\n**Contribution 2 (Barycentre Shift):** The barycentre shift affects all $k_{\\text{alive}}$ walkers. The magnitude of the shift is bounded by:\n\n$$\n\\|\\mu_v^{\\text{new}} - \\mu_v^{\\text{old}}\\| \\leq \\frac{n_{\\text{clone}}}{k_{\\text{alive}}} \\cdot 2V_{\\max}\n$$\n\nThe contribution to variance change from barycentre shift across all walkers is bounded by:\n\n$$\n\\left|\\frac{1}{N}\\sum_{i \\in \\mathcal{A}} \\left(\\|v_i - \\mu_v^{\\text{new}}\\|^2 - \\|v_i - \\mu_v^{\\text{old}}\\|^2\\right)\\right| \\leq \\frac{k_{\\text{alive}}}{N} \\cdot 4V_{\\max} \\cdot \\|\\mu_v^{\\text{new}} - \\mu_v^{\\text{old}}\\| \\leq \\frac{8n_{\\text{clone}}V_{\\max}^2}{N}\n$$\n\n**Contribution 3 (Status Changes):** Dead walkers contribute zero to the sum. When a walker revives, it adds a term $\\frac{1}{N}\\|v_i - \\mu_v\\|^2 \\leq \\frac{4V_{\\max}^2}{N}$. The number of revivals equals the number of deaths, which is at most $n_{\\text{clone}}$.\n\n**Total Bound:** Combining all contributions:\n\n$$\n\\begin{aligned}\n|\\Delta V_{Var,v}| &\\leq \\frac{n_{\\text{clone}}}{N} \\cdot 8(\\alpha_{\\text{restitution}}^2 + 2) V_{\\max}^2 + \\frac{8n_{\\text{clone}}V_{\\max}^2}{N} + \\frac{4n_{\\text{clone}}V_{\\max}^2}{N} \\\\\n&= \\frac{n_{\\text{clone}}}{N} \\cdot \\left[8(\\alpha_{\\text{restitution}}^2 + 2) + 8 + 4\\right] V_{\\max}^2 \\\\\n&= \\frac{n_{\\text{clone}}}{N} \\cdot 8(\\alpha_{\\text{restitution}}^2 + 4) V_{\\max}^2\n\\end{aligned}\n$$\n\nSince $n_{\\text{clone}} = f_{\\text{clone}} \\cdot N$ by definition:\n\n$$\n|\\Delta V_{Var,v}| \\leq f_{\\text{clone}} \\cdot 8(\\alpha_{\\text{restitution}}^2 + 4) V_{\\max}^2\n$$\n\n**Part 4: State-Independence of the Bound**\n\nThe bound depends only on:\n- $f_{\\text{clone}}$: the cloning fraction (algorithmic parameter)\n- $\\alpha_{\\text{restitution}}$: the restitution coefficient (algorithmic parameter)\n- $V_{\\max}^2$: the velocity domain bound\n\nThe critical claim is that $V_{\\max}$ is state-independent. This is guaranteed by {prf:ref}`axiom-velocity-regularization`. Any walker with $\\|v_i\\|^2 \\gg V_{\\text{thresh}}^2$ has reward:\n\n$$\nR(x_i, v_i) = R_{\\text{pos}}(x_i) - c_{v\\_reg} \\|v_i\\|^2 \\ll R_{\\text{pos}}(x_i) - c_{v\\_reg} V_{\\text{thresh}}^2\n$$\n\nmaking it extremely unfit and a prime target for cloning. This feedback mechanism prevents velocity runaway, ensuring $V_{\\max}$ remains a true constant.\n\n**Conclusion:** Setting:\n\n$$\nC_{\\text{reset}} := 8(\\alpha_{\\text{restitution}}^2 + 4), \\quad V_{\\max,\\text{KE}} := V_{\\max}^2\n$$\n\nwe have proven:\n\n$$\n\\Delta V_{Var,v} \\leq f_{\\text{clone}} \\cdot C_{\\text{reset}} \\cdot V_{\\max,\\text{KE}}\n$$\n\nwhere both $C_{\\text{reset}}$ and $V_{\\max,\\text{KE}}$ are state-independent constants depending only on algorithmic parameters and domain geometry.",
      "raw_directive": "2096: :::\n2097: \n2098: :::{prf:proof}\n2099: :label: proof-prop-bounded-velocity-expansion\n2100: **Proof:**\n2101: \n2102: We will prove that the one-step change in the velocity variance component $V_{Var,v}$ due to cloning is bounded by a state-independent constant. The proof proceeds in four parts: (1) establish the domain of possible velocities, (2) bound the per-walker variance change from velocity reset, (3) bound the total variance change across all cloned walkers, and (4) verify that all bounds are state-independent through the velocity regularization mechanism.\n2103: \n2104: **Part 1: The Velocity Domain and Its Diameter**\n2105: \n2106: By the compactness of the valid position domain $\\mathcal{X}_{\\text{valid}}$ and the Lipschitz continuity of the drift field (see {prf:ref}`axiom-lipschitz-fields`), the velocity domain is implicitly bounded. Specifically:\n2107: \n2108: 1. The kinetic operator includes a friction term $-\\gamma v$ and a bounded drift field $F(x)$ with $\\|F(x)\\| \\leq F_{\\max}$ for all $x \\in \\mathcal{X}_{\\text{valid}}$.\n2109: \n2110: 2. The velocity regularization term in {prf:ref}`axiom-velocity-regularization` ensures walkers with $\\|v\\|^2 > V_{\\text{thresh}}^2$ have extremely low fitness and are preferentially cloned, where $V_{\\text{thresh}}$ is determined by the balance between the positional reward scale and the regularization coefficient $c_{v\\_reg}$.\n2111: \n2112: 3. These mechanisms ensure that in any viable swarm state (where extinction probability is negligible), all walker velocities satisfy $\\|v_i\\| \\leq V_{\\max}$ for a finite constant:\n2113: \n2114: $$\n2115: V_{\\max}^2 := \\max\\left\\{ \\frac{2F_{\\max}}{\\gamma}, V_{\\text{thresh}}^2 \\right\\}\n2116: $$\n2117: \n2118: This bound is **state-independent**, depending only on the domain geometry ($F_{\\max}$), algorithmic parameters ($\\gamma$, $c_{v\\_reg}$), and the reward scale.\n2119: \n2120: **Part 2: Bounding the Per-Walker Variance Change**\n2121: \n2122: Consider a single walker $i$ that is cloned at step $t$. Let $v_i^{\\text{old}}$ be its velocity before cloning and $v_i^{\\text{new}}$ be its velocity after the inelastic collision reset. Let $\\mu_v^{\\text{old}}$ and $\\mu_v^{\\text{new}}$ be the velocity barycentres before and after cloning.\n2123: \n2124: The contribution of walker $i$ to the velocity variance changes as:\n2125: \n2126: $$\n2127: \\Delta_i := \\|v_i^{\\text{new}} - \\mu_v^{\\text{new}}\\|^2 - \\|v_i^{\\text{old}} - \\mu_v^{\\text{old}}\\|^2\n2128: $$\n2129: \n2130: We bound this change using the triangle inequality and the velocity domain bounds. First, note that:\n2131: \n2132: $$\n2133: \\|v_i^{\\text{new}} - \\mu_v^{\\text{new}}\\|^2 \\leq 2\\|v_i^{\\text{new}}\\|^2 + 2\\|\\mu_v^{\\text{new}}\\|^2 \\leq 2V_{\\max}^2 + 2V_{\\max}^2 = 4V_{\\max}^2\n2134: $$\n2135: \n2136: Similarly, $\\|v_i^{\\text{old}} - \\mu_v^{\\text{old}}\\|^2 \\geq 0$. Therefore:\n2137: \n2138: $$\n2139: \\Delta_i \\leq 4V_{\\max}^2\n2140: $$\n2141: \n2142: However, this is a worst-case bound. We can obtain a tighter bound by analyzing the inelastic collision mechanism directly.\n2143: \n2144: **Step 2a: The Inelastic Collision Model**\n2145: \n2146: When walker $i$ is cloned, it participates in an inelastic collision with $M$ companion walkers. Let $v_i^{\\text{old}}$ and $\\{v_j^{\\text{comp}}\\}_{j=1}^M$ be the velocities of the participants. The center-of-mass velocity is:\n2147: \n2148: $$\n2149: V_{\\text{COM}} = \\frac{1}{M+1}\\left(v_i^{\\text{old}} + \\sum_{j=1}^M v_j^{\\text{comp}}\\right)\n2150: $$\n2151: \n2152: The new velocity is computed via:\n2153: \n2154: $$\n2155: v_i^{\\text{new}} = V_{\\text{COM}} + \\alpha_{\\text{restitution}} \\cdot R(u_i)\n2156: $$\n2157: \n2158: where $u_i = v_i^{\\text{old}} - V_{\\text{COM}}$ is the old relative velocity and $R$ is a random rotation. The magnitude change is bounded by:\n2159: \n2160: $$\n2161: \\|v_i^{\\text{new}} - v_i^{\\text{old}}\\|^2 \\leq \\|v_i^{\\text{new}} - V_{\\text{COM}}\\|^2 + \\|V_{\\text{COM}} - v_i^{\\text{old}}\\|^2\n2162: $$\n2163: \n2164: Since $\\|v_i^{\\text{new}} - V_{\\text{COM}}\\| = \\alpha_{\\text{restitution}} \\|u_i\\|$ and $\\|V_{\\text{COM}} - v_i^{\\text{old}}\\| = \\|u_i\\|$:\n2165: \n2166: $$\n2167: \\|v_i^{\\text{new}} - v_i^{\\text{old}}\\|^2 \\leq (\\alpha_{\\text{restitution}}^2 + 1) \\|u_i\\|^2\n2168: $$\n2169: \n2170: The relative velocity magnitude is bounded by:\n2171: \n2172: $$\n2173: \\|u_i\\| = \\|v_i^{\\text{old}} - V_{\\text{COM}}\\| \\leq \\|v_i^{\\text{old}}\\| + \\|V_{\\text{COM}}\\| \\leq V_{\\max} + V_{\\max} = 2V_{\\max}\n2174: $$\n2175: \n2176: Therefore:\n2177: \n2178: $$\n2179: \\|v_i^{\\text{new}} - v_i^{\\text{old}}\\|^2 \\leq 4(\\alpha_{\\text{restitution}}^2 + 1) V_{\\max}^2\n2180: $$\n2181: \n2182: **Part 3: Total Variance Change from All Cloned Walkers**\n2183: \n2184: The velocity variance component of the Lyapunov function is defined (with $N$-normalization) as:\n2185: \n2186: $$\n2187: V_{Var,v}(S_k) = \\frac{1}{N} \\sum_{i \\in \\mathcal{A}(S_k)} \\|v_i - \\mu_v\\|^2\n2188: $$\n2189: \n2190: When a cloning event occurs, let $\\mathcal{C} \\subset \\mathcal{A}(S_k)$ be the set of walkers that are cloned, with $|\\mathcal{C}| = n_{\\text{clone}}$. The change in $V_{Var,v}$ can be decomposed into three contributions:\n2191: \n2192: 1. **Direct variance change from velocity resets** (cloned walkers)\n2193: 2. **Barycentre shift effect** (changes $\\mu_v$, affecting all walkers)\n2194: 3. **Status changes** (deaths and revivals)\n2195: \n2196: We bound each contribution separately.\n2197: \n2198: **Contribution 1 (Direct Reset):** For each cloned walker $i \\in \\mathcal{C}$, the velocity changes from $v_i^{\\text{old}}$ to $v_i^{\\text{new}}$. Using the squared-norm expansion:\n2199: \n2200: $$\n2201: \\begin{aligned}\n2202: &\\|v_i^{\\text{new}} - \\mu_v^{\\text{new}}\\|^2 - \\|v_i^{\\text{old}} - \\mu_v^{\\text{old}}\\|^2 \\\\\n2203: &= \\|v_i^{\\text{new}}\\|^2 - 2\\langle v_i^{\\text{new}}, \\mu_v^{\\text{new}}\\rangle + \\|\\mu_v^{\\text{new}}\\|^2 - \\|v_i^{\\text{old}}\\|^2 + 2\\langle v_i^{\\text{old}}, \\mu_v^{\\text{old}}\\rangle - \\|\\mu_v^{\\text{old}}\\|^2\n2204: \\end{aligned}\n2205: $$\n2206: \n2207: This can be bounded using the fact that $\\|v_i^{\\text{new}} - v_i^{\\text{old}}\\|^2 \\leq 4(\\alpha_{\\text{restitution}}^2 + 1) V_{\\max}^2$ and $\\|\\mu_v^{\\text{new}} - \\mu_v^{\\text{old}}\\|^2$ is also bounded by a similar expression (since the barycentre is an average of velocities, all bounded by $V_{\\max}$).\n2208: \n2209: Through careful algebraic expansion (using $\\|a - b\\|^2 = \\|a\\|^2 - 2\\langle a, b\\rangle + \\|b\\|^2$) and the triangle inequality:\n2210: \n2211: $$\n2212: \\left|\\|v_i^{\\text{new}} - \\mu_v^{\\text{new}}\\|^2 - \\|v_i^{\\text{old}} - \\mu_v^{\\text{old}}\\|^2\\right| \\leq 8(\\alpha_{\\text{restitution}}^2 + 1) V_{\\max}^2 + 8V_{\\max}^2 = 8(\\alpha_{\\text{restitution}}^2 + 2) V_{\\max}^2\n2213: $$\n2214: \n2215: **Contribution 2 (Barycentre Shift):** The barycentre shift affects all $k_{\\text{alive}}$ walkers. The magnitude of the shift is bounded by:\n2216: \n2217: $$\n2218: \\|\\mu_v^{\\text{new}} - \\mu_v^{\\text{old}}\\| \\leq \\frac{n_{\\text{clone}}}{k_{\\text{alive}}} \\cdot 2V_{\\max}\n2219: $$\n2220: \n2221: The contribution to variance change from barycentre shift across all walkers is bounded by:\n2222: \n2223: $$\n2224: \\left|\\frac{1}{N}\\sum_{i \\in \\mathcal{A}} \\left(\\|v_i - \\mu_v^{\\text{new}}\\|^2 - \\|v_i - \\mu_v^{\\text{old}}\\|^2\\right)\\right| \\leq \\frac{k_{\\text{alive}}}{N} \\cdot 4V_{\\max} \\cdot \\|\\mu_v^{\\text{new}} - \\mu_v^{\\text{old}}\\| \\leq \\frac{8n_{\\text{clone}}V_{\\max}^2}{N}\n2225: $$\n2226: \n2227: **Contribution 3 (Status Changes):** Dead walkers contribute zero to the sum. When a walker revives, it adds a term $\\frac{1}{N}\\|v_i - \\mu_v\\|^2 \\leq \\frac{4V_{\\max}^2}{N}$. The number of revivals equals the number of deaths, which is at most $n_{\\text{clone}}$.\n2228: \n2229: **Total Bound:** Combining all contributions:\n2230: \n2231: $$\n2232: \\begin{aligned}\n2233: |\\Delta V_{Var,v}| &\\leq \\frac{n_{\\text{clone}}}{N} \\cdot 8(\\alpha_{\\text{restitution}}^2 + 2) V_{\\max}^2 + \\frac{8n_{\\text{clone}}V_{\\max}^2}{N} + \\frac{4n_{\\text{clone}}V_{\\max}^2}{N} \\\\\n2234: &= \\frac{n_{\\text{clone}}}{N} \\cdot \\left[8(\\alpha_{\\text{restitution}}^2 + 2) + 8 + 4\\right] V_{\\max}^2 \\\\\n2235: &= \\frac{n_{\\text{clone}}}{N} \\cdot 8(\\alpha_{\\text{restitution}}^2 + 4) V_{\\max}^2\n2236: \\end{aligned}\n2237: $$\n2238: \n2239: Since $n_{\\text{clone}} = f_{\\text{clone}} \\cdot N$ by definition:\n2240: \n2241: $$\n2242: |\\Delta V_{Var,v}| \\leq f_{\\text{clone}} \\cdot 8(\\alpha_{\\text{restitution}}^2 + 4) V_{\\max}^2\n2243: $$\n2244: \n2245: **Part 4: State-Independence of the Bound**\n2246: \n2247: The bound depends only on:\n2248: - $f_{\\text{clone}}$: the cloning fraction (algorithmic parameter)\n2249: - $\\alpha_{\\text{restitution}}$: the restitution coefficient (algorithmic parameter)\n2250: - $V_{\\max}^2$: the velocity domain bound\n2251: \n2252: The critical claim is that $V_{\\max}$ is state-independent. This is guaranteed by {prf:ref}`axiom-velocity-regularization`. Any walker with $\\|v_i\\|^2 \\gg V_{\\text{thresh}}^2$ has reward:\n2253: \n2254: $$\n2255: R(x_i, v_i) = R_{\\text{pos}}(x_i) - c_{v\\_reg} \\|v_i\\|^2 \\ll R_{\\text{pos}}(x_i) - c_{v\\_reg} V_{\\text{thresh}}^2\n2256: $$\n2257: \n2258: making it extremely unfit and a prime target for cloning. This feedback mechanism prevents velocity runaway, ensuring $V_{\\max}$ remains a true constant.\n2259: \n2260: **Conclusion:** Setting:\n2261: \n2262: $$\n2263: C_{\\text{reset}} := 8(\\alpha_{\\text{restitution}}^2 + 4), \\quad V_{\\max,\\text{KE}} := V_{\\max}^2\n2264: $$\n2265: \n2266: we have proven:\n2267: \n2268: $$\n2269: \\Delta V_{Var,v} \\leq f_{\\text{clone}} \\cdot C_{\\text{reset}} \\cdot V_{\\max,\\text{KE}}\n2270: $$\n2271: \n2272: where both $C_{\\text{reset}}$ and $V_{\\max,\\text{KE}}$ are state-independent constants depending only on algorithmic parameters and domain geometry.\n2273: ",
      "strategy_summary": "The proof establishes a bounded velocity domain using axioms and regularization, bounds per-walker variance changes via inelastic collision analysis and inequalities, decomposes the total variance change into direct resets, barycenter shifts, and status changes, and confirms the overall bound is state-independent relying on algorithmic parameters and domain properties.",
      "conclusion": {
        "text": "The one-step change in the velocity variance component \u0394 V_{Var,v} due to cloning is bounded by \u0394 V_{Var,v} \u2264 f_{clone} \u00b7 C_{reset} \u00b7 V_{max,KE}, where C_{reset} and V_{max,KE} are state-independent constants.",
        "latex": "\\Delta V_{\\text{Var},v} \\leq f_{\\text{clone}} \\cdot C_{\\text{reset}} \\cdot V_{\\max,\\text{KE}}"
      },
      "assumptions": [
        {
          "text": "Compactness of the valid position domain \\mathcal{X}_{valid}.",
          "latex": "\\text{Compactness of } \\mathcal{X}_{\\text{valid}}"
        },
        {
          "text": "Lipschitz continuity of the drift field F(x) with ||F(x)|| \u2264 F_{max}.",
          "latex": "\\|F(x)\\| \\leq F_{\\max} \\text{ for all } x \\in \\mathcal{X}_{\\text{valid}}"
        },
        {
          "text": "Presence of friction term -\u03b3 v in the kinetic operator.",
          "latex": "-\\gamma v"
        },
        {
          "text": "Velocity regularization via Axiom EG-4 with threshold V_{thresh} and coefficient c_{v_reg}.",
          "latex": "R(x_i, v_i) = R_{\\text{pos}}(x_i) - c_{v_{\\text{reg}}} \\|v_i\\|^2"
        },
        {
          "text": "Cloning fraction f_{clone} and restitution coefficient \u03b1_{restitution} are fixed parameters.",
          "latex": null
        },
        {
          "text": "Number of companion walkers M in cloning is fixed.",
          "latex": null
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "part",
          "text": "Establish the domain of possible velocities using compactness, Lipschitz drift, friction, and regularization to bound ||v_i|| \u2264 V_{max}.",
          "latex": "\\|v_i\\| \\leq V_{\\max}, \\quad V_{\\max}^2 := \\max\\left\\{ \\frac{2F_{\\max}}{\\gamma}, V_{\\text{thresh}}^2 \\right\\}",
          "references": [
            "axiom-eg-4",
            "axiom-lipschitz-drift"
          ],
          "derived_statement": "Velocities are state-independently bounded by V_{max}."
        },
        {
          "order": 2.0,
          "kind": "subpart",
          "text": "For a single cloned walker, bound the variance change \u0394_i using triangle inequality.",
          "latex": "\\Delta_i \\leq 4V_{\\max}^2",
          "references": [],
          "derived_statement": "Per-walker variance change is bounded."
        },
        {
          "order": 2.1,
          "kind": "substep",
          "text": "Model the inelastic collision and bound the velocity change ||v_i^{new} - v_i^{old}||^2.",
          "latex": "\\|v_i^{\\text{new}} - v_i^{\\text{old}}\\|^2 \\leq (\\alpha_{\\text{restitution}}^2 + 1) \\|u_i\\|^2 \\leq 4(\\alpha_{\\text{restitution}}^2 + 1) V_{\\max}^2",
          "references": [],
          "derived_statement": "Tighter bound on velocity perturbation from collision."
        },
        {
          "order": 3.0,
          "kind": "part",
          "text": "Decompose total \u0394 V_{Var,v} into direct reset, barycenter shift, and status changes.",
          "latex": null,
          "references": [
            "def-lyapunov-vvarv"
          ],
          "derived_statement": "Variance change decomposed into three bounded contributions."
        },
        {
          "order": 3.1,
          "kind": "substep",
          "text": "Bound direct reset contribution for cloned walkers.",
          "latex": "\\left| \\|v_i^{\\text{new}} - \\mu_v^{\\text{new}}\\|^2 - \\|v_i^{\\text{old}} - \\mu_v^{\\text{old}}\\|^2 \\right| \\leq 8(\\alpha_{\\text{restitution}}^2 + 2) V_{\\max}^2",
          "references": [],
          "derived_statement": "Direct reset bounded by O(V_{max}^2)."
        },
        {
          "order": 3.2,
          "kind": "substep",
          "text": "Bound barycenter shift effect on all walkers.",
          "latex": "\\|\\mu_v^{\\text{new}} - \\mu_v^{\\text{old}}\\| \\leq \\frac{n_{\\text{clone}}}{k_{\\text{alive}}} \\cdot 2V_{\\max}, \\quad \\text{contribution} \\leq \\frac{8n_{\\text{clone}}V_{\\max}^2}{N}",
          "references": [],
          "derived_statement": "Shift effect bounded by fraction of clones times V_{max}^2."
        },
        {
          "order": 3.3,
          "kind": "substep",
          "text": "Bound status changes (deaths and revivals).",
          "latex": "\\text{contribution} \\leq \\frac{4n_{\\text{clone}}V_{\\max}^2}{N}",
          "references": [],
          "derived_statement": "Status changes add at most O(n_{clone}/N V_{max}^2)."
        },
        {
          "order": 4.0,
          "kind": "part",
          "text": "Combine bounds and verify state-independence using regularization.",
          "latex": "|\\Delta V_{\\text{Var},v}| \\leq f_{\\text{clone}} \\cdot 8(\\alpha_{\\text{restitution}}^2 + 4) V_{\\max}^2",
          "references": [
            "axiom-eg-4"
          ],
          "derived_statement": "Total bound is state-independent."
        }
      ],
      "key_equations": [
        {
          "label": "eq-vmax",
          "latex": "V_{\\max}^2 := \\max\\left\\{ \\frac{2F_{\\max}}{\\gamma}, V_{\\text{thresh}}^2 \\right\\}",
          "role": "Defines the state-independent velocity bound."
        },
        {
          "label": "eq-delta-i",
          "latex": "\\Delta_i := \\|v_i^{\\text{new}} - \\mu_v^{\\text{new}}\\|^2 - \\|v_i^{\\text{old}} - \\mu_v^{\\text{old}}\\|^2",
          "role": "Per-walker variance change."
        },
        {
          "label": "eq-vcom",
          "latex": "V_{\\text{COM}} = \\frac{1}{M+1}\\left(v_i^{\\text{old}} + \\sum_{j=1}^M v_j^{\\text{comp}}\\right)",
          "role": "Center-of-mass velocity in cloning."
        },
        {
          "label": "eq-vnew",
          "latex": "v_i^{\\text{new}} = V_{\\text{COM}} + \\alpha_{\\text{restitution}} \\cdot R(u_i)",
          "role": "Post-collision velocity."
        },
        {
          "label": "eq-vchange",
          "latex": "\\|v_i^{\\text{new}} - v_i^{\\text{old}}\\|^2 \\leq 4(\\alpha_{\\text{restitution}}^2 + 1) V_{\\max}^2",
          "role": "Bound on velocity perturbation."
        },
        {
          "label": "eq-total-bound",
          "latex": "|\\Delta V_{\\text{Var},v}| \\leq \\frac{n_{\\text{clone}}}{N} \\cdot 8(\\alpha_{\\text{restitution}}^2 + 4) V_{\\max}^2",
          "role": "Combined bound before substituting f_{clone}."
        },
        {
          "label": "eq-conclusion",
          "latex": "\\Delta V_{\\text{Var},v} \\leq f_{\\text{clone}} \\cdot C_{\\text{reset}} \\cdot V_{\\max,\\text{KE}}",
          "role": "Final bounded change with constants."
        }
      ],
      "references": [
        "axiom-lipschitz-fields",
        "axiom-velocity-regularization"
      ],
      "math_tools": [
        {
          "toolName": "Triangle Inequality",
          "field": "Analysis",
          "description": "A fundamental inequality for norms stating that the norm of a sum is at most the sum of norms.",
          "roleInProof": "Used repeatedly to bound differences in velocities, barycenters, and variance terms.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Cauchy-Schwarz Inequality"
          ]
        },
        {
          "toolName": "Center of Mass",
          "field": "Classical Mechanics",
          "description": "The average velocity of participating particles in a collision.",
          "roleInProof": "Defines the post-collision velocity in the inelastic cloning mechanism.",
          "levelOfAbstraction": "Concept",
          "relatedTools": []
        },
        {
          "toolName": "Squared Norm Expansion",
          "field": "Linear Algebra",
          "description": "Decomposition of ||a - b||^2 = ||a||^2 - 2<a,b> + ||b||^2 for bounding variance changes.",
          "roleInProof": "Applied to analyze changes in individual and total variance contributions.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Inner Product"
          ]
        },
        {
          "toolName": "Barycenter Shift Bound",
          "field": "Statistics",
          "description": "Bounding the change in mean due to updates in a subset of points.",
          "roleInProof": "Used to bound the effect of cloning on the global velocity mean.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Average"
          ]
        },
        {
          "toolName": "Velocity Regularization",
          "field": "Optimization",
          "description": "Penalty term in fitness to prevent excessive velocities.",
          "roleInProof": "Ensures state-independent velocity bounds via fitness feedback.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "L2 Regularization"
          ]
        }
      ],
      "cases": [
        {
          "name": "Part 1: Velocity Domain",
          "condition": "Compact domain and axioms",
          "summary": "Establishes ||v|| \u2264 V_{max}."
        },
        {
          "name": "Part 2: Per-Walker Change",
          "condition": "Single cloning event",
          "summary": "Bounds \u0394_i \u2264 4 V_{max}^2 using collisions."
        },
        {
          "name": "Part 3: Total Change Decomposition",
          "condition": "Cloning with n_{clone} walkers",
          "summary": "Bounds direct, shift, and status contributions."
        },
        {
          "name": "Part 4: State-Independence",
          "condition": "Regularization active",
          "summary": "Verifies constants depend only on parameters."
        }
      ],
      "remarks": [
        {
          "type": "note",
          "text": "The bound is tight in the worst case but improved by restitution \u03b1 < 1 and regularization preventing high velocities."
        },
        {
          "type": "claim",
          "text": "V_{max} is ensured by feedback in viable states with negligible extinction probability."
        }
      ],
      "gaps": [],
      "tags": [
        "velocity variance",
        "cloning",
        "bounded change",
        "state-independent",
        "inelastic collision",
        "Lyapunov function",
        "regularization",
        "triangle inequality"
      ],
      "document_id": "03_cloning",
      "section": "## 5. The Measurement and Interaction Pipeline",
      "span": {
        "start_line": 2096,
        "end_line": 2273,
        "content_start": 2098,
        "content_end": 2272,
        "header_lines": [
          2097
        ]
      },
      "metadata": {
        "label": "proof-prop-bounded-velocity-expansion"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 5,
        "chapter_file": "chapter_5.json",
        "section_id": "## 5. The Measurement and Interaction Pipeline"
      },
      "generated_at": "2025-11-10T13:26:08.165093+00:00",
      "alt_labels": []
    },
    "tags": [
      "cloning",
      "velocity-variance",
      "bounded-expansion",
      "restitution",
      "walkers",
      "synergistic-dissipation"
    ],
    "content_markdown": ":label: prop-bounded-velocity-expansion\n\nFor any cloning event where a fraction $f_{\\text{clone}}$ of walkers are cloned with restitution coefficient $\\alpha_{\\text{restitution}}$, the change in internal velocity variance from the velocity resets is bounded:\n\n$$\n\\Delta V_{Var,v} \\leq f_{\\text{clone}} \\cdot C_{\\text{reset}} \\cdot V_{\\max,\\text{KE}}\n$$",
    "raw_directive": "2084: The following proposition formalizes the key property that enables the synergistic dissipation framework: the expansion of velocity variance caused by cloning is uniformly bounded.\n2085: \n2086: :::{prf:proposition} Bounded Velocity Variance Expansion from Cloning\n2087: :label: prop-bounded-velocity-expansion\n2088: \n2089: For any cloning event where a fraction $f_{\\text{clone}}$ of walkers are cloned with restitution coefficient $\\alpha_{\\text{restitution}}$, the change in internal velocity variance from the velocity resets is bounded:\n2090: \n2091: $$\n2092: \\Delta V_{Var,v} \\leq f_{\\text{clone}} \\cdot C_{\\text{reset}} \\cdot V_{\\max,\\text{KE}}\n2093: $$\n2094: ",
    "document_id": "03_cloning",
    "section": "## 5. The Measurement and Interaction Pipeline",
    "span": {
      "start_line": 2084,
      "end_line": 2094,
      "content_start": 2087,
      "content_end": 2093,
      "header_lines": [
        2085
      ]
    },
    "references": [
      "axiom-lipschitz-fields",
      "axiom-velocity-regularization"
    ],
    "metadata": {
      "label": "prop-bounded-velocity-expansion"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 5,
      "chapter_file": "chapter_5.json",
      "section_id": "## 5. The Measurement and Interaction Pipeline"
    },
    "generated_at": "2025-11-10T13:26:08.168931+00:00",
    "alt_labels": []
  },
  {
    "label": "prop-satisfiability-of-snr-gamma",
    "title": "Satisfiability of the Signal-to-Noise Condition via Signal Gain",
    "type": "proposition",
    "nl_statement": "For rescaled diversity values defined with a signal gain parameter \u03b3 > 0 and a well-behaved rescale function, any high-error system generating a non-zero raw distance signal admits a sufficiently large \u03b3 satisfying the signal-to-noise condition: the variance signal exceeds the maximum variance of the rescaled diversities.",
    "equations": [
      {
        "label": null,
        "latex": "\\kappa_{\\mathrm{var}}(d') > \\operatorname{Var}_{\\max}(d')"
      }
    ],
    "hypotheses": [
      {
        "text": "Rescaled diversity values are defined as d'_i = g_A(\u03b3 \u00b7 z_{d,i}) + \u03b7, where \u03b3 > 0 is the signal gain parameter and g_A satisfies the Axiom of a Well-Behaved Rescale Function.",
        "latex": "d'_i = g_A(\\gamma \\cdot z_{d,i}) + \\eta, \\quad \\gamma > 0"
      },
      {
        "text": "The system is in a high-error state, meaning Var(x) > R\u00b2_var.",
        "latex": "\\operatorname{Var}(x) > R^{2}_{\\operatorname{var}}"
      },
      {
        "text": "The system generates a non-zero raw distance signal, \u03ba_meas(d) > 0.",
        "latex": "\\kappa_{\\mathrm{meas}}(d) > 0"
      }
    ],
    "conclusion": {
      "text": "There exists a sufficiently large \u03b3 such that the signal-to-noise condition holds: the variance-based signal exceeds the maximum variance of the rescaled diversities.",
      "latex": "\\kappa_{\\mathrm{var}}(d') > \\operatorname{Var}_{\\max}(d')"
    },
    "variables": [
      {
        "symbol": "\u03b3",
        "name": "Signal Gain",
        "description": "User-defined parameter acting as a sensitivity knob to amplify the geometric error signal.",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "parameter",
          "gain",
          "sensitivity"
        ]
      },
      {
        "symbol": "d'_i",
        "name": "Rescaled Diversity",
        "description": "Rescaled diversity values incorporating signal gain and noise.",
        "constraints": [],
        "tags": [
          "diversity",
          "rescaled"
        ]
      },
      {
        "symbol": "z_{d,i}",
        "name": "Raw Diversity",
        "description": "Input to the rescaling function before gain and noise.",
        "constraints": [],
        "tags": [
          "diversity",
          "raw"
        ]
      },
      {
        "symbol": "\u03b7",
        "name": "Noise",
        "description": "Additive noise term in rescaled diversity.",
        "constraints": [],
        "tags": [
          "noise"
        ]
      },
      {
        "symbol": "g_A",
        "name": "Rescale Function",
        "description": "Well-behaved function satisfying the Axiom of a Well-Behaved Rescale Function.",
        "constraints": [],
        "tags": [
          "function",
          "rescaling",
          "axiom"
        ]
      },
      {
        "symbol": "\u03ba_var(d')",
        "name": "Variance Signal",
        "description": "Signal derived from variance of rescaled diversities.",
        "constraints": [],
        "tags": [
          "signal",
          "variance"
        ]
      },
      {
        "symbol": "Var_max(d')",
        "name": "Maximum Variance",
        "description": "Maximum possible variance of the rescaled diversities.",
        "constraints": [],
        "tags": [
          "variance",
          "maximum"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The rescaling function g_A is continuous and invertible to allow amplification via \u03b3.",
        "confidence": 0.9
      },
      {
        "text": "The noise \u03b7 is bounded such that Var_max(d') remains finite and controllable.",
        "confidence": 0.8
      },
      {
        "text": "The high-error state persists under rescaling, maintaining \u03ba_meas(d) > 0.",
        "confidence": 0.95
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-prop-satisfiability-of-snr-gamma",
      "title": null,
      "type": "proof",
      "proves": "prop-satisfiability-of-snr-gamma",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-prop-satisfiability-of-snr-gamma\n\n**Proof.**\n\nThe proof strategy is to show that the guaranteed signal variance of the rescaled values, $\\kappa_var(d')$, scales with $\\gamma^{2}$ in the small-signal limit, while the maximum possible noise, `Var_max(d')`, remains a fixed constant independent of $\\gamma$. This algebraic advantage allows $\\gamma$ to be chosen to ensure the signal always dominates the noise.\n\n**1. The Noise Term (`Var_max(d')`): A Fixed, $\\gamma$-Independent Constant.**\n\nThe **Axiom of a Well-Behaved Rescale Function** requires `g_A` to have a bounded range, which we denote `(g_{A,\\min}, g_{A,\\max})`. Consequently, the rescaled values $d'_i = g_A(\\gamma \u00b7 z_{d,i}) + \\eta$ are always contained within the fixed interval $(g_{A,\\min} + \\eta, g_{A,\\max} + \\eta)$.\n\nThe maximum possible variance for any set of values on this interval is given by Popoviciu's inequality:\n\n$$\n\\operatorname{Var}_{\\max}(d') := \\frac{1}{4}(\\max(d') - \\min(d'))^2 = \\frac{1}{4}(g_{A,\\max} - g_{A,\\min})^2\n$$\n\nThis value is a constant determined solely by the choice of the rescale function `g_A`; it does not depend on the Signal Gain $\\gamma$. For the **Canonical Logistic Rescale function**, `g_A(z) = 2/(1+e^{-z})`, the range is `(0, 2)`, yielding a fixed maximum noise of `Var_max(d') = 1`.\n\nOur goal is to prove that we can choose $\\gamma$ such that the guaranteed signal variance $\\kappa_var(d')$ is greater than this fixed constant.\n\n**2. The Signal Term ($\\kappa_var(d')$): Amplification by $\\gamma$.**\n\nThe signal originates from the raw distance measurements `d`, propagates to the standardized scores `z_d`, and is then amplified.\n\n*   **Raw and Standardized Signal:** From [](#thm-geometry-guarantees-variance), a high-error state guarantees $\\text{Var}(d) \\geq \\kappa_meas(d) > 0$. The Z-scores $z_d = (d - \\mu_d) / \\sigma'_d$ have a variance $\\text{Var}(z_d) = \\text{Var}(d) / (\\sigma'_d)^{2}$. Since the patched standard deviation (see {prf:ref}`def-patched-std-dev-function`) $\\sigma'_d$ is uniformly bounded above by $\\sigma'_max$ ({prf:ref}`def-max-patched-std`), the Z-score variance has a uniform lower bound:\n\n\n$$\n\\operatorname{Var}(z_d) \\ge \\frac{\\kappa_{\\mathrm{meas}}(d)}{(\\sigma'_{\\max})^2} =: \\kappa_{\\mathrm{var}}(z) > 0\n$$\n\n*   **Signal Amplification:** The input to the rescale function is $u_i = \\gammaz_{d,i}$. The variance of this amplified signal is $\\text{Var}(u) = \\gamma^{2}\\text{Var}(z_d) \\geq \\gamma^{2}\\kappa_var(z)$.\n\n*   **Rescaled Signal ($\\kappa_var(d')$):** The rescaled values are $d' = g_A(u) + \\eta$. For any differentiable function, a first-order Taylor expansion around the mean $\\mu_u$ gives $g_A(u_i) \\approx g_A(\\mu_u) + g'_A(\\mu_u)(u_i - \\mu_u)$. The variance is then approximated by:\n\n\n$$\n\\operatorname{Var}(d') = \\operatorname{Var}(g_A(u)) \\approx (g'_A(\\mu_u))^2 \\operatorname{Var}(u)\n$$\n\n    This approximation becomes exact in the limit of small variance relative to the curvature of `g_A`. A more rigorous treatment using the Mean Value Theorem shows that the variance of the output is bounded below by the variance of the input multiplied by the squared infimum of the derivative.\n\n\n$$\n\\operatorname{Var}(d') \\ge (\\inf_{c \\in Z_{\\mathrm{eff}}} g'_A(c))^2 \\operatorname{Var}(u)\n$$\n\n    where `Z_eff` is the effective range of inputs. Let `g'_{\\min} > 0` be the uniform lower bound on the derivative (guaranteed to exist on any compact operational range by the axiom). The guaranteed variance of the rescaled values is thus bounded below by a term proportional to $\\gamma^{2}$:\n\n\n$$\n\\kappa_{\\mathrm{var}}(d') \\ge (g'_{\\min})^2 \\cdot \\gamma^2 \\kappa_{\\mathrm{var}}(z)\n$$\n\n**3. Proving Satisfiability.**\n\nThe Signal-to-Noise Condition is $\\kappa_var(d') > Var_max(d')$. Substituting our results from the steps above:\n\n$$\n(g'_{\\min})^2 \\cdot \\gamma^2 \\kappa_{\\mathrm{var}}(z) > \\frac{1}{4}(g_{A,\\max} - g_{A,\\min})^2\n$$\n\nSolving for the Signal Gain $\\gamma$:\n\n$$\n\\gamma > \\frac{g_{A,\\max} - g_{A,\\min}}{2 \\cdot g'_{\\min} \\cdot \\sqrt{\\kappa_{\\mathrm{var}}(z)}}\n$$\n\nSince $\\kappa_var(z)$ is a fixed positive constant for a given $\\varepsilon$, and `g_A`'s properties (`g_{A,max}`, `g_{A,min}`, `g'_{min}`) are fixed, the right-hand side is a fixed, positive real number. This proves that there always exists a sufficiently large choice of $\\gamma$ that satisfies the condition.\n\n**Conclusion:** The Signal-to-Noise Condition is not a restrictive assumption on the environment but is a design criterion that can always be satisfied by appropriately tuning the algorithm's sensitivity $\\gamma$. This holds for any valid rescale function, including the Canonical choice.",
      "raw_directive": "3494: where `Var_max(d')` is the maximum possible variance of the rescaled values, and $\\kappa_var(d')$ is the guaranteed lower bound on the variance of the rescaled values in the high-error state.\n3495: :::\n3496: :::{prf:proof}\n3497: :label: proof-prop-satisfiability-of-snr-gamma\n3498: \n3499: **Proof.**\n3500: \n3501: The proof strategy is to show that the guaranteed signal variance of the rescaled values, $\\kappa_var(d')$, scales with $\\gamma^{2}$ in the small-signal limit, while the maximum possible noise, `Var_max(d')`, remains a fixed constant independent of $\\gamma$. This algebraic advantage allows $\\gamma$ to be chosen to ensure the signal always dominates the noise.\n3502: \n3503: **1. The Noise Term (`Var_max(d')`): A Fixed, $\\gamma$-Independent Constant.**\n3504: \n3505: The **Axiom of a Well-Behaved Rescale Function** requires `g_A` to have a bounded range, which we denote `(g_{A,\\min}, g_{A,\\max})`. Consequently, the rescaled values $d'_i = g_A(\\gamma \u00b7 z_{d,i}) + \\eta$ are always contained within the fixed interval $(g_{A,\\min} + \\eta, g_{A,\\max} + \\eta)$.\n3506: \n3507: The maximum possible variance for any set of values on this interval is given by Popoviciu's inequality:\n3508: \n3509: $$\n3510: \\operatorname{Var}_{\\max}(d') := \\frac{1}{4}(\\max(d') - \\min(d'))^2 = \\frac{1}{4}(g_{A,\\max} - g_{A,\\min})^2\n3511: $$\n3512: \n3513: This value is a constant determined solely by the choice of the rescale function `g_A`; it does not depend on the Signal Gain $\\gamma$. For the **Canonical Logistic Rescale function**, `g_A(z) = 2/(1+e^{-z})`, the range is `(0, 2)`, yielding a fixed maximum noise of `Var_max(d') = 1`.\n3514: \n3515: Our goal is to prove that we can choose $\\gamma$ such that the guaranteed signal variance $\\kappa_var(d')$ is greater than this fixed constant.\n3516: \n3517: **2. The Signal Term ($\\kappa_var(d')$): Amplification by $\\gamma$.**\n3518: \n3519: The signal originates from the raw distance measurements `d`, propagates to the standardized scores `z_d`, and is then amplified.\n3520: \n3521: *   **Raw and Standardized Signal:** From [](#thm-geometry-guarantees-variance), a high-error state guarantees $\\text{Var}(d) \\geq \\kappa_meas(d) > 0$. The Z-scores $z_d = (d - \\mu_d) / \\sigma'_d$ have a variance $\\text{Var}(z_d) = \\text{Var}(d) / (\\sigma'_d)^{2}$. Since the patched standard deviation (see {prf:ref}`def-patched-std-dev-function`) $\\sigma'_d$ is uniformly bounded above by $\\sigma'_max$ ({prf:ref}`def-max-patched-std`), the Z-score variance has a uniform lower bound:\n3522: \n3523: \n3524: $$\n3525: \\operatorname{Var}(z_d) \\ge \\frac{\\kappa_{\\mathrm{meas}}(d)}{(\\sigma'_{\\max})^2} =: \\kappa_{\\mathrm{var}}(z) > 0\n3526: $$\n3527: \n3528: *   **Signal Amplification:** The input to the rescale function is $u_i = \\gammaz_{d,i}$. The variance of this amplified signal is $\\text{Var}(u) = \\gamma^{2}\\text{Var}(z_d) \\geq \\gamma^{2}\\kappa_var(z)$.\n3529: \n3530: *   **Rescaled Signal ($\\kappa_var(d')$):** The rescaled values are $d' = g_A(u) + \\eta$. For any differentiable function, a first-order Taylor expansion around the mean $\\mu_u$ gives $g_A(u_i) \\approx g_A(\\mu_u) + g'_A(\\mu_u)(u_i - \\mu_u)$. The variance is then approximated by:\n3531: \n3532: \n3533: $$\n3534: \\operatorname{Var}(d') = \\operatorname{Var}(g_A(u)) \\approx (g'_A(\\mu_u))^2 \\operatorname{Var}(u)\n3535: $$\n3536: \n3537:     This approximation becomes exact in the limit of small variance relative to the curvature of `g_A`. A more rigorous treatment using the Mean Value Theorem shows that the variance of the output is bounded below by the variance of the input multiplied by the squared infimum of the derivative.\n3538: \n3539: \n3540: $$\n3541: \\operatorname{Var}(d') \\ge (\\inf_{c \\in Z_{\\mathrm{eff}}} g'_A(c))^2 \\operatorname{Var}(u)\n3542: $$\n3543: \n3544:     where `Z_eff` is the effective range of inputs. Let `g'_{\\min} > 0` be the uniform lower bound on the derivative (guaranteed to exist on any compact operational range by the axiom). The guaranteed variance of the rescaled values is thus bounded below by a term proportional to $\\gamma^{2}$:\n3545: \n3546: \n3547: $$\n3548: \\kappa_{\\mathrm{var}}(d') \\ge (g'_{\\min})^2 \\cdot \\gamma^2 \\kappa_{\\mathrm{var}}(z)\n3549: $$\n3550: \n3551: **3. Proving Satisfiability.**\n3552: \n3553: The Signal-to-Noise Condition is $\\kappa_var(d') > Var_max(d')$. Substituting our results from the steps above:\n3554: \n3555: $$\n3556: (g'_{\\min})^2 \\cdot \\gamma^2 \\kappa_{\\mathrm{var}}(z) > \\frac{1}{4}(g_{A,\\max} - g_{A,\\min})^2\n3557: $$\n3558: \n3559: Solving for the Signal Gain $\\gamma$:\n3560: \n3561: $$\n3562: \\gamma > \\frac{g_{A,\\max} - g_{A,\\min}}{2 \\cdot g'_{\\min} \\cdot \\sqrt{\\kappa_{\\mathrm{var}}(z)}}\n3563: $$\n3564: \n3565: Since $\\kappa_var(z)$ is a fixed positive constant for a given $\\varepsilon$, and `g_A`'s properties (`g_{A,max}`, `g_{A,min}`, `g'_{min}`) are fixed, the right-hand side is a fixed, positive real number. This proves that there always exists a sufficiently large choice of $\\gamma$ that satisfies the condition.\n3566: \n3567: **Conclusion:** The Signal-to-Noise Condition is not a restrictive assumption on the environment but is a design criterion that can always be satisfied by appropriately tuning the algorithm's sensitivity $\\gamma$. This holds for any valid rescale function, including the Canonical choice.\n3568: ",
      "strategy_summary": "The proof demonstrates that the maximum noise variance Var_max(d') is a fixed constant independent of \u03b3, while the guaranteed signal variance \u03ba_var(d') scales quadratically with \u03b3, allowing a sufficiently large \u03b3 to ensure \u03ba_var(d') > Var_max(d') and satisfy the signal-to-noise condition.",
      "conclusion": {
        "text": "The Signal-to-Noise Condition is not a restrictive assumption on the environment but is a design criterion that can always be satisfied by appropriately tuning the algorithm's sensitivity \u03b3. This holds for any valid rescale function, including the Canonical choice.",
        "latex": null
      },
      "assumptions": [
        {
          "text": "Axiom of a Well-Behaved Rescale Function: g_A has a bounded range (g_{A,min}, g_{A,max}) and a uniform lower bound g'_{min} > 0 on its derivative over compact sets.",
          "latex": null
        },
        {
          "text": "High-error state guarantees Var(d) \u2265 \u03ba_meas(d) > 0 (from thm-geometry-guarantees-variance).",
          "latex": null
        },
        {
          "text": "Patched standard deviation \u03c3'_d is bounded above by \u03c3'_{max} (from def-max-patched-std).",
          "latex": null
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "noise-term",
          "text": "The Noise Term (Var_max(d')): A Fixed, \u03b3-Independent Constant. The rescaled values d' are bounded in (g_{A,min} + \u03b7, g_{A,max} + \u03b7), and Popoviciu's inequality gives Var_max(d') = (1/4)(g_{A,max} - g_{A,min})^2, independent of \u03b3.",
          "latex": null,
          "references": [],
          "derived_statement": "Var_max(d') = \\frac{1}{4}(g_{A,\\max} - g_{A,\\min})^2"
        },
        {
          "order": 2.0,
          "kind": "signal-term-raw",
          "text": "Raw and Standardized Signal: Var(d) \u2265 \u03ba_meas(d) > 0, and Var(z_d) = Var(d) / (\u03c3'_d)^2 \u2265 \u03ba_meas(d) / (\u03c3'_{max})^2 = \u03ba_var(z) > 0.",
          "latex": null,
          "references": [
            "thm-geometry-guarantees-variance",
            "def-max-patched-std"
          ],
          "derived_statement": "\\operatorname{Var}(z_d) \\ge \\frac{\\kappa_{\\mathrm{meas}}(d)}{(\\sigma'_{\\max})^2} =: \\kappa_{\\mathrm{var}}(z) > 0"
        },
        {
          "order": 3.0,
          "kind": "signal-term-amplification",
          "text": "Signal Amplification: Input u = \u03b3 z_d, so Var(u) = \u03b3^2 Var(z_d) \u2265 \u03b3^2 \u03ba_var(z).",
          "latex": null,
          "references": [],
          "derived_statement": "\\operatorname{Var}(u) = \\gamma^{2} \\operatorname{Var}(z_d) \\geq \\gamma^{2} \\kappa_{\\mathrm{var}}(z)"
        },
        {
          "order": 4.0,
          "kind": "signal-term-rescaled",
          "text": "Rescaled Signal (\u03ba_var(d')): Using Taylor expansion and Mean Value Theorem, Var(d') \u2265 (inf_{c \u2208 Z_eff} g'_A(c))^2 Var(u), so \u03ba_var(d') \u2265 (g'_{min})^2 \u03b3^2 \u03ba_var(z).",
          "latex": null,
          "references": [],
          "derived_statement": "\\kappa_{\\mathrm{var}}(d') \\ge (g'_{\\min})^2 \\cdot \\gamma^2 \\kappa_{\\mathrm{var}}(z)"
        },
        {
          "order": 5.0,
          "kind": "satisfiability",
          "text": "Proving Satisfiability: Set \u03ba_var(d') > Var_max(d'), yielding \u03b3 > \\frac{g_{A,\\max} - g_{A,\\min}}{2 \\cdot g'_{\\min} \\cdot \\sqrt{\\kappa_{\\mathrm{var}}(z)}}, a fixed positive value, so such a \u03b3 exists.",
          "latex": null,
          "references": [],
          "derived_statement": "\\gamma > \\frac{g_{A,\\max} - g_{A,\\min}}{2 \\cdot g'_{\\min} \\cdot \\sqrt{\\kappa_{\\mathrm{var}}(z)}}"
        }
      ],
      "key_equations": [
        {
          "label": "eq-var-max",
          "latex": "\\operatorname{Var}_{\\max}(d') := \\frac{1}{4}(\\max(d') - \\min(d'))^2 = \\frac{1}{4}(g_{A,\\max} - g_{A,\\min})^2",
          "role": "Defines the fixed maximum variance bound"
        },
        {
          "label": "eq-var-z",
          "latex": "\\operatorname{Var}(z_d) \\ge \\frac{\\kappa_{\\mathrm{meas}}(d)}{(\\sigma'_{\\max})^2} =: \\kappa_{\\mathrm{var}}(z) > 0",
          "role": "Lower bound on Z-score variance"
        },
        {
          "label": "eq-var-u",
          "latex": "\\operatorname{Var}(u) = \\gamma^{2} \\operatorname{Var}(z_d) \\geq \\gamma^{2} \\kappa_{\\mathrm{var}}(z)",
          "role": "Variance after amplification by \u03b3"
        },
        {
          "label": "eq-var-d-approx",
          "latex": "\\operatorname{Var}(d') = \\operatorname{Var}(g_A(u)) \\approx (g'_A(\\mu_u))^2 \\operatorname{Var}(u)",
          "role": "Taylor approximation for rescaled variance"
        },
        {
          "label": "eq-var-d-bound",
          "latex": "\\operatorname{Var}(d') \\ge (\\inf_{c \\in Z_{\\mathrm{eff}}} g'_A(c))^2 \\operatorname{Var}(u)",
          "role": "Rigorous lower bound using Mean Value Theorem"
        },
        {
          "label": "eq-kappa-var",
          "latex": "\\kappa_{\\mathrm{var}}(d') \\ge (g'_{\\min})^2 \\cdot \\gamma^2 \\kappa_{\\mathrm{var}}(z)",
          "role": "Guaranteed lower bound on signal variance"
        },
        {
          "label": "eq-snr-ineq",
          "latex": "(g'_{\\min})^2 \\cdot \\gamma^2 \\kappa_{\\mathrm{var}}(z) > \\frac{1}{4}(g_{A,\\max} - g_{A,\\min})^2",
          "role": "Signal-to-noise condition"
        },
        {
          "label": "eq-gamma-bound",
          "latex": "\\gamma > \\frac{g_{A,\\max} - g_{A,\\min}}{2 \\cdot g'_{\\min} \\cdot \\sqrt{\\kappa_{\\mathrm{var}}(z)}}",
          "role": "Sufficient \u03b3 to satisfy the condition"
        }
      ],
      "references": [
        "def-patched-std-dev-function",
        "def-max-patched-std",
        "thm-geometry-guarantees-variance"
      ],
      "math_tools": [
        {
          "toolName": "Popoviciu's inequality",
          "field": "Statistics",
          "description": "Provides an upper bound on the variance of a bounded random variable as one-quarter the square of the range.",
          "roleInProof": "Used to establish the fixed maximum variance Var_max(d') for rescaled values in a bounded interval.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": []
        },
        {
          "toolName": "Taylor expansion",
          "field": "Calculus",
          "description": "Approximates a differentiable function near a point using its derivatives, particularly the first-order linear approximation.",
          "roleInProof": "Approximates the variance of the rescaled signal g_A(u) as (g'_A(\u03bc_u))^2 Var(u) in the small-signal limit.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Mean Value Theorem"
          ]
        },
        {
          "toolName": "Mean Value Theorem",
          "field": "Calculus",
          "description": "States that for a continuous function on a closed interval, there exists a point where the derivative equals the average rate of change.",
          "roleInProof": "Provides a rigorous lower bound on Var(g_A(u)) using the infimum of the derivative over the effective input range.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Taylor expansion"
          ]
        },
        {
          "toolName": "Z-score standardization",
          "field": "Statistics",
          "description": "Transforms data to have mean zero and variance one by subtracting the mean and dividing by the standard deviation.",
          "roleInProof": "Standardizes raw distances d to z_d, preserving a lower bound on variance via the bounded patched standard deviation.",
          "levelOfAbstraction": "Technique",
          "relatedTools": []
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "goal",
          "text": "Prove that \u03b3 can be chosen such that \u03ba_var(d') > Var_max(d'), ensuring the signal dominates the noise."
        },
        {
          "type": "example",
          "text": "For the Canonical Logistic Rescale g_A(z) = 2/(1 + e^{-z}), Var_max(d') = 1."
        }
      ],
      "gaps": [],
      "tags": [
        "signal-to-noise",
        "variance-bound",
        "rescale-function",
        "signal-amplification",
        "popoviciu-inequality",
        "taylor-expansion",
        "mean-value-theorem",
        "gamma-tuning"
      ],
      "document_id": "03_cloning",
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "span": {
        "start_line": 3494,
        "end_line": 3568,
        "content_start": 3497,
        "content_end": 3567,
        "header_lines": [
          3495
        ]
      },
      "metadata": {
        "label": "proof-prop-satisfiability-of-snr-gamma"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      },
      "generated_at": "2025-11-10T13:26:08.165093+00:00",
      "alt_labels": []
    },
    "tags": [
      "signal-to-noise",
      "satisfiability",
      "signal gain",
      "gamma",
      "diversity",
      "rescaling",
      "learnability"
    ],
    "content_markdown": ":label: prop-satisfiability-of-snr-gamma\n\nLet the rescaled diversity values be defined as $d'_i = g_A(\\gamma \u00b7 z_{d,i}) + \\eta$, where $\\gamma > 0$ is a user-defined **Signal Gain** parameter and `g_A` is any function satisfying the **Axiom of a Well-Behaved Rescale Function** (see {prf:ref}`def-logistic-rescale` for the canonical choice).\n\nFor any system in a high-error state (`Var(x) > R^{2}_var`) that generates a non-zero raw distance signal ($\\kappa_meas(d) > 0$), there exists a sufficiently large choice of $\\gamma$ that satisfies the **Signal-to-Noise Condition**:\n\n$$\n\\kappa_{\\mathrm{var}}(d') > \\operatorname{Var}_{\\max}(d')\n$$",
    "raw_directive": "3481: This section provides the formal proof that this condition, which we call the **Signal-to-Noise Condition**, is not an unstated assumption but a satisfiable criterion that can be met by a valid choice of the algorithm's user-defined parameters. We prove this by introducing a **Signal Gain** parameter, $\\gamma$, which acts as a sensitivity knob for the algorithm. This proves that the system is fundamentally \"learnable\": the signal generated by geometric error can always be amplified sufficiently to overcome the worst-case statistical noise, ensuring that a true difference between the high-error and low-error populations is always detectable.\n3482: \n3483: :::{prf:proposition} **(Satisfiability of the Signal-to-Noise Condition via Signal Gain)**\n3484: :label: prop-satisfiability-of-snr-gamma\n3485: \n3486: Let the rescaled diversity values be defined as $d'_i = g_A(\\gamma \u00b7 z_{d,i}) + \\eta$, where $\\gamma > 0$ is a user-defined **Signal Gain** parameter and `g_A` is any function satisfying the **Axiom of a Well-Behaved Rescale Function** (see {prf:ref}`def-logistic-rescale` for the canonical choice).\n3487: \n3488: For any system in a high-error state (`Var(x) > R^{2}_var`) that generates a non-zero raw distance signal ($\\kappa_meas(d) > 0$), there exists a sufficiently large choice of $\\gamma$ that satisfies the **Signal-to-Noise Condition**:\n3489: \n3490: $$\n3491: \\kappa_{\\mathrm{var}}(d') > \\operatorname{Var}_{\\max}(d')\n3492: $$\n3493: ",
    "document_id": "03_cloning",
    "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
    "span": {
      "start_line": 3481,
      "end_line": 3493,
      "content_start": 3484,
      "content_end": 3492,
      "header_lines": [
        3482
      ]
    },
    "references": [
      "def-logistic-rescale",
      "def-patched-std-dev-function",
      "def-max-patched-std",
      "thm-geometry-guarantees-variance"
    ],
    "metadata": {
      "label": "prop-satisfiability-of-snr-gamma"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 7,
      "chapter_file": "chapter_7.json",
      "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
    },
    "generated_at": "2025-11-10T13:26:08.168931+00:00",
    "alt_labels": []
  },
  {
    "label": "prop-corrective-signal-bound",
    "title": "Lower Bound on the Corrective Diversity Signal",
    "type": "proposition",
    "nl_statement": "In the high-error regime where the variance of rescaled diversity values d' is bounded below by a positive constant and the Signal-to-Noise Condition holds, the expected logarithmic gap in d' between high-error and low-error populations is bounded below by a positive N-uniform constant.",
    "equations": [
      {
        "label": null,
        "latex": "\\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right) > 0"
      }
    ],
    "hypotheses": [
      {
        "text": "Swarm state in high-error regime with \\(\\operatorname{Var}(d') \\ge \\kappa_{d', \\text{var}} > 0\\)",
        "latex": "\\operatorname{Var}(d') \\ge \\kappa_{d', \\text{var}} > 0"
      },
      {
        "text": "Signal-to-Noise Condition satisfied: \\(\\kappa_{d', \\text{var}} > \\operatorname{Var}_{\\max}(d')\\)",
        "latex": "\\kappa_{d', \\text{var}} > \\operatorname{Var}_{\\max}(d')"
      }
    ],
    "conclusion": {
      "text": "Expected logarithmic gap bounded below: \\(\\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right) > 0\\)",
      "latex": "\\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right) > 0"
    },
    "variables": [
      {
        "symbol": "d'",
        "name": "rescaled diversity",
        "description": "Rescaled diversity values in the swarm state",
        "constraints": [
          "variance bounded below"
        ],
        "tags": [
          "diversity",
          "rescaled"
        ]
      },
      {
        "symbol": "\\kappa_{d', \\text{var}}",
        "name": "diversity variance constant",
        "description": "Positive lower bound on variance of d'",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "constant",
          "variance"
        ]
      },
      {
        "symbol": "H_k",
        "name": "high-error population",
        "description": "Population in high geometric error state at iteration k",
        "constraints": [],
        "tags": [
          "population",
          "high-error"
        ]
      },
      {
        "symbol": "L_k",
        "name": "low-error population",
        "description": "Population in low geometric error state at iteration k",
        "constraints": [],
        "tags": [
          "population",
          "low-error"
        ]
      },
      {
        "symbol": "\\kappa_{d', \\text{mean}}",
        "name": "diversity mean constant",
        "description": "Parameter related to mean separation in diversity",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "constant",
          "mean"
        ]
      },
      {
        "symbol": "g_{A,\\max}",
        "name": "maximum growth rate",
        "description": "Upper bound on growth rate of A",
        "constraints": [],
        "tags": [
          "growth",
          "maximum"
        ]
      },
      {
        "symbol": "\\eta",
        "name": "noise parameter",
        "description": "Small positive noise or regularization term",
        "constraints": [
          "> 0"
        ],
        "tags": [
          "noise",
          "regularization"
        ]
      },
      {
        "symbol": "N",
        "name": "population size",
        "description": "Swarm or population size, uniform bound independent of N",
        "constraints": [],
        "tags": [
          "size",
          "uniform"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "All constants like \\(\\kappa\\) are positive and N-independent",
        "confidence": 0.9
      },
      {
        "text": "d' is positive-valued for logarithmic expectation to be defined",
        "confidence": 0.8
      },
      {
        "text": "System parameters ensure the bound is strictly positive",
        "confidence": 0.95
      }
    ],
    "local_refs": [
      "lem-variance-to-mean-separation"
    ],
    "proof": {
      "label": "proof-prop-corrective-signal-bound",
      "title": null,
      "type": "proof",
      "proves": "prop-corrective-signal-bound",
      "proof_type": "reference",
      "proof_status": "complete",
      "content_markdown": ":label: proof-prop-corrective-signal-bound\n\n**Proof.**\n\nThe proof proceeds in two steps. First, we translate the guaranteed variance into a guaranteed separation between the means of the high-error and low-error populations. Second, we translate this mean separation into a guaranteed separation in the expected logarithms.\n\n**1. From Variance to Mean Separation:**\nThe premises state that $\\operatorname{Var}(d') \\ge \\kappa_{d', \\text{var}}$ and that the Signal-to-Noise Condition is satisfied. The population fractions $f_H$ and $f_L$ are N-uniform and bounded below by a constant $f_{\\min} > 0$. We apply [](#lem-variance-to-mean-separation) directly. This yields a guaranteed separation between the means of the rescaled diversity values:\n\n$$\n|\\mu_{d'}(H_k) - \\mu_{d'}(L_k)| \\ge \\kappa_{d', \\text{mean}} > 0\n$$\n\nThe direction of this inequality is also guaranteed. The geometric analysis in Chapter 6 ({prf:ref}`lem-geometric-separation-of-partition`) established that high-error walkers are systematically more isolated, which implies their expected raw distance-to-companion is larger: $\\mathbb{E}[d|H_k] > \\mathbb{E}[d|L_k]$. Since the standardization and rescaling operators (specifically the monotonic rescale function $g_A$) preserve the ordering of the means, this inequality propagates through the entire pipeline. This guarantees that the mean of the *rescaled* diversity values is also larger for the high-error set, $\\mu_{d'}(H_k) > \\mu_{d'}(L_k)$. We can therefore remove the absolute value and state the inequality directionally.\n\n**2. From Mean Separation to Logarithmic Mean Separation:**\nWe now have a guaranteed mean separation, $\\mu_{d'}(H_k) \\ge \\mu_{d'}(L_k) + \\kappa_{d', \\text{mean}}$. The rescaled values $d'$ are contained in the compact interval $[\\eta, g_{A,\\max}+\\eta]$. We apply [](#lem-log-gap-lower-bound) with $X$ representing the distribution of $d'$ in $H_k$, $Y$ representing the distribution in $L_k$, $\\kappa = \\kappa_{d', \\text{mean}}$, and $V_{\\max} = g_{A,\\max}+\\eta$.\nThe lemma gives the stated result directly:\n\n$$\n\\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right)\n$$\n\nSince $\\kappa_{d', \\text{mean}} > 0$, the argument of the logarithm is strictly greater than 1, ensuring the lower bound is strictly positive.",
      "raw_directive": "4309: where $\\kappa_{d', \\text{mean}} := \\frac{1}{\\sqrt{f_H f_L}}\\sqrt{\\kappa_{d', \\text{var}} - \\operatorname{Var}_{\\max}(d')}$.\n4310: :::\n4311: :::{prf:proof}\n4312: :label: proof-prop-corrective-signal-bound\n4313: \n4314: **Proof.**\n4315: \n4316: The proof proceeds in two steps. First, we translate the guaranteed variance into a guaranteed separation between the means of the high-error and low-error populations. Second, we translate this mean separation into a guaranteed separation in the expected logarithms.\n4317: \n4318: **1. From Variance to Mean Separation:**\n4319: The premises state that $\\operatorname{Var}(d') \\ge \\kappa_{d', \\text{var}}$ and that the Signal-to-Noise Condition is satisfied. The population fractions $f_H$ and $f_L$ are N-uniform and bounded below by a constant $f_{\\min} > 0$. We apply [](#lem-variance-to-mean-separation) directly. This yields a guaranteed separation between the means of the rescaled diversity values:\n4320: \n4321: $$\n4322: |\\mu_{d'}(H_k) - \\mu_{d'}(L_k)| \\ge \\kappa_{d', \\text{mean}} > 0\n4323: $$\n4324: \n4325: The direction of this inequality is also guaranteed. The geometric analysis in Chapter 6 ({prf:ref}`lem-geometric-separation-of-partition`) established that high-error walkers are systematically more isolated, which implies their expected raw distance-to-companion is larger: $\\mathbb{E}[d|H_k] > \\mathbb{E}[d|L_k]$. Since the standardization and rescaling operators (specifically the monotonic rescale function $g_A$) preserve the ordering of the means, this inequality propagates through the entire pipeline. This guarantees that the mean of the *rescaled* diversity values is also larger for the high-error set, $\\mu_{d'}(H_k) > \\mu_{d'}(L_k)$. We can therefore remove the absolute value and state the inequality directionally.\n4326: \n4327: **2. From Mean Separation to Logarithmic Mean Separation:**\n4328: We now have a guaranteed mean separation, $\\mu_{d'}(H_k) \\ge \\mu_{d'}(L_k) + \\kappa_{d', \\text{mean}}$. The rescaled values $d'$ are contained in the compact interval $[\\eta, g_{A,\\max}+\\eta]$. We apply [](#lem-log-gap-lower-bound) with $X$ representing the distribution of $d'$ in $H_k$, $Y$ representing the distribution in $L_k$, $\\kappa = \\kappa_{d', \\text{mean}}$, and $V_{\\max} = g_{A,\\max}+\\eta$.\n4329: The lemma gives the stated result directly:\n4330: \n4331: $$\n4332: \\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right)\n4333: $$\n4334: \n4335: Since $\\kappa_{d', \\text{mean}} > 0$, the argument of the logarithm is strictly greater than 1, ensuring the lower bound is strictly positive.\n4336: ",
      "strategy_summary": "The proof applies a variance-to-mean separation lemma to guarantee a positive difference in means between high-error and low-error populations, leveraging the signal-to-noise condition and population fractions. It then uses a log-gap lower bound lemma to translate this mean separation into a positive lower bound on the difference in expected logarithms of rescaled diversity values.",
      "conclusion": {
        "text": "The expected logarithmic separation is bounded below by a positive quantity: $\\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right) > 0$.",
        "latex": "\\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right)"
      },
      "assumptions": [
        {
          "text": "Var(d') \u2265 \u03ba_{d', var}",
          "latex": "\\operatorname{Var}(d') \\ge \\kappa_{d', \\text{var}}"
        },
        {
          "text": "Signal-to-Noise Condition is satisfied",
          "latex": null
        },
        {
          "text": "Population fractions f_H and f_L are N-uniform and bounded below by f_min > 0",
          "latex": "f_H, f_L \\ge f_{\\min} > 0"
        },
        {
          "text": "Rescaled values d' in [\u03b7, g_{A,max} + \u03b7]",
          "latex": "d' \\in [\\eta, g_{A,\\max} + \\eta]"
        },
        {
          "text": "High-error walkers have larger expected raw distance: E[d|H_k] > E[d|L_k]",
          "latex": "\\mathbb{E}[d|H_k] > \\mathbb{E}[d|L_k]"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "lemma-application",
          "text": "Apply the variance-to-mean separation lemma using Var(d') \u2265 \u03ba_{d', var}, signal-to-noise condition, and population fractions f_H, f_L \u2265 f_min > 0.",
          "latex": null,
          "references": [
            "lem-variance-to-mean-separation"
          ],
          "derived_statement": "|\u03bc_{d'}(H_k) - \u03bc_{d'}(L_k)| \u2265 \u03ba_{d', mean} > 0"
        },
        {
          "order": 1.1,
          "kind": "directional-argument",
          "text": "Geometric separation (from Chapter 6) ensures E[d|H_k] > E[d|L_k], preserved through monotonic rescaling, so \u03bc_{d'}(H_k) > \u03bc_{d'}(L_k) + \u03ba_{d', mean}.",
          "latex": null,
          "references": [
            "lem-geometric-separation-of-partition"
          ],
          "derived_statement": "\u03bc_{d'}(H_k) \u2265 \u03bc_{d'}(L_k) + \u03ba_{d', mean}"
        },
        {
          "order": 2.0,
          "kind": "lemma-application",
          "text": "Apply the log-gap lower bound lemma with X ~ d'|H_k, Y ~ d'|L_k, \u03ba = \u03ba_{d', mean}, V_max = g_{A,max} + \u03b7.",
          "latex": null,
          "references": [
            "lem-log-gap-lower-bound"
          ],
          "derived_statement": "E[ln(d')|H_k] - E[ln(d')|L_k] \u2265 ln(1 + \u03ba_{d', mean}/(g_{A,max} + \u03b7)) > 0"
        }
      ],
      "key_equations": [
        {
          "label": "eq-mean-separation",
          "latex": "|\\mu_{d'}(H_k) - \\mu_{d'}(L_k)| \\ge \\kappa_{d', \\text{mean}} > 0",
          "role": "Guaranteed mean separation"
        },
        {
          "label": "eq-directional-mean",
          "latex": "\\mu_{d'}(H_k) \\ge \\mu_{d'}(L_k) + \\kappa_{d', \\text{mean}}",
          "role": "Directional mean separation"
        },
        {
          "label": "eq-log-separation",
          "latex": "\\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right)",
          "role": "Final logarithmic separation bound"
        }
      ],
      "references": [
        "lem-geometric-separation-of-partition",
        "lem-variance-to-mean-separation",
        "lem-log-gap-lower-bound"
      ],
      "math_tools": [
        {
          "toolName": "Variance-to-Mean Separation",
          "field": "Statistics",
          "description": "A lemma that bounds the separation between population means given a minimum variance and signal-to-noise conditions.",
          "roleInProof": "Used to establish a guaranteed positive mean separation in rescaled diversity values between high-error and low-error sets.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Signal-to-Noise Condition"
          ]
        },
        {
          "toolName": "Log-Gap Lower Bound",
          "field": "Analysis",
          "description": "A lemma providing a lower bound on the difference of expected logarithms of random variables with separated means and bounded support.",
          "roleInProof": "Applied to derive a positive lower bound on the expected log-difference from the mean separation.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Jensen's Inequality"
          ]
        },
        {
          "toolName": "N-Uniform Distribution",
          "field": "Probability",
          "description": "A distribution assumption ensuring population fractions are bounded away from zero.",
          "roleInProof": "Supports the application of the variance-to-mean separation lemma by guaranteeing non-degenerate population sizes.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Population Fractions"
          ]
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "directionality",
          "text": "The inequality direction is preserved due to the monotonicity of the rescaling function and the geometric isolation of high-error walkers."
        },
        {
          "type": "positivity",
          "text": "Since \u03ba_{d', mean} > 0, the logarithmic lower bound is strictly positive."
        }
      ],
      "gaps": [],
      "tags": [
        "variance-separation",
        "mean-separation",
        "logarithmic-gap",
        "corrective-signal",
        "population-fractions",
        "signal-to-noise"
      ],
      "document_id": "03_cloning",
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "span": {
        "start_line": 4309,
        "end_line": 4336,
        "content_start": 4312,
        "content_end": 4335,
        "header_lines": [
          4310
        ]
      },
      "metadata": {
        "label": "proof-prop-corrective-signal-bound"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      },
      "generated_at": "2025-11-10T13:26:08.165093+00:00",
      "alt_labels": []
    },
    "tags": [
      "swarm optimization",
      "diversity signal",
      "high-error regime",
      "logarithmic gap",
      "signal-to-noise",
      "corrective bound"
    ],
    "content_markdown": ":label: prop-corrective-signal-bound\n\nLet a swarm state be in the high-error regime, such that the variance of its rescaled diversity values, `d'`, is bounded below, $\\operatorname{Var}(d') \\ge \\kappa_{d', \\text{var}} > 0$. Let the system parameters be chosen such that the Signal-to-Noise Condition of [](#lem-variance-to-mean-separation) is satisfied, i.e., $\\kappa_{d', \\text{var}} > \\operatorname{Var}_{\\max}(d')$.\n\nThen the expected logarithmic gap in the diversity signal between the high-error population $H_k$ and the low-error population $L_k$ is bounded below by a strictly positive, N-uniform constant:\n\n$$\n\\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right) > 0\n$$",
    "raw_directive": "4296: This proposition forges the complete link from a macroscopic state of high geometric error to a guaranteed, non-vanishing corrective signal in the logarithmic space of the fitness potential.\n4297: \n4298: :::{prf:proposition} **(Lower Bound on the Corrective Diversity Signal)**\n4299: :label: prop-corrective-signal-bound\n4300: \n4301: Let a swarm state be in the high-error regime, such that the variance of its rescaled diversity values, `d'`, is bounded below, $\\operatorname{Var}(d') \\ge \\kappa_{d', \\text{var}} > 0$. Let the system parameters be chosen such that the Signal-to-Noise Condition of [](#lem-variance-to-mean-separation) is satisfied, i.e., $\\kappa_{d', \\text{var}} > \\operatorname{Var}_{\\max}(d')$.\n4302: \n4303: Then the expected logarithmic gap in the diversity signal between the high-error population $H_k$ and the low-error population $L_k$ is bounded below by a strictly positive, N-uniform constant:\n4304: \n4305: $$\n4306: \\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right) > 0\n4307: $$\n4308: ",
    "document_id": "03_cloning",
    "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
    "span": {
      "start_line": 4296,
      "end_line": 4308,
      "content_start": 4299,
      "content_end": 4307,
      "header_lines": [
        4297
      ]
    },
    "references": [
      "lem-geometric-separation-of-partition",
      "lem-variance-to-mean-separation",
      "lem-log-gap-lower-bound"
    ],
    "metadata": {
      "label": "prop-corrective-signal-bound"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 7,
      "chapter_file": "chapter_7.json",
      "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
    },
    "generated_at": "2025-11-10T13:26:08.168931+00:00",
    "alt_labels": []
  },
  {
    "label": "prop-adversarial-signal-bound-naive",
    "title": "Worst-Case Upper Bound on the Adversarial Reward Signal",
    "type": "proposition",
    "nl_statement": "For any swarm state, the maximum possible expected logarithmic gap in the rescaled reward signal $r'$ between the low-error and high-error populations is uniformly bounded above by $\\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)$.",
    "equations": [
      {
        "label": null,
        "latex": "\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)"
      }
    ],
    "hypotheses": [
      {
        "text": "For any swarm state",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "The maximum possible expected logarithmic gap in the rescaled reward signal $r'$ between the low-error and high-error populations is uniformly bounded above by a constant derived only from the rescale function's range: $\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)$",
      "latex": "\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)"
    },
    "variables": [
      {
        "symbol": "r'",
        "name": "r'",
        "description": "Rescaled reward signal",
        "constraints": [],
        "tags": [
          "reward"
        ]
      },
      {
        "symbol": "L_k",
        "name": "L_k",
        "description": "Low-error population at step k",
        "constraints": [],
        "tags": [
          "population",
          "low-error"
        ]
      },
      {
        "symbol": "H_k",
        "name": "H_k",
        "description": "High-error population at step k",
        "constraints": [],
        "tags": [
          "population",
          "high-error"
        ]
      },
      {
        "symbol": "g_{A,\\max}",
        "name": "g_{A,max}",
        "description": "Maximum value of the adversarial gain function",
        "constraints": [
          "Maximum in rescale range"
        ],
        "tags": [
          "adversarial",
          "gain"
        ]
      },
      {
        "symbol": "\\eta",
        "name": "\u03b7",
        "description": "Parameter in rescale function",
        "constraints": [
          "Positive scalar"
        ],
        "tags": [
          "rescale",
          "parameter"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The rescale function has a defined range allowing $g_{A,\\max}$ and $\\eta$ as parameters",
        "confidence": 0.9
      },
      {
        "text": "Expectations are well-defined over low-error and high-error populations",
        "confidence": 0.8
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-prop-adversarial-signal-bound-naive",
      "title": null,
      "type": "proof",
      "proves": "prop-adversarial-signal-bound-naive",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-prop-adversarial-signal-bound-naive\n\n**Proof.**\n\nThe proof finds the maximum possible separation by considering the most extreme allowable configuration of mean rewards, unconstrained by any landscape regularity.\n\n**1. Bounding the Maximum Possible Mean Separation:**\nThe rescaled reward values $r'$ are contained in the interval $[\\eta, g_{A,\\max}+\\eta]$. The mean reward for any subpopulation, e.g., $\\mu_{r'}(L_k)$, must also lie within this interval. The absolute difference between the means of any two subpopulations is therefore bounded by the total width of this interval:\n\n$$\n|\\mu_{r'}(L_k) - \\mu_{r'}(H_k)| \\le (g_{A,\\max}+\\eta) - \\eta = g_{A,\\max}\n$$\n\nWe define the maximum possible mean separation as $\\kappa_{r', \\text{mean, max}} := g_{A,\\max}$. This represents the most adversarial scenario, where the low-error set $L_k$ achieves the maximum possible mean reward ($g_{A,\\max} + \\eta$) and the high-error set $H_k$ achieves the minimum possible mean reward ($\\eta$), maximizing the gap between them.\n\n**2. From Mean Separation to Logarithmic Mean Separation:**\nWe now seek an upper bound for the expression $\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]$. We apply [](#lem-log-gap-upper-bound). Let $X$ represent the distribution of $r'$ in $L_k$ and $Y$ represent the distribution in $H_k$. We use the maximum possible mean separation $\\kappa = \\kappa_{r', \\text{mean, max}}$ and note that the minimum value for any $r'$ is $V_{\\min} = \\eta$.\nThe lemma gives the stated result directly:\n\n$$\n|\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]| \\le \\ln\\left(1 + \\frac{\\kappa_{r', \\text{mean, max}}}{\\eta}\\right) = \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)\n$$\n\nThis provides a uniform upper bound on the magnitude of the adversarial signal under the weakest possible assumptions.",
      "raw_directive": "4352: \n4353: :::\n4354: :::{prf:proof}\n4355: :label: proof-prop-adversarial-signal-bound-naive\n4356: \n4357: **Proof.**\n4358: \n4359: The proof finds the maximum possible separation by considering the most extreme allowable configuration of mean rewards, unconstrained by any landscape regularity.\n4360: \n4361: **1. Bounding the Maximum Possible Mean Separation:**\n4362: The rescaled reward values $r'$ are contained in the interval $[\\eta, g_{A,\\max}+\\eta]$. The mean reward for any subpopulation, e.g., $\\mu_{r'}(L_k)$, must also lie within this interval. The absolute difference between the means of any two subpopulations is therefore bounded by the total width of this interval:\n4363: \n4364: $$\n4365: |\\mu_{r'}(L_k) - \\mu_{r'}(H_k)| \\le (g_{A,\\max}+\\eta) - \\eta = g_{A,\\max}\n4366: $$\n4367: \n4368: We define the maximum possible mean separation as $\\kappa_{r', \\text{mean, max}} := g_{A,\\max}$. This represents the most adversarial scenario, where the low-error set $L_k$ achieves the maximum possible mean reward ($g_{A,\\max} + \\eta$) and the high-error set $H_k$ achieves the minimum possible mean reward ($\\eta$), maximizing the gap between them.\n4369: \n4370: **2. From Mean Separation to Logarithmic Mean Separation:**\n4371: We now seek an upper bound for the expression $\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]$. We apply [](#lem-log-gap-upper-bound). Let $X$ represent the distribution of $r'$ in $L_k$ and $Y$ represent the distribution in $H_k$. We use the maximum possible mean separation $\\kappa = \\kappa_{r', \\text{mean, max}}$ and note that the minimum value for any $r'$ is $V_{\\min} = \\eta$.\n4372: The lemma gives the stated result directly:\n4373: \n4374: $$\n4375: |\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]| \\le \\ln\\left(1 + \\frac{\\kappa_{r', \\text{mean, max}}}{\\eta}\\right) = \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)\n4376: $$\n4377: \n4378: This provides a uniform upper bound on the magnitude of the adversarial signal under the weakest possible assumptions.\n4379: ",
      "strategy_summary": "The proof derives an upper bound on the adversarial signal by first establishing the maximum possible mean separation of rescaled rewards between subpopulations, then applying a lemma to convert this into a bound on the difference of expected logarithms.",
      "conclusion": {
        "text": "|\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]| \\le \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)",
        "latex": "|\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]| \\le \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)"
      },
      "assumptions": [
        {
          "text": "Rescaled reward values r' are contained in the interval [\\eta, g_{A,\\max} + \\eta]",
          "latex": "r' \\in [\\eta, g_{A,\\max} + \\eta]"
        },
        {
          "text": "Mean rewards of subpopulations lie within the support interval of r'",
          "latex": "\\mu_{r'}(L_k), \\mu_{r'}(H_k) \\in [\\eta, g_{A,\\max} + \\eta]"
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "bounding",
          "text": "Bound the maximum possible mean separation between subpopulations L_k and H_k. Since means lie within the interval [\\eta, g_{A,\\max} + \\eta], the difference is at most g_{A,\\max}. Define \\kappa_{r', \\text{mean, max}} := g_{A,\\max}.",
          "latex": "|\\mu_{r'}(L_k) - \\mu_{r'}(H_k)| \\le g_{A,\\max}",
          "references": [],
          "derived_statement": "\\kappa_{r', \\text{mean, max}} := g_{A,\\max}"
        },
        {
          "order": 2.0,
          "kind": "application",
          "text": "Apply the log-gap upper bound lemma to the distributions of r' in L_k and H_k, using \\kappa = \\kappa_{r', \\text{mean, max}} and minimum value V_{\\min} = \\eta, yielding the bound on the logarithmic mean separation.",
          "latex": "|\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]| \\le \\ln\\left(1 + \\frac{\\kappa_{r', \\text{mean, max}}}{\\eta}\\right) = \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)",
          "references": [
            "lem-log-gap-upper-bound"
          ],
          "derived_statement": null
        }
      ],
      "key_equations": [
        {
          "label": "eq-mean-separation",
          "latex": "|\\mu_{r'}(L_k) - \\mu_{r'}(H_k)| \\le g_{A,\\max}",
          "role": "Bounds the maximum mean difference"
        },
        {
          "label": "eq-log-separation",
          "latex": "|\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]| \\le \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)",
          "role": "Final upper bound on adversarial signal"
        }
      ],
      "references": [
        "lem-log-gap-upper-bound"
      ],
      "math_tools": [
        {
          "toolName": "Bound on mean separation",
          "field": "Probability",
          "description": "The difference in means of random variables supported on a bounded interval is at most the length of the interval.",
          "roleInProof": "Used to bound the maximum possible difference in expected rescaled rewards between low- and high-error sets.",
          "levelOfAbstraction": "Technique",
          "relatedTools": []
        },
        {
          "toolName": "Log-gap upper bound lemma",
          "field": "Probability",
          "description": "An upper bound on the difference of expected logarithms of two distributions based on their mean difference and the minimum support value.",
          "roleInProof": "Applied to obtain the logarithmic mean separation bound from the mean separation and minimum reward value.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Jensen's inequality"
          ]
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "explanation",
          "text": "This bound holds under the weakest assumptions, considering the most adversarial configuration of mean rewards without landscape regularity constraints."
        }
      ],
      "gaps": [],
      "tags": [
        "adversarial signal",
        "mean separation",
        "logarithmic bound",
        "upper bound",
        "rescaled rewards",
        "expectation",
        "proof"
      ],
      "document_id": "03_cloning",
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "span": {
        "start_line": 4352,
        "end_line": 4379,
        "content_start": 4355,
        "content_end": 4378,
        "header_lines": [
          4353
        ]
      },
      "metadata": {
        "label": "proof-prop-adversarial-signal-bound-naive"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      },
      "generated_at": "2025-11-10T13:26:08.165093+00:00",
      "alt_labels": []
    },
    "tags": [
      "adversarial",
      "reward-signal",
      "upper-bound",
      "naive",
      "stability",
      "swarm-state",
      "rescale-function"
    ],
    "content_markdown": ":label: prop-adversarial-signal-bound-naive\n\nFor any swarm state, the maximum possible expected logarithmic gap in the rescaled reward signal, $r'$, between the low-error and high-error populations is uniformly bounded above by a constant derived only from the rescale function's range:\n\n$$\n\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)",
    "raw_directive": "4342: Before deriving the final stability condition, it is instructive to first establish a \"naive\" upper bound on the adversarial reward signal. This bound considers the absolute worst-case scenario allowed by the range of the rescale function, without yet invoking the axioms that constrain the reward landscape's structure. This will serve as a baseline to demonstrate the critical importance of those axioms.\n4343: \n4344: :::{prf:proposition} **(Worst-Case Upper Bound on the Adversarial Reward Signal)**\n4345: :label: prop-adversarial-signal-bound-naive\n4346: \n4347: For any swarm state, the maximum possible expected logarithmic gap in the rescaled reward signal, $r'$, between the low-error and high-error populations is uniformly bounded above by a constant derived only from the rescale function's range:\n4348: \n4349: $$\n4350: \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)\n4351: $$",
    "document_id": "03_cloning",
    "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
    "span": {
      "start_line": 4342,
      "end_line": 4351,
      "content_start": 4345,
      "content_end": 4350,
      "header_lines": [
        4343
      ]
    },
    "references": [
      "lem-log-gap-upper-bound"
    ],
    "metadata": {
      "label": "prop-adversarial-signal-bound-naive"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 7,
      "chapter_file": "chapter_7.json",
      "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
    },
    "generated_at": "2025-11-10T13:26:08.168931+00:00",
    "alt_labels": []
  },
  {
    "label": "prop-raw-reward-mean-gap-bound",
    "title": "Lipschitz Bound on the Raw Reward Mean Gap",
    "type": "proposition",
    "nl_statement": "The absolute difference between the mean raw rewards of the high-error population H_k and low-error population L_k is bounded by the product of the Lipschitz constant L_R of the positional reward component and the diameter D_valid of the valid domain X_valid.",
    "equations": [
      {
        "label": null,
        "latex": "|\\mu_R(L_k) - \\mu_R(H_k)| \\le L_{R} \\cdot D_{\\mathrm{valid}} =: \\kappa_{\\mathrm{raw},r,\\text{adv}}"
      }
    ],
    "hypotheses": [
      {
        "text": "The reward function's positional component R_pos(x) is Lipschitz continuous on the valid domain X_valid with constant L_R, as per the Axiom of Reward Regularity.",
        "latex": null
      },
      {
        "text": "The diameter of X_valid is D_valid.",
        "latex": null
      },
      {
        "text": "For any swarm, considering high-error population H_k and low-error population L_k.",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "The absolute difference between the mean raw rewards is uniformly bounded: |\u03bc_R(L_k) - \u03bc_R(H_k)| \u2264 L_R \u00b7 D_valid =: \u03ba_raw,r,adv",
      "latex": "|\\mu_R(L_k) - \\mu_R(H_k)| \\le L_{R} \\cdot D_{\\mathrm{valid}} =: \\kappa_{\\mathrm{raw},r,\\text{adv}}"
    },
    "variables": [
      {
        "symbol": "R_pos",
        "name": "positional reward component",
        "description": "Lipschitz continuous function on X_valid",
        "constraints": [
          "Lipschitz with constant L_R"
        ],
        "tags": [
          "reward",
          "positional"
        ]
      },
      {
        "symbol": "L_R",
        "name": "Lipschitz constant",
        "description": "Constant for Lipschitz continuity of R_pos",
        "constraints": [
          "positive real"
        ],
        "tags": [
          "Lipschitz"
        ]
      },
      {
        "symbol": "X_valid",
        "name": "valid domain",
        "description": "Domain where R_pos is defined and Lipschitz",
        "constraints": [
          "compact",
          "diameter D_valid"
        ],
        "tags": [
          "domain"
        ]
      },
      {
        "symbol": "D_valid",
        "name": "domain diameter",
        "description": "Diameter of X_valid",
        "constraints": [
          "non-negative real"
        ],
        "tags": [
          "diameter"
        ]
      },
      {
        "symbol": "H_k",
        "name": "high-error population",
        "description": "Subset of swarm with high error at iteration k",
        "constraints": [
          "subset of swarm"
        ],
        "tags": [
          "population",
          "high-error"
        ]
      },
      {
        "symbol": "L_k",
        "name": "low-error population",
        "description": "Subset of swarm with low error at iteration k",
        "constraints": [
          "subset of swarm"
        ],
        "tags": [
          "population",
          "low-error"
        ]
      },
      {
        "symbol": "\u03bc_R",
        "name": "mean raw reward",
        "description": "Expected value of raw reward over a population",
        "constraints": [],
        "tags": [
          "mean",
          "reward"
        ]
      },
      {
        "symbol": "\u03ba_raw,r,adv",
        "name": "raw reward mean gap bound",
        "description": "Defined bound L_R \u00b7 D_valid",
        "constraints": [
          "non-negative real"
        ],
        "tags": [
          "bound",
          "gap"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "Populations H_k and L_k are non-empty subsets of the swarm within X_valid.",
        "confidence": 0.9
      },
      {
        "text": "Raw reward R is composed of positional component R_pos, inheriting its Lipschitz property.",
        "confidence": 0.8
      },
      {
        "text": "The swarm operates within the valid domain X_valid.",
        "confidence": 1.0
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-prop-raw-reward-mean-gap-bound",
      "title": null,
      "type": "proof",
      "proves": "prop-raw-reward-mean-gap-bound",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-prop-raw-reward-mean-gap-bound\n\n**Proof.**\nThe mean reward difference is $|\\frac{1}{|L_k|}\\sum_{l \\in L_k} R(x_l) - \\frac{1}{|H_k|}\\sum_{h \\in H_k} R(x_h)|$. This can be rewritten as the average difference over all pairs: $\\frac{1}{|L_k||H_k|} |\\sum_{l,h} (R(x_l) - R(x_h))|$.\n\nBy the triangle inequality and the Lipschitz property:\n\n$$\n|\\sum_{l,h} (R(x_l) - R(x_h))| \\le \\sum_{l,h} |R(x_l) - R(x_h)| \\le \\sum_{l,h} L_{R} \\cdot d(x_l, x_h)\n$$\n\nThe distance between any two points $x_l, x_h$ in the valid domain is bounded by its diameter, $D_{\\mathrm{valid}}$. There are $|L_k||H_k|$ pairs in the sum.\n\n$$\n\\le \\sum_{l,h} L_{R} \\cdot D_{\\mathrm{valid}} = |L_k||H_k| \\cdot L_{R} \\cdot D_{\\mathrm{valid}}\n$$\n\nDividing by $|L_k||H_k|$ gives the final bound. This maximum possible raw reward gap, $\\kappa_{\\mathrm{raw},r,\\text{adv}}$, represents the tightest axiom-based constraint on how deceptive the landscape can be.",
      "raw_directive": "4417: \n4418: :::\n4419: :::{prf:proof}\n4420: :label: proof-prop-raw-reward-mean-gap-bound\n4421: \n4422: **Proof.**\n4423: The mean reward difference is $|\\frac{1}{|L_k|}\\sum_{l \\in L_k} R(x_l) - \\frac{1}{|H_k|}\\sum_{h \\in H_k} R(x_h)|$. This can be rewritten as the average difference over all pairs: $\\frac{1}{|L_k||H_k|} |\\sum_{l,h} (R(x_l) - R(x_h))|$.\n4424: \n4425: By the triangle inequality and the Lipschitz property:\n4426: \n4427: $$\n4428: |\\sum_{l,h} (R(x_l) - R(x_h))| \\le \\sum_{l,h} |R(x_l) - R(x_h)| \\le \\sum_{l,h} L_{R} \\cdot d(x_l, x_h)\n4429: $$\n4430: \n4431: The distance between any two points $x_l, x_h$ in the valid domain is bounded by its diameter, $D_{\\mathrm{valid}}$. There are $|L_k||H_k|$ pairs in the sum.\n4432: \n4433: $$\n4434: \\le \\sum_{l,h} L_{R} \\cdot D_{\\mathrm{valid}} = |L_k||H_k| \\cdot L_{R} \\cdot D_{\\mathrm{valid}}\n4435: $$\n4436: \n4437: Dividing by $|L_k||H_k|$ gives the final bound. This maximum possible raw reward gap, $\\kappa_{\\mathrm{raw},r,\\text{adv}}$, represents the tightest axiom-based constraint on how deceptive the landscape can be.\n4438: ",
      "strategy_summary": "The proof rewrites the mean reward difference as an average over pairwise differences, applies the triangle inequality to bound the absolute sum by the sum of absolutes, uses the Lipschitz property to relate reward differences to distances, and further bounds distances by the domain diameter to obtain the final upper bound.",
      "conclusion": {
        "text": "The mean reward difference is bounded by L_R \\cdot D_{\\mathrm{valid}}, denoted as \\kappa_{\\mathrm{raw},r,\\text{adv}}.",
        "latex": "\\left| \\frac{1}{|L_k|} \\sum_{l \\in L_k} R(x_l) - \\frac{1}{|H_k|} \\sum_{h \\in H_k} R(x_h) \\right| \\leq L_R D_{\\mathrm{valid}} = \\kappa_{\\mathrm{raw},r,\\text{adv}}"
      },
      "assumptions": [
        {
          "text": "The reward function R is L_R-Lipschitz continuous.",
          "latex": "|R(x) - R(y)| \\leq L_R d(x, y)"
        },
        {
          "text": "The valid domain has finite diameter D_{\\mathrm{valid}}.",
          "latex": "D_{\\mathrm{valid}} = \\sup_{x,y \\in \\mathrm{valid}} d(x,y) < \\infty"
        }
      ],
      "steps": [],
      "key_equations": [
        {
          "label": "eq-mean-reward-diff",
          "latex": "\\left| \\frac{1}{|L_k|} \\sum_{l \\in L_k} R(x_l) - \\frac{1}{|H_k|} \\sum_{h \\in H_k} R(x_h) \\right| = \\frac{1}{|L_k| |H_k|} \\left| \\sum_{l,h} (R(x_l) - R(x_h)) \\right|",
          "role": "initial-expression"
        },
        {
          "label": "eq-triangle-bound",
          "latex": "\\left| \\sum_{l,h} (R(x_l) - R(x_h)) \\right| \\leq \\sum_{l,h} |R(x_l) - R(x_h)|",
          "role": "triangle-inequality"
        },
        {
          "label": "eq-lipschitz-bound",
          "latex": "\\sum_{l,h} |R(x_l) - R(x_h)| \\leq L_R \\sum_{l,h} d(x_l, x_h)",
          "role": "lipschitz-application"
        },
        {
          "label": "eq-diameter-bound",
          "latex": "L_R \\sum_{l,h} d(x_l, x_h) \\leq |L_k| |H_k| L_R D_{\\mathrm{valid}}",
          "role": "diameter-bound"
        },
        {
          "label": "eq-final-gap",
          "latex": "\\left| \\frac{1}{|L_k|} \\sum_{l} R(x_l) - \\frac{1}{|H_k|} \\sum_{h} R(x_h) \\right| \\leq L_R D_{\\mathrm{valid}} = \\kappa_{\\mathrm{raw},r,\\text{adv}}",
          "role": "conclusion"
        }
      ],
      "references": [],
      "math_tools": [
        {
          "toolName": "Triangle Inequality",
          "field": "Analysis",
          "description": "For any numbers a_i, |\\sum a_i| \\leq \\sum |a_i|.",
          "roleInProof": "Bounds the absolute value of the sum of reward differences by the sum of their absolute values.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Lipschitz Continuity"
          ]
        },
        {
          "toolName": "Lipschitz Continuity",
          "field": "Analysis",
          "description": "A function f satisfies |f(x) - f(y)| \\leq L d(x, y) for some constant L and distance d.",
          "roleInProof": "Bounds each individual |R(x_l) - R(x_h)| by L_R times the distance d(x_l, x_h).",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Triangle Inequality"
          ]
        },
        {
          "toolName": "Diameter of a Metric Space",
          "field": "Metric Spaces",
          "description": "The diameter D of a set is the supremum of distances d(x, y) for x, y in the set.",
          "roleInProof": "Provides an upper bound D_{\\mathrm{valid}} for all pairwise distances d(x_l, x_h) in the valid domain.",
          "levelOfAbstraction": "Concept",
          "relatedTools": []
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "interpretation",
          "text": "This maximum possible raw reward gap, \\kappa_{\\mathrm{raw},r,\\text{adv}}, represents the tightest axiom-based constraint on how deceptive the landscape can be."
        }
      ],
      "gaps": [],
      "tags": [
        "reward-gap",
        "lipschitz-continuity",
        "triangle-inequality",
        "diameter-bound",
        "mean-difference"
      ],
      "document_id": "03_cloning",
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "span": {
        "start_line": 4417,
        "end_line": 4438,
        "content_start": 4420,
        "content_end": 4437,
        "header_lines": [
          4418
        ]
      },
      "metadata": {
        "label": "proof-prop-raw-reward-mean-gap-bound"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      },
      "generated_at": "2025-11-10T13:26:08.165093+00:00",
      "alt_labels": []
    },
    "tags": [
      "Lipschitz",
      "reward",
      "mean gap",
      "bound",
      "swarm",
      "population",
      "domain diameter"
    ],
    "content_markdown": ":label: prop-raw-reward-mean-gap-bound\n\nLet the reward function's positional component, $R_{\\text{pos}}(x)$, be Lipschitz continuous on the valid domain $\\mathcal{X}_{\\text{valid}}$ with constant $L_{R}$, as per the **Axiom of Reward Regularity**. Let the diameter of $\\mathcal{X}_{\\text{valid}}$ be $D_{\\text{valid}}$.\n\nFor any swarm, the absolute difference between the mean raw rewards of the high-error population $H_k$ and the low-error population $L_k$ is uniformly bounded:\n\n$$\n|\\mu_R(L_k) - \\mu_R(H_k)| \\le L_{R} \\cdot D_{\\mathrm{valid}} =: \\kappa_{\\mathrm{raw},r,\\text{adv}}",
    "raw_directive": "4405: The following propositions build a chain of reasoning from the Lipschitz continuity of the raw reward function to a final, tight bound on the expected logarithmic gap.\n4406: \n4407: :::{prf:proposition} **(Lipschitz Bound on the Raw Reward Mean Gap)**\n4408: :label: prop-raw-reward-mean-gap-bound\n4409: \n4410: Let the reward function's positional component, $R_{\\text{pos}}(x)$, be Lipschitz continuous on the valid domain $\\mathcal{X}_{\\text{valid}}$ with constant $L_{R}$, as per the **Axiom of Reward Regularity**. Let the diameter of $\\mathcal{X}_{\\text{valid}}$ be $D_{\\text{valid}}$.\n4411: \n4412: For any swarm, the absolute difference between the mean raw rewards of the high-error population $H_k$ and the low-error population $L_k$ is uniformly bounded:\n4413: \n4414: $$\n4415: |\\mu_R(L_k) - \\mu_R(H_k)| \\le L_{R} \\cdot D_{\\mathrm{valid}} =: \\kappa_{\\mathrm{raw},r,\\text{adv}}\n4416: $$",
    "document_id": "03_cloning",
    "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
    "span": {
      "start_line": 4405,
      "end_line": 4416,
      "content_start": 4408,
      "content_end": 4415,
      "header_lines": [
        4406
      ]
    },
    "references": [],
    "metadata": {
      "label": "prop-raw-reward-mean-gap-bound"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 7,
      "chapter_file": "chapter_7.json",
      "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
    },
    "generated_at": "2025-11-10T13:26:08.168931+00:00",
    "alt_labels": []
  },
  {
    "label": "prop-log-reward-gap-axiom-bound",
    "title": "Axiom-Based Bound on the Logarithmic Reward Gap",
    "type": "proposition",
    "nl_statement": "Under the Axiom of Reward Regularity, the expected logarithmic gap in the rescaled reward signal is bounded above by the logarithm of one plus the rescaled condition number divided by eta.",
    "equations": [
      {
        "label": null,
        "latex": "\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R \\cdot D_{\\mathrm{valid}})}{\\eta}\\right)"
      }
    ],
    "hypotheses": [
      {
        "text": "Axiom of Reward Regularity holds.",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "The expected logarithmic gap in the rescaled reward signal is bounded.",
      "latex": "\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R \\cdot D_{\\mathrm{valid}})}{\\eta}\\right)"
    },
    "variables": [
      {
        "symbol": "r'",
        "name": "rescaled reward",
        "description": "Rescaled reward signal.",
        "constraints": [],
        "tags": [
          "reward"
        ]
      },
      {
        "symbol": "L_k",
        "name": "low condition set",
        "description": "Low condition indicator at step k.",
        "constraints": [],
        "tags": [
          "condition",
          "low"
        ]
      },
      {
        "symbol": "H_k",
        "name": "high condition set",
        "description": "High condition indicator at step k.",
        "constraints": [],
        "tags": [
          "condition",
          "high"
        ]
      },
      {
        "symbol": "\\kappa_{\\mathrm{rescaled}}",
        "name": "rescaled condition number",
        "description": "Condition number for rescaled parameters.",
        "constraints": [],
        "tags": [
          "condition",
          "number"
        ]
      },
      {
        "symbol": "L_R",
        "name": "reward Lipschitz constant",
        "description": "Lipschitz constant for rewards.",
        "constraints": [],
        "tags": [
          "Lipschitz",
          "reward"
        ]
      },
      {
        "symbol": "D_{\\mathrm{valid}}",
        "name": "valid dataset diameter",
        "description": "Diameter of the valid dataset.",
        "constraints": [],
        "tags": [
          "dataset",
          "diameter"
        ]
      },
      {
        "symbol": "\\eta",
        "name": "learning rate",
        "description": "Step size or regularization parameter.",
        "constraints": [],
        "tags": [
          "rate",
          "eta"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The rescaled reward signal is positive to ensure logarithms are defined.",
        "confidence": 0.8
      },
      {
        "text": "The condition number and parameters are well-defined and positive.",
        "confidence": 0.9
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-prop-log-reward-gap-axiom-bound",
      "title": null,
      "type": "proof",
      "proves": "prop-log-reward-gap-axiom-bound",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-prop-log-reward-gap-axiom-bound\n\n**Proof.**\n\nThe proof proceeds in three direct steps. First, we establish a uniform upper bound on the maximum possible *microscopic* gap between any two rescaled reward values, using the Lipschitz axiom. Second, we argue that the gap between the *means* of any two subpopulations cannot exceed this maximum microscopic gap. Finally, we apply the upper-bound lemma for logarithmic gaps to this bounded mean separation.\n\n**1. Bounding the Maximum Microscopic Rescaled Gap.**\nLet $r'_a$ and $r'_b$ be the rescaled reward values for any two walkers $a$ and $b$. We seek an upper bound for $|r'_a - r'_b|$.\n\n$$\n|r'_a - r'_b| = |g_A(z_a) - g_A(z_b)|\n$$\n\nSince the rescale function $g_A$ is Lipschitz with constant $L_g$ (its maximum derivative), we have:\n\n$$\n|r'_a - r'_b| \\le L_g |z_a - z_b| = L_g \\left| \\frac{R_a - \\mu_R}{\\sigma'_R} - \\frac{R_b - \\mu_R}{\\sigma'_R} \\right| = \\frac{L_g}{\\sigma'_R} |R_a - R_b|\n$$\n\nThe patched standard deviation (see {prf:ref}`def-patched-std-dev-function`) $\\sigma'_R$ is uniformly bounded below by $\\sigma'_{\\min,\\text{patch}} > 0$. The raw reward gap $|R_a - R_b|$ is bounded by the Lipschitz property: $|R_a - R_b| \\le L_R D_{\\text{valid}}$. Combining these gives a uniform upper bound on the microscopic rescaled gap:\n\n$$\n|r'_a - r'_b| \\le \\frac{L_g}{\\sigma'_{\\min,\\text{patch}}} (L_R D_{\\text{valid}}) = \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})\n$$\n\nThis is precisely the result of applying the signal propagation function $\\kappa_{\\mathrm{rescaled}}$ to the maximum possible raw reward gap.\n\n**2. Bounding the Macroscopic Mean Separation.**\nThe absolute difference between the mean rescaled rewards of the low-error and high-error sets, $|\\mu_{r'}(L_k) - \\mu_{r'}(H_k)|$, is a weighted average of the differences between all cross-set pairs. As such, it cannot be larger than the maximum possible difference between any single pair. Therefore:\n\n$$\n|\\mu_{r'}(L_k) - \\mu_{r'}(H_k)| \\le \\max_{a,b} |r'_a - r'_b| \\le \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})\n$$\n\nWe define this upper bound on the mean separation as $\\kappa_{r',\\text{mean,adv}} := \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})$.\n\n**3. From Mean Separation to Logarithmic Mean Separation.**\nWe now have a valid upper bound on the mean separation, which is the required premise for [](#lem-log-gap-upper-bound). We apply this lemma with:\n*   $X$ representing the distribution of $r'$ in $L_k$.\n*   $Y$ representing the distribution of $r'$ in $H_k$.\n*   $\\kappa = \\kappa_{r',\\text{mean,adv}}$.\n*   $V_{\\min} = \\eta$ (the minimum value for any rescaled value $r'$).\n\nThe lemma directly yields the stated result:\n\n$$\n|\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]| \\le \\ln\\left(1 + \\frac{\\kappa_{r',\\text{mean,adv}}}{\\eta}\\right) = \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})}{\\eta}\\right)\n$$\n\nSince we are interested in the one-sided difference, this bound also holds for $\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]$.",
      "raw_directive": "4453: where $\\kappa_{\\mathrm{rescaled}}(\\cdot)$ is the signal propagation function.\n4454: :::\n4455: :::{prf:proof}\n4456: :label: proof-prop-log-reward-gap-axiom-bound\n4457: \n4458: **Proof.**\n4459: \n4460: The proof proceeds in three direct steps. First, we establish a uniform upper bound on the maximum possible *microscopic* gap between any two rescaled reward values, using the Lipschitz axiom. Second, we argue that the gap between the *means* of any two subpopulations cannot exceed this maximum microscopic gap. Finally, we apply the upper-bound lemma for logarithmic gaps to this bounded mean separation.\n4461: \n4462: **1. Bounding the Maximum Microscopic Rescaled Gap.**\n4463: Let $r'_a$ and $r'_b$ be the rescaled reward values for any two walkers $a$ and $b$. We seek an upper bound for $|r'_a - r'_b|$.\n4464: \n4465: $$\n4466: |r'_a - r'_b| = |g_A(z_a) - g_A(z_b)|\n4467: $$\n4468: \n4469: Since the rescale function $g_A$ is Lipschitz with constant $L_g$ (its maximum derivative), we have:\n4470: \n4471: $$\n4472: |r'_a - r'_b| \\le L_g |z_a - z_b| = L_g \\left| \\frac{R_a - \\mu_R}{\\sigma'_R} - \\frac{R_b - \\mu_R}{\\sigma'_R} \\right| = \\frac{L_g}{\\sigma'_R} |R_a - R_b|\n4473: $$\n4474: \n4475: The patched standard deviation (see {prf:ref}`def-patched-std-dev-function`) $\\sigma'_R$ is uniformly bounded below by $\\sigma'_{\\min,\\text{patch}} > 0$. The raw reward gap $|R_a - R_b|$ is bounded by the Lipschitz property: $|R_a - R_b| \\le L_R D_{\\text{valid}}$. Combining these gives a uniform upper bound on the microscopic rescaled gap:\n4476: \n4477: $$\n4478: |r'_a - r'_b| \\le \\frac{L_g}{\\sigma'_{\\min,\\text{patch}}} (L_R D_{\\text{valid}}) = \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})\n4479: $$\n4480: \n4481: This is precisely the result of applying the signal propagation function $\\kappa_{\\mathrm{rescaled}}$ to the maximum possible raw reward gap.\n4482: \n4483: **2. Bounding the Macroscopic Mean Separation.**\n4484: The absolute difference between the mean rescaled rewards of the low-error and high-error sets, $|\\mu_{r'}(L_k) - \\mu_{r'}(H_k)|$, is a weighted average of the differences between all cross-set pairs. As such, it cannot be larger than the maximum possible difference between any single pair. Therefore:\n4485: \n4486: $$\n4487: |\\mu_{r'}(L_k) - \\mu_{r'}(H_k)| \\le \\max_{a,b} |r'_a - r'_b| \\le \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})\n4488: $$\n4489: \n4490: We define this upper bound on the mean separation as $\\kappa_{r',\\text{mean,adv}} := \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})$.\n4491: \n4492: **3. From Mean Separation to Logarithmic Mean Separation.**\n4493: We now have a valid upper bound on the mean separation, which is the required premise for [](#lem-log-gap-upper-bound). We apply this lemma with:\n4494: *   $X$ representing the distribution of $r'$ in $L_k$.\n4495: *   $Y$ representing the distribution of $r'$ in $H_k$.\n4496: *   $\\kappa = \\kappa_{r',\\text{mean,adv}}$.\n4497: *   $V_{\\min} = \\eta$ (the minimum value for any rescaled value $r'$).\n4498: \n4499: The lemma directly yields the stated result:\n4500: \n4501: $$\n4502: |\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]| \\le \\ln\\left(1 + \\frac{\\kappa_{r',\\text{mean,adv}}}{\\eta}\\right) = \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})}{\\eta}\\right)\n4503: $$\n4504: \n4505: Since we are interested in the one-sided difference, this bound also holds for $\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k]$.\n4506: ",
      "strategy_summary": "The proof first derives a uniform bound on the maximum difference between any two rescaled reward values using Lipschitz continuity of the rescaling function and raw reward properties. It then shows that the difference in means between subpopulations is at most this maximum pairwise difference. Finally, it applies a referenced lemma on logarithmic gaps to obtain the bound on the expected log-reward difference.",
      "conclusion": {
        "text": "The one-sided expected logarithmic reward gap is bounded as E[ln(r') | L_k] - E[ln(r') | H_k] \u2264 ln(1 + \u03ba_rescaled(L_R D_valid) / \u03b7).",
        "latex": "$\\mathbb{E}[\\ln(r') \\mid L_k] - \\mathbb{E}[\\ln(r') \\mid H_k] \\le \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})}{\\eta}\\right)$"
      },
      "assumptions": [
        {
          "text": "The rescaling function g_A is Lipschitz continuous with constant L_g (maximum derivative).",
          "latex": null
        },
        {
          "text": "The patched standard deviation \u03c3'_R is bounded below by \u03c3'_{min,patch} > 0.",
          "latex": null
        },
        {
          "text": "Raw rewards satisfy |R_a - R_b| \u2264 L_R D_valid due to Lipschitz property.",
          "latex": null
        },
        {
          "text": "Rescaled rewards r' are bounded below by \u03b7 > 0.",
          "latex": null
        }
      ],
      "steps": [
        {
          "order": 1.0,
          "kind": "bound",
          "text": "Establish a uniform upper bound on the maximum microscopic gap |r'_a - r'_b| between any two rescaled reward values using Lipschitz continuity of g_A, the lower bound on \u03c3'_R, and the bound on raw reward differences.",
          "latex": null,
          "references": [],
          "derived_statement": "|r'_a - r'_b| \u2264 \u03ba_rescaled(L_R D_valid)"
        },
        {
          "order": 2.0,
          "kind": "inequality",
          "text": "Argue that the mean separation |\u03bc_{r'}(L_k) - \u03bc_{r'}(H_k)| between low-error and high-error subpopulations cannot exceed the maximum microscopic gap, as it is a weighted average of pairwise differences.",
          "latex": null,
          "references": [],
          "derived_statement": "|\u03bc_{r'}(L_k) - \u03bc_{r'}(H_k)| \u2264 \u03ba_rescaled(L_R D_valid)"
        },
        {
          "order": 3.0,
          "kind": "application",
          "text": "Apply the lemma for upper-bounding logarithmic gaps to the bounded mean separation, with X as the distribution of r' in L_k, Y in H_k, \u03ba = \u03ba_{r',mean,adv} = \u03ba_rescaled(L_R D_valid), and V_min = \u03b7, yielding the bound on the expected log-reward difference.",
          "latex": null,
          "references": [
            "lem-log-gap-upper-bound"
          ],
          "derived_statement": "|E[ln(r') | L_k] - E[ln(r') | H_k]| \u2264 ln(1 + \u03ba_rescaled(L_R D_valid)/\u03b7)"
        }
      ],
      "key_equations": [
        {
          "label": "eq-micro-gap",
          "latex": "$|r'_a - r'_b| = |g_A(z_a) - g_A(z_b)| \\le L_g |z_a - z_b| = \\frac{L_g}{\\sigma'_R} |R_a - R_b| \\le \\frac{L_g}{\\sigma'_{\\min,\\text{patch}}} (L_R D_{\\text{valid}}) = \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})$",
          "role": "Bounds the maximum microscopic rescaled reward gap."
        },
        {
          "label": "eq-mean-sep",
          "latex": "$|\\mu_{r'}(L_k) - \\mu_{r'}(H_k)| \\le \\max_{a,b} |r'_a - r'_b| \\le \\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})$",
          "role": "Bounds the macroscopic mean separation by the microscopic maximum."
        },
        {
          "label": "eq-log-bound",
          "latex": "$|\\mathbb{E}[\\ln(r') \\mid L_k] - \\mathbb{E}[\\ln(r') \\mid H_k]| \\le \\ln\\left(1 + \\frac{\\kappa_{r',\\text{mean,adv}}}{\\eta}\\right) = \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R D_{\\text{valid}})}{\\eta}\\right)$",
          "role": "Final bound on the logarithmic mean separation using the referenced lemma."
        }
      ],
      "references": [
        "def-patched-std-dev-function",
        "lem-log-gap-upper-bound"
      ],
      "math_tools": [
        {
          "toolName": "Lipschitz Continuity",
          "field": "Real Analysis",
          "description": "A function f is Lipschitz continuous with constant L if |f(x) - f(y)| \u2264 L |x - y| for all x, y, often derived from bounded derivatives.",
          "roleInProof": "Applied to the rescaling function g_A and raw rewards R to bound microscopic differences in rescaled rewards |r'_a - r'_b| in terms of raw reward gaps.",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Bounded Derivative"
          ]
        },
        {
          "toolName": "Expectation of Differences",
          "field": "Probability Theory",
          "description": "The absolute difference between expectations |E[X] - E[Y]| is bounded by the expected absolute difference E[|X - Y|], which in turn is at most the maximum |X - Y| for bounded supports.",
          "roleInProof": "Used to bound the mean rescaled reward separation |\u03bc_{r'}(L_k) - \u03bc_{r'}(H_k)| by the maximum microscopic gap.",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Jensen's Inequality"
          ]
        },
        {
          "toolName": "Logarithmic Gap Bound",
          "field": "Inequality Theory",
          "description": "A lemma providing an upper bound on the difference in expected logarithms of two distributions based on their mean separation and a minimum value.",
          "roleInProof": "Directly applied to bound the difference in expected log-rescaled rewards using the established mean separation.",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Jensen's Inequality",
            "Concavity of Logarithm"
          ]
        }
      ],
      "cases": [],
      "remarks": [
        {
          "type": "note",
          "text": "The bound applies to the one-sided difference E[ln(r') | L_k] - E[ln(r') | H_k] since the absolute value bound implies the one-sided version."
        }
      ],
      "gaps": [],
      "tags": [
        "Lipschitz continuity",
        "rescaled rewards",
        "mean separation",
        "logarithmic bound",
        "signal propagation",
        "microscopic gap",
        "adversarial robustness"
      ],
      "document_id": "03_cloning",
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "span": {
        "start_line": 4453,
        "end_line": 4506,
        "content_start": 4456,
        "content_end": 4505,
        "header_lines": [
          4454
        ]
      },
      "metadata": {
        "label": "proof-prop-log-reward-gap-axiom-bound"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      },
      "generated_at": "2025-11-10T13:26:08.165093+00:00",
      "alt_labels": []
    },
    "tags": [
      "reward",
      "gap",
      "logarithmic",
      "rescaled",
      "axiom",
      "bound",
      "expectation"
    ],
    "content_markdown": ":label: prop-log-reward-gap-axiom-bound\n\nUnder the **Axiom of Reward Regularity**, the expected logarithmic gap in the rescaled reward signal is bounded by:\n\n$$\n\\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R \\cdot D_{\\mathrm{valid}})}{\\eta}\\right)\n$$",
    "raw_directive": "4442: This raw reward gap now propagates through the measurement pipeline.\n4443: \n4444: :::{prf:proposition} **(Axiom-Based Bound on the Logarithmic Reward Gap)**\n4445: :label: prop-log-reward-gap-axiom-bound\n4446: \n4447: Under the **Axiom of Reward Regularity**, the expected logarithmic gap in the rescaled reward signal is bounded by:\n4448: \n4449: $$\n4450: \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R \\cdot D_{\\mathrm{valid}})}{\\eta}\\right)\n4451: $$\n4452: ",
    "document_id": "03_cloning",
    "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
    "span": {
      "start_line": 4442,
      "end_line": 4452,
      "content_start": 4445,
      "content_end": 4451,
      "header_lines": [
        4443
      ]
    },
    "references": [
      "def-patched-std-dev-function",
      "lem-log-gap-upper-bound"
    ],
    "metadata": {
      "label": "prop-log-reward-gap-axiom-bound"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 7,
      "chapter_file": "chapter_7.json",
      "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
    },
    "generated_at": "2025-11-10T13:26:08.168931+00:00",
    "alt_labels": []
  },
  {
    "label": "prop-n-uniformity-keystone",
    "title": "N-Uniformity of Keystone Constants",
    "type": "proposition",
    "nl_statement": "The Keystone constants \u03c7(\u03b5) and g_max(\u03b5) are strictly independent of the swarm size N; for any fixed system parameters, finite positive constants \u03c7\u2080(\u03b5) and g\u2080(\u03b5) exist such that for all N \u2265 2, \u03c7(\u03b5) = \u03c7\u2080(\u03b5) and g_max(\u03b5) = g\u2080(\u03b5).",
    "equations": [
      {
        "label": null,
        "latex": "\\chi(\\epsilon) = \\chi_0(\\epsilon) \\quad \\text{and} \\quad g_{\\max}(\\epsilon) = g_0(\\epsilon)"
      }
    ],
    "hypotheses": [
      {
        "text": "Fixed choice of system parameters (\u03b5, domain, pipeline parameters, etc.)",
        "latex": null
      },
      {
        "text": "N \u2265 2, where N is the swarm size",
        "latex": null
      },
      {
        "text": "Finite positive constants \u03c7\u2080(\u03b5) and g\u2080(\u03b5) exist independent of N",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "\u03c7(\u03b5) = \u03c7\u2080(\u03b5) and g_max(\u03b5) = g\u2080(\u03b5)",
      "latex": "\\chi(\\epsilon) = \\chi_0(\\epsilon) \\quad \\text{and} \\quad g_{\\max}(\\epsilon) = g_0(\\epsilon)"
    },
    "variables": [
      {
        "symbol": "\\epsilon",
        "name": "epsilon",
        "description": "Accuracy or tolerance parameter",
        "constraints": [],
        "tags": [
          "parameter",
          "accuracy"
        ]
      },
      {
        "symbol": "N",
        "name": "swarm size",
        "description": "Number of agents in the swarm",
        "constraints": [
          "integer",
          "N \\geq 2"
        ],
        "tags": [
          "swarm",
          "size"
        ]
      },
      {
        "symbol": "\\chi(\\epsilon)",
        "name": "chi epsilon",
        "description": "Keystone constant for convergence",
        "constraints": [
          "positive"
        ],
        "tags": [
          "keystone",
          "constant"
        ]
      },
      {
        "symbol": "g_{\\max}(\\epsilon)",
        "name": "g max epsilon",
        "description": "Maximum gain Keystone constant",
        "constraints": [
          "positive"
        ],
        "tags": [
          "keystone",
          "gain"
        ]
      },
      {
        "symbol": "\\chi_0(\\epsilon)",
        "name": "chi zero epsilon",
        "description": "N-independent base for \u03c7(\u03b5)",
        "constraints": [
          "finite",
          "positive"
        ],
        "tags": [
          "keystone",
          "base"
        ]
      },
      {
        "symbol": "g_0(\\epsilon)",
        "name": "g zero epsilon",
        "description": "N-independent base for g_max(\u03b5)",
        "constraints": [
          "finite",
          "positive"
        ],
        "tags": [
          "keystone",
          "base"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "\u03c7\u2080(\u03b5) and g\u2080(\u03b5) are finite and positive for fixed \u03b5",
        "confidence": 1.0
      },
      {
        "text": "System parameters are fixed, excluding N",
        "confidence": 1.0
      },
      {
        "text": "Constants are O(1) as N \u2192 \u221e",
        "confidence": 0.9
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-prop-n-uniformity-keystone",
      "title": null,
      "type": "proof",
      "proves": "prop-n-uniformity-keystone",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":label: proof-prop-n-uniformity-keystone\n\n**Proof.**\n\nWe verify N-independence by systematically checking every component in the definitions of $\\chi(\\epsilon) = p_u(\\epsilon) \\cdot c_{\\text{err}}(\\epsilon)$ and $g_{\\max}(\\epsilon) = \\max(p_u(\\epsilon) \\cdot g_{\\text{err}}(\\epsilon), \\chi(\\epsilon)R^2_{\\text{spread}})$.\n\n**Part 1: N-Independence of $p_u(\\epsilon)$**\n\nFrom Section 8.6.1.1, $p_u(\\epsilon)$ is defined as:\n\n$$\np_u(\\epsilon) = \\frac{1}{p_{\\max}} \\left( \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}})} \\right)\n$$\n\nWe verify each component:\n- $p_{\\max}$: User-defined parameter, independent of $N$ \u2713\n- $\\varepsilon_{\\text{clone}}$: User-defined parameter, independent of $N$ \u2713\n- $V_{\\text{pot,max}} = (g_{A,\\max} + \\eta)^{\\alpha+\\beta}$: Depends only on pipeline parameters ($g_{A,\\max}$, $\\eta$, $\\alpha$, $\\beta$), all independent of $N$ \u2713\n- $\\kappa_{V,\\text{gap}}(\\epsilon)$: The fitness potential gap. We trace its dependencies:\n  - $\\kappa_{\\text{meas}}(\\epsilon)$: From [](#thm-geometry-guarantees-variance), this depends on the phase-space separation constants $D_H(\\epsilon)$ and $R_L(\\epsilon)$, which are defined in terms of:\n    - Geometric properties of the outlier/cluster definitions ($\\epsilon_O$, $D_{\\text{diam}}(\\epsilon)$): Independent of $N$ \u2713\n    - Domain diameter $D_{\\text{valid}}$: Independent of $N$ \u2713\n    - Velocity bounds: Independent of $N$ \u2713\n  - Pipeline transformations (standardization, rescaling): Depend only on ($g'_{\\min}$, $\\sigma'_{\\max}$, $\\eta$), all independent of $N$ \u2713\n\n**Conclusion:** $p_u(\\epsilon)$ is strictly independent of $N$.\n\n**Part 2: N-Independence of $c_{\\text{err}}(\\epsilon)$**\n\nFrom Section 8.6.1.2, $c_{\\text{err}}(\\epsilon) \\propto \\lambda_2 \\cdot c_H \\cdot f_{UH}(\\epsilon)$. We verify each component:\n\n- $\\lambda_2$: The minimum eigenvalue from the Coercivity Lemma for the Lyapunov function. From Lemma 3.4.1 (referenced but not shown), this depends only on the Lyapunov structure constants ($b$, $\\lambda_v$), which are parameters of the function definition, independent of $N$ \u2713\n\n- $c_H$: The variance concentration constant from [](#lem-variance-concentration-Hk). From the proof (lines 3828-3908):\n  - **Mean-field regime**: $c_H = 1 - \\epsilon_O$, where $\\epsilon_O$ is the outlier threshold parameter, independent of $N$ \u2713\n  - **Local-interaction regime**: $c_H = \\min\\{1-\\epsilon_O, (1-\\epsilon_O)R^2_{\\text{means}}/R^2_{\\text{var}}\\}$, where:\n    - $R^2_{\\text{means}} = R^2_{\\text{var}} - (D_{\\text{diam}}(\\epsilon)/2)^2$: Depends only on variance threshold and cluster diameter, both independent of $N$ \u2713\n\n- $f_{UH}(\\epsilon)$: The overlap fraction from [](#thm-unfit-high-error-overlap-fraction). This depends on:\n  - Population fraction lower bounds $f_U(\\epsilon)$ and $f_H(\\epsilon)$ from Chapters 6-7\n  - From {prf:ref}`lem-outlier-fraction-lower-bound` and 6.4.3, these fractions are **defined as N-uniform constants** - they are constructed precisely to be independent of swarm size \u2713\n  - The proof uses only geometric properties (phase-space packing, variance decomposition) that scale with the number of walkers but produce **fractions** that remain constant \u2713\n\n**Conclusion:** $c_{\\text{err}}(\\epsilon)$ is strictly independent of $N$.\n\n**Part 3: N-Independence of $g_{\\text{err}}(\\epsilon)$**\n\nFrom Section 8.6.2.1:\n\n$$\ng_{err}(\\epsilon) := g'_{err} + (1 - f_{UH}(\\epsilon)) \\cdot 4D_{\\mathrm{valid}}^2\n$$\n\n- $g'_{\\text{err}}$: A constant from {prf:ref}`lem-variance-concentration-Hk` involving domain diameter, independent of $N$ \u2713\n- $f_{UH}(\\epsilon)$: Already verified as N-independent in Part 2 \u2713\n- $D_{\\text{valid}}$: Domain diameter, independent of $N$ \u2713\n\n**Conclusion:** $g_{\\text{err}}(\\epsilon)$ is strictly independent of $N$.\n\n**Part 4: N-Independence of $g_{\\max}(\\epsilon)$ and $\\chi(\\epsilon)$**\n\nSince all components are N-independent:\n\n$$\n\\chi(\\epsilon) = p_u(\\epsilon) \\cdot c_{err}(\\epsilon) \\quad \\text{(product of N-independent terms)} \\quad \u2713\n$$\n\n$$\ng_{\\max}(\\epsilon) = \\max(p_u(\\epsilon) \\cdot g_{err}(\\epsilon), \\chi(\\epsilon) R^2_{\\text{spread}}) \\quad \\text{(max of N-independent terms)} \\quad \u2713\n$$\n\nwhere $R^2_{\\text{spread}}$ is the variance threshold, a fixed constant independent of $N$ \u2713\n\n**Conclusion:** Both $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are strictly independent of $N$, depending only on $\\epsilon$ and fixed system parameters.",
      "raw_directive": "5573: :::\n5574: \n5575: :::{prf:proof}\n5576: :label: proof-prop-n-uniformity-keystone\n5577: \n5578: **Proof.**\n5579: \n5580: We verify N-independence by systematically checking every component in the definitions of $\\chi(\\epsilon) = p_u(\\epsilon) \\cdot c_{\\text{err}}(\\epsilon)$ and $g_{\\max}(\\epsilon) = \\max(p_u(\\epsilon) \\cdot g_{\\text{err}}(\\epsilon), \\chi(\\epsilon)R^2_{\\text{spread}})$.\n5581: \n5582: **Part 1: N-Independence of $p_u(\\epsilon)$**\n5583: \n5584: From Section 8.6.1.1, $p_u(\\epsilon)$ is defined as:\n5585: \n5586: $$\n5587: p_u(\\epsilon) = \\frac{1}{p_{\\max}} \\left( \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}})} \\right)\n5588: $$\n5589: \n5590: We verify each component:\n5591: - $p_{\\max}$: User-defined parameter, independent of $N$ \u2713\n5592: - $\\varepsilon_{\\text{clone}}$: User-defined parameter, independent of $N$ \u2713\n5593: - $V_{\\text{pot,max}} = (g_{A,\\max} + \\eta)^{\\alpha+\\beta}$: Depends only on pipeline parameters ($g_{A,\\max}$, $\\eta$, $\\alpha$, $\\beta$), all independent of $N$ \u2713\n5594: - $\\kappa_{V,\\text{gap}}(\\epsilon)$: The fitness potential gap. We trace its dependencies:\n5595:   - $\\kappa_{\\text{meas}}(\\epsilon)$: From [](#thm-geometry-guarantees-variance), this depends on the phase-space separation constants $D_H(\\epsilon)$ and $R_L(\\epsilon)$, which are defined in terms of:\n5596:     - Geometric properties of the outlier/cluster definitions ($\\epsilon_O$, $D_{\\text{diam}}(\\epsilon)$): Independent of $N$ \u2713\n5597:     - Domain diameter $D_{\\text{valid}}$: Independent of $N$ \u2713\n5598:     - Velocity bounds: Independent of $N$ \u2713\n5599:   - Pipeline transformations (standardization, rescaling): Depend only on ($g'_{\\min}$, $\\sigma'_{\\max}$, $\\eta$), all independent of $N$ \u2713\n5600: \n5601: **Conclusion:** $p_u(\\epsilon)$ is strictly independent of $N$.\n5602: \n5603: **Part 2: N-Independence of $c_{\\text{err}}(\\epsilon)$**\n5604: \n5605: From Section 8.6.1.2, $c_{\\text{err}}(\\epsilon) \\propto \\lambda_2 \\cdot c_H \\cdot f_{UH}(\\epsilon)$. We verify each component:\n5606: \n5607: - $\\lambda_2$: The minimum eigenvalue from the Coercivity Lemma for the Lyapunov function. From Lemma 3.4.1 (referenced but not shown), this depends only on the Lyapunov structure constants ($b$, $\\lambda_v$), which are parameters of the function definition, independent of $N$ \u2713\n5608: \n5609: - $c_H$: The variance concentration constant from [](#lem-variance-concentration-Hk). From the proof (lines 3828-3908):\n5610:   - **Mean-field regime**: $c_H = 1 - \\epsilon_O$, where $\\epsilon_O$ is the outlier threshold parameter, independent of $N$ \u2713\n5611:   - **Local-interaction regime**: $c_H = \\min\\{1-\\epsilon_O, (1-\\epsilon_O)R^2_{\\text{means}}/R^2_{\\text{var}}\\}$, where:\n5612:     - $R^2_{\\text{means}} = R^2_{\\text{var}} - (D_{\\text{diam}}(\\epsilon)/2)^2$: Depends only on variance threshold and cluster diameter, both independent of $N$ \u2713\n5613: \n5614: - $f_{UH}(\\epsilon)$: The overlap fraction from [](#thm-unfit-high-error-overlap-fraction). This depends on:\n5615:   - Population fraction lower bounds $f_U(\\epsilon)$ and $f_H(\\epsilon)$ from Chapters 6-7\n5616:   - From {prf:ref}`lem-outlier-fraction-lower-bound` and 6.4.3, these fractions are **defined as N-uniform constants** - they are constructed precisely to be independent of swarm size \u2713\n5617:   - The proof uses only geometric properties (phase-space packing, variance decomposition) that scale with the number of walkers but produce **fractions** that remain constant \u2713\n5618: \n5619: **Conclusion:** $c_{\\text{err}}(\\epsilon)$ is strictly independent of $N$.\n5620: \n5621: **Part 3: N-Independence of $g_{\\text{err}}(\\epsilon)$**\n5622: \n5623: From Section 8.6.2.1:\n5624: \n5625: $$\n5626: g_{err}(\\epsilon) := g'_{err} + (1 - f_{UH}(\\epsilon)) \\cdot 4D_{\\mathrm{valid}}^2\n5627: $$\n5628: \n5629: - $g'_{\\text{err}}$: A constant from {prf:ref}`lem-variance-concentration-Hk` involving domain diameter, independent of $N$ \u2713\n5630: - $f_{UH}(\\epsilon)$: Already verified as N-independent in Part 2 \u2713\n5631: - $D_{\\text{valid}}$: Domain diameter, independent of $N$ \u2713\n5632: \n5633: **Conclusion:** $g_{\\text{err}}(\\epsilon)$ is strictly independent of $N$.\n5634: \n5635: **Part 4: N-Independence of $g_{\\max}(\\epsilon)$ and $\\chi(\\epsilon)$**\n5636: \n5637: Since all components are N-independent:\n5638: \n5639: $$\n5640: \\chi(\\epsilon) = p_u(\\epsilon) \\cdot c_{err}(\\epsilon) \\quad \\text{(product of N-independent terms)} \\quad \u2713\n5641: $$\n5642: \n5643: $$\n5644: g_{\\max}(\\epsilon) = \\max(p_u(\\epsilon) \\cdot g_{err}(\\epsilon), \\chi(\\epsilon) R^2_{\\text{spread}}) \\quad \\text{(max of N-independent terms)} \\quad \u2713\n5645: $$\n5646: \n5647: where $R^2_{\\text{spread}}$ is the variance threshold, a fixed constant independent of $N$ \u2713\n5648: \n5649: **Conclusion:** Both $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are strictly independent of $N$, depending only on $\\epsilon$ and fixed system parameters.\n5650: ",
      "strategy_summary": "The proof establishes N-independence of \u03c7(\u03b5) and g_max(\u03b5) by decomposing them into constituent components, verifying each one's independence from N through tracing dependencies to geometric, parametric, and probabilistic properties that do not scale with swarm size.",
      "conclusion": {
        "text": "Both \u03c7(\u03b5) and g_max(\u03b5) are strictly independent of N, depending only on \u03b5 and fixed system parameters.",
        "latex": "\\chi(\\epsilon) \\text{ and } g_{\\max}(\\epsilon) \\text{ are strictly independent of } N"
      },
      "assumptions": [],
      "steps": [
        {
          "order": 1.0,
          "kind": "decomposition",
          "text": "Decompose p_u(\u03b5) and verify each component's N-independence: p_max, \u03b5_clone, V_pot,max, \u03ba_V,gap(\u03b5) via thm-geometry-guarantees-variance.",
          "latex": null,
          "references": [
            "thm-geometry-guarantees-variance"
          ],
          "derived_statement": "p_u(\u03b5) is N-independent"
        },
        {
          "order": 2.0,
          "kind": "decomposition",
          "text": "Decompose c_err(\u03b5) \u221d \u03bb\u2082 \u00b7 c_H \u00b7 f_UH(\u03b5): \u03bb\u2082 via Lemma 3.4.1, c_H via lem-variance-concentration-Hk (mean-field and local regimes), f_UH(\u03b5) via thm-unfit-high-error-overlap-fraction and population fractions.",
          "latex": null,
          "references": [
            "lem-variance-concentration-Hk",
            "thm-unfit-high-error-overlap-fraction",
            "lem-outlier-fraction-lower-bound"
          ],
          "derived_statement": "c_err(\u03b5) is N-independent"
        },
        {
          "order": 3.0,
          "kind": "decomposition",
          "text": "Decompose g_err(\u03b5) = g'_err + (1 - f_UH(\u03b5)) \u00b7 4 D_valid\u00b2, verifying each term.",
          "latex": null,
          "references": [
            "lem-variance-concentration-Hk"
          ],
          "derived_statement": "g_err(\u03b5) is N-independent"
        },
        {
          "order": 4.0,
          "kind": "composition",
          "text": "Compose \u03c7(\u03b5) = p_u(\u03b5) \u00b7 c_err(\u03b5) and g_max(\u03b5) = max(p_u(\u03b5) \u00b7 g_err(\u03b5), \u03c7(\u03b5) R\u00b2_spread), confirming overall N-independence.",
          "latex": null,
          "references": [],
          "derived_statement": "\u03c7(\u03b5) and g_max(\u03b5) are N-independent"
        }
      ],
      "key_equations": [
        {
          "label": "eq-p_u",
          "latex": "p_u(\\epsilon) = \\frac{1}{p_{\\max}} \\left( \\frac{\\kappa_{V,\\text{gap}}(\\epsilon)}{2(V_{\\text{pot,max}} + \\varepsilon_{\\text{clone}})} \\right)",
          "role": "Definition of p_u(\u03b5)"
        },
        {
          "label": "eq-g_err",
          "latex": "g_{\\text{err}}(\\epsilon) := g'_{\\text{err}} + (1 - f_{UH}(\\epsilon)) \\cdot 4D_{\\mathrm{valid}}^2",
          "role": "Definition of g_err(\u03b5)"
        },
        {
          "label": "eq-chi",
          "latex": "\\chi(\\epsilon) = p_u(\\epsilon) \\cdot c_{\\text{err}}(\\epsilon)",
          "role": "Definition of \u03c7(\u03b5)"
        },
        {
          "label": "eq-g_max",
          "latex": "g_{\\max}(\\epsilon) = \\max(p_u(\\epsilon) \\cdot g_{\\text{err}}(\\epsilon), \\chi(\\epsilon)R^2_{\\text{spread}})",
          "role": "Definition of g_max(\u03b5)"
        }
      ],
      "references": [
        "lem-outlier-fraction-lower-bound",
        "lem-variance-concentration-Hk",
        "thm-geometry-guarantees-variance",
        "thm-unfit-high-error-overlap-fraction"
      ],
      "math_tools": [
        {
          "toolName": "Coercivity Lemma",
          "field": "Dynamical Systems",
          "description": "A lemma bounding the coercivity of Lyapunov functions via minimum eigenvalues.",
          "roleInProof": "Provides N-independent bound for \u03bb\u2082 in c_err(\u03b5).",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Lyapunov Stability"
          ]
        },
        {
          "toolName": "Variance Concentration",
          "field": "Probability Theory",
          "description": "Technique for bounding variance in mean-field or local-interaction regimes using geometric constants.",
          "roleInProof": "Yields N-independent c_H in c_err(\u03b5).",
          "levelOfAbstraction": "Technique",
          "relatedTools": [
            "Chebyshev Inequality"
          ]
        },
        {
          "toolName": "Phase-Space Geometry",
          "field": "Geometric Analysis",
          "description": "Analysis of separations and diameters in phase space for outlier and cluster definitions.",
          "roleInProof": "Ensures \u03ba_V,gap(\u03b5) and related terms are N-independent in p_u(\u03b5).",
          "levelOfAbstraction": "Concept",
          "relatedTools": [
            "Packing Arguments"
          ]
        },
        {
          "toolName": "Overlap Fraction Bounds",
          "field": "Optimization",
          "description": "Lower bounds on fractions of unfit and high-error populations using geometric packing.",
          "roleInProof": "Provides N-uniform f_UH(\u03b5) in c_err(\u03b5) and g_err(\u03b5).",
          "levelOfAbstraction": "Theorem/Lemma",
          "relatedTools": [
            "Fractional Coverage"
          ]
        }
      ],
      "cases": [
        {
          "name": "Part 1",
          "condition": "p_u(\u03b5) components",
          "summary": "All components (p_max, \u03b5_clone, V_pot,max, \u03ba_V,gap(\u03b5)) are N-independent."
        },
        {
          "name": "Part 2",
          "condition": "c_err(\u03b5) components",
          "summary": "\u03bb\u2082, c_H (mean-field/local), f_UH(\u03b5) all N-independent."
        },
        {
          "name": "Part 3",
          "condition": "g_err(\u03b5) components",
          "summary": "g'_err, f_UH(\u03b5), D_valid all N-independent."
        },
        {
          "name": "Part 4",
          "condition": "Composite functions",
          "summary": "Products and max of N-independent terms remain N-independent."
        }
      ],
      "remarks": [
        {
          "type": "conclusion",
          "text": "p_u(\u03b5) is strictly independent of N."
        },
        {
          "type": "conclusion",
          "text": "c_err(\u03b5) is strictly independent of N."
        },
        {
          "type": "conclusion",
          "text": "g_err(\u03b5) is strictly independent of N."
        }
      ],
      "gaps": [],
      "tags": [
        "n-independence",
        "uniformity",
        "verification",
        "components",
        "swarm-systems",
        "geometric-properties",
        "variance-concentration"
      ],
      "document_id": "03_cloning",
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "span": {
        "start_line": 5573,
        "end_line": 5650,
        "content_start": 5576,
        "content_end": 5649,
        "header_lines": [
          5574
        ]
      },
      "metadata": {
        "label": "proof-prop-n-uniformity-keystone"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 8,
        "chapter_file": "chapter_8.json",
        "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma"
      },
      "generated_at": "2025-11-10T13:26:08.165093+00:00",
      "alt_labels": []
    },
    "tags": [
      "keystone constants",
      "N-uniformity",
      "swarm size independence",
      "chi epsilon",
      "g max epsilon",
      "asymptotic O(1)"
    ],
    "content_markdown": ":label: prop-n-uniformity-keystone\n\nThe Keystone constants $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are strictly independent of the swarm size $N$. More precisely, for any fixed choice of system parameters ($\\epsilon$, domain, pipeline parameters, etc.), there exist finite positive constants $\\chi_0(\\epsilon)$ and $g_0(\\epsilon)$ such that for all $N \\geq 2$:\n\n$$\n\\chi(\\epsilon) = \\chi_0(\\epsilon) \\quad \\text{and} \\quad g_{\\max}(\\epsilon) = g_0(\\epsilon)\n$$",
    "raw_directive": "5561: We now provide a formal verification that both Keystone constants, $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$, are **strictly independent of the swarm size N**, establishing that they are $O(1)$ as $N \\to \\infty$.\n5562: \n5563: :::{prf:proposition} N-Uniformity of Keystone Constants\n5564: :label: prop-n-uniformity-keystone\n5565: \n5566: The Keystone constants $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are strictly independent of the swarm size $N$. More precisely, for any fixed choice of system parameters ($\\epsilon$, domain, pipeline parameters, etc.), there exist finite positive constants $\\chi_0(\\epsilon)$ and $g_0(\\epsilon)$ such that for all $N \\geq 2$:\n5567: \n5568: $$\n5569: \\chi(\\epsilon) = \\chi_0(\\epsilon) \\quad \\text{and} \\quad g_{\\max}(\\epsilon) = g_0(\\epsilon)\n5570: $$\n5571: ",
    "document_id": "03_cloning",
    "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
    "span": {
      "start_line": 5561,
      "end_line": 5571,
      "content_start": 5564,
      "content_end": 5570,
      "header_lines": [
        5562
      ]
    },
    "references": [
      "lem-outlier-fraction-lower-bound",
      "lem-variance-concentration-Hk",
      "thm-geometry-guarantees-variance",
      "thm-unfit-high-error-overlap-fraction"
    ],
    "metadata": {
      "label": "prop-n-uniformity-keystone"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 8,
      "chapter_file": "chapter_8.json",
      "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma"
    },
    "generated_at": "2025-11-10T13:26:08.168931+00:00",
    "alt_labels": []
  },
  {
    "label": "prop-expected-displacement-cloning",
    "title": "Expected Displacement Under Cloning",
    "type": "proposition",
    "nl_statement": "For walker i with cloning probability p_i, the expected squared position displacement satisfies E[||\u0394x_i||\u00b2 | S] \u2264 p_i \u00b7 D_max\u00b2, where D_max is the maximum distance in the valid domain or a bound on the jitter kernel range.",
    "equations": [
      {
        "label": null,
        "latex": "\\mathbb{E}[\\|\\Delta x_i\\|^2 \\mid S] \\leq p_i \\cdot D_{\\text{max}}^2"
      }
    ],
    "hypotheses": [
      {
        "text": "Walker i has cloning probability p_i.",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "The expected squared position displacement satisfies the bound.",
      "latex": "\\mathbb{E}[\\|\\Delta x_i\\|^2 \\mid S] \\leq p_i \\cdot D_{\\text{max}}^2"
    },
    "variables": [
      {
        "symbol": "i",
        "name": "walker index",
        "description": "Index identifying the specific walker.",
        "constraints": [],
        "tags": [
          "index"
        ]
      },
      {
        "symbol": "p_i",
        "name": "cloning probability",
        "description": "Probability that walker i undergoes cloning.",
        "constraints": [
          "0 \\leq p_i \\leq 1"
        ],
        "tags": [
          "probability"
        ]
      },
      {
        "symbol": "\\Delta x_i",
        "name": "position displacement",
        "description": "Change in position for walker i.",
        "constraints": [],
        "tags": [
          "displacement",
          "position"
        ]
      },
      {
        "symbol": "S",
        "name": "conditioning state",
        "description": "State or condition under which the expectation is taken.",
        "constraints": [],
        "tags": [
          "state",
          "condition"
        ]
      },
      {
        "symbol": "D_{\\text{max}}",
        "name": "maximum distance",
        "description": "Maximum distance in the valid domain or bound on the jitter kernel range.",
        "constraints": [
          "D_{\\text{max}} > 0"
        ],
        "tags": [
          "bound",
          "distance",
          "domain"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The position displacements are bounded within the valid domain.",
        "confidence": 1.0
      },
      {
        "text": "S is a valid conditioning event or state.",
        "confidence": 0.9
      }
    ],
    "local_refs": [],
    "proof": {
      "label": "proof-prop-expected-displacement-cloning",
      "title": null,
      "type": "proof",
      "proves": "prop-expected-displacement-cloning",
      "proof_type": "direct",
      "proof_status": "complete",
      "content_markdown": ":::{prf:proof}\n:label: proof-prop-expected-displacement-cloning\n**Proof.**\n\nThe walker clones with probability $p_i$, in which case its position is sampled from $\\mathcal{Q}_\\delta(x_{c_i}, \\cdot)$, yielding displacement bounded by $D_{\\text{max}}$.\n\nWith probability $1 - p_i$, the walker persists and has zero displacement.\n\nTherefore:\n\n$$\n\\mathbb{E}[\\|\\Delta x_i\\|^2 \\mid S] = p_i \\cdot \\mathbb{E}[\\|\\Delta x_i\\|^2 \\mid S, a_i = \\text{clone}] + (1-p_i) \\cdot 0 \\leq p_i \\cdot D_{\\text{max}}^2\n$$",
      "raw_directive": "6262: :::\n6263: \n6264: :::{prf:proof}\n6265: :label: proof-prop-expected-displacement-cloning\n6266: **Proof.**\n6267: \n6268: The walker clones with probability $p_i$, in which case its position is sampled from $\\mathcal{Q}_\\delta(x_{c_i}, \\cdot)$, yielding displacement bounded by $D_{\\text{max}}$.\n6269: \n6270: With probability $1 - p_i$, the walker persists and has zero displacement.\n6271: \n6272: Therefore:\n6273: \n6274: $$\n6275: \\mathbb{E}[\\|\\Delta x_i\\|^2 \\mid S] = p_i \\cdot \\mathbb{E}[\\|\\Delta x_i\\|^2 \\mid S, a_i = \\text{clone}] + (1-p_i) \\cdot 0 \\leq p_i \\cdot D_{\\text{max}}^2\n6276: $$\n6277: ",
      "strategy_summary": "The proof conditions the expected squared displacement on the cloning action, computing it as a weighted average of the displacement under cloning (bounded by D_max) and zero under persistence.",
      "conclusion": {
        "text": null,
        "latex": null
      },
      "assumptions": [],
      "steps": [],
      "key_equations": [],
      "references": [],
      "math_tools": [
        {
          "toolName": "Conditional Expectation",
          "field": "Probability Theory",
          "description": "The expected value of a random variable given additional information or events.",
          "roleInProof": "Used to express the overall expectation as a convex combination based on the cloning probability.",
          "levelOfAbstraction": "Technique",
          "relatedTools": []
        }
      ],
      "cases": [
        {
          "name": "Cloning",
          "condition": "a_i = clone with probability p_i",
          "summary": "Position sampled from Q_\u03b4(x_{c_i}, \u00b7), ||\u0394x_i|| \u2264 D_max so E[||\u0394x_i||\u00b2 | clone] \u2264 D_max\u00b2"
        },
        {
          "name": "Persistence",
          "condition": "a_i = persist with probability 1 - p_i",
          "summary": "Zero displacement, E[||\u0394x_i||\u00b2 | persist] = 0"
        }
      ],
      "remarks": [],
      "gaps": [],
      "tags": [
        "expected displacement",
        "cloning",
        "probability",
        "bounding",
        "conditional expectation"
      ],
      "document_id": "03_cloning",
      "section": "## 9.5. Key Quantities for Drift Analysis",
      "span": {
        "start_line": 6262,
        "end_line": 6277,
        "content_start": 6264,
        "content_end": 6276,
        "header_lines": [
          6263
        ]
      },
      "metadata": {
        "label": "proof-prop-expected-displacement-cloning"
      },
      "registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 13,
        "chapter_file": "chapter_13.json",
        "section_id": "## 9.5. Key Quantities for Drift Analysis"
      },
      "generated_at": "2025-11-10T13:26:08.165093+00:00",
      "alt_labels": []
    },
    "tags": [
      "cloning",
      "expected displacement",
      "squared norm",
      "walker",
      "probability",
      "bound"
    ],
    "content_markdown": ":label: prop-expected-displacement-cloning\n\nFor walker $i$ with cloning probability $p_i$, the expected squared position displacement satisfies:\n\n$$\n\\mathbb{E}[\\|\\Delta x_i\\|^2 \\mid S] \\leq p_i \\cdot D_{\\text{max}}^2\n$$\n\nwhere $D_{\\text{max}}$ is the maximum distance in the valid domain (or a suitable bound on the jitter kernel range).",
    "raw_directive": "6248: :::\n6249: \n6250: :::{prf:proposition} Expected Displacement Under Cloning\n6251: :label: prop-expected-displacement-cloning\n6252: \n6253: For walker $i$ with cloning probability $p_i$, the expected squared position displacement satisfies:\n6254: \n6255: $$\n6256: \\mathbb{E}[\\|\\Delta x_i\\|^2 \\mid S] \\leq p_i \\cdot D_{\\text{max}}^2\n6257: $$\n6258: \n6259: where $D_{\\text{max}}$ is the maximum distance in the valid domain (or a suitable bound on the jitter kernel range).\n6260: ",
    "document_id": "03_cloning",
    "section": "## 9.5. Key Quantities for Drift Analysis",
    "span": {
      "start_line": 6248,
      "end_line": 6260,
      "content_start": 6251,
      "content_end": 6259,
      "header_lines": [
        6249
      ]
    },
    "references": [],
    "metadata": {
      "label": "prop-expected-displacement-cloning"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 13,
      "chapter_file": "chapter_13.json",
      "section_id": "## 9.5. Key Quantities for Drift Analysis"
    },
    "generated_at": "2025-11-10T13:26:08.168931+00:00",
    "alt_labels": []
  },
  {
    "label": "prop-kinetic-necessity",
    "title": "Necessity of the Kinetic Operator",
    "type": "proposition",
    "nl_statement": "The cloning operator alone cannot guarantee convergence to a quasi-stationary distribution due to unbounded velocity variance accumulation, inter-swarm divergence, and lack of velocity equilibrium; the kinetic operator is essential for contraction via friction and drift.",
    "equations": [],
    "hypotheses": [
      {
        "text": "Velocity variance accumulation: The bounded expansion +C_v per step can accumulate without bound over infinite time if not countered.",
        "latex": null
      },
      {
        "text": "Inter-swarm divergence: The bounded expansion +C_W means the two coupled swarms can drift arbitrarily far apart without inter-swarm correction.",
        "latex": null
      },
      {
        "text": "No velocity equilibrium: Cloning has no mechanism to dissipate kinetic energy toward a target distribution - it only redistributes it through collisions.",
        "latex": null
      }
    ],
    "conclusion": {
      "text": "The kinetic operator is essential to contract V_{\text{Var},v} via Langevin friction (overcoming C_v) and contract V_W via hypocoercive drift and confining potential (overcoming C_W).",
      "latex": null
    },
    "variables": [
      {
        "symbol": "C_v",
        "name": "Velocity expansion constant",
        "description": "Bounded expansion per step for velocity variance.",
        "constraints": [
          "bounded",
          "positive"
        ],
        "tags": [
          "velocity",
          "expansion",
          "constant"
        ]
      },
      {
        "symbol": "C_W",
        "name": "Swarm expansion constant",
        "description": "Bounded expansion per step for inter-swarm distance.",
        "constraints": [
          "bounded",
          "positive"
        ],
        "tags": [
          "swarm",
          "expansion",
          "constant"
        ]
      },
      {
        "symbol": "V_{\\text{Var},v}",
        "name": "Velocity variance",
        "description": "Measure of variance in velocity components.",
        "constraints": [],
        "tags": [
          "variance",
          "velocity"
        ]
      },
      {
        "symbol": "V_W",
        "name": "Inter-swarm variance",
        "description": "Measure of distance between two coupled swarms.",
        "constraints": [],
        "tags": [
          "variance",
          "swarm"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "System evolves over infinite time.",
        "confidence": 0.9
      },
      {
        "text": "No additional correction mechanisms beyond cloning.",
        "confidence": 1.0
      },
      {
        "text": "Coupled swarms are present in the model.",
        "confidence": 0.8
      }
    ],
    "local_refs": [],
    "proof": null,
    "tags": [
      "cloning operator",
      "kinetic operator",
      "convergence",
      "quasi-stationary distribution",
      "variance accumulation",
      "inter-swarm divergence"
    ],
    "content_markdown": ":label: prop-kinetic-necessity\n\nThe cloning operator alone cannot guarantee convergence to a quasi-stationary distribution. Specifically:\n\n1. **Velocity variance accumulation:** The bounded expansion $+C_v$ per step can accumulate without bound over infinite time if not countered.\n\n2. **Inter-swarm divergence:** The bounded expansion $+C_W$ means the two coupled swarms can drift arbitrarily far apart without inter-swarm correction.\n\n3. **No velocity equilibrium:** Cloning has no mechanism to dissipate kinetic energy toward a target distribution - it only redistributes it through collisions.\n\nTherefore, the **kinetic operator is essential** to:\n- Contract $V_{\\text{Var},v}$ via Langevin friction (overcoming $C_v$)",
    "raw_directive": "8195: ### 12.3.3. Why Cloning Alone Cannot Achieve Convergence\n8196: \n8197: :::{prf:proposition} Necessity of the Kinetic Operator\n8198: :label: prop-kinetic-necessity\n8199: \n8200: The cloning operator alone cannot guarantee convergence to a quasi-stationary distribution. Specifically:\n8201: \n8202: 1. **Velocity variance accumulation:** The bounded expansion $+C_v$ per step can accumulate without bound over infinite time if not countered.\n8203: \n8204: 2. **Inter-swarm divergence:** The bounded expansion $+C_W$ means the two coupled swarms can drift arbitrarily far apart without inter-swarm correction.\n8205: \n8206: 3. **No velocity equilibrium:** Cloning has no mechanism to dissipate kinetic energy toward a target distribution - it only redistributes it through collisions.\n8207: \n8208: Therefore, the **kinetic operator is essential** to:\n8209: - Contract $V_{\\text{Var},v}$ via Langevin friction (overcoming $C_v$)\n8210: - Contract $V_W$ via hypocoercive drift and confining potential (overcoming $C_W$)",
    "document_id": "03_cloning",
    "section": "## 12.3. The Complete Lyapunov Drift Under Cloning",
    "span": {
      "start_line": 8195,
      "end_line": 8210,
      "content_start": 8198,
      "content_end": 8209,
      "header_lines": [
        8196
      ]
    },
    "references": [],
    "metadata": {
      "label": "prop-kinetic-necessity"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 31,
      "chapter_file": "chapter_31.json",
      "section_id": "## 12.3. The Complete Lyapunov Drift Under Cloning"
    },
    "generated_at": "2025-11-10T13:26:08.168931+00:00",
    "alt_labels": []
  },
  {
    "label": "prop-coupling-constant-existence",
    "title": "Existence of Valid Coupling Constants",
    "type": "proposition",
    "nl_statement": "There exist positive coupling constants \\(c_V, c_B > 0\\) that satisfy the synergistic drift condition, provided the algorithmic parameters meet conditions on cloning quality and responsiveness, kinetic friction and confinement, noise levels, and a balance inequality.",
    "equations": [
      {
        "label": null,
        "latex": "\\frac{\\kappa_x}{\\text{(kinetic diffusion)}} > 1, \\quad \\frac{\\kappa_v}{\\text{(cloning velocity expansion)}} > 1, \\quad \\frac{\\kappa_W}{C_W} > 1"
      }
    ],
    "hypotheses": [
      {
        "text": "Sufficient measurement quality: \u03b5 > \u03b5_min for detectable variance",
        "latex": null
      },
      {
        "text": "Sufficient cloning responsiveness: \u03b5_clone small, p_max large",
        "latex": null
      },
      {
        "text": "Sufficient fitness weight on rewards: \u03b2 > 0 for boundary detection",
        "latex": null
      },
      {
        "text": "Sufficient friction: \u03b3 > \u03b3_min for velocity dissipation",
        "latex": null
      },
      {
        "text": "Sufficient confinement: \u2225\u2207U(x)\u2225 large enough far from equilibrium",
        "latex": null
      },
      {
        "text": "Small enough noise: \u03c3_v\u00b2 to prevent excessive velocity heating",
        "latex": null
      },
      {
        "text": "Balance condition",
        "latex": "\\frac{\\kappa_x}{\\text{(kinetic diffusion)}} > 1, \\quad \\frac{\\kappa_v}{\\text{(cloning velocity expansion)}} > 1, \\quad \\frac{\\kappa_W}{C_W} > 1"
      }
    ],
    "conclusion": {
      "text": "There exist coupling constants c_V, c_B > 0 that satisfy the synergistic drift condition",
      "latex": "There exist coupling constants $c_V, c_B > 0$ that satisfy the synergistic drift condition"
    },
    "variables": [
      {
        "symbol": "c_V",
        "name": "coupling constant V",
        "description": "Positive coupling constant for velocity component",
        "constraints": [
          "c_V > 0"
        ],
        "tags": [
          "coupling"
        ]
      },
      {
        "symbol": "c_B",
        "name": "coupling constant B",
        "description": "Positive coupling constant for boundary component",
        "constraints": [
          "c_B > 0"
        ],
        "tags": [
          "coupling"
        ]
      },
      {
        "symbol": "\u03b5",
        "name": "measurement quality",
        "description": "Parameter for measurement quality in cloning",
        "constraints": [
          "\u03b5 > \u03b5_min"
        ],
        "tags": [
          "cloning"
        ]
      },
      {
        "symbol": "\u03b5_min",
        "name": "minimum measurement quality",
        "description": "Threshold for detectable variance",
        "constraints": [],
        "tags": [
          "cloning"
        ]
      },
      {
        "symbol": "\u03b5_clone",
        "name": "cloning responsiveness",
        "description": "Parameter controlling cloning sensitivity",
        "constraints": [
          "small"
        ],
        "tags": [
          "cloning"
        ]
      },
      {
        "symbol": "p_max",
        "name": "maximum cloning probability",
        "description": "Upper bound on cloning probability",
        "constraints": [
          "large"
        ],
        "tags": [
          "cloning"
        ]
      },
      {
        "symbol": "\u03b2",
        "name": "fitness weight",
        "description": "Weight on rewards for boundary detection",
        "constraints": [
          "\u03b2 > 0"
        ],
        "tags": [
          "fitness"
        ]
      },
      {
        "symbol": "\u03b3",
        "name": "friction coefficient",
        "description": "Kinetic friction for velocity dissipation",
        "constraints": [
          "\u03b3 > \u03b3_min"
        ],
        "tags": [
          "kinetic"
        ]
      },
      {
        "symbol": "\u03b3_min",
        "name": "minimum friction",
        "description": "Threshold for sufficient friction",
        "constraints": [],
        "tags": [
          "kinetic"
        ]
      },
      {
        "symbol": "\u03c3_v\u00b2",
        "name": "velocity noise variance",
        "description": "Noise level to control velocity heating",
        "constraints": [
          "small"
        ],
        "tags": [
          "noise"
        ]
      },
      {
        "symbol": "\u03ba_x",
        "name": "position scaling",
        "description": "Scaling factor for position in balance",
        "constraints": [],
        "tags": [
          "balance"
        ]
      },
      {
        "symbol": "\u03ba_v",
        "name": "velocity scaling",
        "description": "Scaling factor for velocity in balance",
        "constraints": [],
        "tags": [
          "balance"
        ]
      },
      {
        "symbol": "\u03ba_W",
        "name": "weight scaling",
        "description": "Scaling factor for weights in balance",
        "constraints": [],
        "tags": [
          "balance"
        ]
      },
      {
        "symbol": "C_W",
        "name": "weight constant",
        "description": "Constant bounding weight complexity",
        "constraints": [],
        "tags": [
          "balance"
        ]
      }
    ],
    "implicit_assumptions": [
      {
        "text": "The synergistic drift condition is well-defined based on prior kinetic and cloning dynamics",
        "confidence": 0.9
      },
      {
        "text": "Algorithmic parameters are positive real numbers where applicable",
        "confidence": 1.0
      }
    ],
    "local_refs": [],
    "proof": null,
    "tags": [
      "existence",
      "coupling constants",
      "synergistic drift",
      "cloning parameters",
      "kinetic parameters",
      "balance condition"
    ],
    "content_markdown": ":label: prop-coupling-constant-existence\n\nThere exist coupling constants $c_V, c_B > 0$ that satisfy the synergistic drift condition, provided the algorithmic parameters satisfy:\n\n**Cloning Parameters:**\n- Sufficient measurement quality: $\\epsilon > \\epsilon_{\\min}$ for detectable variance\n- Sufficient cloning responsiveness: $\\varepsilon_{\\text{clone}}$ small, $p_{\\max}$ large\n- Sufficient fitness weight on rewards: $\\beta > 0$ for boundary detection\n\n**Kinetic Parameters:**\n- Sufficient friction: $\\gamma > \\gamma_{\\min}$ for velocity dissipation\n- Sufficient confinement: $\\|\\nabla U(x)\\|$ large enough far from equilibrium\n- Small enough noise: $\\sigma_v^2$ to prevent excessive velocity heating\n\n**Balance Condition:**\n\n$$\n\\frac{\\kappa_x}{\\text{(kinetic diffusion)}} > 1, \\quad \\frac{\\kappa_v}{\\text{(cloning velocity expansion)}} > 1, \\quad \\frac{\\kappa_W}{C_W} > 1",
    "raw_directive": "8343: ### 12.4.3. Parameter Balancing\n8344: \n8345: :::{prf:proposition} Existence of Valid Coupling Constants\n8346: :label: prop-coupling-constant-existence\n8347: \n8348: There exist coupling constants $c_V, c_B > 0$ that satisfy the synergistic drift condition, provided the algorithmic parameters satisfy:\n8349: \n8350: **Cloning Parameters:**\n8351: - Sufficient measurement quality: $\\epsilon > \\epsilon_{\\min}$ for detectable variance\n8352: - Sufficient cloning responsiveness: $\\varepsilon_{\\text{clone}}$ small, $p_{\\max}$ large\n8353: - Sufficient fitness weight on rewards: $\\beta > 0$ for boundary detection\n8354: \n8355: **Kinetic Parameters:**\n8356: - Sufficient friction: $\\gamma > \\gamma_{\\min}$ for velocity dissipation\n8357: - Sufficient confinement: $\\|\\nabla U(x)\\|$ large enough far from equilibrium\n8358: - Small enough noise: $\\sigma_v^2$ to prevent excessive velocity heating\n8359: \n8360: **Balance Condition:**\n8361: \n8362: $$\n8363: \\frac{\\kappa_x}{\\text{(kinetic diffusion)}} > 1, \\quad \\frac{\\kappa_v}{\\text{(cloning velocity expansion)}} > 1, \\quad \\frac{\\kappa_W}{C_W} > 1\n8364: $$",
    "document_id": "03_cloning",
    "section": "## 12.4. The Synergistic Dissipation Framework",
    "span": {
      "start_line": 8343,
      "end_line": 8364,
      "content_start": 8346,
      "content_end": 8363,
      "header_lines": [
        8344
      ]
    },
    "references": [],
    "metadata": {
      "label": "prop-coupling-constant-existence"
    },
    "registry_context": {
      "stage": "directives",
      "document_id": "03_cloning",
      "chapter_index": 32,
      "chapter_file": "chapter_32.json",
      "section_id": "## 12.4. The Synergistic Dissipation Framework"
    },
    "generated_at": "2025-11-10T13:26:08.168931+00:00",
    "alt_labels": []
  }
]