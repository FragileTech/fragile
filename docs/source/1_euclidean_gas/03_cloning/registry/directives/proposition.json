{
  "document_id": "03_cloning",
  "stage": "directives",
  "directive_type": "proposition",
  "generated_at": "2025-11-12T22:37:33.907857+00:00",
  "count": 12,
  "items": [
    {
      "directive_type": "proposition",
      "label": "prop-barrier-existence",
      "title": "Existence of a Global Smooth Barrier Function",
      "start_line": 215,
      "end_line": 224,
      "header_lines": [
        216
      ],
      "content_start": 218,
      "content_end": 223,
      "content": "218: :label: prop-barrier-existence\n219: \n220: Let $\\mathcal{X}_{\\text{valid}}$ satisfy the conditions of {prf:ref}`axiom-domain-regularity`. Then there exists a function $\\varphi: \\mathcal{X}_{\\text{valid}} \\to \\mathbb{R}$ with the following properties:\n221: 1.  **Smoothness:** $\\varphi(x)$ is $C^{\\infty}$-smooth on $\\mathcal{X}_{\\text{valid}}$.\n222: 2.  **Positivity:** $\\varphi(x)$ is strictly positive for all $x \\in \\mathcal{X}_{\\text{valid}}$.\n223: 3.  **Boundary Divergence:** $\\varphi(x) \\to \\infty$ as $x \\to \\partial \\mathcal{X}_{\\text{valid}}$.",
      "metadata": {
        "label": "prop-barrier-existence"
      },
      "section": "## 2. The Coupled State Space and State Differences",
      "references": [
        "axiom-domain-regularity",
        "def-boundary-potential-recall",
        "def-full-synergistic-lyapunov-function"
      ],
      "raw_directive": "215: Under the assumption of a regular domain, we can state and prove the existence of our desired barrier function.\n216: \n217: :::{prf:proposition} Existence of a Global Smooth Barrier Function\n218: :label: prop-barrier-existence\n219: \n220: Let $\\mathcal{X}_{\\text{valid}}$ satisfy the conditions of {prf:ref}`axiom-domain-regularity`. Then there exists a function $\\varphi: \\mathcal{X}_{\\text{valid}} \\to \\mathbb{R}$ with the following properties:\n221: 1.  **Smoothness:** $\\varphi(x)$ is $C^{\\infty}$-smooth on $\\mathcal{X}_{\\text{valid}}$.\n222: 2.  **Positivity:** $\\varphi(x)$ is strictly positive for all $x \\in \\mathcal{X}_{\\text{valid}}$.\n223: 3.  **Boundary Divergence:** $\\varphi(x) \\to \\infty$ as $x \\to \\partial \\mathcal{X}_{\\text{valid}}$.\n224: ",
      "_registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 2,
        "chapter_file": "chapter_2.json",
        "section_id": "## 2. The Coupled State Space and State Differences"
      }
    },
    {
      "directive_type": "proposition",
      "label": "prop-lyapunov-necessity",
      "title": "Necessity of the Augmented Lyapunov Structure",
      "start_line": 932,
      "end_line": 986,
      "header_lines": [
        933
      ],
      "content_start": 935,
      "content_end": 985,
      "content": "935: :label: prop-lyapunov-necessity\n936: \n937: The Lyapunov function $V_{\\text{total}} = W_h^2 + c_V V_{\\text{Var}} + c_B W_b$ with three distinct weighted components is mathematically necessary for the following reasons:\n938: \n939: **1. Complementary Information Content**\n940: \n941: The two kinematic components measure fundamentally different aspects of swarm ({prf:ref}`def-swarm-and-state-space`) error:\n942: \n943: - **$W_h^2(\\mu_1, \\mu_2)$**: Measures how far apart the two swarms are **as distributions**. This is the squared Wasserstein distance ({prf:ref}`def-n-particle-displacement-metric`) between the full empirical measures $\\mu_1$ and $\\mu_2$. It quantifies the minimal transport cost to transform one swarm 's distribution into the other's.\n944: \n945: - **$V_{\\text{Var}}(S_1, S_2)$**: Measures the **internal dispersion within each swarm**. This is the sum of the internal variances (positional and velocity) of each swarm's alive-walker population.\n946: \n947: These quantities contain **non-redundant information**:\n948: - A system can have **small $W_h^2$ but large $V_{\\text{Var}}$**: Both swarms have similar empirical measures (so Wasserstein distance is small), but each swarm is internally highly dispersed (large variance).\n949: - A system can have **small $V_{\\text{Var}}$ but large $W_h^2$**: Both swarms are internally tight clusters (small variance), but the two tight clusters are far apart in phase space (large Wasserstein distance).\n950: \n951: **2. Operator-Specific Targeting**\n952: \n953: The two stochastic operators act on fundamentally different error components:\n954: \n955: - **The Cloning Operator $\\Psi_{\\text{clone}}$**: Acts **within** each swarm independently. It selects walkers based on their fitness **relative to their own swarm's distribution**. The cloning mechanism directly targets $V_{\\text{Var}}$ by eliminating low-fitness walkers and duplicating high-fitness walkers, thereby reducing the internal spread of each swarm's distribution.\n956: \n957: - **The Kinetic Operator $\\Psi_{\\text{kin}}$**: Contains a drift term $F(x)$ (the negative gradient of a confining potential) that acts on walker positions. This drift causes walkers in both swarms to move toward regions of lower potential, thereby moving both swarms' barycenters toward the same equilibrium. This directly targets $W_h^2$ by reducing the distance between the swarms' centers of mass.\n958: \n959: **3. Synergistic Dissipation Necessity**\n960: \n961: Neither operator can contract the full hypocoercive norm $\\|\\!(\\delta x, \\delta v)\\!\\|_h^2 = \\|\\delta x\\|^2 + \\lambda_v \\|\\delta v\\|^2$ in both position and velocity simultaneously:\n962: \n963: - **Velocity Desynchronization from Cloning**: When the cloning operator duplicates a walker, it adds Gaussian jitter to the velocity: $v_{\\text{new}} = v_{\\text{parent}} + \\mathcal{N}(0, \\delta^2 I_d)$. This randomization **breaks velocity correlations** between swarms, causing the velocity component of the structural error to increase (expansion of the velocity-related parts of $W_h^2$). Additionally, the cloning mechanism creates a distribution of velocities within each swarm that may increase $V_{\\text{Var},v}$.\n964: \n965: - **Positional Diffusion from Kinetic Noise**: The Langevin equation for the kinetic step includes a diffusion term: $dx = (\\text{drift terms}) \\, dt + \\sigma \\, dW$. This stochastic noise **desynchronizes positions** between the two swarms' trajectories, causing positional components to expand. It also contributes to an increase in $V_{\\text{Var},x}$ within each swarm.\n966: \n967: **4. The Weighted Sum as a Solution**\n968: \n969: The augmented Lyapunov function resolves this by allowing us to **balance expansions against contractions**:\n970: \n971: $$\n972: \\mathbb{E}[V_{\\text{total}}(t+1) - V_{\\text{total}}(t)] = \\underbrace{\\mathbb{E}[\\Delta W_h^2]}_{\\Psi_{\\text{clone}}: +, \\ \\Psi_{\\text{kin}}: -} + c_V \\underbrace{\\mathbb{E}[\\Delta V_{\\text{Var}}]}_{\\Psi_{\\text{clone}}: -, \\ \\Psi_{\\text{kin}}: +} + c_B \\underbrace{\\mathbb{E}[\\Delta W_b]}_{\\text{both: } -}\n973: $$\n974: \n975: By choosing the coupling constant $c_V$ appropriately, we can ensure that:\n976: - The **strong contraction** of $V_{\\text{Var}}$ under $\\Psi_{\\text{clone}}$ (weighted by $c_V$) **dominates** the bounded expansion of $W_h^2$ under $\\Psi_{\\text{clone}}$.\n977: - The **strong contraction** of $W_h^2$ under $\\Psi_{\\text{kin}}$ **dominates** the bounded expansion of $c_V V_{\\text{Var}}$ under $\\Psi_{\\text{kin}}$.\n978: \n979: This yields **net negative drift**: $\\mathbb{E}[V_{\\text{total}}(t+1) - V_{\\text{total}}(t)] \\leq -\\kappa V_{\\text{total}}(t) + C$ for some $\\kappa > 0$.\n980: \n981: **5. The Boundary Term $W_b$**\n982: \n983: The term $c_B W_b$ ensures that walkers near the boundary $\\partial \\mathcal{X}_{\\text{valid}}$ are penalized. Both operators have mechanisms that contract this term:\n984: - **$\\Psi_{\\text{clone}}$**: Walkers near the boundary have lower survival probability and are thus eliminated and replaced by clones of interior walkers.\n985: - **$\\Psi_{\\text{kin}}$**: The confining potential $U(x)$ and force field $F(x) = -\\nabla U(x)$ (see {prf:ref}`axiom-lipschitz-fields`) push walkers away from the boundary.",
      "metadata": {
        "label": "prop-lyapunov-necessity"
      },
      "section": "## 3. The Augmented Hypocoercive Lyapunov Function",
      "references": [
        "def-swarm-and-state-space",
        "def-n-particle-displacement-metric",
        "axiom-lipschitz-fields"
      ],
      "raw_directive": "932: The inclusion of both $W_h^2$ (inter-swarm error) and $V_{\\text{Var}}$ (intra-swarm error) in the Lyapunov function is not merely convenient but mathematically necessary. This subsection explains why the specific weighted-sum structure is required for proving convergence.\n933: \n934: :::{prf:proposition} Necessity of the Augmented Lyapunov Structure\n935: :label: prop-lyapunov-necessity\n936: \n937: The Lyapunov function $V_{\\text{total}} = W_h^2 + c_V V_{\\text{Var}} + c_B W_b$ with three distinct weighted components is mathematically necessary for the following reasons:\n938: \n939: **1. Complementary Information Content**\n940: \n941: The two kinematic components measure fundamentally different aspects of swarm ({prf:ref}`def-swarm-and-state-space`) error:\n942: \n943: - **$W_h^2(\\mu_1, \\mu_2)$**: Measures how far apart the two swarms are **as distributions**. This is the squared Wasserstein distance ({prf:ref}`def-n-particle-displacement-metric`) between the full empirical measures $\\mu_1$ and $\\mu_2$. It quantifies the minimal transport cost to transform one swarm 's distribution into the other's.\n944: \n945: - **$V_{\\text{Var}}(S_1, S_2)$**: Measures the **internal dispersion within each swarm**. This is the sum of the internal variances (positional and velocity) of each swarm's alive-walker population.\n946: \n947: These quantities contain **non-redundant information**:\n948: - A system can have **small $W_h^2$ but large $V_{\\text{Var}}$**: Both swarms have similar empirical measures (so Wasserstein distance is small), but each swarm is internally highly dispersed (large variance).\n949: - A system can have **small $V_{\\text{Var}}$ but large $W_h^2$**: Both swarms are internally tight clusters (small variance), but the two tight clusters are far apart in phase space (large Wasserstein distance).\n950: \n951: **2. Operator-Specific Targeting**\n952: \n953: The two stochastic operators act on fundamentally different error components:\n954: \n955: - **The Cloning Operator $\\Psi_{\\text{clone}}$**: Acts **within** each swarm independently. It selects walkers based on their fitness **relative to their own swarm's distribution**. The cloning mechanism directly targets $V_{\\text{Var}}$ by eliminating low-fitness walkers and duplicating high-fitness walkers, thereby reducing the internal spread of each swarm's distribution.\n956: \n957: - **The Kinetic Operator $\\Psi_{\\text{kin}}$**: Contains a drift term $F(x)$ (the negative gradient of a confining potential) that acts on walker positions. This drift causes walkers in both swarms to move toward regions of lower potential, thereby moving both swarms' barycenters toward the same equilibrium. This directly targets $W_h^2$ by reducing the distance between the swarms' centers of mass.\n958: \n959: **3. Synergistic Dissipation Necessity**\n960: \n961: Neither operator can contract the full hypocoercive norm $\\|\\!(\\delta x, \\delta v)\\!\\|_h^2 = \\|\\delta x\\|^2 + \\lambda_v \\|\\delta v\\|^2$ in both position and velocity simultaneously:\n962: \n963: - **Velocity Desynchronization from Cloning**: When the cloning operator duplicates a walker, it adds Gaussian jitter to the velocity: $v_{\\text{new}} = v_{\\text{parent}} + \\mathcal{N}(0, \\delta^2 I_d)$. This randomization **breaks velocity correlations** between swarms, causing the velocity component of the structural error to increase (expansion of the velocity-related parts of $W_h^2$). Additionally, the cloning mechanism creates a distribution of velocities within each swarm that may increase $V_{\\text{Var},v}$.\n964: \n965: - **Positional Diffusion from Kinetic Noise**: The Langevin equation for the kinetic step includes a diffusion term: $dx = (\\text{drift terms}) \\, dt + \\sigma \\, dW$. This stochastic noise **desynchronizes positions** between the two swarms' trajectories, causing positional components to expand. It also contributes to an increase in $V_{\\text{Var},x}$ within each swarm.\n966: \n967: **4. The Weighted Sum as a Solution**\n968: \n969: The augmented Lyapunov function resolves this by allowing us to **balance expansions against contractions**:\n970: \n971: $$\n972: \\mathbb{E}[V_{\\text{total}}(t+1) - V_{\\text{total}}(t)] = \\underbrace{\\mathbb{E}[\\Delta W_h^2]}_{\\Psi_{\\text{clone}}: +, \\ \\Psi_{\\text{kin}}: -} + c_V \\underbrace{\\mathbb{E}[\\Delta V_{\\text{Var}}]}_{\\Psi_{\\text{clone}}: -, \\ \\Psi_{\\text{kin}}: +} + c_B \\underbrace{\\mathbb{E}[\\Delta W_b]}_{\\text{both: } -}\n973: $$\n974: \n975: By choosing the coupling constant $c_V$ appropriately, we can ensure that:\n976: - The **strong contraction** of $V_{\\text{Var}}$ under $\\Psi_{\\text{clone}}$ (weighted by $c_V$) **dominates** the bounded expansion of $W_h^2$ under $\\Psi_{\\text{clone}}$.\n977: - The **strong contraction** of $W_h^2$ under $\\Psi_{\\text{kin}}$ **dominates** the bounded expansion of $c_V V_{\\text{Var}}$ under $\\Psi_{\\text{kin}}$.\n978: \n979: This yields **net negative drift**: $\\mathbb{E}[V_{\\text{total}}(t+1) - V_{\\text{total}}(t)] \\leq -\\kappa V_{\\text{total}}(t) + C$ for some $\\kappa > 0$.\n980: \n981: **5. The Boundary Term $W_b$**\n982: \n983: The term $c_B W_b$ ensures that walkers near the boundary $\\partial \\mathcal{X}_{\\text{valid}}$ are penalized. Both operators have mechanisms that contract this term:\n984: - **$\\Psi_{\\text{clone}}$**: Walkers near the boundary have lower survival probability and are thus eliminated and replaced by clones of interior walkers.\n985: - **$\\Psi_{\\text{kin}}$**: The confining potential $U(x)$ and force field $F(x) = -\\nabla U(x)$ (see {prf:ref}`axiom-lipschitz-fields`) push walkers away from the boundary.\n986: ",
      "_registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 3,
        "chapter_file": "chapter_3.json",
        "section_id": "## 3. The Augmented Hypocoercive Lyapunov Function"
      }
    },
    {
      "directive_type": "proposition",
      "label": "prop-bounded-velocity-expansion",
      "title": "Bounded Velocity Variance Expansion from Cloning",
      "start_line": 2115,
      "end_line": 2125,
      "header_lines": [
        2116
      ],
      "content_start": 2118,
      "content_end": 2124,
      "content": "2118: :label: prop-bounded-velocity-expansion\n2119: \n2120: For any cloning event where a fraction $f_{\\text{clone}}$ of walkers are cloned with restitution coefficient $\\alpha_{\\text{restitution}}$, the change in internal velocity variance from the velocity resets is bounded:\n2121: \n2122: $$\n2123: \\Delta V_{Var,v} \\leq f_{\\text{clone}} \\cdot C_{\\text{reset}} \\cdot V_{\\max,\\text{KE}}\n2124: $$",
      "metadata": {
        "label": "prop-bounded-velocity-expansion"
      },
      "section": "## 5. The Measurement and Interaction Pipeline",
      "references": [
        "def-walker"
      ],
      "raw_directive": "2115: The following proposition formalizes the key property that enables the synergistic dissipation framework: the expansion of velocity variance caused by cloning is uniformly bounded.\n2116: \n2117: :::{prf:proposition} Bounded Velocity Variance Expansion from Cloning\n2118: :label: prop-bounded-velocity-expansion\n2119: \n2120: For any cloning event where a fraction $f_{\\text{clone}}$ of walkers are cloned with restitution coefficient $\\alpha_{\\text{restitution}}$, the change in internal velocity variance from the velocity resets is bounded:\n2121: \n2122: $$\n2123: \\Delta V_{Var,v} \\leq f_{\\text{clone}} \\cdot C_{\\text{reset}} \\cdot V_{\\max,\\text{KE}}\n2124: $$\n2125: ",
      "_registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 5,
        "chapter_file": "chapter_5.json",
        "section_id": "## 5. The Measurement and Interaction Pipeline"
      }
    },
    {
      "directive_type": "proposition",
      "label": "prop-satisfiability-of-snr-gamma",
      "title": "**(Satisfiability of the Signal-to-Noise Condition via Signal Gain)**",
      "start_line": 3514,
      "end_line": 3526,
      "header_lines": [
        3515
      ],
      "content_start": 3517,
      "content_end": 3525,
      "content": "3517: :label: prop-satisfiability-of-snr-gamma\n3518: \n3519: Let the rescaled diversity values be defined as $d'_i = g_A(\\gamma · z_{d,i}) + \\eta$, where $\\gamma > 0$ is a user-defined **Signal Gain** parameter and `g_A` is any function satisfying the **Axiom of a Well-Behaved Rescale Function ({prf:ref}`def-canonical-logistic-rescale-function-example`)** (see {prf:ref}`def-logistic-rescale` for the canonical choice).\n3520: \n3521: For any system in a high-error state (`Var(x) > R^{2}_var`) that generates a non-zero raw distance signal ($\\kappa_meas(d) > 0$), there exists a sufficiently large choice of $\\gamma$ that satisfies the **Signal-to-Noise Condition**:\n3522: \n3523: $$\n3524: \\kappa_{\\mathrm{var}}(d') > \\operatorname{Var}_{\\max}(d')\n3525: $$",
      "metadata": {
        "label": "prop-satisfiability-of-snr-gamma"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "references": [
        "def-canonical-logistic-rescale-function-example",
        "def-logistic-rescale"
      ],
      "raw_directive": "3514: This section provides the formal proof that this condition, which we call the **Signal-to-Noise Condition**, is not an unstated assumption but a satisfiable criterion that can be met by a valid choice of the algorithm's user-defined parameters. We prove this by introducing a **Signal Gain** parameter, $\\gamma$, which acts as a sensitivity knob for the algorithm. This proves that the system is fundamentally \"learnable\": the signal generated by geometric error can always be amplified sufficiently to overcome the worst-case statistical noise, ensuring that a true difference between the high-error and low-error populations is always detectable.\n3515: \n3516: :::{prf:proposition} **(Satisfiability of the Signal-to-Noise Condition via Signal Gain)**\n3517: :label: prop-satisfiability-of-snr-gamma\n3518: \n3519: Let the rescaled diversity values be defined as $d'_i = g_A(\\gamma · z_{d,i}) + \\eta$, where $\\gamma > 0$ is a user-defined **Signal Gain** parameter and `g_A` is any function satisfying the **Axiom of a Well-Behaved Rescale Function ({prf:ref}`def-canonical-logistic-rescale-function-example`)** (see {prf:ref}`def-logistic-rescale` for the canonical choice).\n3520: \n3521: For any system in a high-error state (`Var(x) > R^{2}_var`) that generates a non-zero raw distance signal ($\\kappa_meas(d) > 0$), there exists a sufficiently large choice of $\\gamma$ that satisfies the **Signal-to-Noise Condition**:\n3522: \n3523: $$\n3524: \\kappa_{\\mathrm{var}}(d') > \\operatorname{Var}_{\\max}(d')\n3525: $$\n3526: ",
      "_registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      }
    },
    {
      "directive_type": "proposition",
      "label": "prop-corrective-signal-bound",
      "title": "**(Lower Bound on the Corrective Diversity Signal)**",
      "start_line": 4329,
      "end_line": 4343,
      "header_lines": [
        4330
      ],
      "content_start": 4332,
      "content_end": 4342,
      "content": "4332: :label: prop-corrective-signal-bound\n4333: \n4334: Let a swarm ({prf:ref}`def-swarm-and-state-space`) state be in the high-error regime, such that the variance of its rescaled diversity values, `d'`, is bounded below, $\\operatorname{Var}(d') \\ge \\kappa_{d', \\text{var}} > 0$. Let the system parameters be chosen such that the Signal-to-Noise Condition of [](#lem-variance-to-mean-separation) is satisfied, i.e., $\\kappa_{d', \\text{var}} > \\operatorname{Var}_{\\max}(d')$.\n4335: \n4336: Then the expected logarithmic gap in the diversity signal between the high-error population $H_k$ and the low-error population $L_k$ is bounded below by a strictly positive, N-uniform constant:\n4337: \n4338: $$\n4339: \\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right) > 0\n4340: $$\n4341: \n4342: where $\\kappa_{d', \\text{mean}} := \\frac{1}{\\sqrt{f_H f_L}}\\sqrt{\\kappa_{d', \\text{var}} - \\operatorname{Var}_{\\max}(d')}$.",
      "metadata": {
        "label": "prop-corrective-signal-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "references": [
        "def-swarm-and-state-space",
        "thm-stability-condition-final-corrected"
      ],
      "raw_directive": "4329: This proposition forges the complete link from a macroscopic state of high geometric error to a guaranteed, non-vanishing corrective signal in the logarithmic space of the fitness potential.\n4330: \n4331: :::{prf:proposition} **(Lower Bound on the Corrective Diversity Signal)**\n4332: :label: prop-corrective-signal-bound\n4333: \n4334: Let a swarm ({prf:ref}`def-swarm-and-state-space`) state be in the high-error regime, such that the variance of its rescaled diversity values, `d'`, is bounded below, $\\operatorname{Var}(d') \\ge \\kappa_{d', \\text{var}} > 0$. Let the system parameters be chosen such that the Signal-to-Noise Condition of [](#lem-variance-to-mean-separation) is satisfied, i.e., $\\kappa_{d', \\text{var}} > \\operatorname{Var}_{\\max}(d')$.\n4335: \n4336: Then the expected logarithmic gap in the diversity signal between the high-error population $H_k$ and the low-error population $L_k$ is bounded below by a strictly positive, N-uniform constant:\n4337: \n4338: $$\n4339: \\mathbb{E}[\\ln(d')|H_k] - \\mathbb{E}[\\ln(d')|L_k] \\ge \\ln\\left(1 + \\frac{\\kappa_{d', \\text{mean}}}{g_{A,\\max}+\\eta}\\right) > 0\n4340: $$\n4341: \n4342: where $\\kappa_{d', \\text{mean}} := \\frac{1}{\\sqrt{f_H f_L}}\\sqrt{\\kappa_{d', \\text{var}} - \\operatorname{Var}_{\\max}(d')}$.\n4343: ",
      "_registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      }
    },
    {
      "directive_type": "proposition",
      "label": "prop-adversarial-signal-bound-naive",
      "title": "**(Worst-Case Upper Bound on the Adversarial Reward Signal)**",
      "start_line": 4377,
      "end_line": 4386,
      "header_lines": [
        4378
      ],
      "content_start": 4380,
      "content_end": 4385,
      "content": "4380: :label: prop-adversarial-signal-bound-naive\n4381: \n4382: For any swarm ({prf:ref}`def-swarm-and-state-space`) state, the maximum possible expected logarithmic gap in the rescaled reward signal, $r'$, between the low-error and high-error populations is uniformly bounded above by a constant derived only from the rescale function ({prf:ref}`def-canonical-logistic-rescale-function-example`)'s range:\n4383: \n4384: $$\n4385: \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)",
      "metadata": {
        "label": "prop-adversarial-signal-bound-naive"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "references": [
        "def-swarm-and-state-space",
        "def-canonical-logistic-rescale-function-example"
      ],
      "raw_directive": "4377: Before deriving the final stability condition, it is instructive to first establish a \"naive\" upper bound on the adversarial reward signal. This bound considers the absolute worst-case scenario allowed by the range of the rescale function, without yet invoking the axioms that constrain the reward landscape's structure. This will serve as a baseline to demonstrate the critical importance of those axioms.\n4378: \n4379: :::{prf:proposition} **(Worst-Case Upper Bound on the Adversarial Reward Signal)**\n4380: :label: prop-adversarial-signal-bound-naive\n4381: \n4382: For any swarm ({prf:ref}`def-swarm-and-state-space`) state, the maximum possible expected logarithmic gap in the rescaled reward signal, $r'$, between the low-error and high-error populations is uniformly bounded above by a constant derived only from the rescale function ({prf:ref}`def-canonical-logistic-rescale-function-example`)'s range:\n4383: \n4384: $$\n4385: \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{g_{A,\\max}}{\\eta}\\right)\n4386: $$",
      "_registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      }
    },
    {
      "directive_type": "proposition",
      "label": "prop-raw-reward-mean-gap-bound",
      "title": "**(Lipschitz Bound on the Raw Reward Mean Gap)**",
      "start_line": 4440,
      "end_line": 4451,
      "header_lines": [
        4441
      ],
      "content_start": 4443,
      "content_end": 4450,
      "content": "4443: :label: prop-raw-reward-mean-gap-bound\n4444: \n4445: Let the reward function's positional component, $R_{\\text{pos}}(x)$, be Lipschitz continuous on the valid domain $\\mathcal{X}_{\\text{valid}}$ with constant $L_{R}$, as per the **Axiom of Reward Regularity ({prf:ref}`axiom-reward-regularity`)**. Let the diameter of $\\mathcal{X}_{\\text{valid}}$ be $D_{\\text{valid}}$.\n4446: \n4447: For any swarm ({prf:ref}`def-swarm-and-state-space`), the absolute difference between the mean raw rewards of the high-error population $H_k$ and the low-error population $L_k$ is uniformly bounded:\n4448: \n4449: $$\n4450: |\\mu_R(L_k) - \\mu_R(H_k)| \\le L_{R} \\cdot D_{\\mathrm{valid}} =: \\kappa_{\\mathrm{raw},r,\\text{adv}}",
      "metadata": {
        "label": "prop-raw-reward-mean-gap-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "references": [
        "axiom-reward-regularity",
        "def-swarm-and-state-space"
      ],
      "raw_directive": "4440: The following propositions build a chain of reasoning from the Lipschitz continuity of the raw reward function to a final, tight bound on the expected logarithmic gap.\n4441: \n4442: :::{prf:proposition} **(Lipschitz Bound on the Raw Reward Mean Gap)**\n4443: :label: prop-raw-reward-mean-gap-bound\n4444: \n4445: Let the reward function's positional component, $R_{\\text{pos}}(x)$, be Lipschitz continuous on the valid domain $\\mathcal{X}_{\\text{valid}}$ with constant $L_{R}$, as per the **Axiom of Reward Regularity ({prf:ref}`axiom-reward-regularity`)**. Let the diameter of $\\mathcal{X}_{\\text{valid}}$ be $D_{\\text{valid}}$.\n4446: \n4447: For any swarm ({prf:ref}`def-swarm-and-state-space`), the absolute difference between the mean raw rewards of the high-error population $H_k$ and the low-error population $L_k$ is uniformly bounded:\n4448: \n4449: $$\n4450: |\\mu_R(L_k) - \\mu_R(H_k)| \\le L_{R} \\cdot D_{\\mathrm{valid}} =: \\kappa_{\\mathrm{raw},r,\\text{adv}}\n4451: $$",
      "_registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      }
    },
    {
      "directive_type": "proposition",
      "label": "prop-log-reward-gap-axiom-bound",
      "title": "**(Axiom-Based Bound on the Logarithmic Reward Gap)**",
      "start_line": 4477,
      "end_line": 4487,
      "header_lines": [
        4478
      ],
      "content_start": 4480,
      "content_end": 4486,
      "content": "4480: :label: prop-log-reward-gap-axiom-bound\n4481: \n4482: Under the **Axiom of Reward Regularity ({prf:ref}`axiom-reward-regularity`)**, the expected logarithmic gap in the rescaled reward signal is bounded by:\n4483: \n4484: $$\n4485: \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R \\cdot D_{\\mathrm{valid}})}{\\eta}\\right)\n4486: $$",
      "metadata": {
        "label": "prop-log-reward-gap-axiom-bound"
      },
      "section": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation",
      "references": [
        "axiom-reward-regularity"
      ],
      "raw_directive": "4477: This raw reward gap now propagates through the measurement pipeline.\n4478: \n4479: :::{prf:proposition} **(Axiom-Based Bound on the Logarithmic Reward Gap)**\n4480: :label: prop-log-reward-gap-axiom-bound\n4481: \n4482: Under the **Axiom of Reward Regularity ({prf:ref}`axiom-reward-regularity`)**, the expected logarithmic gap in the rescaled reward signal is bounded by:\n4483: \n4484: $$\n4485: \\mathbb{E}[\\ln(r')|L_k] - \\mathbb{E}[\\ln(r')|H_k] \\le \\ln\\left(1 + \\frac{\\kappa_{\\mathrm{rescaled}}(L_R \\cdot D_{\\mathrm{valid}})}{\\eta}\\right)\n4486: $$\n4487: ",
      "_registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 7. The Corrective Nature of Fitness: From Signal Generation to Intelligent Adaptation"
      }
    },
    {
      "directive_type": "proposition",
      "label": "prop-n-uniformity-keystone",
      "title": "N-Uniformity of Keystone Constants",
      "start_line": 5601,
      "end_line": 5611,
      "header_lines": [
        5602
      ],
      "content_start": 5604,
      "content_end": 5610,
      "content": "5604: :label: prop-n-uniformity-keystone\n5605: \n5606: The Keystone constants $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are strictly independent of the swarm ({prf:ref}`def-swarm-and-state-space`) size $N$. More precisely, for any fixed choice of system parameters ($\\epsilon$, domain, pipeline parameters, etc.), there exist finite positive constants $\\chi_0(\\epsilon)$ and $g_0(\\epsilon)$ such that for all $N \\geq 2$:\n5607: \n5608: $$\n5609: \\chi(\\epsilon) = \\chi_0(\\epsilon) \\quad \\text{and} \\quad g_{\\max}(\\epsilon) = g_0(\\epsilon)\n5610: $$",
      "metadata": {
        "label": "prop-n-uniformity-keystone"
      },
      "section": "## 8. The N-Uniform Quantitative Keystone Lemma",
      "references": [
        "def-swarm-and-state-space"
      ],
      "raw_directive": "5601: We now provide a formal verification that both Keystone constants, $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$, are **strictly independent of the swarm size N**, establishing that they are $O(1)$ as $N \\to \\infty$.\n5602: \n5603: :::{prf:proposition} N-Uniformity of Keystone Constants\n5604: :label: prop-n-uniformity-keystone\n5605: \n5606: The Keystone constants $\\chi(\\epsilon)$ and $g_{\\max}(\\epsilon)$ are strictly independent of the swarm ({prf:ref}`def-swarm-and-state-space`) size $N$. More precisely, for any fixed choice of system parameters ($\\epsilon$, domain, pipeline parameters, etc.), there exist finite positive constants $\\chi_0(\\epsilon)$ and $g_0(\\epsilon)$ such that for all $N \\geq 2$:\n5607: \n5608: $$\n5609: \\chi(\\epsilon) = \\chi_0(\\epsilon) \\quad \\text{and} \\quad g_{\\max}(\\epsilon) = g_0(\\epsilon)\n5610: $$\n5611: ",
      "_registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 8,
        "chapter_file": "chapter_8.json",
        "section_id": "## 8. The N-Uniform Quantitative Keystone Lemma"
      }
    },
    {
      "directive_type": "proposition",
      "label": "prop-expected-displacement-cloning",
      "title": "Expected Displacement Under Cloning",
      "start_line": 6292,
      "end_line": 6304,
      "header_lines": [
        6293
      ],
      "content_start": 6295,
      "content_end": 6303,
      "content": "6295: :label: prop-expected-displacement-cloning\n6296: \n6297: For walker ({prf:ref}`def-walker`) $i$ with cloning probability $p_i$, the expected squared position displacement satisfies:\n6298: \n6299: $$\n6300: \\mathbb{E}[\\|\\Delta x_i\\|^2 \\mid S] \\leq p_i \\cdot D_{\\text{max}}^2\n6301: $$\n6302: \n6303: where $D_{\\text{max}}$ is the maximum distance in the valid domain (or a suitable bound on the jitter kernel range).",
      "metadata": {
        "label": "prop-expected-displacement-cloning"
      },
      "section": "## 9.5. Key Quantities for Drift Analysis",
      "references": [
        "def-walker"
      ],
      "raw_directive": "6292: :::\n6293: \n6294: :::{prf:proposition} Expected Displacement Under Cloning\n6295: :label: prop-expected-displacement-cloning\n6296: \n6297: For walker ({prf:ref}`def-walker`) $i$ with cloning probability $p_i$, the expected squared position displacement satisfies:\n6298: \n6299: $$\n6300: \\mathbb{E}[\\|\\Delta x_i\\|^2 \\mid S] \\leq p_i \\cdot D_{\\text{max}}^2\n6301: $$\n6302: \n6303: where $D_{\\text{max}}$ is the maximum distance in the valid domain (or a suitable bound on the jitter kernel range).\n6304: ",
      "_registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 13,
        "chapter_file": "chapter_13.json",
        "section_id": "## 9.5. Key Quantities for Drift Analysis"
      }
    },
    {
      "directive_type": "proposition",
      "label": "prop-kinetic-necessity",
      "title": "Necessity of the Kinetic Operator",
      "start_line": 8246,
      "end_line": 8261,
      "header_lines": [
        8247
      ],
      "content_start": 8249,
      "content_end": 8260,
      "content": "8249: :label: prop-kinetic-necessity\n8250: \n8251: The cloning operator ({prf:ref}`def-cloning-operator`) alone cannot guarantee convergence to a quasi-stationary distribution ({prf:ref}`def-qsd`). Specifically:\n8252: \n8253: 1. **Velocity variance accumulation:** The bounded expansion $+C_v$ per step can accumulate without bound over infinite time if not countered.\n8254: \n8255: 2. **Inter-swarm ({prf:ref}`def-swarm-and-state-space`) divergence:** The bounded expansion $+C_W$ means the two coupled swarms can drift arbitrarily far apart without inter-swarm correction.\n8256: \n8257: 3. **No velocity equilibrium:** Cloning has no mechanism to dissipate kinetic energy toward a target distribution - it only redistributes it through collisions.\n8258: \n8259: Therefore, the **kinetic operator is essential** to:\n8260: - Contract $V_{\\text{Var},v}$ via Langevin friction (overcoming $C_v$)",
      "metadata": {
        "label": "prop-kinetic-necessity"
      },
      "section": "## 12.3. The Complete Lyapunov Drift Under Cloning",
      "references": [
        "def-cloning-operator",
        "def-qsd",
        "def-swarm-and-state-space"
      ],
      "raw_directive": "8246: ### 12.3.3. Why Cloning Alone Cannot Achieve Convergence\n8247: \n8248: :::{prf:proposition} Necessity of the Kinetic Operator\n8249: :label: prop-kinetic-necessity\n8250: \n8251: The cloning operator ({prf:ref}`def-cloning-operator`) alone cannot guarantee convergence to a quasi-stationary distribution ({prf:ref}`def-qsd`). Specifically:\n8252: \n8253: 1. **Velocity variance accumulation:** The bounded expansion $+C_v$ per step can accumulate without bound over infinite time if not countered.\n8254: \n8255: 2. **Inter-swarm ({prf:ref}`def-swarm-and-state-space`) divergence:** The bounded expansion $+C_W$ means the two coupled swarms can drift arbitrarily far apart without inter-swarm correction.\n8256: \n8257: 3. **No velocity equilibrium:** Cloning has no mechanism to dissipate kinetic energy toward a target distribution - it only redistributes it through collisions.\n8258: \n8259: Therefore, the **kinetic operator is essential** to:\n8260: - Contract $V_{\\text{Var},v}$ via Langevin friction (overcoming $C_v$)\n8261: - Contract $V_W$ via hypocoercive drift and confining potential (overcoming $C_W$)",
      "_registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 31,
        "chapter_file": "chapter_31.json",
        "section_id": "## 12.3. The Complete Lyapunov Drift Under Cloning"
      }
    },
    {
      "directive_type": "proposition",
      "label": "prop-coupling-constant-existence",
      "title": "Existence of Valid Coupling Constants",
      "start_line": 8394,
      "end_line": 8415,
      "header_lines": [
        8395
      ],
      "content_start": 8397,
      "content_end": 8414,
      "content": "8397: :label: prop-coupling-constant-existence\n8398: \n8399: There exist coupling constants $c_V, c_B > 0$ that satisfy the synergistic drift condition, provided the algorithmic parameters satisfy:\n8400: \n8401: **Cloning Parameters:**\n8402: - Sufficient measurement quality: $\\epsilon > \\epsilon_{\\min}$ for detectable variance\n8403: - Sufficient cloning responsiveness: $\\varepsilon_{\\text{clone}}$ small, $p_{\\max}$ large\n8404: - Sufficient fitness weight on rewards: $\\beta > 0$ for boundary detection\n8405: \n8406: **Kinetic Parameters:**\n8407: - Sufficient friction: $\\gamma > \\gamma_{\\min}$ for velocity dissipation\n8408: - Sufficient confinement: $\\|\\nabla U(x)\\|$ large enough far from equilibrium\n8409: - Small enough noise: $\\sigma_v^2$ to prevent excessive velocity heating\n8410: \n8411: **Balance Condition:**\n8412: \n8413: $$\n8414: \\frac{\\kappa_x}{\\text{(kinetic diffusion)}} > 1, \\quad \\frac{\\kappa_v}{\\text{(cloning velocity expansion)}} > 1, \\quad \\frac{\\kappa_W}{C_W} > 1",
      "metadata": {
        "label": "prop-coupling-constant-existence"
      },
      "section": "## 12.4. The Synergistic Dissipation Framework",
      "references": [],
      "raw_directive": "8394: ### 12.4.3. Parameter Balancing\n8395: \n8396: :::{prf:proposition} Existence of Valid Coupling Constants\n8397: :label: prop-coupling-constant-existence\n8398: \n8399: There exist coupling constants $c_V, c_B > 0$ that satisfy the synergistic drift condition, provided the algorithmic parameters satisfy:\n8400: \n8401: **Cloning Parameters:**\n8402: - Sufficient measurement quality: $\\epsilon > \\epsilon_{\\min}$ for detectable variance\n8403: - Sufficient cloning responsiveness: $\\varepsilon_{\\text{clone}}$ small, $p_{\\max}$ large\n8404: - Sufficient fitness weight on rewards: $\\beta > 0$ for boundary detection\n8405: \n8406: **Kinetic Parameters:**\n8407: - Sufficient friction: $\\gamma > \\gamma_{\\min}$ for velocity dissipation\n8408: - Sufficient confinement: $\\|\\nabla U(x)\\|$ large enough far from equilibrium\n8409: - Small enough noise: $\\sigma_v^2$ to prevent excessive velocity heating\n8410: \n8411: **Balance Condition:**\n8412: \n8413: $$\n8414: \\frac{\\kappa_x}{\\text{(kinetic diffusion)}} > 1, \\quad \\frac{\\kappa_v}{\\text{(cloning velocity expansion)}} > 1, \\quad \\frac{\\kappa_W}{C_W} > 1\n8415: $$",
      "_registry_context": {
        "stage": "directives",
        "document_id": "03_cloning",
        "chapter_index": 32,
        "chapter_file": "chapter_32.json",
        "section_id": "## 12.4. The Synergistic Dissipation Framework"
      }
    }
  ]
}