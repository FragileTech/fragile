{
  "document_id": "06_convergence",
  "stage": "directives",
  "directive_type": "algorithm",
  "generated_at": "2025-11-12T22:37:33.915978+00:00",
  "count": 3,
  "items": [
    {
      "directive_type": "algorithm",
      "label": "alg-param-selection",
      "title": "Parameter Selection for Optimal Convergence",
      "start_line": 1953,
      "end_line": 2034,
      "header_lines": [
        1954
      ],
      "content_start": 1956,
      "content_end": 2033,
      "content": "1956: :label: alg-param-selection\n1957: \n1958: **Input:** Problem dimension $d$, budget $N$, landscape curvature estimate $\\lambda_{\\min}$\n1959: \n1960: **Goal:** Choose $(\\gamma, \\lambda, \\sigma_v, \\tau, d_{\\text{safe}}, \\kappa_{\\text{wall}})$ to maximize $\\kappa_{\\text{total}}$ while keeping $C_{\\text{total}}$ reasonable.\n1961: \n1962: **Step 1: Balance friction and cloning**\n1963: \n1964: Choose $\\gamma \\sim \\lambda$ to avoid bottlenecks:\n1965: \n1966: $$\n1967: \\gamma = \\lambda = \\sqrt{\\lambda_{\\min}}\n1968: $$\n1969: \n1970: **Justification:**\n1971: - If $\\gamma \\ll \\lambda$: velocity thermalization is the bottleneck ($\\kappa_{\\text{total}} \\sim 2\\gamma$)\n1972: - If $\\lambda \\ll \\gamma$: positional contraction is the bottleneck ($\\kappa_{\\text{total}} \\sim \\lambda$)\n1973: - Balanced: $\\kappa_{\\text{total}} \\sim \\min(2\\gamma, \\lambda) = \\sqrt{\\lambda_{\\min}}$\n1974: \n1975: **Step 2: Choose noise intensity for exploration**\n1976: \n1977: Set thermal noise to match desired exploration scale $\\sigma_{\\text{explore}}$:\n1978: \n1979: $$\n1980: \\sigma_v = \\sqrt{\\gamma \\sigma_{\\text{explore}}^2}\n1981: $$\n1982: \n1983: **Justification:** The equilibrium positional variance is:\n1984: \n1985: $$\n1986: V_{\\text{Var},x}^{\\text{eq}} \\sim \\frac{\\sigma_v^2 \\tau^2}{\\gamma \\lambda} \\sim \\sigma_{\\text{explore}}^2\n1987: $$\n1988: \n1989: **Step 3: Choose timestep from stability**\n1990: \n1991: Use CFL-like condition:\n1992: \n1993: $$\n1994: \\tau = \\frac{c_{\\text{CFL}}}{\\sqrt{\\gamma \\lambda_{\\max}}}\n1995: $$\n1996: \n1997: where $\\lambda_{\\max}$ is the largest curvature and $c_{\\text{CFL}} \\sim 0.1 - 0.5$.\n1998: \n1999: **Justification:** Ensures:\n2000: - BAOAB stability: $\\gamma \\tau \\ll 1$\n2001: - Symplectic accuracy: $\\sqrt{\\lambda_{\\max}} \\tau \\ll 1$\n2002: - Weak error: $O(\\tau^2)$ corrections negligible\n2003: \n2004: **Step 4: Set boundary parameters for safety**\n2005: \n2006: Choose Safe Harbor distance from swarm variance:\n2007: \n2008: $$\n2009: d_{\\text{safe}} = 3\\sqrt{V_{\\text{Var},x}^{\\text{eq}}} \\sim 3\\sigma_{\\text{explore}}\n2010: $$\n2011: \n2012: Choose boundary stiffness from extinction tolerance:\n2013: \n2014: $$\n2015: \\kappa_{\\text{wall}} = \\frac{\\lambda f_{\\text{typical}}}{\\Delta f_{\\text{desired}}}\n2016: $$\n2017: \n2018: to ensure $P(\\text{extinction per step}) \\lesssim e^{-\\Theta(N)}$.\n2019: \n2020: **Step 5: Scale with swarm size**\n2021: \n2022: For dimension $d$ and desired Wasserstein accuracy $\\epsilon_W$:\n2023: \n2024: $$\n2025: N \\geq \\left(\\frac{\\sigma_v^2 \\tau}{\\epsilon_W^2 \\kappa_W}\\right)^d\n2026: $$\n2027: \n2028: **Output:** Optimized parameters $(\\gamma^*, \\lambda^*, \\sigma_v^*, \\tau^*, d_{\\text{safe}}^*, \\kappa_{\\text{wall}}^*)$\n2029: \n2030: **Expected performance:**\n2031: \n2032: $$\n2033: \\kappa_{\\text{total}} \\sim \\sqrt{\\lambda_{\\min}}, \\quad",
      "metadata": {
        "label": "alg-param-selection"
      },
      "section": "## 5. Explicit Parameter Dependence and Convergence Rates",
      "references": [],
      "raw_directive": "1953: Based on the explicit formulas, here is a practical strategy for choosing parameters:\n1954: \n1955: :::{prf:algorithm} Parameter Selection for Optimal Convergence\n1956: :label: alg-param-selection\n1957: \n1958: **Input:** Problem dimension $d$, budget $N$, landscape curvature estimate $\\lambda_{\\min}$\n1959: \n1960: **Goal:** Choose $(\\gamma, \\lambda, \\sigma_v, \\tau, d_{\\text{safe}}, \\kappa_{\\text{wall}})$ to maximize $\\kappa_{\\text{total}}$ while keeping $C_{\\text{total}}$ reasonable.\n1961: \n1962: **Step 1: Balance friction and cloning**\n1963: \n1964: Choose $\\gamma \\sim \\lambda$ to avoid bottlenecks:\n1965: \n1966: $$\n1967: \\gamma = \\lambda = \\sqrt{\\lambda_{\\min}}\n1968: $$\n1969: \n1970: **Justification:**\n1971: - If $\\gamma \\ll \\lambda$: velocity thermalization is the bottleneck ($\\kappa_{\\text{total}} \\sim 2\\gamma$)\n1972: - If $\\lambda \\ll \\gamma$: positional contraction is the bottleneck ($\\kappa_{\\text{total}} \\sim \\lambda$)\n1973: - Balanced: $\\kappa_{\\text{total}} \\sim \\min(2\\gamma, \\lambda) = \\sqrt{\\lambda_{\\min}}$\n1974: \n1975: **Step 2: Choose noise intensity for exploration**\n1976: \n1977: Set thermal noise to match desired exploration scale $\\sigma_{\\text{explore}}$:\n1978: \n1979: $$\n1980: \\sigma_v = \\sqrt{\\gamma \\sigma_{\\text{explore}}^2}\n1981: $$\n1982: \n1983: **Justification:** The equilibrium positional variance is:\n1984: \n1985: $$\n1986: V_{\\text{Var},x}^{\\text{eq}} \\sim \\frac{\\sigma_v^2 \\tau^2}{\\gamma \\lambda} \\sim \\sigma_{\\text{explore}}^2\n1987: $$\n1988: \n1989: **Step 3: Choose timestep from stability**\n1990: \n1991: Use CFL-like condition:\n1992: \n1993: $$\n1994: \\tau = \\frac{c_{\\text{CFL}}}{\\sqrt{\\gamma \\lambda_{\\max}}}\n1995: $$\n1996: \n1997: where $\\lambda_{\\max}$ is the largest curvature and $c_{\\text{CFL}} \\sim 0.1 - 0.5$.\n1998: \n1999: **Justification:** Ensures:\n2000: - BAOAB stability: $\\gamma \\tau \\ll 1$\n2001: - Symplectic accuracy: $\\sqrt{\\lambda_{\\max}} \\tau \\ll 1$\n2002: - Weak error: $O(\\tau^2)$ corrections negligible\n2003: \n2004: **Step 4: Set boundary parameters for safety**\n2005: \n2006: Choose Safe Harbor distance from swarm variance:\n2007: \n2008: $$\n2009: d_{\\text{safe}} = 3\\sqrt{V_{\\text{Var},x}^{\\text{eq}}} \\sim 3\\sigma_{\\text{explore}}\n2010: $$\n2011: \n2012: Choose boundary stiffness from extinction tolerance:\n2013: \n2014: $$\n2015: \\kappa_{\\text{wall}} = \\frac{\\lambda f_{\\text{typical}}}{\\Delta f_{\\text{desired}}}\n2016: $$\n2017: \n2018: to ensure $P(\\text{extinction per step}) \\lesssim e^{-\\Theta(N)}$.\n2019: \n2020: **Step 5: Scale with swarm size**\n2021: \n2022: For dimension $d$ and desired Wasserstein accuracy $\\epsilon_W$:\n2023: \n2024: $$\n2025: N \\geq \\left(\\frac{\\sigma_v^2 \\tau}{\\epsilon_W^2 \\kappa_W}\\right)^d\n2026: $$\n2027: \n2028: **Output:** Optimized parameters $(\\gamma^*, \\lambda^*, \\sigma_v^*, \\tau^*, d_{\\text{safe}}^*, \\kappa_{\\text{wall}}^*)$\n2029: \n2030: **Expected performance:**\n2031: \n2032: $$\n2033: \\kappa_{\\text{total}} \\sim \\sqrt{\\lambda_{\\min}}, \\quad\n2034: T_{\\text{mix}} \\sim \\frac{5}{\\sqrt{\\lambda_{\\min}}}",
      "_registry_context": {
        "stage": "directives",
        "document_id": "06_convergence",
        "chapter_index": 5,
        "chapter_file": "chapter_5.json",
        "section_id": "## 5. Explicit Parameter Dependence and Convergence Rates"
      }
    },
    {
      "directive_type": "algorithm",
      "label": "alg-projected-gradient-ascent",
      "title": "Projected Gradient Ascent for Parameter Optimization",
      "start_line": 3299,
      "end_line": 3403,
      "header_lines": [
        3300
      ],
      "content_start": 3302,
      "content_end": 3402,
      "content": "3302: :label: alg-projected-gradient-ascent\n3303: \n3304: **Input:**\n3305: - Landscape: $(\\lambda_{\\min}, \\lambda_{\\max}, d)$\n3306: - Constraints: $(N_{\\max}, \\lambda_{\\max}, V_{\\max}, \\ldots)$\n3307: - Initial guess: $\\mathbf{P}_0$ (from closed-form solution)\n3308: \n3309: **Output:** Optimal parameters $\\mathbf{P}^*$, achieved rate $\\kappa_{\\text{total}}^*$\n3310: \n3311: **Algorithm:**\n3312: \n3313: ```python\n3314: def optimize_parameters_constrained(landscape, constraints, P_init, max_iter=100):\n3315:     P = P_init\n3316:     alpha = 0.1  # Step size\n3317: \n3318:     for iter in range(max_iter):\n3319:         # Step 1: Compute current rates\n3320:         kappa = compute_rates(P, landscape)\n3321:         #   kappa = [kappa_x(P), kappa_v(P), kappa_W(P), kappa_b(P)]\n3322: \n3323:         kappa_total = min(kappa)\n3324: \n3325:         # Step 2: Identify active constraints (rates equal to minimum)\n3326:         active = [i for i in range(4) if abs(kappa[i] - kappa_total) < 1e-6]\n3327: \n3328:         # Step 3: Compute subgradient\n3329:         if len(active) == 1:\n3330:             # Unique minimum: gradient is M_kappa[active[0], :]\n3331:             grad = M_kappa[active[0], :]\n3332:         else:\n3333:             # Multiple minima: convex combination of gradients\n3334:             grad = mean(M_kappa[active, :], axis=0)\n3335: \n3336:         # Step 4: Gradient ascent step\n3337:         P_new = P * (1 + alpha * grad)  # Multiplicative update\n3338: \n3339:         # Step 5: Project onto feasible set\n3340:         P_new = project_onto_constraints(P_new, constraints)\n3341: \n3342:         # Step 6: Check convergence\n3343:         rel_change = norm(P_new - P) / norm(P)\n3344:         if rel_change < 1e-4:\n3345:             break\n3346: \n3347:         # Step 7: Adaptive step size\n3348:         kappa_new = min(compute_rates(P_new, landscape))\n3349:         if kappa_new > kappa_total:\n3350:             alpha *= 1.2  # Increase step (things are improving)\n3351:         else:\n3352:             alpha *= 0.5  # Decrease step (overshot)\n3353:             P_new = P     # Reject step\n3354: \n3355:         P = P_new\n3356: \n3357:     return P, kappa_total\n3358: ```\n3359: \n3360: **Helper functions:**\n3361: \n3362: ```python\n3363: def compute_rates(P, landscape):\n3364:     \"\"\"Compute all four rates from parameters.\"\"\"\n3365:     lambda_val = P['lambda']\n3366:     gamma = P['gamma']\n3367:     tau = P['tau']\n3368:     lambda_alg = P['lambda_alg']\n3369:     epsilon_c = P['epsilon_c']\n3370:     kappa_wall = P['kappa_wall']\n3371: \n3372:     # Use formulas from Chapter 7\n3373:     c_fit = estimate_fitness_correlation(lambda_alg, epsilon_c)\n3374: \n3375:     kappa_x = lambda_val * c_fit * (1 - 0.1*tau)\n3376:     kappa_v = 2 * gamma * (1 - 0.1*tau)\n3377:     kappa_W = 0.5 * gamma / (1 + gamma/landscape['lambda_min'])\n3378:     kappa_b = min(lambda_val, kappa_wall + gamma)\n3379: \n3380:     return [kappa_x, kappa_v, kappa_W, kappa_b]\n3381: \n3382: def project_onto_constraints(P, constraints):\n3383:     \"\"\"Project parameters onto feasible set.\"\"\"\n3384:     P_proj = P.copy()\n3385: \n3386:     # Box constraints\n3387:     if 'N_max' in constraints:\n3388:         P_proj['N'] = min(P['N'], constraints['N_max'])\n3389:     if 'lambda_max' in constraints:\n3390:         P_proj['lambda'] = min(P['lambda'], constraints['lambda_max'])\n3391: \n3392:     # Stability constraints\n3393:     P_proj['tau'] = min(P['tau'], 0.5/P['gamma'])\n3394:     P_proj['tau'] = min(P_proj['tau'], 1/sqrt(constraints['lambda_max']))\n3395: \n3396:     # Positivity\n3397:     for key in P_proj:\n3398:         P_proj[key] = max(P_proj[key], 1e-6)\n3399: \n3400:     # Restitution bound\n3401:     P_proj['alpha_rest'] = clip(P['alpha_rest'], 0, 1)\n3402: ",
      "metadata": {
        "label": "alg-projected-gradient-ascent"
      },
      "section": "## 6. Spectral Analysis of Parameter Coupling",
      "references": [],
      "raw_directive": "3299: When constraints are active (e.g., limited memory $N \\leq N_{\\max}$ or communication budget $\\lambda \\leq \\lambda_{\\max}$), we need iterative optimization.\n3300: \n3301: :::{prf:algorithm} Projected Gradient Ascent for Parameter Optimization\n3302: :label: alg-projected-gradient-ascent\n3303: \n3304: **Input:**\n3305: - Landscape: $(\\lambda_{\\min}, \\lambda_{\\max}, d)$\n3306: - Constraints: $(N_{\\max}, \\lambda_{\\max}, V_{\\max}, \\ldots)$\n3307: - Initial guess: $\\mathbf{P}_0$ (from closed-form solution)\n3308: \n3309: **Output:** Optimal parameters $\\mathbf{P}^*$, achieved rate $\\kappa_{\\text{total}}^*$\n3310: \n3311: **Algorithm:**\n3312: \n3313: ```python\n3314: def optimize_parameters_constrained(landscape, constraints, P_init, max_iter=100):\n3315:     P = P_init\n3316:     alpha = 0.1  # Step size\n3317: \n3318:     for iter in range(max_iter):\n3319:         # Step 1: Compute current rates\n3320:         kappa = compute_rates(P, landscape)\n3321:         #   kappa = [kappa_x(P), kappa_v(P), kappa_W(P), kappa_b(P)]\n3322: \n3323:         kappa_total = min(kappa)\n3324: \n3325:         # Step 2: Identify active constraints (rates equal to minimum)\n3326:         active = [i for i in range(4) if abs(kappa[i] - kappa_total) < 1e-6]\n3327: \n3328:         # Step 3: Compute subgradient\n3329:         if len(active) == 1:\n3330:             # Unique minimum: gradient is M_kappa[active[0], :]\n3331:             grad = M_kappa[active[0], :]\n3332:         else:\n3333:             # Multiple minima: convex combination of gradients\n3334:             grad = mean(M_kappa[active, :], axis=0)\n3335: \n3336:         # Step 4: Gradient ascent step\n3337:         P_new = P * (1 + alpha * grad)  # Multiplicative update\n3338: \n3339:         # Step 5: Project onto feasible set\n3340:         P_new = project_onto_constraints(P_new, constraints)\n3341: \n3342:         # Step 6: Check convergence\n3343:         rel_change = norm(P_new - P) / norm(P)\n3344:         if rel_change < 1e-4:\n3345:             break\n3346: \n3347:         # Step 7: Adaptive step size\n3348:         kappa_new = min(compute_rates(P_new, landscape))\n3349:         if kappa_new > kappa_total:\n3350:             alpha *= 1.2  # Increase step (things are improving)\n3351:         else:\n3352:             alpha *= 0.5  # Decrease step (overshot)\n3353:             P_new = P     # Reject step\n3354: \n3355:         P = P_new\n3356: \n3357:     return P, kappa_total\n3358: ```\n3359: \n3360: **Helper functions:**\n3361: \n3362: ```python\n3363: def compute_rates(P, landscape):\n3364:     \"\"\"Compute all four rates from parameters.\"\"\"\n3365:     lambda_val = P['lambda']\n3366:     gamma = P['gamma']\n3367:     tau = P['tau']\n3368:     lambda_alg = P['lambda_alg']\n3369:     epsilon_c = P['epsilon_c']\n3370:     kappa_wall = P['kappa_wall']\n3371: \n3372:     # Use formulas from Chapter 7\n3373:     c_fit = estimate_fitness_correlation(lambda_alg, epsilon_c)\n3374: \n3375:     kappa_x = lambda_val * c_fit * (1 - 0.1*tau)\n3376:     kappa_v = 2 * gamma * (1 - 0.1*tau)\n3377:     kappa_W = 0.5 * gamma / (1 + gamma/landscape['lambda_min'])\n3378:     kappa_b = min(lambda_val, kappa_wall + gamma)\n3379: \n3380:     return [kappa_x, kappa_v, kappa_W, kappa_b]\n3381: \n3382: def project_onto_constraints(P, constraints):\n3383:     \"\"\"Project parameters onto feasible set.\"\"\"\n3384:     P_proj = P.copy()\n3385: \n3386:     # Box constraints\n3387:     if 'N_max' in constraints:\n3388:         P_proj['N'] = min(P['N'], constraints['N_max'])\n3389:     if 'lambda_max' in constraints:\n3390:         P_proj['lambda'] = min(P['lambda'], constraints['lambda_max'])\n3391: \n3392:     # Stability constraints\n3393:     P_proj['tau'] = min(P['tau'], 0.5/P['gamma'])\n3394:     P_proj['tau'] = min(P_proj['tau'], 1/sqrt(constraints['lambda_max']))\n3395: \n3396:     # Positivity\n3397:     for key in P_proj:\n3398:         P_proj[key] = max(P_proj[key], 1e-6)\n3399: \n3400:     # Restitution bound\n3401:     P_proj['alpha_rest'] = clip(P['alpha_rest'], 0, 1)\n3402: \n3403:     return P_proj",
      "_registry_context": {
        "stage": "directives",
        "document_id": "06_convergence",
        "chapter_index": 6,
        "chapter_file": "chapter_6.json",
        "section_id": "## 6. Spectral Analysis of Parameter Coupling"
      }
    },
    {
      "directive_type": "algorithm",
      "label": "alg-adaptive-tuning",
      "title": "Adaptive Parameter Tuning",
      "start_line": 3488,
      "end_line": 3580,
      "header_lines": [
        3489
      ],
      "content_start": 3491,
      "content_end": 3579,
      "content": "3491: :label: alg-adaptive-tuning\n3492: \n3493: **Input:**\n3494: - Swarm system (black box)\n3495: - Initial parameter guess $\\mathbf{P}_0$\n3496: - Measurement window $T_{\\text{sample}}$\n3497: \n3498: **Output:** Tuned parameters $\\mathbf{P}_{\\text{tuned}}$\n3499: \n3500: **Algorithm:**\n3501: \n3502: ```python\n3503: def adaptive_tuning(swarm_system, P_init, n_iterations=10, T_sample=1000):\n3504:     \"\"\"\n3505:     Iteratively improve parameters using empirical measurements.\n3506:     \"\"\"\n3507:     P = P_init\n3508: \n3509:     for iter in range(n_iterations):\n3510:         # Step 1: Run swarm for T_sample steps\n3511:         trajectory = swarm_system.run(P, steps=T_sample)\n3512: \n3513:         # Step 2: Estimate rates from trajectory\n3514:         kappa_emp = estimate_rates_from_trajectory(trajectory)\n3515:         #   Returns: [kappa_x_emp, kappa_v_emp, kappa_W_emp, kappa_b_emp]\n3516: \n3517:         # Step 3: Identify bottleneck\n3518:         i_bottleneck = argmin(kappa_emp)\n3519:         kappa_min = kappa_emp[i_bottleneck]\n3520: \n3521:         bottleneck_names = ['Position', 'Velocity', 'Wasserstein', 'Boundary']\n3522:         print(f\"Iter {iter}: Bottleneck = {bottleneck_names[i_bottleneck]}, \"\n3523:               f\"κ = {kappa_min:.4f}\")\n3524: \n3525:         # Step 4: Compute adjustment direction using sensitivity matrix\n3526:         grad = M_kappa[i_bottleneck, :]  # Which parameters affect bottleneck?\n3527: \n3528:         # Step 5: Adaptive step size based on gap to target\n3529:         # Estimate achievable rate from landscape (if known roughly)\n3530:         kappa_target = estimate_achievable_rate(swarm_system)\n3531:         gap = kappa_target - kappa_min\n3532: \n3533:         if gap > 0:\n3534:             alpha = 0.2 * gap / kappa_min  # Proportional adjustment\n3535:         else:\n3536:             alpha = 0.05  # Small refinement\n3537: \n3538:         # Step 6: Update parameters\n3539:         P_new = {}\n3540:         for j, param_name in enumerate(param_names):\n3541:             P_new[param_name] = P[param_name] * (1 + alpha * grad[j])\n3542: \n3543:         # Step 7: Project onto feasible set\n3544:         P_new = project_onto_constraints(P_new, get_system_constraints())\n3545: \n3546:         # Step 8: Validate improvement\n3547:         trajectory_new = swarm_system.run(P_new, steps=T_sample//2)\n3548:         kappa_new = estimate_rates_from_trajectory(trajectory_new)\n3549: \n3550:         if min(kappa_new) > min(kappa_emp):\n3551:             P = P_new  # Accept\n3552:             print(f\"  → Accepted: κ_new = {min(kappa_new):.4f}\")\n3553:         else:\n3554:             alpha *= 0.5  # Reduce step size, try again\n3555:             print(f\"  → Rejected: Reducing step size\")\n3556: \n3557:     return P\n3558: \n3559: def estimate_rates_from_trajectory(trajectory):\n3560:     \"\"\"\n3561:     Extract empirical convergence rates from swarm trajectory.\n3562: \n3563:     Method: Fit exponential decay to Lyapunov components:\n3564:         V_i(t) ≈ C_i/κ_i + (V_i(0) - C_i/κ_i) * exp(-κ_i * t)\n3565: \n3566:     Extract κ_i from exponential fit.\n3567:     \"\"\"\n3568:     # Extract Lyapunov components over time\n3569:     V_Var_x = [compute_variance(traj.positions) for traj in trajectory]\n3570:     V_Var_v = [compute_variance(traj.velocities) for traj in trajectory]\n3571:     V_W = [compute_wasserstein(traj, reference) for traj in trajectory]\n3572:     W_b = [compute_boundary_potential(traj) for traj in trajectory]\n3573: \n3574:     # Fit exponential decay: V(t) = C + A * exp(-kappa * t)\n3575:     kappa_x = fit_exponential_rate(V_Var_x, trajectory.times)\n3576:     kappa_v = fit_exponential_rate(V_Var_v, trajectory.times)\n3577:     kappa_W = fit_exponential_rate(V_W, trajectory.times)\n3578:     kappa_b = fit_exponential_rate(W_b, trajectory.times)\n3579: ",
      "metadata": {
        "label": "alg-adaptive-tuning"
      },
      "section": "## 6. Spectral Analysis of Parameter Coupling",
      "references": [],
      "raw_directive": "3488: When the landscape is unknown or model assumptions are violated, adapt parameters based on measured convergence.\n3489: \n3490: :::{prf:algorithm} Adaptive Parameter Tuning\n3491: :label: alg-adaptive-tuning\n3492: \n3493: **Input:**\n3494: - Swarm system (black box)\n3495: - Initial parameter guess $\\mathbf{P}_0$\n3496: - Measurement window $T_{\\text{sample}}$\n3497: \n3498: **Output:** Tuned parameters $\\mathbf{P}_{\\text{tuned}}$\n3499: \n3500: **Algorithm:**\n3501: \n3502: ```python\n3503: def adaptive_tuning(swarm_system, P_init, n_iterations=10, T_sample=1000):\n3504:     \"\"\"\n3505:     Iteratively improve parameters using empirical measurements.\n3506:     \"\"\"\n3507:     P = P_init\n3508: \n3509:     for iter in range(n_iterations):\n3510:         # Step 1: Run swarm for T_sample steps\n3511:         trajectory = swarm_system.run(P, steps=T_sample)\n3512: \n3513:         # Step 2: Estimate rates from trajectory\n3514:         kappa_emp = estimate_rates_from_trajectory(trajectory)\n3515:         #   Returns: [kappa_x_emp, kappa_v_emp, kappa_W_emp, kappa_b_emp]\n3516: \n3517:         # Step 3: Identify bottleneck\n3518:         i_bottleneck = argmin(kappa_emp)\n3519:         kappa_min = kappa_emp[i_bottleneck]\n3520: \n3521:         bottleneck_names = ['Position', 'Velocity', 'Wasserstein', 'Boundary']\n3522:         print(f\"Iter {iter}: Bottleneck = {bottleneck_names[i_bottleneck]}, \"\n3523:               f\"κ = {kappa_min:.4f}\")\n3524: \n3525:         # Step 4: Compute adjustment direction using sensitivity matrix\n3526:         grad = M_kappa[i_bottleneck, :]  # Which parameters affect bottleneck?\n3527: \n3528:         # Step 5: Adaptive step size based on gap to target\n3529:         # Estimate achievable rate from landscape (if known roughly)\n3530:         kappa_target = estimate_achievable_rate(swarm_system)\n3531:         gap = kappa_target - kappa_min\n3532: \n3533:         if gap > 0:\n3534:             alpha = 0.2 * gap / kappa_min  # Proportional adjustment\n3535:         else:\n3536:             alpha = 0.05  # Small refinement\n3537: \n3538:         # Step 6: Update parameters\n3539:         P_new = {}\n3540:         for j, param_name in enumerate(param_names):\n3541:             P_new[param_name] = P[param_name] * (1 + alpha * grad[j])\n3542: \n3543:         # Step 7: Project onto feasible set\n3544:         P_new = project_onto_constraints(P_new, get_system_constraints())\n3545: \n3546:         # Step 8: Validate improvement\n3547:         trajectory_new = swarm_system.run(P_new, steps=T_sample//2)\n3548:         kappa_new = estimate_rates_from_trajectory(trajectory_new)\n3549: \n3550:         if min(kappa_new) > min(kappa_emp):\n3551:             P = P_new  # Accept\n3552:             print(f\"  → Accepted: κ_new = {min(kappa_new):.4f}\")\n3553:         else:\n3554:             alpha *= 0.5  # Reduce step size, try again\n3555:             print(f\"  → Rejected: Reducing step size\")\n3556: \n3557:     return P\n3558: \n3559: def estimate_rates_from_trajectory(trajectory):\n3560:     \"\"\"\n3561:     Extract empirical convergence rates from swarm trajectory.\n3562: \n3563:     Method: Fit exponential decay to Lyapunov components:\n3564:         V_i(t) ≈ C_i/κ_i + (V_i(0) - C_i/κ_i) * exp(-κ_i * t)\n3565: \n3566:     Extract κ_i from exponential fit.\n3567:     \"\"\"\n3568:     # Extract Lyapunov components over time\n3569:     V_Var_x = [compute_variance(traj.positions) for traj in trajectory]\n3570:     V_Var_v = [compute_variance(traj.velocities) for traj in trajectory]\n3571:     V_W = [compute_wasserstein(traj, reference) for traj in trajectory]\n3572:     W_b = [compute_boundary_potential(traj) for traj in trajectory]\n3573: \n3574:     # Fit exponential decay: V(t) = C + A * exp(-kappa * t)\n3575:     kappa_x = fit_exponential_rate(V_Var_x, trajectory.times)\n3576:     kappa_v = fit_exponential_rate(V_Var_v, trajectory.times)\n3577:     kappa_W = fit_exponential_rate(V_W, trajectory.times)\n3578:     kappa_b = fit_exponential_rate(W_b, trajectory.times)\n3579: \n3580:     return [kappa_x, kappa_v, kappa_W, kappa_b]",
      "_registry_context": {
        "stage": "directives",
        "document_id": "06_convergence",
        "chapter_index": 6,
        "chapter_file": "chapter_6.json",
        "section_id": "## 6. Spectral Analysis of Parameter Coupling"
      }
    }
  ]
}