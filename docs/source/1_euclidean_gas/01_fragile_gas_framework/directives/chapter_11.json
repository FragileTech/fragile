{
  "chapter_index": 11,
  "section_id": "## 11. Distance-to-Companion Measurement",
  "directive_count": 28,
  "hints": [
    {
      "directive_type": "definition",
      "label": "def-distance-to-companion-measurement",
      "title": "Distance-to-Companion Measurement",
      "start_line": 2170,
      "end_line": 2180,
      "header_lines": [
        2171
      ],
      "content_start": 2172,
      "content_end": 2179,
      "content": "2172: :label: def-distance-to-companion-measurement\n2173: The distance value $d_i$ for walker $i$ is the result of a two-stage sampling process. First, a **potential companion** index, denoted $c_{\\text{pot}}(i)$, is sampled from the **Companion Selection Measure** $\\mathbb{C}_i$. Second, the **Algorithmic Distance** is computed to that specific companion.\n2174: This process is equivalent to sampling a single value from the **Distance-to-Companion Measure** $\\mathbb{D}_i$, which is the pushforward of $\\mathbb{C}_i$ by the distance function $D_i(j) = d_{\\text{alg}}(x_i, x_j)$.\n2175: $$\n2176: \n2177: d_i := d_{\\text{alg}}(x_i, x_{c_{\\text{pot}}(i)}) \\quad \\text{where} \\quad c_{\\text{pot}}(i) \\sim \\mathbb{C}_i(\\cdot)\n2178: \n2179: $$",
      "metadata": {
        "label": "def-distance-to-companion-measurement"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2170: ### 10.1 Definition of Distance-to-Companion Measurement\n2171: :::{prf:definition} Distance-to-Companion Measurement\n2172: :label: def-distance-to-companion-measurement\n2173: The distance value $d_i$ for walker $i$ is the result of a two-stage sampling process. First, a **potential companion** index, denoted $c_{\\text{pot}}(i)$, is sampled from the **Companion Selection Measure** $\\mathbb{C}_i$. Second, the **Algorithmic Distance** is computed to that specific companion.\n2174: This process is equivalent to sampling a single value from the **Distance-to-Companion Measure** $\\mathbb{D}_i$, which is the pushforward of $\\mathbb{C}_i$ by the distance function $D_i(j) = d_{\\text{alg}}(x_i, x_j)$.\n2175: $$\n2176: \n2177: d_i := d_{\\text{alg}}(x_i, x_{c_{\\text{pot}}(i)}) \\quad \\text{where} \\quad c_{\\text{pot}}(i) \\sim \\mathbb{C}_i(\\cdot)\n2178: \n2179: $$\n2180: The **Raw Distance Vector Operator**, denoted $\\mathbf{d}(\\mathcal{S})$, maps a swarm state $\\mathcal{S}$ to a *distribution* over N-dimensional vectors. A single realization of this vector is produced by performing the distance-to-companion measurement independently for each alive walker. For dead walkers, the component is zero."
    },
    {
      "directive_type": "lemma",
      "label": "lem-single-walker-positional-error",
      "title": "Bound on Single-Walker Error from Positional Change",
      "start_line": 2191,
      "end_line": 2200,
      "header_lines": [
        2192
      ],
      "content_start": 2193,
      "content_end": 2199,
      "content": "2193: :label: lem-single-walker-positional-error\n2194: Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two swarm states. For a given walker $i$ that is alive in swarm $\\mathcal{S}_1$ ($s_{1,i}=1$), let $\\mathbb{C}_i(\\mathcal{S}_1)$ be its companion selection measure.\n2195: The absolute error in its expected distance due to the positional displacement of the walkers between the two states, evaluated over the fixed companion set from $\\mathcal{S}_1$, is bounded by the sum of its own displacement and the average displacement of its potential companions.\n2196: $$\n2197: \n2198: \\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,i}, x_{1,c}) \\right] - \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right| \\le d_{\\text{alg}}(x_{1,i}, x_{2,i}) + \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c}) \\right]\n2199: ",
      "metadata": {
        "label": "lem-single-walker-positional-error"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2191: #### 10.2.1 Lemma: Bound on Single-Walker Error from Positional Change\n2192: :::{prf:lemma} Bound on Single-Walker Error from Positional Change\n2193: :label: lem-single-walker-positional-error\n2194: Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two swarm states. For a given walker $i$ that is alive in swarm $\\mathcal{S}_1$ ($s_{1,i}=1$), let $\\mathbb{C}_i(\\mathcal{S}_1)$ be its companion selection measure.\n2195: The absolute error in its expected distance due to the positional displacement of the walkers between the two states, evaluated over the fixed companion set from $\\mathcal{S}_1$, is bounded by the sum of its own displacement and the average displacement of its potential companions.\n2196: $$\n2197: \n2198: \\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,i}, x_{1,c}) \\right] - \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right| \\le d_{\\text{alg}}(x_{1,i}, x_{2,i}) + \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c}) \\right]\n2199: \n2200: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-single-walker-positional-error",
      "title": null,
      "start_line": 2201,
      "end_line": 2232,
      "header_lines": [
        2202
      ],
      "content_start": 2203,
      "content_end": 2231,
      "content": "2203: :label: proof-lem-single-walker-positional-error\n2204: **Proof.**\n2205: Let $\\Delta_{\\text{pos},i}$ denote the absolute error term we wish to bound.\n2206: 1.  **Apply Linearity of Expectation:**\n2207:     We combine the two terms into a single expectation.\n2208: $$\n2209: \n2210:     \\Delta_{\\text{pos},i} = \\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,i}, x_{1,c}) - d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right|\n2211: \n2212: \n2213: $$\n2214: 2.  **Move the Absolute Value Inside the Expectation:**\n2215:     Using Jensen's inequality, $|\\mathbb{E}[X]| \\le \\mathbb{E}[|X|]$, we move the absolute value inside, which provides an upper bound:\n2216: $$\n2217: \n2218:     \\Delta_{\\text{pos},i} \\le \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ \\left| d_{\\text{alg}}(x_{1,i}, x_{1,c}) - d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right| \\right]\n2219: \n2220: \n2221: $$\n2222: 3.  **Apply the Reverse Triangle Inequality:**\n2223:     The term inside the expectation is the absolute difference between two distance values. We apply the **reverse triangle inequality**, which states that for any points $a,b,c,d$ in a metric space $(M,d)$, $|d(a,b) - d(c,d)| \\le d(a,c) + d(b,d)$. Applying this to the Euclidean algorithmic distance $d_{\\text{alg}}$ yields:\n2224: $$\n2225: \n2226:     \\left| d_{\\text{alg}}(x_{1,i}, x_{1,c}) - d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right| \\le d_{\\text{alg}}(x_{1,i}, x_{2,i}) + d_{\\text{alg}}(x_{1,c}, x_{2,c})\n2227: \n2228: \n2229: $$\n2230: 4.  **Finalize the Bound:**\n2231:     We substitute this inequality back into the expression from Step 2. By linearity of expectation, the first term, $d_{\\text{alg}}(x_{1,i}, x_{2,i})$, is a constant with respect to the expectation over the companion index $c$. This gives the final bound.",
      "metadata": {
        "label": "proof-lem-single-walker-positional-error"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2201: :::\n2202: :::{prf:proof}\n2203: :label: proof-lem-single-walker-positional-error\n2204: **Proof.**\n2205: Let $\\Delta_{\\text{pos},i}$ denote the absolute error term we wish to bound.\n2206: 1.  **Apply Linearity of Expectation:**\n2207:     We combine the two terms into a single expectation.\n2208: $$\n2209: \n2210:     \\Delta_{\\text{pos},i} = \\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,i}, x_{1,c}) - d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right|\n2211: \n2212: \n2213: $$\n2214: 2.  **Move the Absolute Value Inside the Expectation:**\n2215:     Using Jensen's inequality, $|\\mathbb{E}[X]| \\le \\mathbb{E}[|X|]$, we move the absolute value inside, which provides an upper bound:\n2216: $$\n2217: \n2218:     \\Delta_{\\text{pos},i} \\le \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ \\left| d_{\\text{alg}}(x_{1,i}, x_{1,c}) - d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right| \\right]\n2219: \n2220: \n2221: $$\n2222: 3.  **Apply the Reverse Triangle Inequality:**\n2223:     The term inside the expectation is the absolute difference between two distance values. We apply the **reverse triangle inequality**, which states that for any points $a,b,c,d$ in a metric space $(M,d)$, $|d(a,b) - d(c,d)| \\le d(a,c) + d(b,d)$. Applying this to the Euclidean algorithmic distance $d_{\\text{alg}}$ yields:\n2224: $$\n2225: \n2226:     \\left| d_{\\text{alg}}(x_{1,i}, x_{1,c}) - d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right| \\le d_{\\text{alg}}(x_{1,i}, x_{2,i}) + d_{\\text{alg}}(x_{1,c}, x_{2,c})\n2227: \n2228: \n2229: $$\n2230: 4.  **Finalize the Bound:**\n2231:     We substitute this inequality back into the expression from Step 2. By linearity of expectation, the first term, $d_{\\text{alg}}(x_{1,i}, x_{2,i})$, is a constant with respect to the expectation over the companion index $c$. This gives the final bound.\n2232: **Q.E.D.**"
    },
    {
      "directive_type": "lemma",
      "label": "lem-single-walker-structural-error",
      "title": "Bound on Single-Walker Error from Structural Change",
      "start_line": 2234,
      "end_line": 2244,
      "header_lines": [
        2235
      ],
      "content_start": 2236,
      "content_end": 2243,
      "content": "2236: :label: lem-single-walker-structural-error\n2237: Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two swarm states with alive sets $\\mathcal{A}_1$ and $\\mathcal{A}_2$. Let walker $i$ be alive in both swarms ($i \\in \\mathcal{A}_1 \\cap \\mathcal{A}_2$), and let the initial swarm have at least two alive walkers, $k_1=|\\mathcal{A}_1| \\ge 2$. Let the walker positions $\\mathbf{x}_2$ from the second swarm be fixed for the analysis. Let $n_c$ be the total number of status changes in the swarm.\n2238: The absolute error in the expected distance for walker $i$ due to the change in the companion selection measure is bounded by:\n2239: $$\n2240: \n2241: \\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] - \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_2)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right| \\le \\frac{2 D_{\\mathcal{Y}}}{k_1 - 1} \\cdot n_c\n2242: \n2243: $$",
      "metadata": {
        "label": "lem-single-walker-structural-error"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2234: #### 10.2.2 Lemma: Bound on Single-Walker Error from Structural Change\n2235: :::{prf:lemma} Bound on Single-Walker Error from Structural Change\n2236: :label: lem-single-walker-structural-error\n2237: Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two swarm states with alive sets $\\mathcal{A}_1$ and $\\mathcal{A}_2$. Let walker $i$ be alive in both swarms ($i \\in \\mathcal{A}_1 \\cap \\mathcal{A}_2$), and let the initial swarm have at least two alive walkers, $k_1=|\\mathcal{A}_1| \\ge 2$. Let the walker positions $\\mathbf{x}_2$ from the second swarm be fixed for the analysis. Let $n_c$ be the total number of status changes in the swarm.\n2238: The absolute error in the expected distance for walker $i$ due to the change in the companion selection measure is bounded by:\n2239: $$\n2240: \n2241: \\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] - \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_2)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right| \\le \\frac{2 D_{\\mathcal{Y}}}{k_1 - 1} \\cdot n_c\n2242: \n2243: $$\n2244: where $D_{\\mathcal{Y}}$ is the diameter of the algorithmic space."
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-single-walker-structural-error",
      "title": null,
      "start_line": 2245,
      "end_line": 2264,
      "header_lines": [
        2246
      ],
      "content_start": 2247,
      "content_end": 2263,
      "content": "2247: :label: proof-lem-single-walker-structural-error\n2248: **Proof.**\n2249: This result is a direct application of the [](#thm-total-error-status-bound).\n2250: 1.  **Identify the Function and Bound:**\n2251:     Let the function being evaluated be $f(c) := d_{\\text{alg}}(x_{2,i}, x_{2,c})$. This function measures the distance from walker $i$ to a potential companion $c$. The distance in the algorithmic space is, by definition, bounded by the space's diameter, $D_{\\mathcal{Y}}$. Therefore, we have a uniform bound $M_f = D_{\\mathcal{Y}}$.\n2252: 2.  **Identify the Support Sets:**\n2253:     Let $S_1 = \\mathbb{C}_i(\\mathcal{S}_1)$ and $S_2 = \\mathbb{C}_i(\\mathcal{S}_2)$ be the companion support sets for walker $i$ in the two swarms. Since walker $i$ is alive in $\\mathcal{S}_1$ and the precondition states $k_1 \\ge 2$, the initial support set is $S_1 = \\mathcal{A}_1 \\setminus \\{i\\}$, and its size is $|S_1| = k_1 - 1 > 0$.\n2254: 3.  **Apply [](#thm-total-error-status-bound):**\n2255:     [](#thm-total-error-status-bound) provides a general bound for the change in expectation due to a change in the support set:\n2256: $$\n2257: \n2258:     \\text{Error} \\le \\frac{2 M_f}{|S_1|} \\cdot n_c\n2259: \n2260: \n2261: $$\n2262: 4.  **Substitute and Finalize:**\n2263:     We substitute our specific function bound $M_f = D_{\\mathcal{Y}}$ and the support set size $|S_1| = k_1 - 1$ into the general formula. This immediately yields the stated bound.",
      "metadata": {
        "label": "proof-lem-single-walker-structural-error"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2245: :::\n2246: :::{prf:proof}\n2247: :label: proof-lem-single-walker-structural-error\n2248: **Proof.**\n2249: This result is a direct application of the [](#thm-total-error-status-bound).\n2250: 1.  **Identify the Function and Bound:**\n2251:     Let the function being evaluated be $f(c) := d_{\\text{alg}}(x_{2,i}, x_{2,c})$. This function measures the distance from walker $i$ to a potential companion $c$. The distance in the algorithmic space is, by definition, bounded by the space's diameter, $D_{\\mathcal{Y}}$. Therefore, we have a uniform bound $M_f = D_{\\mathcal{Y}}$.\n2252: 2.  **Identify the Support Sets:**\n2253:     Let $S_1 = \\mathbb{C}_i(\\mathcal{S}_1)$ and $S_2 = \\mathbb{C}_i(\\mathcal{S}_2)$ be the companion support sets for walker $i$ in the two swarms. Since walker $i$ is alive in $\\mathcal{S}_1$ and the precondition states $k_1 \\ge 2$, the initial support set is $S_1 = \\mathcal{A}_1 \\setminus \\{i\\}$, and its size is $|S_1| = k_1 - 1 > 0$.\n2254: 3.  **Apply [](#thm-total-error-status-bound):**\n2255:     [](#thm-total-error-status-bound) provides a general bound for the change in expectation due to a change in the support set:\n2256: $$\n2257: \n2258:     \\text{Error} \\le \\frac{2 M_f}{|S_1|} \\cdot n_c\n2259: \n2260: \n2261: $$\n2262: 4.  **Substitute and Finalize:**\n2263:     We substitute our specific function bound $M_f = D_{\\mathcal{Y}}$ and the support set size $|S_1| = k_1 - 1$ into the general formula. This immediately yields the stated bound.\n2264: **Q.E.D.**"
    },
    {
      "directive_type": "lemma",
      "label": "lem-single-walker-own-status-error",
      "title": "Bound on Single-Walker Error from Own Status Change",
      "start_line": 2266,
      "end_line": 2274,
      "header_lines": [
        2267
      ],
      "content_start": 2268,
      "content_end": 2273,
      "content": "2268: :label: lem-single-walker-own-status-error\n2269: Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two swarm states. For any walker $i$ whose survival status changes ($s_{1,i} \\neq s_{2,i}$), the absolute difference in its expected raw distance measurement is bounded by the diameter of the algorithmic space, $D_{\\mathcal{Y}}$.\n2270: $$\n2271: \n2272: \\left| \\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)] \\right| \\le D_{\\mathcal{Y}}\n2273: ",
      "metadata": {
        "label": "lem-single-walker-own-status-error"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2266: #### 10.2.3 Lemma: Bound on Single-Walker Error from Own Status Change\n2267: :::{prf:lemma} Bound on Single-Walker Error from Own Status Change\n2268: :label: lem-single-walker-own-status-error\n2269: Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two swarm states. For any walker $i$ whose survival status changes ($s_{1,i} \\neq s_{2,i}$), the absolute difference in its expected raw distance measurement is bounded by the diameter of the algorithmic space, $D_{\\mathcal{Y}}$.\n2270: $$\n2271: \n2272: \\left| \\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)] \\right| \\le D_{\\mathcal{Y}}\n2273: \n2274: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-single-walker-own-status-error",
      "title": null,
      "start_line": 2275,
      "end_line": 2283,
      "header_lines": [
        2276
      ],
      "content_start": 2277,
      "content_end": 2282,
      "content": "2277: :label: proof-lem-single-walker-own-status-error\n2278: **Proof.**\n2279: The proof considers the two possible cases for a status change.\n2280: 1.  **Case 1: Walker Dies ($s_{1,i}=1 \\to s_{2,i}=0$)**: The expected distance in state $\\mathcal{S}_1$ is $\\mathbb{E}[d_i(\\mathcal{S}_1)]$, which must lie in the interval $[0, D_{\\mathcal{Y}}]$. The expected distance in state $\\mathcal{S}_2$ is defined to be $\\mathbb{E}[d_i(\\mathcal{S}_2)] = 0$. The absolute difference is therefore $|\\mathbb{E}[d_i(\\mathcal{S}_1)] - 0| \\le D_{\\mathcal{Y}}$.\n2281: 2.  **Case 2: Walker is Revived ($s_{1,i}=0 \\to s_{2,i}=1$)**: The logic is symmetric. $\\mathbb{E}[d_i(\\mathcal{S}_1)] = 0$ and $\\mathbb{E}[d_i(\\mathcal{S}_2)] \\in [0, D_{\\mathcal{Y}}]$. The absolute difference is again bounded by $D_{\\mathcal{Y}}$.\n2282: In both cases, the bound holds.",
      "metadata": {
        "label": "proof-lem-single-walker-own-status-error"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2275: :::\n2276: :::{prf:proof}\n2277: :label: proof-lem-single-walker-own-status-error\n2278: **Proof.**\n2279: The proof considers the two possible cases for a status change.\n2280: 1.  **Case 1: Walker Dies ($s_{1,i}=1 \\to s_{2,i}=0$)**: The expected distance in state $\\mathcal{S}_1$ is $\\mathbb{E}[d_i(\\mathcal{S}_1)]$, which must lie in the interval $[0, D_{\\mathcal{Y}}]$. The expected distance in state $\\mathcal{S}_2$ is defined to be $\\mathbb{E}[d_i(\\mathcal{S}_2)] = 0$. The absolute difference is therefore $|\\mathbb{E}[d_i(\\mathcal{S}_1)] - 0| \\le D_{\\mathcal{Y}}$.\n2281: 2.  **Case 2: Walker is Revived ($s_{1,i}=0 \\to s_{2,i}=1$)**: The logic is symmetric. $\\mathbb{E}[d_i(\\mathcal{S}_1)] = 0$ and $\\mathbb{E}[d_i(\\mathcal{S}_2)] \\in [0, D_{\\mathcal{Y}}]$. The absolute difference is again bounded by $D_{\\mathcal{Y}}$.\n2282: In both cases, the bound holds.\n2283: **Q.E.D.**"
    },
    {
      "directive_type": "proof",
      "label": "proof-thm-total-expected-distance-error-decomposition",
      "title": null,
      "start_line": 2292,
      "end_line": 2298,
      "header_lines": [
        2293
      ],
      "content_start": 2294,
      "content_end": 2297,
      "content": "2294: :label: proof-thm-total-expected-distance-error-decomposition\n2295: **Proof.**\n2296: This decomposition is an identity that follows directly from partitioning the set of all walker indices $\\{1, ..., N\\}$ into two disjoint subsets: those whose survival status is the same in both swarms, and those whose status changes. The total sum of squared errors over all walkers is simply the sum of the errors over these two partitions.\n2297: The set of walkers whose error contribution could be non-zero is the union of the alive sets, $\\mathcal{A}(\\mathcal{S}_1) \\cup \\mathcal{A}(\\mathcal{S}_2)$. We partition this set into stable walkers, $\\mathcal{A}_{\\text{stable}} = \\mathcal{A}(\\mathcal{S}_1) \\cap \\mathcal{A}(\\mathcal{S}_2)$, and unstable walkers, whose indices lie in the symmetric difference of the alive sets, $\\mathcal{A}_{\\text{unstable}} = \\mathcal{A}(\\mathcal{S}_1) \\Delta \\mathcal{A}(\\mathcal{S}_2)$. For any walker **i** that is dead in both states, its expected distance is 0 in both states, so its error contribution is 0.",
      "metadata": {
        "label": "proof-thm-total-expected-distance-error-decomposition"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2292: $$\n2293: :::{prf:proof}\n2294: :label: proof-thm-total-expected-distance-error-decomposition\n2295: **Proof.**\n2296: This decomposition is an identity that follows directly from partitioning the set of all walker indices $\\{1, ..., N\\}$ into two disjoint subsets: those whose survival status is the same in both swarms, and those whose status changes. The total sum of squared errors over all walkers is simply the sum of the errors over these two partitions.\n2297: The set of walkers whose error contribution could be non-zero is the union of the alive sets, $\\mathcal{A}(\\mathcal{S}_1) \\cup \\mathcal{A}(\\mathcal{S}_2)$. We partition this set into stable walkers, $\\mathcal{A}_{\\text{stable}} = \\mathcal{A}(\\mathcal{S}_1) \\cap \\mathcal{A}(\\mathcal{S}_2)$, and unstable walkers, whose indices lie in the symmetric difference of the alive sets, $\\mathcal{A}_{\\text{unstable}} = \\mathcal{A}(\\mathcal{S}_1) \\Delta \\mathcal{A}(\\mathcal{S}_2)$. For any walker **i** that is dead in both states, its expected distance is 0 in both states, so its error contribution is 0.\n2298: **Q.E.D.**"
    },
    {
      "directive_type": "lemma",
      "label": "lem-total-squared-error-unstable",
      "title": "Bound on the Total Squared Error for Unstable Walkers",
      "start_line": 2300,
      "end_line": 2309,
      "header_lines": [
        2301
      ],
      "content_start": 2302,
      "content_end": 2308,
      "content": "2302: :label: lem-total-squared-error-unstable\n2303: Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two swarm states. The total squared error in the expected raw distance from the set of unstable walkers, $\\mathcal{A}_{\\text{unstable}}$, is bounded by the total number of status changes:\n2304: $$\n2305: \n2306: \\sum_{i \\in \\mathcal{A}_{\\text{unstable}}} \\big|\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]\\big|^2 \\le D_{\\mathcal{Y}}^2 \\sum_{j=1}^N (s_{1,j} - s_{2,j})^2,\n2307: \n2308: $$",
      "metadata": {
        "label": "lem-total-squared-error-unstable"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2300: #### 10.2.5 Lemma: Bound on the Total Squared Error for Unstable Walkers\n2301: :::{prf:lemma} Bound on the Total Squared Error for Unstable Walkers\n2302: :label: lem-total-squared-error-unstable\n2303: Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two swarm states. The total squared error in the expected raw distance from the set of unstable walkers, $\\mathcal{A}_{\\text{unstable}}$, is bounded by the total number of status changes:\n2304: $$\n2305: \n2306: \\sum_{i \\in \\mathcal{A}_{\\text{unstable}}} \\big|\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]\\big|^2 \\le D_{\\mathcal{Y}}^2 \\sum_{j=1}^N (s_{1,j} - s_{2,j})^2,\n2307: \n2308: $$\n2309: where $D_{\\mathcal{Y}}$ is the diameter of the algorithmic space."
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-total-squared-error-unstable",
      "title": null,
      "start_line": 2310,
      "end_line": 2317,
      "header_lines": [
        2311
      ],
      "content_start": 2312,
      "content_end": 2316,
      "content": "2312: :label: proof-lem-total-squared-error-unstable\n2313: **Proof.** For any unstable walker $i$ (i.e., $s_{1,i}\\neq s_{2,i}$), [](#lem-single-walker-own-status-error) gives\n2314: $|\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]| \\le D_{\\mathcal{Y}}$.\n2315: Squaring and summing over all unstable walkers yields the stated bound, since the count\n2316: $\\big|\\mathcal{A}_{\\text{unstable}}\\big| = \\sum_{j=1}^N (s_{1,j}-s_{2,j})^2$.",
      "metadata": {
        "label": "proof-lem-total-squared-error-unstable"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2310: :::\n2311: :::{prf:proof}\n2312: :label: proof-lem-total-squared-error-unstable\n2313: **Proof.** For any unstable walker $i$ (i.e., $s_{1,i}\\neq s_{2,i}$), [](#lem-single-walker-own-status-error) gives\n2314: $|\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]| \\le D_{\\mathcal{Y}}$.\n2315: Squaring and summing over all unstable walkers yields the stated bound, since the count\n2316: $\\big|\\mathcal{A}_{\\text{unstable}}\\big| = \\sum_{j=1}^N (s_{1,j}-s_{2,j})^2$.\n2317: **Q.E.D.**"
    },
    {
      "directive_type": "lemma",
      "label": "lem-total-squared-error-stable",
      "title": "Bound on the Total Squared Error for Stable Walkers",
      "start_line": 2319,
      "end_line": 2327,
      "header_lines": [
        2320
      ],
      "content_start": 2321,
      "content_end": 2326,
      "content": "2321: :label: lem-total-squared-error-stable\n2322: Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two swarm states with $|\\mathcal{A}(\\mathcal{S}_1)|=k_1 \\ge 2$. The total squared error in the expected raw distance from the set of stable walkers, $\\mathcal{A}_{\\text{stable}} = \\mathcal{A}(\\mathcal{S}_1) \\cap \\mathcal{A}(\\mathcal{S}_2)$, is bounded as follows:\n2323: $$\n2324: \n2325: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} |\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]|^2 \\le 12 \\cdot \\Delta_{\\text{pos}}^2(\\mathcal{S}_1, \\mathcal{S}_2) + \\frac{8 k_1 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c(\\mathcal{S}_1, \\mathcal{S}_2)^2\n2326: ",
      "metadata": {
        "label": "lem-total-squared-error-stable"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2319: #### 10.2.6. Lemma: Bound on the Total Squared Error for Stable Walkers\n2320: :::{prf:lemma} Bound on the Total Squared Error for Stable Walkers\n2321: :label: lem-total-squared-error-stable\n2322: Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two swarm states with $|\\mathcal{A}(\\mathcal{S}_1)|=k_1 \\ge 2$. The total squared error in the expected raw distance from the set of stable walkers, $\\mathcal{A}_{\\text{stable}} = \\mathcal{A}(\\mathcal{S}_1) \\cap \\mathcal{A}(\\mathcal{S}_2)$, is bounded as follows:\n2323: $$\n2324: \n2325: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} |\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]|^2 \\le 12 \\cdot \\Delta_{\\text{pos}}^2(\\mathcal{S}_1, \\mathcal{S}_2) + \\frac{8 k_1 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c(\\mathcal{S}_1, \\mathcal{S}_2)^2\n2326: \n2327: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-total-squared-error-stable",
      "title": null,
      "start_line": 2328,
      "end_line": 2352,
      "header_lines": [
        2329
      ],
      "content_start": 2330,
      "content_end": 2351,
      "content": "2330: :label: proof-lem-total-squared-error-stable\n2331: **Proof.**\n2332: The total error for a single stable walker is first decomposed into a positional component and a structural component. The squared L2-norm of the total error over all stable walkers is then bounded by combining the established bounds for the sum of the squares of these individual components.\n2333: 1.  **Decomposition of Total Stable Error:** From [](#sub-lem-stable-walker-error-decomposition), the total squared error for stable walkers is bounded by twice the sum of the squared positional and structural error components:\n2334: $$\n2335: \n2336: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} |\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]|^2 \\le 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 + 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2\n2337: \n2338: $$\n2339: 2.  **Bound the Positional Component:** From [](#sub-lem-stable-positional-error-bound), the total squared positional error component is bounded by:\n2340: $$\n2341: \n2342: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 6 \\cdot \\Delta_{\\text{pos}}^2(\\mathcal{S}_1, \\mathcal{S}_2)\n2343: \n2344: $$\n2345: 3.  **Bound the Structural Component:** From [](#sub-lem-stable-structural-error-bound), the total squared structural error component is bounded by:\n2346: $$\n2347: \n2348: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2 \\le \\frac{4 k_1 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c(\\mathcal{S}_1, \\mathcal{S}_2)^2\n2349: \n2350: $$\n2351: 4.  **Combine the Bounds:** Substituting the bounds from steps 2 and 3 into the inequality from step 1 and multiplying by the factor of 2 yields the final result stated in the lemma.",
      "metadata": {
        "label": "proof-lem-total-squared-error-stable"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2328: :::\n2329: :::{prf:proof}\n2330: :label: proof-lem-total-squared-error-stable\n2331: **Proof.**\n2332: The total error for a single stable walker is first decomposed into a positional component and a structural component. The squared L2-norm of the total error over all stable walkers is then bounded by combining the established bounds for the sum of the squares of these individual components.\n2333: 1.  **Decomposition of Total Stable Error:** From [](#sub-lem-stable-walker-error-decomposition), the total squared error for stable walkers is bounded by twice the sum of the squared positional and structural error components:\n2334: $$\n2335: \n2336: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} |\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]|^2 \\le 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 + 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2\n2337: \n2338: $$\n2339: 2.  **Bound the Positional Component:** From [](#sub-lem-stable-positional-error-bound), the total squared positional error component is bounded by:\n2340: $$\n2341: \n2342: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 6 \\cdot \\Delta_{\\text{pos}}^2(\\mathcal{S}_1, \\mathcal{S}_2)\n2343: \n2344: $$\n2345: 3.  **Bound the Structural Component:** From [](#sub-lem-stable-structural-error-bound), the total squared structural error component is bounded by:\n2346: $$\n2347: \n2348: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2 \\le \\frac{4 k_1 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c(\\mathcal{S}_1, \\mathcal{S}_2)^2\n2349: \n2350: $$\n2351: 4.  **Combine the Bounds:** Substituting the bounds from steps 2 and 3 into the inequality from step 1 and multiplying by the factor of 2 yields the final result stated in the lemma.\n2352: **Q.E.D.**"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-sub-stable-walker-error-decomposition",
      "title": null,
      "start_line": 2362,
      "end_line": 2374,
      "header_lines": [
        2363
      ],
      "content_start": 2364,
      "content_end": 2373,
      "content": "2364: :label: proof-lem-sub-stable-walker-error-decomposition\n2365: **Proof.**\n2366: 1.  **Decompose Single-Walker Error:** For each stable walker $i \\in \\mathcal{A}_{\\text{stable}}$, we introduce the intermediate term $\\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} [d_{\\text{alg}}(x_{2,i}, x_{2,c})]$ and apply the triangle inequality:\n2367: $$\n2368: \n2369: |\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]| \\le \\underbrace{\\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,i}, x_{1,c}) \\right] - \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right|}_{\\Delta_{\\text{pos},i}} + \\underbrace{\\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] - \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_2)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right|}_{\\Delta_{\\text{struct},i}}\n2370: \n2371: $$\n2372: The term $\\Delta_{\\text{pos},i}$ is the error from positional change over a fixed companion set, bounded by [](#lem-single-walker-positional-error). The term $\\Delta_{\\text{struct},i}$ is the error from structural change with fixed positions, bounded by [](#lem-single-walker-structural-error).\n2373: 2.  **Bound the Squared Sum:** Using the elementary inequality $(a+b)^2 \\le 2a^2 + 2b^2$, we can bound the square of the single-walker error. Summing over all $i \\in \\mathcal{A}_{\\text{stable}}$ yields the inequality stated in the lemma.",
      "metadata": {
        "label": "proof-lem-sub-stable-walker-error-decomposition"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2362: $$\n2363: :::{prf:proof}\n2364: :label: proof-lem-sub-stable-walker-error-decomposition\n2365: **Proof.**\n2366: 1.  **Decompose Single-Walker Error:** For each stable walker $i \\in \\mathcal{A}_{\\text{stable}}$, we introduce the intermediate term $\\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} [d_{\\text{alg}}(x_{2,i}, x_{2,c})]$ and apply the triangle inequality:\n2367: $$\n2368: \n2369: |\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]| \\le \\underbrace{\\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,i}, x_{1,c}) \\right] - \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right|}_{\\Delta_{\\text{pos},i}} + \\underbrace{\\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] - \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_2)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right|}_{\\Delta_{\\text{struct},i}}\n2370: \n2371: $$\n2372: The term $\\Delta_{\\text{pos},i}$ is the error from positional change over a fixed companion set, bounded by [](#lem-single-walker-positional-error). The term $\\Delta_{\\text{struct},i}$ is the error from structural change with fixed positions, bounded by [](#lem-single-walker-structural-error).\n2373: 2.  **Bound the Squared Sum:** Using the elementary inequality $(a+b)^2 \\le 2a^2 + 2b^2$, we can bound the square of the single-walker error. Summing over all $i \\in \\mathcal{A}_{\\text{stable}}$ yields the inequality stated in the lemma.\n2374: **Q.E.D.**"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-sub-stable-positional-error-bound",
      "title": null,
      "start_line": 2383,
      "end_line": 2430,
      "header_lines": [
        2384
      ],
      "content_start": 2385,
      "content_end": 2429,
      "content": "2385: :label: proof-lem-sub-stable-positional-error-bound\n2386: **Proof.**\n2387: 1.  **Bound the Single-Walker Squared Error:** We start with the bound on $\\Delta_{\\text{pos},i}$ from [](#lem-single-walker-positional-error) and apply the inequality $(a+b)^2 \\le 2a^2 + 2b^2$:\n2388: $$\n2389: \n2390: (\\Delta_{\\text{pos},i})^2 \\le 2 \\cdot d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 2 \\cdot \\left( \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c}) \\right] \\right)^2\n2391: \n2392: $$\n2393: 2.  **Apply Jensen's Inequality:** For the second term, we apply Jensen's inequality, $(\\mathbb{E}[X])^2 \\le \\mathbb{E}[X^2]$, to move the square inside the expectation:\n2394: $$\n2395: \n2396: (\\Delta_{\\text{pos},i})^2 \\le 2 \\cdot d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 2 \\cdot \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\right]\n2397: \n2398: $$\n2399: 3.  **Sum Over All Stable Walkers:** We sum this inequality over all $i \\in \\mathcal{A}_{\\text{stable}}$.\n2400: $$\n2401: \n2402: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\right]\n2403: \n2404: $$\n2405: 4.  **Analyze the Second Term's Double Summation:** The second term is a double summation. We expand the expectation:\n2406: $$\n2407: \n2408: 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\left( \\frac{1}{|\\mathcal{A}_1 \\setminus \\{i\\}|} \\sum_{c \\in \\mathcal{A}_1 \\setminus \\{i\\}} d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\right)\n2409: \n2410: $$\n2411: Consider a specific squared distance term $d_{\\text{alg}}(x_{1,j}, x_{2,j})^2$ where $j \\in \\mathcal{A}_1$. This term appears in the inner sum for every $i \\in \\mathcal{A}_{\\text{stable}}$ such that $i \\neq j$. The number of such appearances is $|\\mathcal{A}_{\\text{stable}} \\setminus \\{j\\}|$, which is bounded above by $|\\mathcal{A}_{\\text{stable}}|$. The normalization factor is $\\frac{1}{k_1-1}$. Therefore, the double summation is bounded by:\n2412: $$\n2413: \n2414: \\le \\frac{2}{k_1 - 1} \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\sum_{c \\in \\mathcal{A}_1 \\setminus \\{i\\}} d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\le \\frac{2|\\mathcal{A}_{\\text{stable}}|}{k_1 - 1} \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2415: \n2416: $$\n2417: Since $|\\mathcal{A}_{\\text{stable}}| \\le k_1$, and for $k_1 \\ge 2$, the fraction $k_1/(k_1-1) \\le 2$, the entire second term from Step 3 is bounded by $4 \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2$.\n2418: 5.  **Combine and Finalize:** Substituting this back into the inequality from Step 3:\n2419: $$\n2420: \n2421: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 4 \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2422: \n2423: $$\n2424: Both sums can be bounded by the sum over all $N$ walkers, which is the definition of $\\Delta_{\\text{pos}}^2$:\n2425: $$\n2426: \n2427: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 6 \\cdot \\Delta_{\\text{pos}}^2(\\mathcal{S}_1, \\mathcal{S}_2)\n2428: \n2429: $$",
      "metadata": {
        "label": "proof-lem-sub-stable-positional-error-bound"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2383: $$\n2384: :::{prf:proof}\n2385: :label: proof-lem-sub-stable-positional-error-bound\n2386: **Proof.**\n2387: 1.  **Bound the Single-Walker Squared Error:** We start with the bound on $\\Delta_{\\text{pos},i}$ from [](#lem-single-walker-positional-error) and apply the inequality $(a+b)^2 \\le 2a^2 + 2b^2$:\n2388: $$\n2389: \n2390: (\\Delta_{\\text{pos},i})^2 \\le 2 \\cdot d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 2 \\cdot \\left( \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c}) \\right] \\right)^2\n2391: \n2392: $$\n2393: 2.  **Apply Jensen's Inequality:** For the second term, we apply Jensen's inequality, $(\\mathbb{E}[X])^2 \\le \\mathbb{E}[X^2]$, to move the square inside the expectation:\n2394: $$\n2395: \n2396: (\\Delta_{\\text{pos},i})^2 \\le 2 \\cdot d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 2 \\cdot \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\right]\n2397: \n2398: $$\n2399: 3.  **Sum Over All Stable Walkers:** We sum this inequality over all $i \\in \\mathcal{A}_{\\text{stable}}$.\n2400: $$\n2401: \n2402: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\right]\n2403: \n2404: $$\n2405: 4.  **Analyze the Second Term's Double Summation:** The second term is a double summation. We expand the expectation:\n2406: $$\n2407: \n2408: 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\left( \\frac{1}{|\\mathcal{A}_1 \\setminus \\{i\\}|} \\sum_{c \\in \\mathcal{A}_1 \\setminus \\{i\\}} d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\right)\n2409: \n2410: $$\n2411: Consider a specific squared distance term $d_{\\text{alg}}(x_{1,j}, x_{2,j})^2$ where $j \\in \\mathcal{A}_1$. This term appears in the inner sum for every $i \\in \\mathcal{A}_{\\text{stable}}$ such that $i \\neq j$. The number of such appearances is $|\\mathcal{A}_{\\text{stable}} \\setminus \\{j\\}|$, which is bounded above by $|\\mathcal{A}_{\\text{stable}}|$. The normalization factor is $\\frac{1}{k_1-1}$. Therefore, the double summation is bounded by:\n2412: $$\n2413: \n2414: \\le \\frac{2}{k_1 - 1} \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\sum_{c \\in \\mathcal{A}_1 \\setminus \\{i\\}} d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\le \\frac{2|\\mathcal{A}_{\\text{stable}}|}{k_1 - 1} \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2415: \n2416: $$\n2417: Since $|\\mathcal{A}_{\\text{stable}}| \\le k_1$, and for $k_1 \\ge 2$, the fraction $k_1/(k_1-1) \\le 2$, the entire second term from Step 3 is bounded by $4 \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2$.\n2418: 5.  **Combine and Finalize:** Substituting this back into the inequality from Step 3:\n2419: $$\n2420: \n2421: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 4 \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2422: \n2423: $$\n2424: Both sums can be bounded by the sum over all $N$ walkers, which is the definition of $\\Delta_{\\text{pos}}^2$:\n2425: $$\n2426: \n2427: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 6 \\cdot \\Delta_{\\text{pos}}^2(\\mathcal{S}_1, \\mathcal{S}_2)\n2428: \n2429: $$\n2430: **Q.E.D.**"
    },
    {
      "directive_type": "proof",
      "label": "proof-lem-sub-stable-structural-error-bound",
      "title": null,
      "start_line": 2440,
      "end_line": 2464,
      "header_lines": [
        2441
      ],
      "content_start": 2442,
      "content_end": 2463,
      "content": "2442: :label: proof-lem-sub-stable-structural-error-bound\n2443: **Proof.**\n2444: The proof proceeds by taking the established bound for the structural error of a single stable walker and summing its square over all stable walkers.\n2445: 1.  **Bound the Single-Walker Squared Error:** We start with the bound on the structural error component for a single walker $i$, $\\Delta_{\\text{struct},i}$, as established in [](#lem-single-walker-structural-error). The bound is:\n2446: $$\n2447: \n2448: |\\Delta_{\\text{struct},i}| \\le \\frac{2 D_{\\mathcal{Y}}}{k_1 - 1} \\cdot n_c(\\mathcal{S}_1, \\mathcal{S}_2)\n2449: \n2450: $$\n2451: Squaring this expression provides a deterministic bound for the squared error of a single stable walker:\n2452: $$\n2453: \n2454: (\\Delta_{\\text{struct},i})^2 \\le \\frac{4 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c(\\mathcal{S}_1, \\mathcal{S}_2)^2\n2455: \n2456: $$\n2457: 2.  **Sum Over All Stable Walkers:** We sum this inequality over all stable walkers $i \\in \\mathcal{A}_{\\text{stable}}$. Since the derived bound is identical for every stable walker, we multiply the single-walker bound by the number of stable walkers, $|\\mathcal{A}_{\\text{stable}}|$.\n2458: $$\n2459: \n2460: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2 \\le |\\mathcal{A}_{\\text{stable}}| \\cdot \\frac{4 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c(\\mathcal{S}_1, \\mathcal{S}_2)^2\n2461: \n2462: $$\n2463: 3.  **Finalize:** Using the fact that the number of stable walkers is bounded by the initial number of alive walkers, $|\\mathcal{A}_{\\text{stable}}| \\le |\\mathcal{A}(\\mathcal{S}_1)| = k_1$, we arrive at the final bound stated in the sub-lemma.",
      "metadata": {
        "label": "proof-lem-sub-stable-structural-error-bound"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2440: $$\n2441: :::{prf:proof}\n2442: :label: proof-lem-sub-stable-structural-error-bound\n2443: **Proof.**\n2444: The proof proceeds by taking the established bound for the structural error of a single stable walker and summing its square over all stable walkers.\n2445: 1.  **Bound the Single-Walker Squared Error:** We start with the bound on the structural error component for a single walker $i$, $\\Delta_{\\text{struct},i}$, as established in [](#lem-single-walker-structural-error). The bound is:\n2446: $$\n2447: \n2448: |\\Delta_{\\text{struct},i}| \\le \\frac{2 D_{\\mathcal{Y}}}{k_1 - 1} \\cdot n_c(\\mathcal{S}_1, \\mathcal{S}_2)\n2449: \n2450: $$\n2451: Squaring this expression provides a deterministic bound for the squared error of a single stable walker:\n2452: $$\n2453: \n2454: (\\Delta_{\\text{struct},i})^2 \\le \\frac{4 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c(\\mathcal{S}_1, \\mathcal{S}_2)^2\n2455: \n2456: $$\n2457: 2.  **Sum Over All Stable Walkers:** We sum this inequality over all stable walkers $i \\in \\mathcal{A}_{\\text{stable}}$. Since the derived bound is identical for every stable walker, we multiply the single-walker bound by the number of stable walkers, $|\\mathcal{A}_{\\text{stable}}|$.\n2458: $$\n2459: \n2460: \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2 \\le |\\mathcal{A}_{\\text{stable}}| \\cdot \\frac{4 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c(\\mathcal{S}_1, \\mathcal{S}_2)^2\n2461: \n2462: $$\n2463: 3.  **Finalize:** Using the fact that the number of stable walkers is bounded by the initial number of alive walkers, $|\\mathcal{A}_{\\text{stable}}| \\le |\\mathcal{A}(\\mathcal{S}_1)| = k_1$, we arrive at the final bound stated in the sub-lemma.\n2464: **Q.E.D.**"
    },
    {
      "directive_type": "proof",
      "label": "proof-line-2408",
      "title": null,
      "start_line": 2466,
      "end_line": 2479,
      "header_lines": [
        2467
      ],
      "content_start": 2468,
      "content_end": 2478,
      "content": "2468: :label: proof-line-2408\n2469: **Proof.**\n2470: 1.  **Analyze a Single Unstable Walker:** Let $i$ be an unstable walker, meaning its status $s_i$ changes. From [](#lem-single-walker-own-status-error), the absolute error in its expected distance is bounded by $D_{\\mathcal{Y}}$. Therefore, the squared error for any single unstable walker is bounded by $D_{\\mathcal{Y}}^2$.\n2471: 2.  **Sum Over All Unstable Walkers:** The set of unstable walkers, $\\mathcal{A}_{\\text{unstable}}$, is precisely the set of indices where $s_{1,i} \\neq s_{2,i}$. The number of walkers in this set is $|\\mathcal{A}_{\\text{unstable}}| = \\sum_{j=1}^N (s_{1,j} - s_{2,j})^2$, since $(s_{1,j} - s_{2,j})^2$ is 1 if the status changes and 0 otherwise.\n2472: 3.  **Combine and Finalize:** The total squared error from unstable walkers is the sum of their individual squared errors. Since each is bounded by $D_{\\mathcal{Y}}^2$, the total sum is bounded by the number of such walkers multiplied by this bound:\n2473: $$\n2474: \n2475:     \\sum_{i \\in \\mathcal{A}_{\\text{unstable}}} |\\dots|^2 \\le |\\mathcal{A}_{\\text{unstable}}| \\cdot D_{\\mathcal{Y}}^2 = D_{\\mathcal{Y}}^2 \\sum_{j=1}^N (s_{1,j} - s_{2,j})^2\n2476: \n2477: \n2478: $$",
      "metadata": {
        "label": "proof-line-2408"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2466: #### 10.2.5 Lemma: Bound on the Total Squared Error for Unstable Walkers\n2467: :::{prf:proof}\n2468: :label: proof-line-2408\n2469: **Proof.**\n2470: 1.  **Analyze a Single Unstable Walker:** Let $i$ be an unstable walker, meaning its status $s_i$ changes. From [](#lem-single-walker-own-status-error), the absolute error in its expected distance is bounded by $D_{\\mathcal{Y}}$. Therefore, the squared error for any single unstable walker is bounded by $D_{\\mathcal{Y}}^2$.\n2471: 2.  **Sum Over All Unstable Walkers:** The set of unstable walkers, $\\mathcal{A}_{\\text{unstable}}$, is precisely the set of indices where $s_{1,i} \\neq s_{2,i}$. The number of walkers in this set is $|\\mathcal{A}_{\\text{unstable}}| = \\sum_{j=1}^N (s_{1,j} - s_{2,j})^2$, since $(s_{1,j} - s_{2,j})^2$ is 1 if the status changes and 0 otherwise.\n2472: 3.  **Combine and Finalize:** The total squared error from unstable walkers is the sum of their individual squared errors. Since each is bounded by $D_{\\mathcal{Y}}^2$, the total sum is bounded by the number of such walkers multiplied by this bound:\n2473: $$\n2474: \n2475:     \\sum_{i \\in \\mathcal{A}_{\\text{unstable}}} |\\dots|^2 \\le |\\mathcal{A}_{\\text{unstable}}| \\cdot D_{\\mathcal{Y}}^2 = D_{\\mathcal{Y}}^2 \\sum_{j=1}^N (s_{1,j} - s_{2,j})^2\n2476: \n2477: \n2478: $$\n2479: **Q.E.D.**"
    },
    {
      "directive_type": "proof",
      "label": "proof-line-2422",
      "title": null,
      "start_line": 2481,
      "end_line": 2508,
      "header_lines": [
        2482
      ],
      "content_start": 2483,
      "content_end": 2507,
      "content": "2483: :label: proof-line-2422\n2484: **Proof.**\n2485: The total error for a single stable walker is first decomposed into a positional component and a structural component. The squared L2-norm of the total error over all stable walkers is then bounded by combining the established bounds for the sum of the squares of these individual components.\n2486: 1.  **Decomposition of Total Stable Error:** From [](#sub-lem-stable-walker-error-decomposition), the total squared error for stable walkers is bounded by twice the sum of the squared positional and structural error components:\n2487: $$\n2488: \n2489:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} |\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]|^2 \\le 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 + 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2\n2490: \n2491: \n2492: $$\n2493: 2.  **Bound the Positional Component:** From [](#sub-lem-stable-positional-error-bound), the total squared positional error component is bounded by:\n2494: $$\n2495: \n2496:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 6 \\sum_{j=1}^N d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2497: \n2498: \n2499: $$\n2500: 3.  **Bound the Structural Component:** From [](#sub-lem-stable-structural-error-bound), the total squared structural error component is bounded by:\n2501: $$\n2502: \n2503:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2 \\le \\frac{4 k_1 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\left( \\sum_{j=1}^N (s_{1,j} - s_{2,j})^2 \\right)^2\n2504: \n2505: \n2506: $$\n2507: 4.  **Combine the Bounds:** Substituting the bounds from steps 2 and 3 into the inequality from step 1 and multiplying by the factor of 2 yields the final result stated in the lemma.",
      "metadata": {
        "label": "proof-line-2422"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2481: #### 10.2.6 Lemma: Bound on the Total Squared Error for Stable Walkers\n2482: :::{prf:proof}\n2483: :label: proof-line-2422\n2484: **Proof.**\n2485: The total error for a single stable walker is first decomposed into a positional component and a structural component. The squared L2-norm of the total error over all stable walkers is then bounded by combining the established bounds for the sum of the squares of these individual components.\n2486: 1.  **Decomposition of Total Stable Error:** From [](#sub-lem-stable-walker-error-decomposition), the total squared error for stable walkers is bounded by twice the sum of the squared positional and structural error components:\n2487: $$\n2488: \n2489:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} |\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]|^2 \\le 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 + 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2\n2490: \n2491: \n2492: $$\n2493: 2.  **Bound the Positional Component:** From [](#sub-lem-stable-positional-error-bound), the total squared positional error component is bounded by:\n2494: $$\n2495: \n2496:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 6 \\sum_{j=1}^N d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2497: \n2498: \n2499: $$\n2500: 3.  **Bound the Structural Component:** From [](#sub-lem-stable-structural-error-bound), the total squared structural error component is bounded by:\n2501: $$\n2502: \n2503:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2 \\le \\frac{4 k_1 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\left( \\sum_{j=1}^N (s_{1,j} - s_{2,j})^2 \\right)^2\n2504: \n2505: \n2506: $$\n2507: 4.  **Combine the Bounds:** Substituting the bounds from steps 2 and 3 into the inequality from step 1 and multiplying by the factor of 2 yields the final result stated in the lemma.\n2508: **Q.E.D.**"
    },
    {
      "directive_type": "proof",
      "label": "proof-line-2450",
      "title": null,
      "start_line": 2510,
      "end_line": 2523,
      "header_lines": [
        2511
      ],
      "content_start": 2512,
      "content_end": 2522,
      "content": "2512: :label: proof-line-2450\n2513: **Proof.**\n2514: 1.  **Decompose Single-Walker Error:** For each stable walker $i \\in \\mathcal{A}_{\\text{stable}}$, we introduce the intermediate term $\\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} [d_{\\text{alg}}(x_{2,i}, x_{2,c})]$ and apply the triangle inequality:\n2515: $$\n2516: \n2517:     |\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]| \\le \\underbrace{\\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,i}, x_{1,c}) \\right] - \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right|}_{\\Delta_{\\text{pos},i}} + \\underbrace{\\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] - \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_2)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right|}_{\\Delta_{\\text{struct},i}}\n2518: \n2519: \n2520: $$\n2521: The term $\\Delta_{\\text{pos},i}$ is the error from positional change over a fixed companion set, bounded by [](#lem-single-walker-positional-error). The term $\\Delta_{\\text{struct},i}$ is the error from structural change with fixed positions, bounded by [](#lem-single-walker-structural-error).\n2522: 2.  **Bound the Squared Sum:** Using the elementary inequality $(a+b)^2 \\le 2a^2 + 2b^2$, we can bound the square of the single-walker error. Summing over all $i \\in \\mathcal{A}_{\\text{stable}}$ yields the inequality stated in the lemma.",
      "metadata": {
        "label": "proof-line-2450"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2510: ##### 10.2.6.1 Sub-Lemma: Decomposition of Stable Walker Error\n2511: :::{prf:proof}\n2512: :label: proof-line-2450\n2513: **Proof.**\n2514: 1.  **Decompose Single-Walker Error:** For each stable walker $i \\in \\mathcal{A}_{\\text{stable}}$, we introduce the intermediate term $\\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} [d_{\\text{alg}}(x_{2,i}, x_{2,c})]$ and apply the triangle inequality:\n2515: $$\n2516: \n2517:     |\\mathbb{E}[d_i(\\mathcal{S}_1)] - \\mathbb{E}[d_i(\\mathcal{S}_2)]| \\le \\underbrace{\\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,i}, x_{1,c}) \\right] - \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right|}_{\\Delta_{\\text{pos},i}} + \\underbrace{\\left| \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] - \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_2)} \\left[ d_{\\text{alg}}(x_{2,i}, x_{2,c}) \\right] \\right|}_{\\Delta_{\\text{struct},i}}\n2518: \n2519: \n2520: $$\n2521: The term $\\Delta_{\\text{pos},i}$ is the error from positional change over a fixed companion set, bounded by [](#lem-single-walker-positional-error). The term $\\Delta_{\\text{struct},i}$ is the error from structural change with fixed positions, bounded by [](#lem-single-walker-structural-error).\n2522: 2.  **Bound the Squared Sum:** Using the elementary inequality $(a+b)^2 \\le 2a^2 + 2b^2$, we can bound the square of the single-walker error. Summing over all $i \\in \\mathcal{A}_{\\text{stable}}$ yields the inequality stated in the lemma.\n2523: **Q.E.D.**"
    },
    {
      "directive_type": "proof",
      "label": "proof-line-2464",
      "title": null,
      "start_line": 2525,
      "end_line": 2586,
      "header_lines": [
        2526
      ],
      "content_start": 2527,
      "content_end": 2585,
      "content": "2527: :label: proof-line-2464\n2528: **Proof.**\n2529: 1.  **Bound the Single-Walker Squared Error:** We start with the bound on $\\Delta_{\\text{pos},i}$ from [](#lem-single-walker-positional-error) and apply the inequality $(a+b)^2 \\le 2a^2 + 2b^2$:\n2530: $$\n2531: \n2532:     (\\Delta_{\\text{pos},i})^2 \\le 2 \\cdot d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 2 \\cdot \\left( \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c}) \\right] \\right)^2\n2533: \n2534: \n2535: $$\n2536: 2.  **Apply Jensen's Inequality:** For the second term, we apply Jensen's inequality, $(\\mathbb{E}[X])^2 \\le \\mathbb{E}[X^2]$, to move the square inside the expectation:\n2537: $$\n2538: \n2539:     (\\Delta_{\\text{pos},i})^2 \\le 2 \\cdot d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 2 \\cdot \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\right]\n2540: \n2541: \n2542: $$\n2543: 3.  **Sum Over All Stable Walkers:** We sum this inequality over all $i \\in \\mathcal{A}_{\\text{stable}}$.\n2544: $$\n2545: \n2546:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\right]\n2547: \n2548: \n2549: $$\n2550: 4.  **Analyze the Second Term's Double Summation:** The second term is a double summation. We expand the expectation:\n2551: $$\n2552: \n2553:     2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\left( \\frac{1}{k_1 - 1} \\sum_{c \\in \\mathcal{A}_1 \\setminus \\{i\\}} d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\right) = \\frac{2}{k_1 - 1} \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\sum_{c \\in \\mathcal{A}_1 \\setminus \\{i\\}} d_{\\text{alg}}(x_{1,c}, x_{2,c})^2\n2554: \n2555: \n2556: $$\n2557: Consider a specific squared distance term $d_{\\text{alg}}(x_{1,j}, x_{2,j})^2$ where $j \\in \\mathcal{A}_1$. This term appears in the inner sum for every $i \\in \\mathcal{A}_{\\text{stable}}$ such that $i \\neq j$. The number of such appearances is $|\\mathcal{A}_{\\text{stable}} \\setminus \\{j\\}|$, which is bounded above by $|\\mathcal{A}_{\\text{stable}}|$. Therefore, the double summation is bounded by:\n2558: $$\n2559: \n2560:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\sum_{c \\in \\mathcal{A}_1 \\setminus \\{i\\}} (\\dots)^2 \\le |\\mathcal{A}_{\\text{stable}}| \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2561: \n2562: \n2563: $$\n2564: Since $|\\mathcal{A}_{\\text{stable}}| \\le k_1$, the entire second term from Step 3 is bounded by:\n2565: $$\n2566: \n2567:     \\frac{2}{k_1 - 1} \\cdot k_1 \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2568: \n2569: \n2570: $$\n2571: For $k_1 \\ge 2$, the fraction $k_1/(k_1-1) \\le 2$. The term is therefore bounded by $4 \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2$.\n2572: 5.  **Combine and Finalize:** Substituting this back into the inequality from Step 3:\n2573: $$\n2574: \n2575:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 4 \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2576: \n2577: \n2578: $$\n2579: Both sums can be bounded by the sum over all $N$ walkers, giving the final result:\n2580: $$\n2581: \n2582:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 6 \\sum_{j=1}^N d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2583: \n2584: \n2585: $$",
      "metadata": {
        "label": "proof-line-2464"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2525: ##### 10.2.6.2 Sub-Lemma: Bounding the Positional Error Component\n2526: :::{prf:proof}\n2527: :label: proof-line-2464\n2528: **Proof.**\n2529: 1.  **Bound the Single-Walker Squared Error:** We start with the bound on $\\Delta_{\\text{pos},i}$ from [](#lem-single-walker-positional-error) and apply the inequality $(a+b)^2 \\le 2a^2 + 2b^2$:\n2530: $$\n2531: \n2532:     (\\Delta_{\\text{pos},i})^2 \\le 2 \\cdot d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 2 \\cdot \\left( \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c}) \\right] \\right)^2\n2533: \n2534: \n2535: $$\n2536: 2.  **Apply Jensen's Inequality:** For the second term, we apply Jensen's inequality, $(\\mathbb{E}[X])^2 \\le \\mathbb{E}[X^2]$, to move the square inside the expectation:\n2537: $$\n2538: \n2539:     (\\Delta_{\\text{pos},i})^2 \\le 2 \\cdot d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 2 \\cdot \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\right]\n2540: \n2541: \n2542: $$\n2543: 3.  **Sum Over All Stable Walkers:** We sum this inequality over all $i \\in \\mathcal{A}_{\\text{stable}}$.\n2544: $$\n2545: \n2546:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\mathbb{E}_{c \\sim \\mathbb{C}_i(\\mathcal{S}_1)} \\left[ d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\right]\n2547: \n2548: \n2549: $$\n2550: 4.  **Analyze the Second Term's Double Summation:** The second term is a double summation. We expand the expectation:\n2551: $$\n2552: \n2553:     2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\left( \\frac{1}{k_1 - 1} \\sum_{c \\in \\mathcal{A}_1 \\setminus \\{i\\}} d_{\\text{alg}}(x_{1,c}, x_{2,c})^2 \\right) = \\frac{2}{k_1 - 1} \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\sum_{c \\in \\mathcal{A}_1 \\setminus \\{i\\}} d_{\\text{alg}}(x_{1,c}, x_{2,c})^2\n2554: \n2555: \n2556: $$\n2557: Consider a specific squared distance term $d_{\\text{alg}}(x_{1,j}, x_{2,j})^2$ where $j \\in \\mathcal{A}_1$. This term appears in the inner sum for every $i \\in \\mathcal{A}_{\\text{stable}}$ such that $i \\neq j$. The number of such appearances is $|\\mathcal{A}_{\\text{stable}} \\setminus \\{j\\}|$, which is bounded above by $|\\mathcal{A}_{\\text{stable}}|$. Therefore, the double summation is bounded by:\n2558: $$\n2559: \n2560:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} \\sum_{c \\in \\mathcal{A}_1 \\setminus \\{i\\}} (\\dots)^2 \\le |\\mathcal{A}_{\\text{stable}}| \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2561: \n2562: \n2563: $$\n2564: Since $|\\mathcal{A}_{\\text{stable}}| \\le k_1$, the entire second term from Step 3 is bounded by:\n2565: $$\n2566: \n2567:     \\frac{2}{k_1 - 1} \\cdot k_1 \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2568: \n2569: \n2570: $$\n2571: For $k_1 \\ge 2$, the fraction $k_1/(k_1-1) \\le 2$. The term is therefore bounded by $4 \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2$.\n2572: 5.  **Combine and Finalize:** Substituting this back into the inequality from Step 3:\n2573: $$\n2574: \n2575:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 2 \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} d_{\\text{alg}}(x_{1,i}, x_{2,i})^2 + 4 \\sum_{j \\in \\mathcal{A}_1} d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2576: \n2577: \n2578: $$\n2579: Both sums can be bounded by the sum over all $N$ walkers, giving the final result:\n2580: $$\n2581: \n2582:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{pos},i})^2 \\le 6 \\sum_{j=1}^N d_{\\text{alg}}(x_{1,j}, x_{2,j})^2\n2583: \n2584: \n2585: $$\n2586: **Q.E.D.**"
    },
    {
      "directive_type": "proof",
      "label": "proof-line-2526",
      "title": null,
      "start_line": 2588,
      "end_line": 2621,
      "header_lines": [
        2589
      ],
      "content_start": 2590,
      "content_end": 2620,
      "content": "2590: :label: proof-line-2526\n2591: **Proof.**\n2592: The proof proceeds by taking the established bound for the structural error of a single stable walker and summing its square over all stable walkers in the set $\\mathcal{A}_{\\text{stable}}$.\n2593: 1.  **Bound the Single-Walker Squared Error:** We start with the bound on the structural error component for a single walker $i$, $\\Delta_{\\text{struct},i}$, as established in [](#lem-single-walker-structural-error). Let $n_c = \\sum_{j=1}^N (s_{1,j} - s_{2,j})^2$ be the total number of status changes. The bound from [](#lem-single-walker-structural-error) is:\n2594: $$\n2595: \n2596:     |\\Delta_{\\text{struct},i}| \\le \\frac{2 D_{\\mathcal{Y}}}{k_1 - 1} \\cdot n_c\n2597: \n2598: \n2599: $$\n2600: Squaring this expression provides a deterministic bound for the squared error of a single stable walker:\n2601: $$\n2602: \n2603:     (\\Delta_{\\text{struct},i})^2 \\le \\frac{4 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c^2\n2604: \n2605: \n2606: $$\n2607: 2.  **Sum Over All Stable Walkers:** We sum this inequality over all stable walkers $i \\in \\mathcal{A}_{\\text{stable}}$. Since the derived bound is identical for every stable walker and does not depend on the index $i$, we multiply the single-walker bound by the number of stable walkers, $|\\mathcal{A}_{\\text{stable}}|$.\n2608: $$\n2609: \n2610:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2 \\le |\\mathcal{A}_{\\text{stable}}| \\cdot \\frac{4 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c^2\n2611: \n2612: \n2613: $$\n2614: 3.  **Finalize:** Using the fact that the number of stable walkers is bounded by the initial number of alive walkers, $|\\mathcal{A}_{\\text{stable}}| \\le |\\mathcal{A}(\\mathcal{S}_1)| = k_1$, and substituting the definition of $n_c^2$, we arrive at the final bound stated in the sub-lemma.\n2615: $$\n2616: \n2617:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2 \\le \\frac{4 k_1 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c^2 = \\frac{4 k_1 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\left( \\sum_{j=1}^N (s_{1,j} - s_{2,j})^2 \\right)^2\n2618: \n2619: \n2620: $$",
      "metadata": {
        "label": "proof-line-2526"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2588: ##### 10.2.6.3 Sub-Lemma: Bounding the Structural Error Component for Stable Walkers\n2589: :::{prf:proof}\n2590: :label: proof-line-2526\n2591: **Proof.**\n2592: The proof proceeds by taking the established bound for the structural error of a single stable walker and summing its square over all stable walkers in the set $\\mathcal{A}_{\\text{stable}}$.\n2593: 1.  **Bound the Single-Walker Squared Error:** We start with the bound on the structural error component for a single walker $i$, $\\Delta_{\\text{struct},i}$, as established in [](#lem-single-walker-structural-error). Let $n_c = \\sum_{j=1}^N (s_{1,j} - s_{2,j})^2$ be the total number of status changes. The bound from [](#lem-single-walker-structural-error) is:\n2594: $$\n2595: \n2596:     |\\Delta_{\\text{struct},i}| \\le \\frac{2 D_{\\mathcal{Y}}}{k_1 - 1} \\cdot n_c\n2597: \n2598: \n2599: $$\n2600: Squaring this expression provides a deterministic bound for the squared error of a single stable walker:\n2601: $$\n2602: \n2603:     (\\Delta_{\\text{struct},i})^2 \\le \\frac{4 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c^2\n2604: \n2605: \n2606: $$\n2607: 2.  **Sum Over All Stable Walkers:** We sum this inequality over all stable walkers $i \\in \\mathcal{A}_{\\text{stable}}$. Since the derived bound is identical for every stable walker and does not depend on the index $i$, we multiply the single-walker bound by the number of stable walkers, $|\\mathcal{A}_{\\text{stable}}|$.\n2608: $$\n2609: \n2610:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2 \\le |\\mathcal{A}_{\\text{stable}}| \\cdot \\frac{4 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c^2\n2611: \n2612: \n2613: $$\n2614: 3.  **Finalize:** Using the fact that the number of stable walkers is bounded by the initial number of alive walkers, $|\\mathcal{A}_{\\text{stable}}| \\le |\\mathcal{A}(\\mathcal{S}_1)| = k_1$, and substituting the definition of $n_c^2$, we arrive at the final bound stated in the sub-lemma.\n2615: $$\n2616: \n2617:     \\sum_{i \\in \\mathcal{A}_{\\text{stable}}} (\\Delta_{\\text{struct},i})^2 \\le \\frac{4 k_1 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c^2 = \\frac{4 k_1 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\left( \\sum_{j=1}^N (s_{1,j} - s_{2,j})^2 \\right)^2\n2618: \n2619: \n2620: $$\n2621: **Q.E.D.**"
    },
    {
      "directive_type": "theorem",
      "label": "thm-expected-raw-distance-bound",
      "title": "Bound on the Expected Raw Distance Vector Change",
      "start_line": 2624,
      "end_line": 2637,
      "header_lines": [
        2625
      ],
      "content_start": 2626,
      "content_end": 2636,
      "content": "2626: :label: thm-expected-raw-distance-bound\n2627: Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two swarm states, with $|\\mathcal{A}(\\mathcal{S}_1)| = k_1 \\ge 2$. Let $\\mathbb{E}[\\mathbf{d}(\\mathcal{S})]$ be the $N$-dimensional vector of expected raw distances.\n2628: The squared Euclidean distance between the expected raw distance vectors of the two swarms is deterministically bounded by a function of the displacement components:\n2629: $$\n2630: \n2631: \\| \\mathbb{E}[\\mathbf{d}(\\mathcal{S}_1)] - \\mathbb{E}[\\mathbf{d}(\\mathcal{S}_2)] \\|_2^2 \\le C_{\\text{pos},d} \\cdot \\Delta_{\\text{pos}}^2(\\mathcal{S}_1, \\mathcal{S}_2) + C_{\\text{status},d}^{(1)} \\cdot n_c(\\mathcal{S}_1, \\mathcal{S}_2) + C_{\\text{status},d}^{(2)}(k_1) \\cdot n_c^2(\\mathcal{S}_1, \\mathcal{S}_2)\n2632: \n2633: $$\n2634: where the **Expected Distance Error Coefficients** are defined as:\n2635: *   $C_{\\text{pos},d} := 12$\n2636: *   $C_{\\text{status},d}^{(1)} := D_{\\mathcal{Y}}^2$",
      "metadata": {
        "label": "thm-expected-raw-distance-bound"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2624: This theorem consolidates the error bounds from the preceding lemmas to establish a deterministic bound on the change of the *expected* raw distance vector. This bound is expressed as a function of the formal displacement components ([](#def-displacement-components)) and serves as the deterministic part of the continuity axiom for the distance operator.\n2625: :::{prf:theorem} Bound on the Expected Raw Distance Vector Change\n2626: :label: thm-expected-raw-distance-bound\n2627: Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two swarm states, with $|\\mathcal{A}(\\mathcal{S}_1)| = k_1 \\ge 2$. Let $\\mathbb{E}[\\mathbf{d}(\\mathcal{S})]$ be the $N$-dimensional vector of expected raw distances.\n2628: The squared Euclidean distance between the expected raw distance vectors of the two swarms is deterministically bounded by a function of the displacement components:\n2629: $$\n2630: \n2631: \\| \\mathbb{E}[\\mathbf{d}(\\mathcal{S}_1)] - \\mathbb{E}[\\mathbf{d}(\\mathcal{S}_2)] \\|_2^2 \\le C_{\\text{pos},d} \\cdot \\Delta_{\\text{pos}}^2(\\mathcal{S}_1, \\mathcal{S}_2) + C_{\\text{status},d}^{(1)} \\cdot n_c(\\mathcal{S}_1, \\mathcal{S}_2) + C_{\\text{status},d}^{(2)}(k_1) \\cdot n_c^2(\\mathcal{S}_1, \\mathcal{S}_2)\n2632: \n2633: $$\n2634: where the **Expected Distance Error Coefficients** are defined as:\n2635: *   $C_{\\text{pos},d} := 12$\n2636: *   $C_{\\text{status},d}^{(1)} := D_{\\mathcal{Y}}^2$\n2637: *   $C_{\\text{status},d}^{(2)}(k_1) := \\frac{8 k_1 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2}$"
    },
    {
      "directive_type": "proof",
      "label": "proof-thm-expected-raw-distance-bound",
      "title": null,
      "start_line": 2638,
      "end_line": 2648,
      "header_lines": [
        2639
      ],
      "content_start": 2640,
      "content_end": 2647,
      "content": "2640: :label: proof-thm-expected-raw-distance-bound\n2641: **Proof.**\n2642: The proof is a direct consequence of decomposing the total error and applying the bounds established in the preceding lemmas.\n2643: 1.  **Decomposition of Total Error:** Following [](#thm-total-expected-distance-error-decomposition), the total squared error is the sum of the error from the set of stable walkers ($E^2_{\\text{stable}}$) and the set of unstable walkers ($E^2_{\\text{unstable}}$).\n2644: 2.  **Bound Error Components:**\n2645:     *   The error from unstable walkers, $E^2_{\\text{unstable}}$, is bounded by [](#lem-total-squared-error-unstable): $E^2_{\\text{unstable}} \\le D_{\\mathcal{Y}}^2 \\cdot n_c$.\n2646:     *   The error from stable walkers, $E^2_{\\text{stable}}$, is bounded by [](#lem-total-squared-error-stable): $E^2_{\\text{stable}} \\le 12 \\cdot \\Delta_{\\text{pos}}^2 + \\frac{8 k_1 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c^2$.\n2647: 3.  **Combine Bounds:** Summing the two bounds gives the final inequality. This theorem recasts that result by explicitly naming the coefficients for each displacement component, formalizing the bound for use in subsequent proofs.",
      "metadata": {
        "label": "proof-thm-expected-raw-distance-bound"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2638: :::\n2639: :::{prf:proof}\n2640: :label: proof-thm-expected-raw-distance-bound\n2641: **Proof.**\n2642: The proof is a direct consequence of decomposing the total error and applying the bounds established in the preceding lemmas.\n2643: 1.  **Decomposition of Total Error:** Following [](#thm-total-expected-distance-error-decomposition), the total squared error is the sum of the error from the set of stable walkers ($E^2_{\\text{stable}}$) and the set of unstable walkers ($E^2_{\\text{unstable}}$).\n2644: 2.  **Bound Error Components:**\n2645:     *   The error from unstable walkers, $E^2_{\\text{unstable}}$, is bounded by [](#lem-total-squared-error-unstable): $E^2_{\\text{unstable}} \\le D_{\\mathcal{Y}}^2 \\cdot n_c$.\n2646:     *   The error from stable walkers, $E^2_{\\text{stable}}$, is bounded by [](#lem-total-squared-error-stable): $E^2_{\\text{stable}} \\le 12 \\cdot \\Delta_{\\text{pos}}^2 + \\frac{8 k_1 D_{\\mathcal{Y}}^2}{(k_1 - 1)^2} \\cdot n_c^2$.\n2647: 3.  **Combine Bounds:** Summing the two bounds gives the final inequality. This theorem recasts that result by explicitly naming the coefficients for each displacement component, formalizing the bound for use in subsequent proofs.\n2648: **Q.E.D.**"
    },
    {
      "directive_type": "theorem",
      "label": "thm-expected-raw-distance-k1",
      "title": "Deterministic Behavior of the Expected Raw Distance Vector at $k=1$",
      "start_line": 2651,
      "end_line": 2666,
      "header_lines": [
        2652
      ],
      "content_start": 2653,
      "content_end": 2665,
      "content": "2653: :label: thm-expected-raw-distance-k1\n2654: Let $\\mathcal{S}$ be a swarm state with exactly one alive walker, $|\\mathcal{A}(\\mathcal{S})| = 1$. The N-dimensional vector of expected raw distances, $\\mathbb{E}[\\mathbf{d}(\\mathcal{S})]$, is deterministically the zero vector.\n2655: $$\n2656: \n2657: |\\mathcal{A}(\\mathcal{S})| = 1 \\implies \\mathbb{E}[\\mathbf{d}(\\mathcal{S})] = \\mathbf{0}\n2658: \n2659: $$\n2660: **Implication for Continuity:**\n2661: Any transition between a state $\\mathcal{S}_1$ with $|\\mathcal{A}(\\mathcal{S}_1)| \\ge 2$ and a state $\\mathcal{S}_2$ with $|\\mathcal{A}(\\mathcal{S}_2)| = 1$ induces a discontinuous change in the expected raw distance vector. The magnitude of this change is not governed by the Lipschitz bounds derived for the $k \\geq 2$ regime, but is instead given by the norm of the vector in the $k \\geq 2$ state:\n2662: $$\n2663: \n2664: \\| \\mathbb{E}[\\mathbf{d}(\\mathcal{S}_1)] - \\mathbb{E}[\\mathbf{d}(\\mathcal{S}_2)] \\|_2^2 = \\| \\mathbb{E}[\\mathbf{d}(\\mathcal{S}_1)] \\|_2^2\n2665: ",
      "metadata": {
        "label": "thm-expected-raw-distance-k1"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2651: The continuity analysis in the preceding sections is expressly valid for the $k \\geq 2$ regime. When the number of alive walkers becomes one, the diversity measurement mechanism behaves in a fundamentally different, deterministic manner. The following theorem formally describes this behavior.\n2652: :::{prf:theorem} Deterministic Behavior of the Expected Raw Distance Vector at $k=1$\n2653: :label: thm-expected-raw-distance-k1\n2654: Let $\\mathcal{S}$ be a swarm state with exactly one alive walker, $|\\mathcal{A}(\\mathcal{S})| = 1$. The N-dimensional vector of expected raw distances, $\\mathbb{E}[\\mathbf{d}(\\mathcal{S})]$, is deterministically the zero vector.\n2655: $$\n2656: \n2657: |\\mathcal{A}(\\mathcal{S})| = 1 \\implies \\mathbb{E}[\\mathbf{d}(\\mathcal{S})] = \\mathbf{0}\n2658: \n2659: $$\n2660: **Implication for Continuity:**\n2661: Any transition between a state $\\mathcal{S}_1$ with $|\\mathcal{A}(\\mathcal{S}_1)| \\ge 2$ and a state $\\mathcal{S}_2$ with $|\\mathcal{A}(\\mathcal{S}_2)| = 1$ induces a discontinuous change in the expected raw distance vector. The magnitude of this change is not governed by the Lipschitz bounds derived for the $k \\geq 2$ regime, but is instead given by the norm of the vector in the $k \\geq 2$ state:\n2662: $$\n2663: \n2664: \\| \\mathbb{E}[\\mathbf{d}(\\mathcal{S}_1)] - \\mathbb{E}[\\mathbf{d}(\\mathcal{S}_2)] \\|_2^2 = \\| \\mathbb{E}[\\mathbf{d}(\\mathcal{S}_1)] \\|_2^2\n2665: \n2666: $$"
    },
    {
      "directive_type": "proof",
      "label": "proof-thm-expected-raw-distance-k1",
      "title": null,
      "start_line": 2667,
      "end_line": 2688,
      "header_lines": [
        2668
      ],
      "content_start": 2669,
      "content_end": 2687,
      "content": "2669: :label: proof-thm-expected-raw-distance-k1\n2670: **Proof.**\n2671: The proof follows directly from the definitions of the Raw Value Operator for distance and the Companion Selection Measure for the $k=1$ case. Let $\\mathcal{S}$ be a swarm with $|\\mathcal{A}(\\mathcal{S})| = 1$, and let the single survivor be walker $j$.\n2672: 1.  **Expected Distance for the Survivor (Walker **j**):**\n2673:     *   From the **Companion Selection Measure ([](#def-companion-selection-measure))**, if a walker is the only one alive, it is its own companion. Thus, the companion index is deterministically $c(j) = j$.\n2674:     *   The expected distance for walker $j$ is the expectation over a single outcome:\n2675: $$\n2676: \n2677:         \\mathbb{E}[d_j(\\mathcal{S})] = d_{\\text{alg}}(x_j, x_j) = 0\n2678: \n2679: \n2680: $$\n2681: This holds because $d_{\\text{alg}}$ is a metric, for which the distance from a point to itself is zero.\n2682: 2.  **Expected Distance for Dead Walkers (all **i \u2260 j**):**\n2683:     *   From the definition of the **Raw Value Operator ([](#def-raw-value-operator))**, the raw value for any walker that is not in the alive set is deterministically zero.\n2684:     *   Therefore, for any dead walker $i \\in \\mathcal{D}(\\mathcal{S})$, its expected distance is $\\mathbb{E}[d_i(\\mathcal{S})] = 0$.\n2685: 3.  **Conclusion:**\n2686:     Since the expected distance is zero for the single alive walker and for all dead walkers, every component of the N-dimensional vector $\\mathbb{E}[\\mathbf{d}(\\mathcal{S})]$ is zero. This proves that the vector is deterministically the zero vector when $k=1$.\n2687:     The implication for continuity follows directly. For a transition from $\\mathcal{S}_1$ ($k_1 \\ge 2$) to $\\mathcal{S}_2$ ($k_2=1$), the change is $\\| \\mathbb{E}[\\mathbf{d}(\\mathcal{S}_1)] - \\mathbf{0} \\|_2^2$, which is not described by a continuous function of the displacement between the states but represents a discrete jump. This special case is handled by the revival dynamics of the algorithm rather than the continuity framework.",
      "metadata": {
        "label": "proof-thm-expected-raw-distance-k1"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2667: :::\n2668: :::{prf:proof}\n2669: :label: proof-thm-expected-raw-distance-k1\n2670: **Proof.**\n2671: The proof follows directly from the definitions of the Raw Value Operator for distance and the Companion Selection Measure for the $k=1$ case. Let $\\mathcal{S}$ be a swarm with $|\\mathcal{A}(\\mathcal{S})| = 1$, and let the single survivor be walker $j$.\n2672: 1.  **Expected Distance for the Survivor (Walker **j**):**\n2673:     *   From the **Companion Selection Measure ([](#def-companion-selection-measure))**, if a walker is the only one alive, it is its own companion. Thus, the companion index is deterministically $c(j) = j$.\n2674:     *   The expected distance for walker $j$ is the expectation over a single outcome:\n2675: $$\n2676: \n2677:         \\mathbb{E}[d_j(\\mathcal{S})] = d_{\\text{alg}}(x_j, x_j) = 0\n2678: \n2679: \n2680: $$\n2681: This holds because $d_{\\text{alg}}$ is a metric, for which the distance from a point to itself is zero.\n2682: 2.  **Expected Distance for Dead Walkers (all **i \u2260 j**):**\n2683:     *   From the definition of the **Raw Value Operator ([](#def-raw-value-operator))**, the raw value for any walker that is not in the alive set is deterministically zero.\n2684:     *   Therefore, for any dead walker $i \\in \\mathcal{D}(\\mathcal{S})$, its expected distance is $\\mathbb{E}[d_i(\\mathcal{S})] = 0$.\n2685: 3.  **Conclusion:**\n2686:     Since the expected distance is zero for the single alive walker and for all dead walkers, every component of the N-dimensional vector $\\mathbb{E}[\\mathbf{d}(\\mathcal{S})]$ is zero. This proves that the vector is deterministically the zero vector when $k=1$.\n2687:     The implication for continuity follows directly. For a transition from $\\mathcal{S}_1$ ($k_1 \\ge 2$) to $\\mathcal{S}_2$ ($k_2=1$), the change is $\\| \\mathbb{E}[\\mathbf{d}(\\mathcal{S}_1)] - \\mathbf{0} \\|_2^2$, which is not described by a continuous function of the displacement between the states but represents a discrete jump. This special case is handled by the revival dynamics of the algorithm rather than the continuity framework.\n2688: **Q.E.D.**"
    },
    {
      "directive_type": "theorem",
      "label": "thm-distance-operator-satisfies-bounded-variance-axiom",
      "title": "The Distance Operator Satisfies the Bounded Variance Axiom",
      "start_line": 2693,
      "end_line": 2702,
      "header_lines": [
        2694
      ],
      "content_start": 2695,
      "content_end": 2701,
      "content": "2695: :label: thm-distance-operator-satisfies-bounded-variance-axiom\n2696: The **Distance-to-Companion Measurement** operator ($V=d$) satisfies the **Axiom of Bounded Measurement Variance ([](#axiom-bounded-measurement-variance))**. Its maximum measurement variance is deterministically bounded by:\n2697: $$\n2698: \n2699: \\kappa^2_{\\text{variance}} = N \\cdot D_{\\mathcal{Y}}^2\n2700: \n2701: $$",
      "metadata": {
        "label": "thm-distance-operator-satisfies-bounded-variance-axiom"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2693: First, we must prove that our specific stochastic measurement process\u2014the distance-to-companion operator\u2014is compliant with the foundational axiom for bounded variance.\n2694: :::{prf:theorem} The Distance Operator Satisfies the Bounded Variance Axiom\n2695: :label: thm-distance-operator-satisfies-bounded-variance-axiom\n2696: The **Distance-to-Companion Measurement** operator ($V=d$) satisfies the **Axiom of Bounded Measurement Variance ([](#axiom-bounded-measurement-variance))**. Its maximum measurement variance is deterministically bounded by:\n2697: $$\n2698: \n2699: \\kappa^2_{\\text{variance}} = N \\cdot D_{\\mathcal{Y}}^2\n2700: \n2701: $$\n2702: where $N$ is the total number of walkers and $D_{\\mathcal{Y}}$ is the diameter of the algorithmic space."
    },
    {
      "directive_type": "proof",
      "label": "proof-thm-distance-operator-satisfies-bounded-variance-axiom",
      "title": null,
      "start_line": 2703,
      "end_line": 2729,
      "header_lines": [
        2704
      ],
      "content_start": 2705,
      "content_end": 2728,
      "content": "2705: :label: proof-thm-distance-operator-satisfies-bounded-variance-axiom\n2706: **Proof.**\n2707: The proof proceeds by bounding the variance of each component of the N-dimensional raw distance vector.\n2708: 1.  **Decomposition of Total Variance:**\n2709:     The axiom requires a bound on $\\mathbb{E}[\\|\\mathbf{d} - \\mathbb{E}[\\mathbf{d}]\\|_2^2]$. By linearity of expectation, this is:\n2710: $$\n2711: \n2712:     \\mathbb{E}\\left[\\sum_{i=1}^N (d_i - \\mathbb{E}[d_i])^2\\right] = \\sum_{i=1}^N \\mathbb{E}[(d_i - \\mathbb{E}[d_i])^2] = \\sum_{i=1}^N \\operatorname{Var}(d_i)\n2713: \n2714: \n2715: $$\n2716: 2.  **Bound the Variance of a Single Component:**\n2717:     We must bound the variance, $\\operatorname{Var}(d_i)$, for each walker $i$.\n2718:     *   **Case 1: Dead Walker.** If walker $i$ is dead, its raw distance is deterministically zero ($d_i=0$). Therefore, its variance is $\\operatorname{Var}(d_i) = 0$.\n2719:     *   **Case 2: Alive Walker.** If walker $i$ is alive, its raw distance $d_i$ is a random variable. By definition, any distance measurement in the algorithmic space is bounded on the interval $[0, D_{\\mathcal{Y}}]$. For any random variable $X$ bounded on an interval, its variance is bounded by $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\le \\mathbb{E}[X^2]$. Since $d_i \\in [0, D_{\\mathcal{Y}}]$, we have $d_i^2 \\in [0, D_{\\mathcal{Y}}^2]$. The expectation is therefore bounded by $\\mathbb{E}[d_i^2] \\le D_{\\mathcal{Y}}^2$. Thus, for any alive walker, $\\operatorname{Var}(d_i) \\le D_{\\mathcal{Y}}^2$.\n2720: 3.  **Sum Over All Walkers:**\n2721:     The total variance is the sum of the individual variances. Since each of the $N$ terms is bounded above by $D_{\\mathcal{Y}}^2$, the sum is bounded by:\n2722: $$\n2723: \n2724:     \\sum_{i=1}^N \\operatorname{Var}(d_i) \\le N \\cdot D_{\\mathcal{Y}}^2\n2725: \n2726: \n2727: $$\n2728: This provides a uniform bound that holds for any swarm state $\\mathcal{S}$. The axiom is therefore satisfied with $\\kappa^2_{\\text{variance}} = N \\cdot D_{\\mathcal{Y}}^2$.",
      "metadata": {
        "label": "proof-thm-distance-operator-satisfies-bounded-variance-axiom"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2703: :::\n2704: :::{prf:proof}\n2705: :label: proof-thm-distance-operator-satisfies-bounded-variance-axiom\n2706: **Proof.**\n2707: The proof proceeds by bounding the variance of each component of the N-dimensional raw distance vector.\n2708: 1.  **Decomposition of Total Variance:**\n2709:     The axiom requires a bound on $\\mathbb{E}[\\|\\mathbf{d} - \\mathbb{E}[\\mathbf{d}]\\|_2^2]$. By linearity of expectation, this is:\n2710: $$\n2711: \n2712:     \\mathbb{E}\\left[\\sum_{i=1}^N (d_i - \\mathbb{E}[d_i])^2\\right] = \\sum_{i=1}^N \\mathbb{E}[(d_i - \\mathbb{E}[d_i])^2] = \\sum_{i=1}^N \\operatorname{Var}(d_i)\n2713: \n2714: \n2715: $$\n2716: 2.  **Bound the Variance of a Single Component:**\n2717:     We must bound the variance, $\\operatorname{Var}(d_i)$, for each walker $i$.\n2718:     *   **Case 1: Dead Walker.** If walker $i$ is dead, its raw distance is deterministically zero ($d_i=0$). Therefore, its variance is $\\operatorname{Var}(d_i) = 0$.\n2719:     *   **Case 2: Alive Walker.** If walker $i$ is alive, its raw distance $d_i$ is a random variable. By definition, any distance measurement in the algorithmic space is bounded on the interval $[0, D_{\\mathcal{Y}}]$. For any random variable $X$ bounded on an interval, its variance is bounded by $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\le \\mathbb{E}[X^2]$. Since $d_i \\in [0, D_{\\mathcal{Y}}]$, we have $d_i^2 \\in [0, D_{\\mathcal{Y}}^2]$. The expectation is therefore bounded by $\\mathbb{E}[d_i^2] \\le D_{\\mathcal{Y}}^2$. Thus, for any alive walker, $\\operatorname{Var}(d_i) \\le D_{\\mathcal{Y}}^2$.\n2720: 3.  **Sum Over All Walkers:**\n2721:     The total variance is the sum of the individual variances. Since each of the $N$ terms is bounded above by $D_{\\mathcal{Y}}^2$, the sum is bounded by:\n2722: $$\n2723: \n2724:     \\sum_{i=1}^N \\operatorname{Var}(d_i) \\le N \\cdot D_{\\mathcal{Y}}^2\n2725: \n2726: \n2727: $$\n2728: This provides a uniform bound that holds for any swarm state $\\mathcal{S}$. The axiom is therefore satisfied with $\\kappa^2_{\\text{variance}} = N \\cdot D_{\\mathcal{Y}}^2$.\n2729: **Q.E.D.**"
    },
    {
      "directive_type": "theorem",
      "label": "thm-distance-operator-mean-square-continuity",
      "title": "Mean-Square Continuity of the Distance Operator",
      "start_line": 2732,
      "end_line": 2750,
      "header_lines": [
        2733
      ],
      "content_start": 2734,
      "content_end": 2749,
      "content": "2734: :label: thm-distance-operator-mean-square-continuity\n2735: The **Distance-to-Companion Measurement** operator ($V=d$) is mean-square continuous for transitions in the $k \\geq 2$ regime. For any two swarm states $\\mathcal{S}_1$ and $\\mathcal{S}_2$ with $|\\mathcal{A}(\\mathcal{S}_1)|=k_1 \\ge 2$, the expected squared Euclidean distance between the sampled raw distance vectors is deterministically bounded by the function $F_{d,ms}$:\n2736: $$\n2737: \n2738: \\mathbb{E}[\\|\\mathbf{d}(\\mathcal{S}_1) - \\mathbf{d}(\\mathcal{S}_2)\\|_2^2] \\le F_{d,ms}(\\mathcal{S}_1, \\mathcal{S}_2)\n2739: \n2740: $$\n2741: where the **Expected Squared Distance Error Bound** is defined as:\n2742: $$\n2743: \n2744: \\boxed{\n2745: F_{d,ms}(\\mathcal{S}_1, \\mathcal{S}_2) := 6 N D_{\\mathcal{Y}}^2 + 3 \\left( C_{\\text{pos},d} \\cdot \\Delta_{\\text{pos}}^2 + C_{\\text{status},d}^{(1)} \\cdot n_c + C_{\\text{status},d}^{(2)}(k_1) \\cdot n_c^2 \\right)\n2746: }\n2747: \n2748: $$\n2749: and the coefficients $C_{\\dots,d}$ are the deterministic **Expected Distance Error Coefficients** from [](#thm-distance-operator-mean-square-continuity).",
      "metadata": {
        "label": "thm-distance-operator-mean-square-continuity"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2732: With the variance of the distance operator now axiomatically bounded, we can establish its mean-square continuity. This theorem provides the deterministic function that bounds the expected squared error, which is the critical input for the rest of the stability analysis.\n2733: :::{prf:theorem} Mean-Square Continuity of the Distance Operator\n2734: :label: thm-distance-operator-mean-square-continuity\n2735: The **Distance-to-Companion Measurement** operator ($V=d$) is mean-square continuous for transitions in the $k \\geq 2$ regime. For any two swarm states $\\mathcal{S}_1$ and $\\mathcal{S}_2$ with $|\\mathcal{A}(\\mathcal{S}_1)|=k_1 \\ge 2$, the expected squared Euclidean distance between the sampled raw distance vectors is deterministically bounded by the function $F_{d,ms}$:\n2736: $$\n2737: \n2738: \\mathbb{E}[\\|\\mathbf{d}(\\mathcal{S}_1) - \\mathbf{d}(\\mathcal{S}_2)\\|_2^2] \\le F_{d,ms}(\\mathcal{S}_1, \\mathcal{S}_2)\n2739: \n2740: $$\n2741: where the **Expected Squared Distance Error Bound** is defined as:\n2742: $$\n2743: \n2744: \\boxed{\n2745: F_{d,ms}(\\mathcal{S}_1, \\mathcal{S}_2) := 6 N D_{\\mathcal{Y}}^2 + 3 \\left( C_{\\text{pos},d} \\cdot \\Delta_{\\text{pos}}^2 + C_{\\text{status},d}^{(1)} \\cdot n_c + C_{\\text{status},d}^{(2)}(k_1) \\cdot n_c^2 \\right)\n2746: }\n2747: \n2748: $$\n2749: and the coefficients $C_{\\dots,d}$ are the deterministic **Expected Distance Error Coefficients** from [](#thm-distance-operator-mean-square-continuity).\n2750: With the explicit derivation of this function, we have formally proven that the Distance-to-Companion operator is a valid raw value operator that satisfies the **Axiom of Mean-Square Continuity for Raw Values**. This function will now be used as a direct input to the continuity analysis of the standardization operator."
    },
    {
      "directive_type": "proof",
      "label": "proof-thm-distance-operator-mean-square-continuity",
      "title": null,
      "start_line": 2751,
      "end_line": 2795,
      "header_lines": [
        2752
      ],
      "content_start": 2753,
      "content_end": 2794,
      "content": "2753: :label: proof-thm-distance-operator-mean-square-continuity\n2754: **Proof.**\n2755: The proof bounds the total expected squared error by decomposing it into a stochastic variance component and a deterministic mean component. Let $\\mathbf{d}_1 = \\mathbf{d}(\\mathcal{S}_1)$ and $\\mathbf{d}_2 = \\mathbf{d}(\\mathcal{S}_2)$.\n2756: 1.  **Decomposition of Total Error:**\n2757:     We introduce the expectation vectors $\\mathbb{E}[\\mathbf{d}_1]$ and $\\mathbb{E}[\\mathbf{d}_2]$ and use the inequality $\\|A+B+C\\|_2^2 \\le 3(\\|A\\|_2^2 + \\|B\\|_2^2 + \\|C\\|_2^2)$.\n2758: $$\n2759: \n2760:     \\|\\mathbf{d}_1 - \\mathbf{d}_2\\|_2^2 = \\|(\\mathbf{d}_1 - \\mathbb{E}[\\mathbf{d}_1]) + (\\mathbb{E}[\\mathbf{d}_1] - \\mathbb{E}[\\mathbf{d}_2]) - (\\mathbf{d}_2 - \\mathbb{E}[\\mathbf{d}_2])\\|_2^2\n2761: \n2762: \n2763: $$\n2764: $$\n2765:     \\le 3\\|\\mathbf{d}_1 - \\mathbb{E}[\\mathbf{d}_1]\\|_2^2 + 3\\|\\mathbb{E}[\\mathbf{d}_1] - \\mathbb{E}[\\mathbf{d}_2]\\|_2^2 + 3\\|\\mathbf{d}_2 - \\mathbb{E}[\\mathbf{d}_2]\\|_2^2\n2766:     $$\n2767: 2.  **Take the Expectation:**\n2768:     We take the expectation of both sides. By linearity of expectation, this gives:\n2769: $$\n2770: \n2771:     \\mathbb{E}[\\|\\mathbf{d}_1 - \\mathbf{d}_2\\|_2^2] \\le 3\\mathbb{E}[\\|\\mathbf{d}_1 - \\mathbb{E}[\\mathbf{d}_1]\\|_2^2] + 3\\mathbb{E}[\\|\\mathbb{E}[\\mathbf{d}_1] - \\mathbb{E}[\\mathbf{d}_2]\\|_2^2] + 3\\mathbb{E}[\\|\\mathbf{d}_2 - \\mathbb{E}[\\mathbf{d}_2]\\|_2^2]\n2772: \n2773: \n2774: $$\n2775: 3.  **Bound the Components:**\n2776:     *   **Stochastic Variance Terms:** The first and third terms are bounded by the **Axiom of Bounded Measurement Variance**, which we have shown is satisfied by the distance operator in [](#thm-distance-operator-satisfies-bounded-variance-axiom) with $\\kappa^2_{\\text{variance}} = N D_{\\mathcal{Y}}^2$. Therefore:\n2777:         *   $\\mathbb{E}[\\|\\mathbf{d}_1 - \\mathbb{E}[\\mathbf{d}_1]\\|_2^2] \\le N D_{\\mathcal{Y}}^2$\n2778:         *   $\\mathbb{E}[\\|\\mathbf{d}_2 - \\mathbb{E}[\\mathbf{d}_2]\\|_2^2] \\le N D_{\\mathcal{Y}}^2$\n2779:     *   **Deterministic Mean Term:** The middle term involves the squared norm of a deterministic vector difference, so the expectation has no effect. This term is bounded by the analysis in Section 10.3. From [](#thm-distance-operator-mean-square-continuity), we have:\n2780: $$\n2781: \n2782:         \\|\\mathbb{E}[\\mathbf{d}_1] - \\mathbb{E}[\\mathbf{d}_2]\\|_2^2 \\le C_{\\text{pos},d} \\cdot \\Delta_{\\text{pos}}^2 + C_{\\text{status},d}^{(1)} \\cdot n_c + C_{\\text{status},d}^{(2)}(k_1) \\cdot n_c^2\n2783: \n2784: \n2785: $$\n2786: 4.  **Combine the Bounds:**\n2787:     Substituting the bounds from Step 3 into the inequality from Step 2 yields the final result:\n2788: $$\n2789: \n2790:     \\mathbb{E}[\\|\\mathbf{d}_1 - \\mathbf{d}_2\\|_2^2] \\le 3(N D_{\\mathcal{Y}}^2) + 3(\\text{Bound from Thm 10.3.2}) + 3(N D_{\\mathcal{Y}}^2)\n2791: \n2792: \n2793: $$\n2794: This simplifies to the expression for $F_{d,ms}(\\mathcal{S}_1, \\mathcal{S}_2)$ as stated in the theorem.",
      "metadata": {
        "label": "proof-thm-distance-operator-mean-square-continuity"
      },
      "section": "## 11. Distance-to-Companion Measurement",
      "references": [],
      "raw_directive": "2751: :::\n2752: :::{prf:proof}\n2753: :label: proof-thm-distance-operator-mean-square-continuity\n2754: **Proof.**\n2755: The proof bounds the total expected squared error by decomposing it into a stochastic variance component and a deterministic mean component. Let $\\mathbf{d}_1 = \\mathbf{d}(\\mathcal{S}_1)$ and $\\mathbf{d}_2 = \\mathbf{d}(\\mathcal{S}_2)$.\n2756: 1.  **Decomposition of Total Error:**\n2757:     We introduce the expectation vectors $\\mathbb{E}[\\mathbf{d}_1]$ and $\\mathbb{E}[\\mathbf{d}_2]$ and use the inequality $\\|A+B+C\\|_2^2 \\le 3(\\|A\\|_2^2 + \\|B\\|_2^2 + \\|C\\|_2^2)$.\n2758: $$\n2759: \n2760:     \\|\\mathbf{d}_1 - \\mathbf{d}_2\\|_2^2 = \\|(\\mathbf{d}_1 - \\mathbb{E}[\\mathbf{d}_1]) + (\\mathbb{E}[\\mathbf{d}_1] - \\mathbb{E}[\\mathbf{d}_2]) - (\\mathbf{d}_2 - \\mathbb{E}[\\mathbf{d}_2])\\|_2^2\n2761: \n2762: \n2763: $$\n2764: $$\n2765:     \\le 3\\|\\mathbf{d}_1 - \\mathbb{E}[\\mathbf{d}_1]\\|_2^2 + 3\\|\\mathbb{E}[\\mathbf{d}_1] - \\mathbb{E}[\\mathbf{d}_2]\\|_2^2 + 3\\|\\mathbf{d}_2 - \\mathbb{E}[\\mathbf{d}_2]\\|_2^2\n2766:     $$\n2767: 2.  **Take the Expectation:**\n2768:     We take the expectation of both sides. By linearity of expectation, this gives:\n2769: $$\n2770: \n2771:     \\mathbb{E}[\\|\\mathbf{d}_1 - \\mathbf{d}_2\\|_2^2] \\le 3\\mathbb{E}[\\|\\mathbf{d}_1 - \\mathbb{E}[\\mathbf{d}_1]\\|_2^2] + 3\\mathbb{E}[\\|\\mathbb{E}[\\mathbf{d}_1] - \\mathbb{E}[\\mathbf{d}_2]\\|_2^2] + 3\\mathbb{E}[\\|\\mathbf{d}_2 - \\mathbb{E}[\\mathbf{d}_2]\\|_2^2]\n2772: \n2773: \n2774: $$\n2775: 3.  **Bound the Components:**\n2776:     *   **Stochastic Variance Terms:** The first and third terms are bounded by the **Axiom of Bounded Measurement Variance**, which we have shown is satisfied by the distance operator in [](#thm-distance-operator-satisfies-bounded-variance-axiom) with $\\kappa^2_{\\text{variance}} = N D_{\\mathcal{Y}}^2$. Therefore:\n2777:         *   $\\mathbb{E}[\\|\\mathbf{d}_1 - \\mathbb{E}[\\mathbf{d}_1]\\|_2^2] \\le N D_{\\mathcal{Y}}^2$\n2778:         *   $\\mathbb{E}[\\|\\mathbf{d}_2 - \\mathbb{E}[\\mathbf{d}_2]\\|_2^2] \\le N D_{\\mathcal{Y}}^2$\n2779:     *   **Deterministic Mean Term:** The middle term involves the squared norm of a deterministic vector difference, so the expectation has no effect. This term is bounded by the analysis in Section 10.3. From [](#thm-distance-operator-mean-square-continuity), we have:\n2780: $$\n2781: \n2782:         \\|\\mathbb{E}[\\mathbf{d}_1] - \\mathbb{E}[\\mathbf{d}_2]\\|_2^2 \\le C_{\\text{pos},d} \\cdot \\Delta_{\\text{pos}}^2 + C_{\\text{status},d}^{(1)} \\cdot n_c + C_{\\text{status},d}^{(2)}(k_1) \\cdot n_c^2\n2783: \n2784: \n2785: $$\n2786: 4.  **Combine the Bounds:**\n2787:     Substituting the bounds from Step 3 into the inequality from Step 2 yields the final result:\n2788: $$\n2789: \n2790:     \\mathbb{E}[\\|\\mathbf{d}_1 - \\mathbf{d}_2\\|_2^2] \\le 3(N D_{\\mathcal{Y}}^2) + 3(\\text{Bound from Thm 10.3.2}) + 3(N D_{\\mathcal{Y}}^2)\n2791: \n2792: \n2793: $$\n2794: This simplifies to the expression for $F_{d,ms}(\\mathcal{S}_1, \\mathcal{S}_2)$ as stated in the theorem.\n2795: **Q.E.D.**"
    }
  ],
  "validation": {
    "ok": true,
    "errors": []
  }
}