{
  "chapter_index": 11,
  "section_id": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
  "directive_count": 13,
  "hints": [
    {
      "directive_type": "definition",
      "label": "def-fitness-algorithmic",
      "title": "Fitness Potential Construction (Algorithmic Specification)",
      "start_line": 2757,
      "end_line": 2812,
      "header_lines": [
        2758
      ],
      "content_start": 2760,
      "content_end": 2811,
      "content": "2760: :label: def-fitness-algorithmic\n2761: \n2762: For a swarm state $S = \\{(x_i, v_i, s_i)\\}_{i=1}^N$ with alive walkers $A_k = \\{i : s_i = \\text{alive}\\}$, the fitness potential at position $x \\in \\mathcal{X}$ is constructed through the following pipeline:\n2763: \n2764: **Step 1: Measurement Function.** Given a measurement function $d: \\mathcal{X} \\to \\mathbb{R}$ (e.g., reward, diversity score), evaluate:\n2765: \n2766: $$\n2767: d_i = d(x_i) \\quad \\text{for all } i \\in A_k\n2768: $$\n2769: \n2770: **Step 2: Localization Weights.** For localization scale $\\rho > 0$ and localization kernel $K_\\rho(x, x') = \\frac{1}{Z_K(x, \\rho)} \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\rho^2}\\right)$, compute normalized weights:\n2771: \n2772: $$\n2773: w_{ij}(\\rho) = \\frac{K_\\rho(x, x_j)}{\\sum_{\\ell \\in A_k} K_\\rho(x, x_\\ell)}\n2774: $$\n2775: \n2776: where the normalization ensures $\\sum_{j \\in A_k} w_{ij}(\\rho) = 1$.\n2777: \n2778: **Step 3: Localized Moments.** Compute the \u03c1-localized mean and variance at position $x$:\n2779: \n2780: $$\n2781: \\mu_\\rho[f_k, d, x] = \\sum_{j \\in A_k} w_{ij}(\\rho) \\, d_j\n2782: $$\n2783: \n2784: $$\n2785: \\sigma^2_\\rho[f_k, d, x] = \\sum_{j \\in A_k} w_{ij}(\\rho) \\, (d_j - \\mu_\\rho[f_k, d, x])^2\n2786: $$\n2787: \n2788: **Step 4: Regularized Standard Deviation.** Apply numerical regularization using a C\u00b9-smooth patching function:\n2789: \n2790: $$\n2791: \\sigma'_\\rho[f_k, d, x] = \\sigma\\'_{\\text{reg}}\\left(\\sqrt{\\sigma^2_\\rho[f_k, d, x]}\\right)\n2792: $$\n2793: \n2794: where $\\sigma\\'_{\\text{reg}}: [0, \\infty) \\to [\\kappa_{\\text{var,min}}, \\infty)$ is a C\u00b9-smooth function (see Definition {prf:ref}`def-unified-z-score` in `11_geometric_gas.md`) that:\n2795: - Equals $\\kappa_{\\text{var,min}}$ for $\\sigma \\le \\kappa_{\\text{var,min}} - \\delta$\n2796: - Smoothly transitions through a polynomial patch in $[\\kappa_{\\text{var,min}} - \\delta, \\kappa_{\\text{var,min}} + \\delta]$\n2797: - Equals the identity $\\sigma$ for $\\sigma \\ge \\kappa_{\\text{var,min}} + \\delta$\n2798: \n2799: This ensures $V_{\\text{fit}}$ is C\u00b2 everywhere, as required for the Hessian to be well-defined.\n2800: \n2801: **Step 5: Localized Z-Score.** Compute the standardized measurement:\n2802: \n2803: $$\n2804: Z_\\rho[f_k, d, x] = \\frac{d(x) - \\mu_\\rho[f_k, d, x]}{\\sigma'_\\rho[f_k, d, x]}\n2805: $$\n2806: \n2807: **Step 6: Rescale to Bounded Potential.** Apply a smooth, monotone, bounded rescale function $g_A: \\mathbb{R} \\to [0, A]$ (e.g., $g_A(z) = \\frac{A}{1 + e^{-z}}$):\n2808: \n2809: $$\n2810: V_{\\text{fit}}[f_k, \\rho](x) = g_A\\left(Z_\\rho[f_k, d, x]\\right)\n2811: $$",
      "metadata": {
        "label": "def-fitness-algorithmic"
      },
      "section": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "2757: We begin with the algorithmic construction of the fitness potential.\n2758: \n2759: :::{prf:definition} Fitness Potential Construction (Algorithmic Specification)\n2760: :label: def-fitness-algorithmic\n2761: \n2762: For a swarm state $S = \\{(x_i, v_i, s_i)\\}_{i=1}^N$ with alive walkers $A_k = \\{i : s_i = \\text{alive}\\}$, the fitness potential at position $x \\in \\mathcal{X}$ is constructed through the following pipeline:\n2763: \n2764: **Step 1: Measurement Function.** Given a measurement function $d: \\mathcal{X} \\to \\mathbb{R}$ (e.g., reward, diversity score), evaluate:\n2765: \n2766: $$\n2767: d_i = d(x_i) \\quad \\text{for all } i \\in A_k\n2768: $$\n2769: \n2770: **Step 2: Localization Weights.** For localization scale $\\rho > 0$ and localization kernel $K_\\rho(x, x') = \\frac{1}{Z_K(x, \\rho)} \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\rho^2}\\right)$, compute normalized weights:\n2771: \n2772: $$\n2773: w_{ij}(\\rho) = \\frac{K_\\rho(x, x_j)}{\\sum_{\\ell \\in A_k} K_\\rho(x, x_\\ell)}\n2774: $$\n2775: \n2776: where the normalization ensures $\\sum_{j \\in A_k} w_{ij}(\\rho) = 1$.\n2777: \n2778: **Step 3: Localized Moments.** Compute the \u03c1-localized mean and variance at position $x$:\n2779: \n2780: $$\n2781: \\mu_\\rho[f_k, d, x] = \\sum_{j \\in A_k} w_{ij}(\\rho) \\, d_j\n2782: $$\n2783: \n2784: $$\n2785: \\sigma^2_\\rho[f_k, d, x] = \\sum_{j \\in A_k} w_{ij}(\\rho) \\, (d_j - \\mu_\\rho[f_k, d, x])^2\n2786: $$\n2787: \n2788: **Step 4: Regularized Standard Deviation.** Apply numerical regularization using a C\u00b9-smooth patching function:\n2789: \n2790: $$\n2791: \\sigma'_\\rho[f_k, d, x] = \\sigma\\'_{\\text{reg}}\\left(\\sqrt{\\sigma^2_\\rho[f_k, d, x]}\\right)\n2792: $$\n2793: \n2794: where $\\sigma\\'_{\\text{reg}}: [0, \\infty) \\to [\\kappa_{\\text{var,min}}, \\infty)$ is a C\u00b9-smooth function (see Definition {prf:ref}`def-unified-z-score` in `11_geometric_gas.md`) that:\n2795: - Equals $\\kappa_{\\text{var,min}}$ for $\\sigma \\le \\kappa_{\\text{var,min}} - \\delta$\n2796: - Smoothly transitions through a polynomial patch in $[\\kappa_{\\text{var,min}} - \\delta, \\kappa_{\\text{var,min}} + \\delta]$\n2797: - Equals the identity $\\sigma$ for $\\sigma \\ge \\kappa_{\\text{var,min}} + \\delta$\n2798: \n2799: This ensures $V_{\\text{fit}}$ is C\u00b2 everywhere, as required for the Hessian to be well-defined.\n2800: \n2801: **Step 5: Localized Z-Score.** Compute the standardized measurement:\n2802: \n2803: $$\n2804: Z_\\rho[f_k, d, x] = \\frac{d(x) - \\mu_\\rho[f_k, d, x]}{\\sigma'_\\rho[f_k, d, x]}\n2805: $$\n2806: \n2807: **Step 6: Rescale to Bounded Potential.** Apply a smooth, monotone, bounded rescale function $g_A: \\mathbb{R} \\to [0, A]$ (e.g., $g_A(z) = \\frac{A}{1 + e^{-z}}$):\n2808: \n2809: $$\n2810: V_{\\text{fit}}[f_k, \\rho](x) = g_A\\left(Z_\\rho[f_k, d, x]\\right)\n2811: $$\n2812: "
    },
    {
      "directive_type": "remark",
      "label": "unlabeled-remark-75",
      "title": "Algorithmic Meaning of the Fitness Potential",
      "start_line": 2814,
      "end_line": 2822,
      "header_lines": [
        2815
      ],
      "content_start": 2817,
      "content_end": 2821,
      "content": "2817: :class: note\n2818: \n2819: The fitness potential $V_{\\text{fit}}[f_k, \\rho](x)$ encodes **relative performance in a local neighborhood**:\n2820: - High values indicate positions where the measurement $d(x)$ is **above the local mean** (good regions)\n2821: - Low values indicate positions **below the local mean** (poor regions)",
      "metadata": {
        "class": "note"
      },
      "section": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "2814: :::\n2815: \n2816: :::{prf:remark} Algorithmic Meaning of the Fitness Potential\n2817: :class: note\n2818: \n2819: The fitness potential $V_{\\text{fit}}[f_k, \\rho](x)$ encodes **relative performance in a local neighborhood**:\n2820: - High values indicate positions where the measurement $d(x)$ is **above the local mean** (good regions)\n2821: - Low values indicate positions **below the local mean** (poor regions)\n2822: - The Z-score normalization makes this **scale-invariant**: fitness depends on relative position, not absolute measurement values"
    },
    {
      "directive_type": "theorem",
      "label": "thm-explicit-hessian",
      "title": "Explicit Hessian Formula",
      "start_line": 2828,
      "end_line": 2849,
      "header_lines": [
        2829
      ],
      "content_start": 2831,
      "content_end": 2848,
      "content": "2831: :label: thm-explicit-hessian\n2832: \n2833: The Hessian of the fitness potential with respect to position $x \\in \\mathcal{X}$ is:\n2834: \n2835: $$\n2836: H(x, S) = \\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x) = g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z + g'_A(Z) \\, \\nabla^2_x Z\n2837: $$\n2838: \n2839: where:\n2840: - $Z = Z_\\rho[f_k, d, x]$ is the localized Z-score\n2841: - $g'_A(Z)$ and $g''_A(Z)$ are the first and second derivatives of the rescale function\n2842: - $\\nabla_x Z$ and $\\nabla^2_x Z$ are the gradient and Hessian of the Z-score with respect to $x$\n2843: \n2844: **Expanded Form:**\n2845: \n2846: $$\n2847: H(x, S) = \\frac{g''_A(Z)}{\\sigma'^2_\\rho} \\nabla_x d \\otimes \\nabla_x d + \\frac{g'_A(Z)}{\\sigma'_\\rho} \\nabla^2_x d + \\text{(moment correction terms)}\n2848: $$",
      "metadata": {
        "label": "thm-explicit-hessian"
      },
      "section": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "2828: The metric emerges from the **curvature** of the fitness landscape, encoded in the Hessian. We now derive the Hessian explicitly using the chain rule.\n2829: \n2830: :::{prf:theorem} Explicit Hessian Formula\n2831: :label: thm-explicit-hessian\n2832: \n2833: The Hessian of the fitness potential with respect to position $x \\in \\mathcal{X}$ is:\n2834: \n2835: $$\n2836: H(x, S) = \\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x) = g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z + g'_A(Z) \\, \\nabla^2_x Z\n2837: $$\n2838: \n2839: where:\n2840: - $Z = Z_\\rho[f_k, d, x]$ is the localized Z-score\n2841: - $g'_A(Z)$ and $g''_A(Z)$ are the first and second derivatives of the rescale function\n2842: - $\\nabla_x Z$ and $\\nabla^2_x Z$ are the gradient and Hessian of the Z-score with respect to $x$\n2843: \n2844: **Expanded Form:**\n2845: \n2846: $$\n2847: H(x, S) = \\frac{g''_A(Z)}{\\sigma'^2_\\rho} \\nabla_x d \\otimes \\nabla_x d + \\frac{g'_A(Z)}{\\sigma'_\\rho} \\nabla^2_x d + \\text{(moment correction terms)}\n2848: $$\n2849: "
    },
    {
      "directive_type": "proof",
      "label": "unlabeled-proof-112",
      "title": null,
      "start_line": 2851,
      "end_line": 2977,
      "header_lines": [],
      "content_start": 2852,
      "content_end": 2976,
      "content": "2852: \n2853: :::{prf:proof}\n2854: We compute the Hessian by applying the chain rule twice. We first establish a technical lemma for the gradient of the localization weights.\n2855: \n2856: **Lemma: Gradient of Normalized Localization Weights.**\n2857: \n2858: For the normalized Gaussian weights:\n2859: \n2860: $$\n2861: w_{ij}(\\rho) = \\frac{K_\\rho(x, x_j)}{\\sum_{\\ell \\in A_k} K_\\rho(x, x_\\ell)} = \\frac{\\exp\\left(-\\frac{\\|x - x_j\\|^2}{2\\rho^2}\\right)}{\\sum_{\\ell \\in A_k} \\exp\\left(-\\frac{\\|x - x_\\ell\\|^2}{2\\rho^2}\\right)}\n2862: $$\n2863: \n2864: **Gradient derivation:** Using the quotient rule:\n2865: \n2866: $$\n2867: \\nabla_x w_{ij} = \\frac{\\nabla_x K_\\rho(x, x_j) \\cdot \\sum_\\ell K_\\rho(x, x_\\ell) - K_\\rho(x, x_j) \\cdot \\sum_\\ell \\nabla_x K_\\rho(x, x_\\ell)}{\\left(\\sum_\\ell K_\\rho(x, x_\\ell)\\right)^2}\n2868: $$\n2869: \n2870: For the Gaussian kernel, $\\nabla_x K_\\rho(x, x_j) = -\\frac{1}{\\rho^2} K_\\rho(x, x_j) (x - x_j)$. Therefore:\n2871: \n2872: $$\n2873: \\nabla_x w_{ij} = \\frac{-\\frac{1}{\\rho^2} K_\\rho(x, x_j) (x - x_j) \\sum_\\ell K_\\rho(x, x_\\ell) - K_\\rho(x, x_j) \\sum_\\ell \\left(-\\frac{1}{\\rho^2} K_\\rho(x, x_\\ell) (x - x_\\ell)\\right)}{\\left(\\sum_\\ell K_\\rho(x, x_\\ell)\\right)^2}\n2874: $$\n2875: \n2876: Simplifying using $w_{ij} = K_\\rho(x, x_j) / \\sum_\\ell K_\\rho(x, x_\\ell)$:\n2877: \n2878: $$\n2879: \\nabla_x w_{ij} = -\\frac{w_{ij}}{\\rho^2} (x - x_j) + \\frac{w_{ij}}{\\rho^2} \\sum_{\\ell \\in A_k} w_{i\\ell} (x - x_\\ell)\n2880: $$\n2881: \n2882: $$\n2883: = \\frac{1}{\\rho^2} w_{ij} \\left( \\sum_{\\ell \\in A_k} w_{i\\ell} (x - x_\\ell) - (x - x_j) \\right)\n2884: $$\n2885: \n2886: $$\n2887: = \\frac{1}{\\rho^2} \\left( w_{ij} \\sum_{\\ell \\in A_k} w_{i\\ell} (x - x_\\ell) - w_{ij} (x - x_j) \\right)\n2888: $$\n2889: \n2890: which can be rewritten as the formula used in the main proof. $\\square$\n2891: \n2892: **Step 1: First Derivative (Gradient).**\n2893: \n2894: By the chain rule for $V_{\\text{fit}}(x) = g_A(Z_\\rho(x))$:\n2895: \n2896: $$\n2897: \\nabla_x V_{\\text{fit}} = g'_A(Z) \\, \\nabla_x Z_\\rho\n2898: $$\n2899: \n2900: For the Z-score $Z_\\rho = \\frac{d(x) - \\mu_\\rho}{\\sigma'_\\rho}$, we have:\n2901: \n2902: $$\n2903: \\nabla_x Z_\\rho = \\frac{1}{\\sigma'_\\rho} \\left( \\nabla_x d - \\nabla_x \\mu_\\rho \\right) - \\frac{d(x) - \\mu_\\rho}{\\sigma'^2_\\rho} \\nabla_x \\sigma'_\\rho\n2904: $$\n2905: \n2906: **Moment Gradients.** The localized mean depends on $x$ through the weights:\n2907: \n2908: $$\n2909: \\mu_\\rho = \\sum_{j \\in A_k} w_{ij}(\\rho) d_j\n2910: $$\n2911: \n2912: $$\n2913: \\nabla_x \\mu_\\rho = \\sum_{j \\in A_k} (\\nabla_x w_{ij}) d_j\n2914: $$\n2915: \n2916: For the normalized Gaussian weights $w_{ij} = K_\\rho(x, x_j) / \\sum_\\ell K_\\rho(x, x_\\ell)$:\n2917: \n2918: $$\n2919: \\nabla_x w_{ij} = \\frac{1}{\\rho^2} \\left( w_{ij} (x - x_j) - \\sum_{\\ell \\in A_k} w_{i\\ell} w_{ij} (x - x_\\ell) \\right)\n2920: $$\n2921: \n2922: **Critical Telescoping Property:** The normalization constraint $\\sum_j w_{ij} = 1$ implies:\n2923: \n2924: $$\n2925: \\sum_{j \\in A_k} \\nabla_x w_{ij} = 0\n2926: $$\n2927: \n2928: This telescoping property ensures that the gradient of the mean is bounded **independently of $k$** (and thus $N$).\n2929: \n2930: **Step 2: Second Derivative (Hessian).**\n2931: \n2932: Taking another derivative of $\\nabla_x V_{\\text{fit}} = g'_A(Z) \\nabla_x Z$:\n2933: \n2934: $$\n2935: \\nabla^2_x V_{\\text{fit}} = g''_A(Z) \\, (\\nabla_x Z) \\otimes (\\nabla_x Z) + g'_A(Z) \\, \\nabla^2_x Z\n2936: $$\n2937: \n2938: **Term 1 (Outer Product):** The first term is a rank-1 matrix:\n2939: \n2940: $$\n2941: \\left[g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z\\right]_{ab} = g''_A(Z) \\, \\frac{\\partial Z}{\\partial x_a} \\frac{\\partial Z}{\\partial x_b}\n2942: $$\n2943: \n2944: For the Z-score, to leading order (ignoring moment correction terms):\n2945: \n2946: $$\n2947: \\nabla_x Z \\approx \\frac{1}{\\sigma'_\\rho} \\nabla_x d\n2948: $$\n2949: \n2950: Thus:\n2951: \n2952: $$\n2953: g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z \\approx \\frac{g''_A(Z)}{\\sigma'^2_\\rho} \\, \\nabla_x d \\otimes \\nabla_x d\n2954: $$\n2955: \n2956: **Term 2 (Hessian of Z-Score):** The second term involves:\n2957: \n2958: $$\n2959: \\nabla^2_x Z = \\frac{1}{\\sigma'_\\rho} (\\nabla^2_x d - \\nabla^2_x \\mu_\\rho) + \\text{(variance correction terms)}\n2960: $$\n2961: \n2962: The moment Hessian $\\nabla^2_x \\mu_\\rho$ involves second derivatives of the weights $w_{ij}$. By the same telescoping argument, these terms remain bounded.\n2963: \n2964: **Step 3: Explicit Bound.**\n2965: \n2966: Combining both terms and using the bounds:\n2967: - $|g'_A(Z)| \\le g'_{\\max}$, $|g''_A(Z)| \\le g''_{\\max}$ (smoothness of rescale function)\n2968: - $\\|\\nabla_x d\\|_\\infty \\le d'_{\\max}$, $\\|\\nabla^2_x d\\|_\\infty \\le d''_{\\max}$ (regularity of measurement)\n2969: - $\\sigma'_\\rho \\ge \\kappa_{\\text{var,min}}$ (regularization floor)\n2970: - $\\|\\nabla_x \\mu_\\rho\\|, \\|\\nabla^2_x \\mu_\\rho\\| = O(1/\\rho)$ (localization scale dependence)\n2971: \n2972: We obtain the **N-uniform bound**:\n2973: \n2974: $$\n2975: \\|H(x, S)\\| \\le H_{\\max}(\\rho) = \\frac{g''_{\\max} (d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} + \\frac{g'_{\\max} d''_{\\max}}{\\kappa_{\\text{var,min}}} + O(1/\\rho)\n2976: $$",
      "metadata": {},
      "section": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "2851: :::\n2852: \n2853: :::{prf:proof}\n2854: We compute the Hessian by applying the chain rule twice. We first establish a technical lemma for the gradient of the localization weights.\n2855: \n2856: **Lemma: Gradient of Normalized Localization Weights.**\n2857: \n2858: For the normalized Gaussian weights:\n2859: \n2860: $$\n2861: w_{ij}(\\rho) = \\frac{K_\\rho(x, x_j)}{\\sum_{\\ell \\in A_k} K_\\rho(x, x_\\ell)} = \\frac{\\exp\\left(-\\frac{\\|x - x_j\\|^2}{2\\rho^2}\\right)}{\\sum_{\\ell \\in A_k} \\exp\\left(-\\frac{\\|x - x_\\ell\\|^2}{2\\rho^2}\\right)}\n2862: $$\n2863: \n2864: **Gradient derivation:** Using the quotient rule:\n2865: \n2866: $$\n2867: \\nabla_x w_{ij} = \\frac{\\nabla_x K_\\rho(x, x_j) \\cdot \\sum_\\ell K_\\rho(x, x_\\ell) - K_\\rho(x, x_j) \\cdot \\sum_\\ell \\nabla_x K_\\rho(x, x_\\ell)}{\\left(\\sum_\\ell K_\\rho(x, x_\\ell)\\right)^2}\n2868: $$\n2869: \n2870: For the Gaussian kernel, $\\nabla_x K_\\rho(x, x_j) = -\\frac{1}{\\rho^2} K_\\rho(x, x_j) (x - x_j)$. Therefore:\n2871: \n2872: $$\n2873: \\nabla_x w_{ij} = \\frac{-\\frac{1}{\\rho^2} K_\\rho(x, x_j) (x - x_j) \\sum_\\ell K_\\rho(x, x_\\ell) - K_\\rho(x, x_j) \\sum_\\ell \\left(-\\frac{1}{\\rho^2} K_\\rho(x, x_\\ell) (x - x_\\ell)\\right)}{\\left(\\sum_\\ell K_\\rho(x, x_\\ell)\\right)^2}\n2874: $$\n2875: \n2876: Simplifying using $w_{ij} = K_\\rho(x, x_j) / \\sum_\\ell K_\\rho(x, x_\\ell)$:\n2877: \n2878: $$\n2879: \\nabla_x w_{ij} = -\\frac{w_{ij}}{\\rho^2} (x - x_j) + \\frac{w_{ij}}{\\rho^2} \\sum_{\\ell \\in A_k} w_{i\\ell} (x - x_\\ell)\n2880: $$\n2881: \n2882: $$\n2883: = \\frac{1}{\\rho^2} w_{ij} \\left( \\sum_{\\ell \\in A_k} w_{i\\ell} (x - x_\\ell) - (x - x_j) \\right)\n2884: $$\n2885: \n2886: $$\n2887: = \\frac{1}{\\rho^2} \\left( w_{ij} \\sum_{\\ell \\in A_k} w_{i\\ell} (x - x_\\ell) - w_{ij} (x - x_j) \\right)\n2888: $$\n2889: \n2890: which can be rewritten as the formula used in the main proof. $\\square$\n2891: \n2892: **Step 1: First Derivative (Gradient).**\n2893: \n2894: By the chain rule for $V_{\\text{fit}}(x) = g_A(Z_\\rho(x))$:\n2895: \n2896: $$\n2897: \\nabla_x V_{\\text{fit}} = g'_A(Z) \\, \\nabla_x Z_\\rho\n2898: $$\n2899: \n2900: For the Z-score $Z_\\rho = \\frac{d(x) - \\mu_\\rho}{\\sigma'_\\rho}$, we have:\n2901: \n2902: $$\n2903: \\nabla_x Z_\\rho = \\frac{1}{\\sigma'_\\rho} \\left( \\nabla_x d - \\nabla_x \\mu_\\rho \\right) - \\frac{d(x) - \\mu_\\rho}{\\sigma'^2_\\rho} \\nabla_x \\sigma'_\\rho\n2904: $$\n2905: \n2906: **Moment Gradients.** The localized mean depends on $x$ through the weights:\n2907: \n2908: $$\n2909: \\mu_\\rho = \\sum_{j \\in A_k} w_{ij}(\\rho) d_j\n2910: $$\n2911: \n2912: $$\n2913: \\nabla_x \\mu_\\rho = \\sum_{j \\in A_k} (\\nabla_x w_{ij}) d_j\n2914: $$\n2915: \n2916: For the normalized Gaussian weights $w_{ij} = K_\\rho(x, x_j) / \\sum_\\ell K_\\rho(x, x_\\ell)$:\n2917: \n2918: $$\n2919: \\nabla_x w_{ij} = \\frac{1}{\\rho^2} \\left( w_{ij} (x - x_j) - \\sum_{\\ell \\in A_k} w_{i\\ell} w_{ij} (x - x_\\ell) \\right)\n2920: $$\n2921: \n2922: **Critical Telescoping Property:** The normalization constraint $\\sum_j w_{ij} = 1$ implies:\n2923: \n2924: $$\n2925: \\sum_{j \\in A_k} \\nabla_x w_{ij} = 0\n2926: $$\n2927: \n2928: This telescoping property ensures that the gradient of the mean is bounded **independently of $k$** (and thus $N$).\n2929: \n2930: **Step 2: Second Derivative (Hessian).**\n2931: \n2932: Taking another derivative of $\\nabla_x V_{\\text{fit}} = g'_A(Z) \\nabla_x Z$:\n2933: \n2934: $$\n2935: \\nabla^2_x V_{\\text{fit}} = g''_A(Z) \\, (\\nabla_x Z) \\otimes (\\nabla_x Z) + g'_A(Z) \\, \\nabla^2_x Z\n2936: $$\n2937: \n2938: **Term 1 (Outer Product):** The first term is a rank-1 matrix:\n2939: \n2940: $$\n2941: \\left[g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z\\right]_{ab} = g''_A(Z) \\, \\frac{\\partial Z}{\\partial x_a} \\frac{\\partial Z}{\\partial x_b}\n2942: $$\n2943: \n2944: For the Z-score, to leading order (ignoring moment correction terms):\n2945: \n2946: $$\n2947: \\nabla_x Z \\approx \\frac{1}{\\sigma'_\\rho} \\nabla_x d\n2948: $$\n2949: \n2950: Thus:\n2951: \n2952: $$\n2953: g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z \\approx \\frac{g''_A(Z)}{\\sigma'^2_\\rho} \\, \\nabla_x d \\otimes \\nabla_x d\n2954: $$\n2955: \n2956: **Term 2 (Hessian of Z-Score):** The second term involves:\n2957: \n2958: $$\n2959: \\nabla^2_x Z = \\frac{1}{\\sigma'_\\rho} (\\nabla^2_x d - \\nabla^2_x \\mu_\\rho) + \\text{(variance correction terms)}\n2960: $$\n2961: \n2962: The moment Hessian $\\nabla^2_x \\mu_\\rho$ involves second derivatives of the weights $w_{ij}$. By the same telescoping argument, these terms remain bounded.\n2963: \n2964: **Step 3: Explicit Bound.**\n2965: \n2966: Combining both terms and using the bounds:\n2967: - $|g'_A(Z)| \\le g'_{\\max}$, $|g''_A(Z)| \\le g''_{\\max}$ (smoothness of rescale function)\n2968: - $\\|\\nabla_x d\\|_\\infty \\le d'_{\\max}$, $\\|\\nabla^2_x d\\|_\\infty \\le d''_{\\max}$ (regularity of measurement)\n2969: - $\\sigma'_\\rho \\ge \\kappa_{\\text{var,min}}$ (regularization floor)\n2970: - $\\|\\nabla_x \\mu_\\rho\\|, \\|\\nabla^2_x \\mu_\\rho\\| = O(1/\\rho)$ (localization scale dependence)\n2971: \n2972: We obtain the **N-uniform bound**:\n2973: \n2974: $$\n2975: \\|H(x, S)\\| \\le H_{\\max}(\\rho) = \\frac{g''_{\\max} (d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} + \\frac{g'_{\\max} d''_{\\max}}{\\kappa_{\\text{var,min}}} + O(1/\\rho)\n2976: $$\n2977: "
    },
    {
      "directive_type": "remark",
      "label": "unlabeled-remark-240",
      "title": "Geometric Interpretation of the Hessian Terms",
      "start_line": 2979,
      "end_line": 2995,
      "header_lines": [
        2980
      ],
      "content_start": 2982,
      "content_end": 2994,
      "content": "2982: :class: note\n2983: \n2984: The two terms in the Hessian have distinct geometric meanings:\n2985: \n2986: **1. Rank-1 Term (Outer Product):** $g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z$\n2987: - Dominant when the rescale function has high curvature ($g''_A$ large)\n2988: - Aligned with the **gradient direction** of the fitness landscape\n2989: - Creates **anisotropy along level sets**: high curvature perpendicular to level sets\n2990: \n2991: **2. Full Hessian Term:** $g'_A(Z) \\, \\nabla^2_x Z$\n2992: - Captures the **intrinsic curvature** of the Z-score manifold\n2993: - Depends on second derivatives of the measurement function $d(x)$\n2994: - Reflects the **geometry of the problem**, not just the fitness magnitude",
      "metadata": {
        "class": "note"
      },
      "section": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "2979: :::\n2980: \n2981: :::{prf:remark} Geometric Interpretation of the Hessian Terms\n2982: :class: note\n2983: \n2984: The two terms in the Hessian have distinct geometric meanings:\n2985: \n2986: **1. Rank-1 Term (Outer Product):** $g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z$\n2987: - Dominant when the rescale function has high curvature ($g''_A$ large)\n2988: - Aligned with the **gradient direction** of the fitness landscape\n2989: - Creates **anisotropy along level sets**: high curvature perpendicular to level sets\n2990: \n2991: **2. Full Hessian Term:** $g'_A(Z) \\, \\nabla^2_x Z$\n2992: - Captures the **intrinsic curvature** of the Z-score manifold\n2993: - Depends on second derivatives of the measurement function $d(x)$\n2994: - Reflects the **geometry of the problem**, not just the fitness magnitude\n2995: "
    },
    {
      "directive_type": "definition",
      "label": "def-metric-explicit",
      "title": "Emergent Riemannian Metric (Explicit Construction)",
      "start_line": 3001,
      "end_line": 3027,
      "header_lines": [
        3002
      ],
      "content_start": 3004,
      "content_end": 3026,
      "content": "3004: :label: def-metric-explicit\n3005: \n3006: For a walker at position $x$ in swarm state $S$, the **emergent Riemannian metric** is defined as:\n3007: \n3008: $$\n3009: g(x, S) = H(x, S) + \\epsilon_\\Sigma I\n3010: $$\n3011: \n3012: where:\n3013: - $H(x, S) = \\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x)$ is the Hessian from {prf:ref}`thm-explicit-hessian`\n3014: - $\\epsilon_\\Sigma > 0$ is the **regularization parameter**\n3015: - $I$ is the $d \\times d$ identity matrix\n3016: \n3017: The corresponding **diffusion tensor** (inverse of the metric) is:\n3018: \n3019: $$\n3020: D_{\\text{reg}}(x, S) = g(x, S)^{-1} = \\left(H(x, S) + \\epsilon_\\Sigma I\\right)^{-1}\n3021: $$\n3022: \n3023: and the **diffusion coefficient matrix** used in the SDE is:\n3024: \n3025: $$\n3026: \\Sigma_{\\text{reg}}(x, S) = D_{\\text{reg}}(x, S)^{1/2} = \\left(H(x, S) + \\epsilon_\\Sigma I\\right)^{-1/2}",
      "metadata": {
        "label": "def-metric-explicit"
      },
      "section": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "3001: The Hessian alone may not be positive definite (e.g., at saddle points or in flat regions). The regularization ensures well-posedness.\n3002: \n3003: :::{prf:definition} Emergent Riemannian Metric (Explicit Construction)\n3004: :label: def-metric-explicit\n3005: \n3006: For a walker at position $x$ in swarm state $S$, the **emergent Riemannian metric** is defined as:\n3007: \n3008: $$\n3009: g(x, S) = H(x, S) + \\epsilon_\\Sigma I\n3010: $$\n3011: \n3012: where:\n3013: - $H(x, S) = \\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x)$ is the Hessian from {prf:ref}`thm-explicit-hessian`\n3014: - $\\epsilon_\\Sigma > 0$ is the **regularization parameter**\n3015: - $I$ is the $d \\times d$ identity matrix\n3016: \n3017: The corresponding **diffusion tensor** (inverse of the metric) is:\n3018: \n3019: $$\n3020: D_{\\text{reg}}(x, S) = g(x, S)^{-1} = \\left(H(x, S) + \\epsilon_\\Sigma I\\right)^{-1}\n3021: $$\n3022: \n3023: and the **diffusion coefficient matrix** used in the SDE is:\n3024: \n3025: $$\n3026: \\Sigma_{\\text{reg}}(x, S) = D_{\\text{reg}}(x, S)^{1/2} = \\left(H(x, S) + \\epsilon_\\Sigma I\\right)^{-1/2}\n3027: $$"
    },
    {
      "directive_type": "theorem",
      "label": "thm-uniform-ellipticity-explicit",
      "title": "Uniform Ellipticity from Regularization",
      "start_line": 3029,
      "end_line": 3062,
      "header_lines": [
        3030
      ],
      "content_start": 3032,
      "content_end": 3061,
      "content": "3032: :label: thm-uniform-ellipticity-explicit\n3033: \n3034: For any choice of algorithmic parameters $(d, \\rho, \\kappa_{\\text{var,min}}, g_A, A, \\epsilon_\\Sigma)$, the metric $g(x, S)$ is **uniformly elliptic** with explicit bounds:\n3035: \n3036: $$\n3037: c_{\\min}(\\rho) I \\preceq g(x, S) \\preceq c_{\\max} I\n3038: $$\n3039: \n3040: where:\n3041: \n3042: $$\n3043: c_{\\min}(\\rho) = \\epsilon_\\Sigma - \\Lambda_-(\\rho), \\quad c_{\\max} = H_{\\max}(\\rho) + \\epsilon_\\Sigma\n3044: $$\n3045: \n3046: and:\n3047: - $\\Lambda_-(\\rho) \\ge 0$ is the **spectral floor**: the maximum magnitude of negative eigenvalues of $H(x, S)$ over all states\n3048: - $H_{\\max}(\\rho)$ is the **spectral ceiling**: the maximum eigenvalue of $H(x, S)$ over all states\n3049: \n3050: **Sufficient Condition for Positive Definiteness:** If $\\epsilon_\\Sigma > \\Lambda_-(\\rho)$, then $g(x, S) \\succ 0$ for all $x, S$.\n3051: \n3052: **Inverse Bounds (Diffusion Tensor):**\n3053: \n3054: $$\n3055: \\frac{1}{c_{\\max}} I \\preceq D_{\\text{reg}}(x, S) \\preceq \\frac{1}{c_{\\min}(\\rho)} I\n3056: $$\n3057: \n3058: **Square Root Bounds (Diffusion Coefficient Matrix):**\n3059: \n3060: $$\n3061: \\frac{1}{\\sqrt{c_{\\max}}} I \\preceq \\Sigma_{\\text{reg}}(x, S) \\preceq \\frac{1}{\\sqrt{c_{\\min}(\\rho)}} I",
      "metadata": {
        "label": "thm-uniform-ellipticity-explicit"
      },
      "section": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "3029: :::\n3030: \n3031: :::{prf:theorem} Uniform Ellipticity from Regularization\n3032: :label: thm-uniform-ellipticity-explicit\n3033: \n3034: For any choice of algorithmic parameters $(d, \\rho, \\kappa_{\\text{var,min}}, g_A, A, \\epsilon_\\Sigma)$, the metric $g(x, S)$ is **uniformly elliptic** with explicit bounds:\n3035: \n3036: $$\n3037: c_{\\min}(\\rho) I \\preceq g(x, S) \\preceq c_{\\max} I\n3038: $$\n3039: \n3040: where:\n3041: \n3042: $$\n3043: c_{\\min}(\\rho) = \\epsilon_\\Sigma - \\Lambda_-(\\rho), \\quad c_{\\max} = H_{\\max}(\\rho) + \\epsilon_\\Sigma\n3044: $$\n3045: \n3046: and:\n3047: - $\\Lambda_-(\\rho) \\ge 0$ is the **spectral floor**: the maximum magnitude of negative eigenvalues of $H(x, S)$ over all states\n3048: - $H_{\\max}(\\rho)$ is the **spectral ceiling**: the maximum eigenvalue of $H(x, S)$ over all states\n3049: \n3050: **Sufficient Condition for Positive Definiteness:** If $\\epsilon_\\Sigma > \\Lambda_-(\\rho)$, then $g(x, S) \\succ 0$ for all $x, S$.\n3051: \n3052: **Inverse Bounds (Diffusion Tensor):**\n3053: \n3054: $$\n3055: \\frac{1}{c_{\\max}} I \\preceq D_{\\text{reg}}(x, S) \\preceq \\frac{1}{c_{\\min}(\\rho)} I\n3056: $$\n3057: \n3058: **Square Root Bounds (Diffusion Coefficient Matrix):**\n3059: \n3060: $$\n3061: \\frac{1}{\\sqrt{c_{\\max}}} I \\preceq \\Sigma_{\\text{reg}}(x, S) \\preceq \\frac{1}{\\sqrt{c_{\\min}(\\rho)}} I\n3062: $$"
    },
    {
      "directive_type": "proof",
      "label": "unlabeled-proof-325",
      "title": null,
      "start_line": 3064,
      "end_line": 3196,
      "header_lines": [],
      "content_start": 3065,
      "content_end": 3195,
      "content": "3065: \n3066: :::{prf:proof}\n3067: **Step 1: Eigenvalue Bounds for $g(x, S)$.**\n3068: \n3069: Let $\\lambda_1, \\ldots, \\lambda_d$ be the eigenvalues of $H(x, S)$. Then the eigenvalues of $g(x, S) = H(x, S) + \\epsilon_\\Sigma I$ are:\n3070: \n3071: $$\n3072: \\lambda_i(g) = \\lambda_i(H) + \\epsilon_\\Sigma, \\quad i = 1, \\ldots, d\n3073: $$\n3074: \n3075: **Lower Bound:** By definition of the spectral floor:\n3076: \n3077: $$\n3078: \\lambda_i(H) \\ge -\\Lambda_-(\\rho) \\quad \\Rightarrow \\quad \\lambda_i(g) \\ge \\epsilon_\\Sigma - \\Lambda_-(\\rho) = c_{\\min}(\\rho)\n3079: $$\n3080: \n3081: If $\\epsilon_\\Sigma > \\Lambda_-(\\rho)$, then $c_{\\min}(\\rho) > 0$ and $g \\succ 0$ (positive definite).\n3082: \n3083: **Upper Bound:** By the Hessian bound from {prf:ref}`thm-explicit-hessian`:\n3084: \n3085: $$\n3086: \\lambda_i(H) \\le \\|H\\| \\le H_{\\max}(\\rho) \\quad \\Rightarrow \\quad \\lambda_i(g) \\le H_{\\max}(\\rho) + \\epsilon_\\Sigma = c_{\\max}\n3087: $$\n3088: \n3089: Therefore $c_{\\min}(\\rho) I \\preceq g(x, S) \\preceq c_{\\max} I$ in the Loewner ordering.\n3090: \n3091: **Step 2: Inverse and Square Root Bounds.**\n3092: \n3093: For a positive definite matrix $A$ with $c_1 I \\preceq A \\preceq c_2 I$, we have:\n3094: \n3095: $$\n3096: \\frac{1}{c_2} I \\preceq A^{-1} \\preceq \\frac{1}{c_1} I\n3097: $$\n3098: \n3099: Applying this to $D_{\\text{reg}} = g^{-1}$ gives the inverse bounds.\n3100: \n3101: For the square root $\\Sigma_{\\text{reg}} = D_{\\text{reg}}^{1/2} = g^{-1/2}$, the eigenvalues are $1/\\sqrt{\\lambda_i(g)}$, giving:\n3102: \n3103: $$\n3104: \\frac{1}{\\sqrt{c_{\\max}}} I \\preceq \\Sigma_{\\text{reg}} \\preceq \\frac{1}{\\sqrt{c_{\\min}(\\rho)}} I\n3105: $$\n3106: \n3107: **Step 3: Explicit Formulas for the Bounds.**\n3108: \n3109: **Part A: Spectral Ceiling $H_{\\max}(\\rho)$.**\n3110: \n3111: From {prf:ref}`thm-explicit-hessian`, we have:\n3112: \n3113: $$\n3114: H_{\\max}(\\rho) = \\frac{g''_{\\max} (d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} + \\frac{g'_{\\max} d''_{\\max}}{\\kappa_{\\text{var,min}}} + O(1/\\rho)\n3115: $$\n3116: \n3117: where:\n3118: - $g''_{\\max} = \\sup_{z \\in \\mathbb{R}} |g''_A(z)|$ is the maximum second derivative of the rescale function\n3119: - $d'_{\\max} = \\sup_{x \\in \\mathcal{X}} \\|\\nabla_x d(x)\\|$ is the Lipschitz constant of the measurement function\n3120: - $d''_{\\max} = \\sup_{x \\in \\mathcal{X}} \\|\\nabla^2_x d(x)\\|_{\\text{op}}$ is the operator norm of the measurement Hessian\n3121: - The $O(1/\\rho)$ term captures moment correction contributions\n3122: \n3123: **Part B: Spectral Floor $\\Lambda_-(\\rho)$.**\n3124: \n3125: The spectral floor $\\Lambda_-(\\rho)$ bounds the maximum magnitude of negative eigenvalues of $H(x, S)$. From the Hessian decomposition:\n3126: \n3127: $$\n3128: H(x, S) = g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z + g'_A(Z) \\, \\nabla^2_x Z\n3129: $$\n3130: \n3131: **Case 1: Convex Rescale Function ($g''_A \\ge 0$) and Convex Measurement ($\\nabla^2 d \\succeq 0$).**\n3132: \n3133: If both $g''_A(z) \\ge 0$ for all $z$ and $\\nabla^2_x d(x) \\succeq 0$ for all $x$ (positive semi-definite), then both terms in $H$ are positive semi-definite:\n3134: - The outer product $\\nabla_x Z \\otimes \\nabla_x Z \\succeq 0$ (rank-1 positive semi-definite)\n3135: - The scaled Hessian term $g'_A(Z) \\nabla^2_x Z \\succeq 0$ (since $g'_A > 0$ by monotonicity)\n3136: \n3137: Therefore $H \\succeq 0$ and $\\Lambda_-(\\rho) = 0$.\n3138: \n3139: **Case 2: General Case (Allowing Indefinite Hessians).**\n3140: \n3141: When the measurement Hessian $\\nabla^2_x d$ can have negative eigenvalues, or when $g''_A$ can be negative, we must bound the most negative eigenvalue:\n3142: \n3143: $$\n3144: \\Lambda_-(\\rho) = \\sup_{x, S} \\max\\{0, -\\lambda_{\\min}(H(x, S))\\}\n3145: $$\n3146: \n3147: For a symmetric matrix $M = A + B$, we have $\\lambda_{\\min}(M) \\ge \\lambda_{\\min}(A) + \\lambda_{\\min}(B)$. Therefore:\n3148: \n3149: $$\n3150: \\lambda_{\\min}(H) \\ge \\lambda_{\\min}(g''_A(Z) \\, \\nabla Z \\otimes \\nabla Z) + \\lambda_{\\min}(g'_A(Z) \\, \\nabla^2 Z)\n3151: $$\n3152: \n3153: **Term 1 (Outer Product):** The minimum eigenvalue of the rank-1 matrix $g''_A \\nabla Z \\otimes \\nabla Z$ is:\n3154: - If $g''_A(Z) \\ge 0$: $\\lambda_{\\min} = 0$ (has $d-1$ zero eigenvalues)\n3155: - If $g''_A(Z) < 0$: $\\lambda_{\\min} = g''_A(Z) \\|\\nabla Z\\|^2 \\ge -|g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}}$\n3156: \n3157: **Term 2 (Scaled Hessian):** For the second term:\n3158: \n3159: $$\n3160: \\lambda_{\\min}(g'_A(Z) \\nabla^2 Z) = g'_A(Z) \\lambda_{\\min}(\\nabla^2 Z)\n3161: $$\n3162: \n3163: If $\\lambda_{\\min}(\\nabla^2 Z) < 0$ (indefinite), then using $g'_A(Z) \\le g'_{\\max}$ and defining:\n3164: \n3165: $$\n3166: d''_{\\min} = \\inf_{x \\in \\mathcal{X}} \\lambda_{\\min}(\\nabla^2_x d(x))\n3167: $$\n3168: \n3169: we have:\n3170: \n3171: $$\n3172: \\lambda_{\\min}(g'_A(Z) \\nabla^2 Z) \\ge g'_{\\min} \\cdot \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\n3173: $$\n3174: \n3175: where $g'_{\\min} = \\inf_{z} g'_A(z) > 0$ by monotonicity.\n3176: \n3177: **Combined Bound:**\n3178: \n3179: $$\n3180: -\\lambda_{\\min}(H) \\le \\max\\left\\{0, |g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} - g'_{\\min} \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\\right\\} + O(1/\\rho)\n3181: $$\n3182: \n3183: Therefore, the **explicit spectral floor bound** is:\n3184: \n3185: $$\n3186: \\Lambda_-(\\rho) = \\max\\left\\{0, |g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} - g'_{\\min} \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\\right\\} + C_{\\text{moment}}(\\rho)\n3187: $$\n3188: \n3189: where $C_{\\text{moment}}(\\rho) = O(1/\\rho)$ captures the moment correction terms and $d''_{\\min} = \\min\\{0, \\inf_x \\lambda_{\\min}(\\nabla^2 d(x))\\}$ is non-positive when the measurement can be concave.\n3190: \n3191: **Sufficient Condition for Positive Definiteness:** The regularization must satisfy:\n3192: \n3193: $$\n3194: \\epsilon_\\Sigma > \\Lambda_-(\\rho) = \\max\\left\\{0, |g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} - g'_{\\min} \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\\right\\} + C_{\\text{moment}}(\\rho)\n3195: $$",
      "metadata": {},
      "section": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "3064: :::\n3065: \n3066: :::{prf:proof}\n3067: **Step 1: Eigenvalue Bounds for $g(x, S)$.**\n3068: \n3069: Let $\\lambda_1, \\ldots, \\lambda_d$ be the eigenvalues of $H(x, S)$. Then the eigenvalues of $g(x, S) = H(x, S) + \\epsilon_\\Sigma I$ are:\n3070: \n3071: $$\n3072: \\lambda_i(g) = \\lambda_i(H) + \\epsilon_\\Sigma, \\quad i = 1, \\ldots, d\n3073: $$\n3074: \n3075: **Lower Bound:** By definition of the spectral floor:\n3076: \n3077: $$\n3078: \\lambda_i(H) \\ge -\\Lambda_-(\\rho) \\quad \\Rightarrow \\quad \\lambda_i(g) \\ge \\epsilon_\\Sigma - \\Lambda_-(\\rho) = c_{\\min}(\\rho)\n3079: $$\n3080: \n3081: If $\\epsilon_\\Sigma > \\Lambda_-(\\rho)$, then $c_{\\min}(\\rho) > 0$ and $g \\succ 0$ (positive definite).\n3082: \n3083: **Upper Bound:** By the Hessian bound from {prf:ref}`thm-explicit-hessian`:\n3084: \n3085: $$\n3086: \\lambda_i(H) \\le \\|H\\| \\le H_{\\max}(\\rho) \\quad \\Rightarrow \\quad \\lambda_i(g) \\le H_{\\max}(\\rho) + \\epsilon_\\Sigma = c_{\\max}\n3087: $$\n3088: \n3089: Therefore $c_{\\min}(\\rho) I \\preceq g(x, S) \\preceq c_{\\max} I$ in the Loewner ordering.\n3090: \n3091: **Step 2: Inverse and Square Root Bounds.**\n3092: \n3093: For a positive definite matrix $A$ with $c_1 I \\preceq A \\preceq c_2 I$, we have:\n3094: \n3095: $$\n3096: \\frac{1}{c_2} I \\preceq A^{-1} \\preceq \\frac{1}{c_1} I\n3097: $$\n3098: \n3099: Applying this to $D_{\\text{reg}} = g^{-1}$ gives the inverse bounds.\n3100: \n3101: For the square root $\\Sigma_{\\text{reg}} = D_{\\text{reg}}^{1/2} = g^{-1/2}$, the eigenvalues are $1/\\sqrt{\\lambda_i(g)}$, giving:\n3102: \n3103: $$\n3104: \\frac{1}{\\sqrt{c_{\\max}}} I \\preceq \\Sigma_{\\text{reg}} \\preceq \\frac{1}{\\sqrt{c_{\\min}(\\rho)}} I\n3105: $$\n3106: \n3107: **Step 3: Explicit Formulas for the Bounds.**\n3108: \n3109: **Part A: Spectral Ceiling $H_{\\max}(\\rho)$.**\n3110: \n3111: From {prf:ref}`thm-explicit-hessian`, we have:\n3112: \n3113: $$\n3114: H_{\\max}(\\rho) = \\frac{g''_{\\max} (d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} + \\frac{g'_{\\max} d''_{\\max}}{\\kappa_{\\text{var,min}}} + O(1/\\rho)\n3115: $$\n3116: \n3117: where:\n3118: - $g''_{\\max} = \\sup_{z \\in \\mathbb{R}} |g''_A(z)|$ is the maximum second derivative of the rescale function\n3119: - $d'_{\\max} = \\sup_{x \\in \\mathcal{X}} \\|\\nabla_x d(x)\\|$ is the Lipschitz constant of the measurement function\n3120: - $d''_{\\max} = \\sup_{x \\in \\mathcal{X}} \\|\\nabla^2_x d(x)\\|_{\\text{op}}$ is the operator norm of the measurement Hessian\n3121: - The $O(1/\\rho)$ term captures moment correction contributions\n3122: \n3123: **Part B: Spectral Floor $\\Lambda_-(\\rho)$.**\n3124: \n3125: The spectral floor $\\Lambda_-(\\rho)$ bounds the maximum magnitude of negative eigenvalues of $H(x, S)$. From the Hessian decomposition:\n3126: \n3127: $$\n3128: H(x, S) = g''_A(Z) \\, \\nabla_x Z \\otimes \\nabla_x Z + g'_A(Z) \\, \\nabla^2_x Z\n3129: $$\n3130: \n3131: **Case 1: Convex Rescale Function ($g''_A \\ge 0$) and Convex Measurement ($\\nabla^2 d \\succeq 0$).**\n3132: \n3133: If both $g''_A(z) \\ge 0$ for all $z$ and $\\nabla^2_x d(x) \\succeq 0$ for all $x$ (positive semi-definite), then both terms in $H$ are positive semi-definite:\n3134: - The outer product $\\nabla_x Z \\otimes \\nabla_x Z \\succeq 0$ (rank-1 positive semi-definite)\n3135: - The scaled Hessian term $g'_A(Z) \\nabla^2_x Z \\succeq 0$ (since $g'_A > 0$ by monotonicity)\n3136: \n3137: Therefore $H \\succeq 0$ and $\\Lambda_-(\\rho) = 0$.\n3138: \n3139: **Case 2: General Case (Allowing Indefinite Hessians).**\n3140: \n3141: When the measurement Hessian $\\nabla^2_x d$ can have negative eigenvalues, or when $g''_A$ can be negative, we must bound the most negative eigenvalue:\n3142: \n3143: $$\n3144: \\Lambda_-(\\rho) = \\sup_{x, S} \\max\\{0, -\\lambda_{\\min}(H(x, S))\\}\n3145: $$\n3146: \n3147: For a symmetric matrix $M = A + B$, we have $\\lambda_{\\min}(M) \\ge \\lambda_{\\min}(A) + \\lambda_{\\min}(B)$. Therefore:\n3148: \n3149: $$\n3150: \\lambda_{\\min}(H) \\ge \\lambda_{\\min}(g''_A(Z) \\, \\nabla Z \\otimes \\nabla Z) + \\lambda_{\\min}(g'_A(Z) \\, \\nabla^2 Z)\n3151: $$\n3152: \n3153: **Term 1 (Outer Product):** The minimum eigenvalue of the rank-1 matrix $g''_A \\nabla Z \\otimes \\nabla Z$ is:\n3154: - If $g''_A(Z) \\ge 0$: $\\lambda_{\\min} = 0$ (has $d-1$ zero eigenvalues)\n3155: - If $g''_A(Z) < 0$: $\\lambda_{\\min} = g''_A(Z) \\|\\nabla Z\\|^2 \\ge -|g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}}$\n3156: \n3157: **Term 2 (Scaled Hessian):** For the second term:\n3158: \n3159: $$\n3160: \\lambda_{\\min}(g'_A(Z) \\nabla^2 Z) = g'_A(Z) \\lambda_{\\min}(\\nabla^2 Z)\n3161: $$\n3162: \n3163: If $\\lambda_{\\min}(\\nabla^2 Z) < 0$ (indefinite), then using $g'_A(Z) \\le g'_{\\max}$ and defining:\n3164: \n3165: $$\n3166: d''_{\\min} = \\inf_{x \\in \\mathcal{X}} \\lambda_{\\min}(\\nabla^2_x d(x))\n3167: $$\n3168: \n3169: we have:\n3170: \n3171: $$\n3172: \\lambda_{\\min}(g'_A(Z) \\nabla^2 Z) \\ge g'_{\\min} \\cdot \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\n3173: $$\n3174: \n3175: where $g'_{\\min} = \\inf_{z} g'_A(z) > 0$ by monotonicity.\n3176: \n3177: **Combined Bound:**\n3178: \n3179: $$\n3180: -\\lambda_{\\min}(H) \\le \\max\\left\\{0, |g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} - g'_{\\min} \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\\right\\} + O(1/\\rho)\n3181: $$\n3182: \n3183: Therefore, the **explicit spectral floor bound** is:\n3184: \n3185: $$\n3186: \\Lambda_-(\\rho) = \\max\\left\\{0, |g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} - g'_{\\min} \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\\right\\} + C_{\\text{moment}}(\\rho)\n3187: $$\n3188: \n3189: where $C_{\\text{moment}}(\\rho) = O(1/\\rho)$ captures the moment correction terms and $d''_{\\min} = \\min\\{0, \\inf_x \\lambda_{\\min}(\\nabla^2 d(x))\\}$ is non-positive when the measurement can be concave.\n3190: \n3191: **Sufficient Condition for Positive Definiteness:** The regularization must satisfy:\n3192: \n3193: $$\n3194: \\epsilon_\\Sigma > \\Lambda_-(\\rho) = \\max\\left\\{0, |g''_{\\max}| \\frac{(d'_{\\max})^2}{\\kappa^2_{\\text{var,min}}} - g'_{\\min} \\frac{d''_{\\min}}{\\kappa_{\\text{var,min}}}\\right\\} + C_{\\text{moment}}(\\rho)\n3195: $$\n3196: "
    },
    {
      "directive_type": "remark",
      "label": "unlabeled-remark-459",
      "title": "Algorithmic Control of Ellipticity",
      "start_line": 3198,
      "end_line": 3228,
      "header_lines": [
        3199
      ],
      "content_start": 3201,
      "content_end": 3227,
      "content": "3201: :class: important\n3202: \n3203: The ellipticity bounds are **fully controlled by algorithmic parameters**:\n3204: \n3205: **1. Regularization Parameter $\\epsilon_\\Sigma$:**\n3206: - **Lower bound:** $c_{\\min}(\\rho) = \\epsilon_\\Sigma - \\Lambda_-(\\rho)$\n3207: - Larger $\\epsilon_\\Sigma$ \u2192 stronger regularization \u2192 more isotropic diffusion\n3208: - Must satisfy $\\epsilon_\\Sigma > \\Lambda_-(\\rho)$ for positive definiteness\n3209: \n3210: **2. Localization Scale $\\rho$:**\n3211: - Affects $H_{\\max}(\\rho)$ through the $O(1/\\rho)$ moment correction terms\n3212: - Smaller $\\rho$ \u2192 tighter localization \u2192 larger $H_{\\max}(\\rho)$ \u2192 wider ellipticity gap\n3213: - Larger $\\rho$ \u2192 global averaging \u2192 more stable $H$ \u2192 narrower ellipticity gap\n3214: \n3215: **3. Variance Regularization $\\kappa_{\\text{var,min}}$:**\n3216: - Appears in denominator of Hessian bound\n3217: - Larger $\\kappa_{\\text{var,min}}$ \u2192 smaller $H_{\\max}(\\rho)$ \u2192 better conditioning\n3218: \n3219: **4. Measurement Regularity $(d'_{\\max}, d''_{\\max})$:**\n3220: - Smoother measurement functions \u2192 smaller Hessian bounds\n3221: - Choice of $d$ (reward, diversity, etc.) directly affects geometry\n3222: \n3223: **5. Rescale Function $(g_A, g'_{\\max}, g''_{\\max})$:**\n3224: - Bounds on derivatives control curvature amplification\n3225: - Saturating rescales (e.g., sigmoid) naturally bound curvature\n3226: \n3227: The convergence rate depends on $c_{\\min}(\\rho)$ (see Chapter 5), so there is a **design tradeoff**:",
      "metadata": {
        "class": "important"
      },
      "section": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "3198: :::\n3199: \n3200: :::{prf:remark} Algorithmic Control of Ellipticity\n3201: :class: important\n3202: \n3203: The ellipticity bounds are **fully controlled by algorithmic parameters**:\n3204: \n3205: **1. Regularization Parameter $\\epsilon_\\Sigma$:**\n3206: - **Lower bound:** $c_{\\min}(\\rho) = \\epsilon_\\Sigma - \\Lambda_-(\\rho)$\n3207: - Larger $\\epsilon_\\Sigma$ \u2192 stronger regularization \u2192 more isotropic diffusion\n3208: - Must satisfy $\\epsilon_\\Sigma > \\Lambda_-(\\rho)$ for positive definiteness\n3209: \n3210: **2. Localization Scale $\\rho$:**\n3211: - Affects $H_{\\max}(\\rho)$ through the $O(1/\\rho)$ moment correction terms\n3212: - Smaller $\\rho$ \u2192 tighter localization \u2192 larger $H_{\\max}(\\rho)$ \u2192 wider ellipticity gap\n3213: - Larger $\\rho$ \u2192 global averaging \u2192 more stable $H$ \u2192 narrower ellipticity gap\n3214: \n3215: **3. Variance Regularization $\\kappa_{\\text{var,min}}$:**\n3216: - Appears in denominator of Hessian bound\n3217: - Larger $\\kappa_{\\text{var,min}}$ \u2192 smaller $H_{\\max}(\\rho)$ \u2192 better conditioning\n3218: \n3219: **4. Measurement Regularity $(d'_{\\max}, d''_{\\max})$:**\n3220: - Smoother measurement functions \u2192 smaller Hessian bounds\n3221: - Choice of $d$ (reward, diversity, etc.) directly affects geometry\n3222: \n3223: **5. Rescale Function $(g_A, g'_{\\max}, g''_{\\max})$:**\n3224: - Bounds on derivatives control curvature amplification\n3225: - Saturating rescales (e.g., sigmoid) naturally bound curvature\n3226: \n3227: The convergence rate depends on $c_{\\min}(\\rho)$ (see Chapter 5), so there is a **design tradeoff**:\n3228: - **Large $\\epsilon_\\Sigma$:** Strong convergence guarantees, but less geometric adaptation"
    },
    {
      "directive_type": "definition",
      "label": "def-emergent-manifold",
      "title": "Emergent Riemannian Manifold",
      "start_line": 3234,
      "end_line": 3271,
      "header_lines": [
        3235
      ],
      "content_start": 3237,
      "content_end": 3270,
      "content": "3237: :label: def-emergent-manifold\n3238: \n3239: The metric $g(x, S)$ from {prf:ref}`def-metric-explicit` endows the state space $\\mathcal{X}$ with the structure of a **Riemannian manifold** $(\\mathcal{X}, g_S)$, where the metric depends parametrically on the swarm state $S$.\n3240: \n3241: **Geometric Quantities:**\n3242: \n3243: **1. Metric Tensor (Index Form):**\n3244: \n3245: $$\n3246: g_{ab}(x, S) = \\left[\\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x)\\right]_{ab} + \\epsilon_\\Sigma \\delta_{ab}\n3247: $$\n3248: \n3249: **2. Inverse Metric (Contravariant Tensor):**\n3250: \n3251: $$\n3252: g^{ab}(x, S) = \\left[\\left(\\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x) + \\epsilon_\\Sigma I\\right)^{-1}\\right]_{ab}\n3253: $$\n3254: \n3255: **3. Volume Element:** The Riemannian volume measure is:\n3256: \n3257: $$\n3258: d\\text{Vol}_g = \\sqrt{\\det g(x, S)} \\, dx\n3259: $$\n3260: \n3261: **4. Geodesic Equation:** Curves $\\gamma(t)$ that minimize length satisfy:\n3262: \n3263: $$\n3264: \\frac{d^2 \\gamma^a}{dt^2} + \\Gamma^a_{bc}(x, S) \\frac{d\\gamma^b}{dt} \\frac{d\\gamma^c}{dt} = 0\n3265: $$\n3266: \n3267: where $\\Gamma^a_{bc}$ are the **Christoffel symbols** computed from $g$ via:\n3268: \n3269: $$\n3270: \\Gamma^a_{bc} = \\frac{1}{2} g^{ad} \\left(\\frac{\\partial g_{db}}{\\partial x^c} + \\frac{\\partial g_{dc}}{\\partial x^b} - \\frac{\\partial g_{bc}}{\\partial x^d}\\right)",
      "metadata": {
        "label": "def-emergent-manifold"
      },
      "section": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "3234: With the metric explicitly constructed, we can characterize the induced geometric structure.\n3235: \n3236: :::{prf:definition} Emergent Riemannian Manifold\n3237: :label: def-emergent-manifold\n3238: \n3239: The metric $g(x, S)$ from {prf:ref}`def-metric-explicit` endows the state space $\\mathcal{X}$ with the structure of a **Riemannian manifold** $(\\mathcal{X}, g_S)$, where the metric depends parametrically on the swarm state $S$.\n3240: \n3241: **Geometric Quantities:**\n3242: \n3243: **1. Metric Tensor (Index Form):**\n3244: \n3245: $$\n3246: g_{ab}(x, S) = \\left[\\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x)\\right]_{ab} + \\epsilon_\\Sigma \\delta_{ab}\n3247: $$\n3248: \n3249: **2. Inverse Metric (Contravariant Tensor):**\n3250: \n3251: $$\n3252: g^{ab}(x, S) = \\left[\\left(\\nabla^2_x V_{\\text{fit}}[f_k, \\rho](x) + \\epsilon_\\Sigma I\\right)^{-1}\\right]_{ab}\n3253: $$\n3254: \n3255: **3. Volume Element:** The Riemannian volume measure is:\n3256: \n3257: $$\n3258: d\\text{Vol}_g = \\sqrt{\\det g(x, S)} \\, dx\n3259: $$\n3260: \n3261: **4. Geodesic Equation:** Curves $\\gamma(t)$ that minimize length satisfy:\n3262: \n3263: $$\n3264: \\frac{d^2 \\gamma^a}{dt^2} + \\Gamma^a_{bc}(x, S) \\frac{d\\gamma^b}{dt} \\frac{d\\gamma^c}{dt} = 0\n3265: $$\n3266: \n3267: where $\\Gamma^a_{bc}$ are the **Christoffel symbols** computed from $g$ via:\n3268: \n3269: $$\n3270: \\Gamma^a_{bc} = \\frac{1}{2} g^{ad} \\left(\\frac{\\partial g_{db}}{\\partial x^c} + \\frac{\\partial g_{dc}}{\\partial x^b} - \\frac{\\partial g_{bc}}{\\partial x^d}\\right)\n3271: $$"
    },
    {
      "directive_type": "proposition",
      "label": "prop-geodesics-fitness",
      "title": "Geodesics Favor High-Fitness Regions",
      "start_line": 3273,
      "end_line": 3295,
      "header_lines": [
        3274
      ],
      "content_start": 3276,
      "content_end": 3294,
      "content": "3276: :label: prop-geodesics-fitness\n3277: \n3278: The geodesics of the emergent metric $g(x, S)$ are **biased toward high-fitness regions**. Specifically:\n3279: \n3280: 1. **Shorter distances in high-fitness regions:** For two points $x_1, x_2 \\in \\mathcal{X}$, the Riemannian distance:\n3281: \n3282: $$\n3283: d_g(x_1, x_2) = \\inf_{\\gamma: x_1 \\to x_2} \\int_0^1 \\sqrt{g_{ab}(\\gamma(t), S) \\dot{\\gamma}^a(t) \\dot{\\gamma}^b(t)} \\, dt\n3284: $$\n3285: \n3286: is **smaller** when the path passes through regions of high $V_{\\text{fit}}$ (low curvature, low metric eigenvalues).\n3287: \n3288: 2. **Geodesics avoid high-curvature regions:** The metric eigenvalues are largest where the Hessian $H$ has large positive eigenvalues, which occurs where the fitness landscape is **most convexly curved**. Geodesics bend away from these regions of high metric density to minimize path length.\n3289: \n3290: 3. **Connection to natural gradient:** The inverse metric $g^{-1} = D_{\\text{reg}}$ defines the **natural gradient**:\n3291: \n3292: $$\n3293: \\nabla^{\\text{nat}} V_{\\text{fit}} = g^{-1} \\nabla V_{\\text{fit}}\n3294: $$",
      "metadata": {
        "label": "prop-geodesics-fitness"
      },
      "section": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "3273: :::\n3274: \n3275: :::{prf:proposition} Geodesics Favor High-Fitness Regions\n3276: :label: prop-geodesics-fitness\n3277: \n3278: The geodesics of the emergent metric $g(x, S)$ are **biased toward high-fitness regions**. Specifically:\n3279: \n3280: 1. **Shorter distances in high-fitness regions:** For two points $x_1, x_2 \\in \\mathcal{X}$, the Riemannian distance:\n3281: \n3282: $$\n3283: d_g(x_1, x_2) = \\inf_{\\gamma: x_1 \\to x_2} \\int_0^1 \\sqrt{g_{ab}(\\gamma(t), S) \\dot{\\gamma}^a(t) \\dot{\\gamma}^b(t)} \\, dt\n3284: $$\n3285: \n3286: is **smaller** when the path passes through regions of high $V_{\\text{fit}}$ (low curvature, low metric eigenvalues).\n3287: \n3288: 2. **Geodesics avoid high-curvature regions:** The metric eigenvalues are largest where the Hessian $H$ has large positive eigenvalues, which occurs where the fitness landscape is **most convexly curved**. Geodesics bend away from these regions of high metric density to minimize path length.\n3289: \n3290: 3. **Connection to natural gradient:** The inverse metric $g^{-1} = D_{\\text{reg}}$ defines the **natural gradient**:\n3291: \n3292: $$\n3293: \\nabla^{\\text{nat}} V_{\\text{fit}} = g^{-1} \\nabla V_{\\text{fit}}\n3294: $$\n3295: "
    },
    {
      "directive_type": "proof",
      "label": "unlabeled-proof-558",
      "title": null,
      "start_line": 3297,
      "end_line": 3329,
      "header_lines": [],
      "content_start": 3298,
      "content_end": 3328,
      "content": "3298: \n3299: :::{prf:proof}\n3300: **Part 1: Distance Formula.**\n3301: \n3302: By definition, the Riemannian distance is the infimum of lengths:\n3303: \n3304: $$\n3305: d_g(x_1, x_2) = \\inf_\\gamma \\int_0^1 \\sqrt{\\dot{\\gamma}^T g(\\gamma(t), S) \\dot{\\gamma}} \\, dt\n3306: $$\n3307: \n3308: In a region where $V_{\\text{fit}}$ is high and flat (low curvature), $H$ has small eigenvalues, so $g = H + \\epsilon_\\Sigma I \\approx \\epsilon_\\Sigma I$ (nearly Euclidean). In a region of low fitness and high curvature, $H$ has large eigenvalues, so $g$ is \"stretched\" (larger metric eigenvalues \u2192 longer distances).\n3309: \n3310: Therefore, paths through high-fitness regions incur smaller length than paths through low-fitness regions, even if the Euclidean distances are equal.\n3311: \n3312: **Part 2: Geodesic Deviation.**\n3313: \n3314: The geodesic equation involves the Christoffel symbols $\\Gamma^a_{bc}$, which depend on $\\partial g/\\partial x$. Since:\n3315: \n3316: $$\n3317: \\frac{\\partial g_{ab}}{\\partial x^c} = \\frac{\\partial}{\\partial x^c} \\left[\\nabla^2 V_{\\text{fit}}\\right]_{ab} = \\left[\\nabla^3 V_{\\text{fit}}\\right]_{abc}\n3318: $$\n3319: \n3320: the geodesic curvature is proportional to the **third derivatives of the fitness potential**. Regions of high curvature (large $\\|\\nabla^3 V_{\\text{fit}}\\|$) exert a \"repulsive force\" on geodesics, causing them to deviate.\n3321: \n3322: **Part 3: Natural Gradient Connection.**\n3323: \n3324: The natural gradient is defined as:\n3325: \n3326: $$\n3327: \\nabla^{\\text{nat}} V_{\\text{fit}} = g^{-1} \\nabla V_{\\text{fit}} = D_{\\text{reg}} \\nabla V_{\\text{fit}}\n3328: $$",
      "metadata": {},
      "section": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "3297: :::\n3298: \n3299: :::{prf:proof}\n3300: **Part 1: Distance Formula.**\n3301: \n3302: By definition, the Riemannian distance is the infimum of lengths:\n3303: \n3304: $$\n3305: d_g(x_1, x_2) = \\inf_\\gamma \\int_0^1 \\sqrt{\\dot{\\gamma}^T g(\\gamma(t), S) \\dot{\\gamma}} \\, dt\n3306: $$\n3307: \n3308: In a region where $V_{\\text{fit}}$ is high and flat (low curvature), $H$ has small eigenvalues, so $g = H + \\epsilon_\\Sigma I \\approx \\epsilon_\\Sigma I$ (nearly Euclidean). In a region of low fitness and high curvature, $H$ has large eigenvalues, so $g$ is \"stretched\" (larger metric eigenvalues \u2192 longer distances).\n3309: \n3310: Therefore, paths through high-fitness regions incur smaller length than paths through low-fitness regions, even if the Euclidean distances are equal.\n3311: \n3312: **Part 2: Geodesic Deviation.**\n3313: \n3314: The geodesic equation involves the Christoffel symbols $\\Gamma^a_{bc}$, which depend on $\\partial g/\\partial x$. Since:\n3315: \n3316: $$\n3317: \\frac{\\partial g_{ab}}{\\partial x^c} = \\frac{\\partial}{\\partial x^c} \\left[\\nabla^2 V_{\\text{fit}}\\right]_{ab} = \\left[\\nabla^3 V_{\\text{fit}}\\right]_{abc}\n3318: $$\n3319: \n3320: the geodesic curvature is proportional to the **third derivatives of the fitness potential**. Regions of high curvature (large $\\|\\nabla^3 V_{\\text{fit}}\\|$) exert a \"repulsive force\" on geodesics, causing them to deviate.\n3321: \n3322: **Part 3: Natural Gradient Connection.**\n3323: \n3324: The natural gradient is defined as:\n3325: \n3326: $$\n3327: \\nabla^{\\text{nat}} V_{\\text{fit}} = g^{-1} \\nabla V_{\\text{fit}} = D_{\\text{reg}} \\nabla V_{\\text{fit}}\n3328: $$\n3329: "
    },
    {
      "directive_type": "theorem",
      "label": "thm-algorithmic-tunability",
      "title": "Algorithmic Tunability of the Emergent Geometry",
      "start_line": 3417,
      "end_line": 3445,
      "header_lines": [
        3418
      ],
      "content_start": 3420,
      "content_end": 3444,
      "content": "3420: :label: thm-algorithmic-tunability\n3421: \n3422: The emergent Riemannian geometry is **completely determined** by the algorithmic parameters. Specifically:\n3423: \n3424: **1. Localization Scale $\\rho$:** Controls the spatial extent of geometric structure.\n3425: - Small $\\rho$: Hyper-local geometry, responds to fine-scale features\n3426: - Large $\\rho$: Global geometry, averages over entire landscape\n3427: \n3428: **2. Regularization $\\epsilon_\\Sigma$:** Controls the deviation from Euclidean geometry.\n3429: - Small $\\epsilon_\\Sigma$: Strong geometric adaptation, metric dominated by Hessian $H$\n3430: - Large $\\epsilon_\\Sigma$: Weak geometric adaptation, metric nearly Euclidean $g \\approx \\epsilon_\\Sigma I$\n3431: \n3432: **3. Variance Regularization $\\kappa_{\\text{var,min}}$:** Controls the conditioning of the Z-score.\n3433: - Small $\\kappa_{\\text{var,min}}$: Sensitive to variance collapse, large Hessian bounds\n3434: - Large $\\kappa_{\\text{var,min}}$: Robust to variance collapse, bounded Hessian\n3435: \n3436: **4. Measurement Function $d$:** Determines **what geometric structure emerges**.\n3437: - Reward: Geometry encodes value landscape\n3438: - Diversity: Geometry encodes novelty structure\n3439: - Custom metrics: User-defined geometric inductive biases\n3440: \n3441: **5. Rescale Function $g_A$:** Controls the **amplification** of curvature.\n3442: - Linear: Direct Hessian of Z-score\n3443: - Sigmoid: Saturated curvature, bounded $g''_A$\n3444: - Custom: Tailored curvature profiles",
      "metadata": {
        "label": "thm-algorithmic-tunability"
      },
      "section": "## 11. Explicit Derivation of the Emergent Metric from Algorithmic Parameters",
      "raw_directive": "3417: $$\n3418: \n3419: :::{prf:theorem} Algorithmic Tunability of the Emergent Geometry\n3420: :label: thm-algorithmic-tunability\n3421: \n3422: The emergent Riemannian geometry is **completely determined** by the algorithmic parameters. Specifically:\n3423: \n3424: **1. Localization Scale $\\rho$:** Controls the spatial extent of geometric structure.\n3425: - Small $\\rho$: Hyper-local geometry, responds to fine-scale features\n3426: - Large $\\rho$: Global geometry, averages over entire landscape\n3427: \n3428: **2. Regularization $\\epsilon_\\Sigma$:** Controls the deviation from Euclidean geometry.\n3429: - Small $\\epsilon_\\Sigma$: Strong geometric adaptation, metric dominated by Hessian $H$\n3430: - Large $\\epsilon_\\Sigma$: Weak geometric adaptation, metric nearly Euclidean $g \\approx \\epsilon_\\Sigma I$\n3431: \n3432: **3. Variance Regularization $\\kappa_{\\text{var,min}}$:** Controls the conditioning of the Z-score.\n3433: - Small $\\kappa_{\\text{var,min}}$: Sensitive to variance collapse, large Hessian bounds\n3434: - Large $\\kappa_{\\text{var,min}}$: Robust to variance collapse, bounded Hessian\n3435: \n3436: **4. Measurement Function $d$:** Determines **what geometric structure emerges**.\n3437: - Reward: Geometry encodes value landscape\n3438: - Diversity: Geometry encodes novelty structure\n3439: - Custom metrics: User-defined geometric inductive biases\n3440: \n3441: **5. Rescale Function $g_A$:** Controls the **amplification** of curvature.\n3442: - Linear: Direct Hessian of Z-score\n3443: - Sigmoid: Saturated curvature, bounded $g''_A$\n3444: - Custom: Tailored curvature profiles\n3445: "
    }
  ],
  "validation": {
    "ok": true,
    "errors": []
  }
}