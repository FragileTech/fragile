{
  "chapter_index": 7,
  "section_id": "## 2. Analysis of the Finite-N to Mean-Field Transition",
  "directive_count": 4,
  "hints": [
    {
      "directive_type": "theorem",
      "label": "thm-finite-n-lsi-preservation",
      "title": "Finite-N LSI Preservation (Proven)",
      "start_line": 525,
      "end_line": 547,
      "header_lines": [
        526
      ],
      "content_start": 528,
      "content_end": 546,
      "content": "528: :label: thm-finite-n-lsi-preservation\n529: \n530: The N-particle cloning operator $\\Psi_{\\text{clone}}: \\Sigma_N \\to \\Sigma_N$ **preserves the LSI** with controlled constant degradation. Specifically, if a distribution $\\mu$ on $\\Sigma_N$ satisfies:\n531: \n532: $$\n533: D_{\\text{KL}}(\\mu \\| \\pi) \\le C_{\\text{LSI}} \\cdot I(\\mu \\| \\pi)\n534: \n535: $$\n536: \n537: then the push-forward $\\Psi_{\\text{clone}}^* \\mu$ satisfies:\n538: \n539: $$\n540: D_{\\text{KL}}(\\Psi_{\\text{clone}}^* \\mu \\| \\Psi_{\\text{clone}}^* \\pi) \\le C'_{\\text{LSI}} \\cdot I(\\Psi_{\\text{clone}}^* \\mu \\| \\Psi_{\\text{clone}}^* \\pi)\n541: \n542: $$\n543: \n544: where $C'_{\\text{LSI}} = C_{\\text{LSI}} \\cdot (1 + O(\\delta^2))$ for cloning noise variance $\\delta^2$.\n545: \n546: **Key mechanism**: The cloning operator introduces small Gaussian noise ($\\delta \\xi$) when copying walkers, which regularizes the Fisher information and prevents LSI constant blow-up.",
      "metadata": {
        "label": "thm-finite-n-lsi-preservation"
      },
      "section": "## 2. Analysis of the Finite-N to Mean-Field Transition",
      "references": [],
      "raw_directive": "525: From [09_kl_convergence.md](../1_euclidean_gas/09_kl_convergence.md), the finite-N cloning operator $\\Psi_{\\text{clone}}$ satisfies:\n526: \n527: :::{prf:theorem} Finite-N LSI Preservation (Proven)\n528: :label: thm-finite-n-lsi-preservation\n529: \n530: The N-particle cloning operator $\\Psi_{\\text{clone}}: \\Sigma_N \\to \\Sigma_N$ **preserves the LSI** with controlled constant degradation. Specifically, if a distribution $\\mu$ on $\\Sigma_N$ satisfies:\n531: \n532: $$\n533: D_{\\text{KL}}(\\mu \\| \\pi) \\le C_{\\text{LSI}} \\cdot I(\\mu \\| \\pi)\n534: \n535: $$\n536: \n537: then the push-forward $\\Psi_{\\text{clone}}^* \\mu$ satisfies:\n538: \n539: $$\n540: D_{\\text{KL}}(\\Psi_{\\text{clone}}^* \\mu \\| \\Psi_{\\text{clone}}^* \\pi) \\le C'_{\\text{LSI}} \\cdot I(\\Psi_{\\text{clone}}^* \\mu \\| \\Psi_{\\text{clone}}^* \\pi)\n541: \n542: $$\n543: \n544: where $C'_{\\text{LSI}} = C_{\\text{LSI}} \\cdot (1 + O(\\delta^2))$ for cloning noise variance $\\delta^2$.\n545: \n546: **Key mechanism**: The cloning operator introduces small Gaussian noise ($\\delta \\xi$) when copying walkers, which regularizes the Fisher information and prevents LSI constant blow-up.\n547: "
    },
    {
      "directive_type": "problem",
      "label": "prob-finite-n-vs-mean-field",
      "title": "Critical Differences Between Finite-N and Mean-Field",
      "start_line": 561,
      "end_line": 573,
      "header_lines": [
        562
      ],
      "content_start": 564,
      "content_end": 572,
      "content": "564: :label: prob-finite-n-vs-mean-field\n565: \n566: | Aspect | Finite-N Cloning | Mean-Field Revival | Implication |\n567: |:-------|:-----------------|:-------------------|:------------|\n568: | **Dimensionality** | Finite $(Nd)$ | Infinite (function space) | Compactness arguments may fail |\n569: | **Noise** | Explicit $\\delta \\xi$ noise | No explicit noise in $\\mathcal{R}$ | Fisher information regularization unclear |\n570: | **Nonlinearity** | Linear in empirical measure | Nonlinear (division by $\\\\|\\rho\\\\|_{L^1}$) | May create singularities |\n571: | **Discreteness** | Discrete selection among N walkers | Continuous sampling from $\\rho$ | Combinatorial structure lost |\n572: | **Companion selection** | Finite sample ($j \\in \\mathcal{A}$) | Integral over $\\rho$ | Correlations differ |",
      "metadata": {
        "label": "prob-finite-n-vs-mean-field"
      },
      "section": "## 2. Analysis of the Finite-N to Mean-Field Transition",
      "references": [],
      "raw_directive": "561: **Potential issues**:\n562: \n563: :::{prf:problem} Critical Differences Between Finite-N and Mean-Field\n564: :label: prob-finite-n-vs-mean-field\n565: \n566: | Aspect | Finite-N Cloning | Mean-Field Revival | Implication |\n567: |:-------|:-----------------|:-------------------|:------------|\n568: | **Dimensionality** | Finite $(Nd)$ | Infinite (function space) | Compactness arguments may fail |\n569: | **Noise** | Explicit $\\delta \\xi$ noise | No explicit noise in $\\mathcal{R}$ | Fisher information regularization unclear |\n570: | **Nonlinearity** | Linear in empirical measure | Nonlinear (division by $\\\\|\\rho\\\\|_{L^1}$) | May create singularities |\n571: | **Discreteness** | Discrete selection among N walkers | Continuous sampling from $\\rho$ | Combinatorial structure lost |\n572: | **Companion selection** | Finite sample ($j \\in \\mathcal{A}$) | Integral over $\\rho$ | Correlations differ |\n573: "
    },
    {
      "directive_type": "theorem",
      "label": "thm-data-processing",
      "title": "Data Processing Inequality (Standard Result)",
      "start_line": 588,
      "end_line": 601,
      "header_lines": [
        589
      ],
      "content_start": 591,
      "content_end": 600,
      "content": "591: :label: thm-data-processing\n592: \n593: For any Markov kernel $K: \\mathcal{X} \\to \\mathcal{P}(\\mathcal{Y})$ and probability measures $\\rho, \\sigma \\in \\mathcal{P}(\\mathcal{X})$:\n594: \n595: $$\n596: D_{\\text{KL}}(K \\rho \\| K \\sigma) \\le D_{\\text{KL}}(\\rho \\| \\sigma)\n597: \n598: $$\n599: \n600: where the push-forward measure is $(K\\rho)(B) = \\int_{\\mathcal{X}} K(x, B) \\, \\rho(dx)$ for measurable $B \\subseteq \\mathcal{Y}$.",
      "metadata": {
        "label": "thm-data-processing"
      },
      "section": "## 2. Analysis of the Finite-N to Mean-Field Transition",
      "references": [],
      "raw_directive": "588: This is analogous to updating a prior $\\rho$ by conditioning on the event \"walker survives.\" Bayesian updates are known to be KL-contractive:\n589: \n590: :::{prf:theorem} Data Processing Inequality (Standard Result)\n591: :label: thm-data-processing\n592: \n593: For any Markov kernel $K: \\mathcal{X} \\to \\mathcal{P}(\\mathcal{Y})$ and probability measures $\\rho, \\sigma \\in \\mathcal{P}(\\mathcal{X})$:\n594: \n595: $$\n596: D_{\\text{KL}}(K \\rho \\| K \\sigma) \\le D_{\\text{KL}}(\\rho \\| \\sigma)\n597: \n598: $$\n599: \n600: where the push-forward measure is $(K\\rho)(B) = \\int_{\\mathcal{X}} K(x, B) \\, \\rho(dx)$ for measurable $B \\subseteq \\mathcal{Y}$.\n601: "
    },
    {
      "directive_type": "proof",
      "label": "proof-thm-data-processing",
      "title": null,
      "start_line": 603,
      "end_line": 763,
      "header_lines": [
        604
      ],
      "content_start": 606,
      "content_end": 762,
      "content": "606: :label: proof-thm-data-processing\n607: \n608: **Historical Context**: This theorem is a fundamental result in information theory, first stated by Kullback and formalized by Shannon. The proof follows the classical approach via the chain rule for relative entropy. Primary references: Cover & Thomas (2006, Theorem 2.8.1), Csisz\u00e1r & K\u00f6rner (2011, Section 1.2).\n609: \n610: **Step 0: Reduction to Finite KL-Divergence**\n611: \n612: If $\\rho \\not\\ll \\sigma$ (i.e., $\\rho$ is not absolutely continuous with respect to $\\sigma$), then $D_{\\text{KL}}(\\rho \\| \\sigma) = +\\infty$ by definition, and the inequality holds trivially. Therefore, we assume **$\\rho \\ll \\sigma$** and $D_{\\text{KL}}(\\rho \\| \\sigma) < \\infty$.\n613: \n614: **Step 1: Construction of Joint Distributions**\n615: \n616: Define probability measures $P$ and $Q$ on the product space $\\mathcal{X} \\times \\mathcal{Y}$ by:\n617: \n618: $$\n619: \\begin{aligned}\n620: P(A \\times B) &:= \\int_A \\rho(dx) \\, K(x, B), \\\\\n621: Q(A \\times B) &:= \\int_A \\sigma(dx) \\, K(x, B)\n622: \\end{aligned}\n623: \n624: $$\n625: \n626: for all measurable rectangles $A \\subseteq \\mathcal{X}$, $B \\subseteq \\mathcal{Y}$. By Carath\u00e9odory's extension theorem, these uniquely extend to probability measures on $\\mathcal{X} \\times \\mathcal{Y}$.\n627: \n628: **Interpretation**: The measure $P$ represents the joint distribution of a Markov chain $(X, Y)$ where $X \\sim \\rho$ and $Y \\sim K(X, \\cdot)$. Similarly, $Q$ corresponds to $X \\sim \\sigma$ and $Y \\sim K(X, \\cdot)$. Both chains use the **same kernel** $K$, differing only in the initial distribution.\n629: \n630: **Step 2: Identification of Marginals**\n631: \n632: The $\\mathcal{Y}$-marginals of $P$ and $Q$ are precisely the push-forward measures:\n633: \n634: $$\n635: P_Y = K\\rho, \\quad Q_Y = K\\sigma\n636: \n637: $$\n638: \n639: **Proof**: For any measurable $B \\subseteq \\mathcal{Y}$:\n640: \n641: $$\n642: \\begin{aligned}\n643: P_Y(B) &= P(\\mathcal{X} \\times B) = \\int_{\\mathcal{X}} \\rho(dx) \\, K(x, B) = (K\\rho)(B)\n644: \\end{aligned}\n645: \n646: $$\n647: \n648: Similarly, $Q_Y(B) = (K\\sigma)(B)$. \u220e\n649: \n650: **Step 3: Absolute Continuity of Joint Measures**\n651: \n652: **Lemma**: If $\\rho \\ll \\sigma$, then $P \\ll Q$ on $\\mathcal{X} \\times \\mathcal{Y}$.\n653: \n654: **Proof**: Let $E \\subseteq \\mathcal{X} \\times \\mathcal{Y}$ be measurable with $Q(E) = 0$. By Fubini's theorem:\n655: \n656: $$\n657: 0 = Q(E) = \\int_{\\mathcal{X}} \\sigma(dx) \\int_{\\mathcal{Y}} \\mathbb{1}_E(x, y) \\, K(x, dy)\n658: \n659: $$\n660: \n661: This implies that for $\\sigma$-almost every $x \\in \\mathcal{X}$:\n662: \n663: $$\n664: K(x, E_x) = 0\n665: \n666: $$\n667: \n668: where $E_x := \\{y \\in \\mathcal{Y} : (x, y) \\in E\\}$. Since $\\rho \\ll \\sigma$, this property holds for $\\rho$-almost every $x$ as well. Therefore:\n669: \n670: $$\n671: P(E) = \\int_{\\mathcal{X}} \\rho(dx) \\, K(x, E_x) = 0\n672: \n673: $$\n674: \n675: Thus $P \\ll Q$. \u220e\n676: \n677: **Step 4: Radon-Nikodym Derivative Factorization**\n678: \n679: The Radon-Nikodym derivative of $P$ with respect to $Q$ satisfies:\n680: \n681: $$\n682: \\frac{dP}{dQ}(x, y) = \\frac{d\\rho}{d\\sigma}(x) \\quad Q\\text{-a.e.}\n683: \n684: $$\n685: \n686: **Proof**: From Step 3, $P \\ll Q$, so $\\frac{dP}{dQ}$ exists. By the disintegration theorem (Kallenberg, Theorem 6.3), we can write:\n687: \n688: $$\n689: \\begin{aligned}\n690: P(dx, dy) &= \\rho(dx) \\, K(x, dy) \\\\\n691: Q(dx, dy) &= \\sigma(dx) \\, K(x, dy)\n692: \\end{aligned}\n693: \n694: $$\n695: \n696: Since the conditional distributions $P_{Y|X=x} = K(x, \\cdot) = Q_{Y|X=x}$ coincide, the Radon-Nikodym derivative factorizes as:\n697: \n698: $$\n699: \\frac{dP}{dQ}(x, y) = \\frac{d\\rho}{d\\sigma}(x) \\cdot 1 = \\frac{d\\rho}{d\\sigma}(x)\n700: \n701: $$\n702: \n703: **Consequence**: The joint divergence is:\n704: \n705: $$\n706: \\begin{aligned}\n707: D(P \\| Q) &= \\int_{\\mathcal{X} \\times \\mathcal{Y}} \\log\\left(\\frac{d\\rho}{d\\sigma}(x)\\right) \\rho(dx) \\, K(x, dy) \\\\\n708: &= \\int_{\\mathcal{X}} \\log\\left(\\frac{d\\rho}{d\\sigma}(x)\\right) \\rho(dx) = D_{\\text{KL}}(\\rho \\| \\sigma)\n709: \\end{aligned}\n710: \n711: $$\n712: \n713: where we used $\\int_{\\mathcal{Y}} K(x, dy) = 1$ (normalization). \u220e\n714: \n715: **Step 5: Chain Rule for Relative Entropy**\n716: \n717: The **chain rule for KL-divergence** (Cover & Thomas 2006, Theorem 2.5.3) states: For probability measures $P, Q$ on $\\mathcal{X} \\times \\mathcal{Y}$ with $P \\ll Q$:\n718: \n719: $$\n720: D(P \\| Q) = D(P_Y \\| Q_Y) + \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, P_Y(dy)\n721: \n722: $$\n723: \n724: where $P_Y, Q_Y$ are the marginals on $\\mathcal{Y}$, and $P_{X|Y=y}, Q_{X|Y=y}$ are the regular conditional probabilities (which exist on standard Borel spaces).\n725: \n726: **Step 6: Derivation of the Data Processing Inequality**\n727: \n728: Applying the chain rule to $P$ and $Q$:\n729: \n730: $$\n731: D(P \\| Q) = D(P_Y \\| Q_Y) + \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, P_Y(dy)\n732: \n733: $$\n734: \n735: From Step 4, $D(P \\| Q) = D_{\\text{KL}}(\\rho \\| \\sigma)$. From Step 2, $P_Y = K\\rho$ and $Q_Y = K\\sigma$. Therefore:\n736: \n737: $$\n738: D_{\\text{KL}}(\\rho \\| \\sigma) = D_{\\text{KL}}(K\\rho \\| K\\sigma) + \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, (K\\rho)(dy)\n739: \n740: $$\n741: \n742: **Key Observation**: KL-divergence is always nonnegative:\n743: \n744: $$\n745: D(P_{X|Y=y} \\| Q_{X|Y=y}) \\ge 0 \\quad \\text{for all } y \\in \\mathcal{Y}\n746: \n747: $$\n748: \n749: Therefore:\n750: \n751: $$\n752: \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, (K\\rho)(dy) \\ge 0\n753: \n754: $$\n755: \n756: Dropping this nonnegative term yields:\n757: \n758: $$\n759: D_{\\text{KL}}(K\\rho \\| K\\sigma) \\le D_{\\text{KL}}(\\rho \\| \\sigma)\n760: \n761: $$\n762: ",
      "metadata": {
        "label": "proof-thm-data-processing"
      },
      "section": "## 2. Analysis of the Finite-N to Mean-Field Transition",
      "references": [],
      "raw_directive": "603: :::\n604: \n605: :::{prf:proof}\n606: :label: proof-thm-data-processing\n607: \n608: **Historical Context**: This theorem is a fundamental result in information theory, first stated by Kullback and formalized by Shannon. The proof follows the classical approach via the chain rule for relative entropy. Primary references: Cover & Thomas (2006, Theorem 2.8.1), Csisz\u00e1r & K\u00f6rner (2011, Section 1.2).\n609: \n610: **Step 0: Reduction to Finite KL-Divergence**\n611: \n612: If $\\rho \\not\\ll \\sigma$ (i.e., $\\rho$ is not absolutely continuous with respect to $\\sigma$), then $D_{\\text{KL}}(\\rho \\| \\sigma) = +\\infty$ by definition, and the inequality holds trivially. Therefore, we assume **$\\rho \\ll \\sigma$** and $D_{\\text{KL}}(\\rho \\| \\sigma) < \\infty$.\n613: \n614: **Step 1: Construction of Joint Distributions**\n615: \n616: Define probability measures $P$ and $Q$ on the product space $\\mathcal{X} \\times \\mathcal{Y}$ by:\n617: \n618: $$\n619: \\begin{aligned}\n620: P(A \\times B) &:= \\int_A \\rho(dx) \\, K(x, B), \\\\\n621: Q(A \\times B) &:= \\int_A \\sigma(dx) \\, K(x, B)\n622: \\end{aligned}\n623: \n624: $$\n625: \n626: for all measurable rectangles $A \\subseteq \\mathcal{X}$, $B \\subseteq \\mathcal{Y}$. By Carath\u00e9odory's extension theorem, these uniquely extend to probability measures on $\\mathcal{X} \\times \\mathcal{Y}$.\n627: \n628: **Interpretation**: The measure $P$ represents the joint distribution of a Markov chain $(X, Y)$ where $X \\sim \\rho$ and $Y \\sim K(X, \\cdot)$. Similarly, $Q$ corresponds to $X \\sim \\sigma$ and $Y \\sim K(X, \\cdot)$. Both chains use the **same kernel** $K$, differing only in the initial distribution.\n629: \n630: **Step 2: Identification of Marginals**\n631: \n632: The $\\mathcal{Y}$-marginals of $P$ and $Q$ are precisely the push-forward measures:\n633: \n634: $$\n635: P_Y = K\\rho, \\quad Q_Y = K\\sigma\n636: \n637: $$\n638: \n639: **Proof**: For any measurable $B \\subseteq \\mathcal{Y}$:\n640: \n641: $$\n642: \\begin{aligned}\n643: P_Y(B) &= P(\\mathcal{X} \\times B) = \\int_{\\mathcal{X}} \\rho(dx) \\, K(x, B) = (K\\rho)(B)\n644: \\end{aligned}\n645: \n646: $$\n647: \n648: Similarly, $Q_Y(B) = (K\\sigma)(B)$. \u220e\n649: \n650: **Step 3: Absolute Continuity of Joint Measures**\n651: \n652: **Lemma**: If $\\rho \\ll \\sigma$, then $P \\ll Q$ on $\\mathcal{X} \\times \\mathcal{Y}$.\n653: \n654: **Proof**: Let $E \\subseteq \\mathcal{X} \\times \\mathcal{Y}$ be measurable with $Q(E) = 0$. By Fubini's theorem:\n655: \n656: $$\n657: 0 = Q(E) = \\int_{\\mathcal{X}} \\sigma(dx) \\int_{\\mathcal{Y}} \\mathbb{1}_E(x, y) \\, K(x, dy)\n658: \n659: $$\n660: \n661: This implies that for $\\sigma$-almost every $x \\in \\mathcal{X}$:\n662: \n663: $$\n664: K(x, E_x) = 0\n665: \n666: $$\n667: \n668: where $E_x := \\{y \\in \\mathcal{Y} : (x, y) \\in E\\}$. Since $\\rho \\ll \\sigma$, this property holds for $\\rho$-almost every $x$ as well. Therefore:\n669: \n670: $$\n671: P(E) = \\int_{\\mathcal{X}} \\rho(dx) \\, K(x, E_x) = 0\n672: \n673: $$\n674: \n675: Thus $P \\ll Q$. \u220e\n676: \n677: **Step 4: Radon-Nikodym Derivative Factorization**\n678: \n679: The Radon-Nikodym derivative of $P$ with respect to $Q$ satisfies:\n680: \n681: $$\n682: \\frac{dP}{dQ}(x, y) = \\frac{d\\rho}{d\\sigma}(x) \\quad Q\\text{-a.e.}\n683: \n684: $$\n685: \n686: **Proof**: From Step 3, $P \\ll Q$, so $\\frac{dP}{dQ}$ exists. By the disintegration theorem (Kallenberg, Theorem 6.3), we can write:\n687: \n688: $$\n689: \\begin{aligned}\n690: P(dx, dy) &= \\rho(dx) \\, K(x, dy) \\\\\n691: Q(dx, dy) &= \\sigma(dx) \\, K(x, dy)\n692: \\end{aligned}\n693: \n694: $$\n695: \n696: Since the conditional distributions $P_{Y|X=x} = K(x, \\cdot) = Q_{Y|X=x}$ coincide, the Radon-Nikodym derivative factorizes as:\n697: \n698: $$\n699: \\frac{dP}{dQ}(x, y) = \\frac{d\\rho}{d\\sigma}(x) \\cdot 1 = \\frac{d\\rho}{d\\sigma}(x)\n700: \n701: $$\n702: \n703: **Consequence**: The joint divergence is:\n704: \n705: $$\n706: \\begin{aligned}\n707: D(P \\| Q) &= \\int_{\\mathcal{X} \\times \\mathcal{Y}} \\log\\left(\\frac{d\\rho}{d\\sigma}(x)\\right) \\rho(dx) \\, K(x, dy) \\\\\n708: &= \\int_{\\mathcal{X}} \\log\\left(\\frac{d\\rho}{d\\sigma}(x)\\right) \\rho(dx) = D_{\\text{KL}}(\\rho \\| \\sigma)\n709: \\end{aligned}\n710: \n711: $$\n712: \n713: where we used $\\int_{\\mathcal{Y}} K(x, dy) = 1$ (normalization). \u220e\n714: \n715: **Step 5: Chain Rule for Relative Entropy**\n716: \n717: The **chain rule for KL-divergence** (Cover & Thomas 2006, Theorem 2.5.3) states: For probability measures $P, Q$ on $\\mathcal{X} \\times \\mathcal{Y}$ with $P \\ll Q$:\n718: \n719: $$\n720: D(P \\| Q) = D(P_Y \\| Q_Y) + \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, P_Y(dy)\n721: \n722: $$\n723: \n724: where $P_Y, Q_Y$ are the marginals on $\\mathcal{Y}$, and $P_{X|Y=y}, Q_{X|Y=y}$ are the regular conditional probabilities (which exist on standard Borel spaces).\n725: \n726: **Step 6: Derivation of the Data Processing Inequality**\n727: \n728: Applying the chain rule to $P$ and $Q$:\n729: \n730: $$\n731: D(P \\| Q) = D(P_Y \\| Q_Y) + \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, P_Y(dy)\n732: \n733: $$\n734: \n735: From Step 4, $D(P \\| Q) = D_{\\text{KL}}(\\rho \\| \\sigma)$. From Step 2, $P_Y = K\\rho$ and $Q_Y = K\\sigma$. Therefore:\n736: \n737: $$\n738: D_{\\text{KL}}(\\rho \\| \\sigma) = D_{\\text{KL}}(K\\rho \\| K\\sigma) + \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, (K\\rho)(dy)\n739: \n740: $$\n741: \n742: **Key Observation**: KL-divergence is always nonnegative:\n743: \n744: $$\n745: D(P_{X|Y=y} \\| Q_{X|Y=y}) \\ge 0 \\quad \\text{for all } y \\in \\mathcal{Y}\n746: \n747: $$\n748: \n749: Therefore:\n750: \n751: $$\n752: \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, (K\\rho)(dy) \\ge 0\n753: \n754: $$\n755: \n756: Dropping this nonnegative term yields:\n757: \n758: $$\n759: D_{\\text{KL}}(K\\rho \\| K\\sigma) \\le D_{\\text{KL}}(\\rho \\| \\sigma)\n760: \n761: $$\n762: \n763: which is the Data Processing Inequality. \u220e"
    }
  ],
  "validation": {
    "ok": true,
    "errors": []
  }
}