{
  "document_id": "16_convergence_mean_field",
  "stage": "directives",
  "directive_type": "proof",
  "generated_at": "2025-11-10T13:24:29.537671+00:00",
  "count": 4,
  "items": [
    {
      "directive_type": "proof",
      "label": "proof-thm-data-processing",
      "title": null,
      "start_line": 603,
      "end_line": 763,
      "header_lines": [
        604
      ],
      "content_start": 606,
      "content_end": 762,
      "content": "606: :label: proof-thm-data-processing\n607: \n608: **Historical Context**: This theorem is a fundamental result in information theory, first stated by Kullback and formalized by Shannon. The proof follows the classical approach via the chain rule for relative entropy. Primary references: Cover & Thomas (2006, Theorem 2.8.1), Csiszár & Körner (2011, Section 1.2).\n609: \n610: **Step 0: Reduction to Finite KL-Divergence**\n611: \n612: If $\\rho \\not\\ll \\sigma$ (i.e., $\\rho$ is not absolutely continuous with respect to $\\sigma$), then $D_{\\text{KL}}(\\rho \\| \\sigma) = +\\infty$ by definition, and the inequality holds trivially. Therefore, we assume **$\\rho \\ll \\sigma$** and $D_{\\text{KL}}(\\rho \\| \\sigma) < \\infty$.\n613: \n614: **Step 1: Construction of Joint Distributions**\n615: \n616: Define probability measures $P$ and $Q$ on the product space $\\mathcal{X} \\times \\mathcal{Y}$ by:\n617: \n618: $$\n619: \\begin{aligned}\n620: P(A \\times B) &:= \\int_A \\rho(dx) \\, K(x, B), \\\\\n621: Q(A \\times B) &:= \\int_A \\sigma(dx) \\, K(x, B)\n622: \\end{aligned}\n623: \n624: $$\n625: \n626: for all measurable rectangles $A \\subseteq \\mathcal{X}$, $B \\subseteq \\mathcal{Y}$. By Carathéodory's extension theorem, these uniquely extend to probability measures on $\\mathcal{X} \\times \\mathcal{Y}$.\n627: \n628: **Interpretation**: The measure $P$ represents the joint distribution of a Markov chain $(X, Y)$ where $X \\sim \\rho$ and $Y \\sim K(X, \\cdot)$. Similarly, $Q$ corresponds to $X \\sim \\sigma$ and $Y \\sim K(X, \\cdot)$. Both chains use the **same kernel** $K$, differing only in the initial distribution.\n629: \n630: **Step 2: Identification of Marginals**\n631: \n632: The $\\mathcal{Y}$-marginals of $P$ and $Q$ are precisely the push-forward measures:\n633: \n634: $$\n635: P_Y = K\\rho, \\quad Q_Y = K\\sigma\n636: \n637: $$\n638: \n639: **Proof**: For any measurable $B \\subseteq \\mathcal{Y}$:\n640: \n641: $$\n642: \\begin{aligned}\n643: P_Y(B) &= P(\\mathcal{X} \\times B) = \\int_{\\mathcal{X}} \\rho(dx) \\, K(x, B) = (K\\rho)(B)\n644: \\end{aligned}\n645: \n646: $$\n647: \n648: Similarly, $Q_Y(B) = (K\\sigma)(B)$. ∎\n649: \n650: **Step 3: Absolute Continuity of Joint Measures**\n651: \n652: **Lemma**: If $\\rho \\ll \\sigma$, then $P \\ll Q$ on $\\mathcal{X} \\times \\mathcal{Y}$.\n653: \n654: **Proof**: Let $E \\subseteq \\mathcal{X} \\times \\mathcal{Y}$ be measurable with $Q(E) = 0$. By Fubini's theorem:\n655: \n656: $$\n657: 0 = Q(E) = \\int_{\\mathcal{X}} \\sigma(dx) \\int_{\\mathcal{Y}} \\mathbb{1}_E(x, y) \\, K(x, dy)\n658: \n659: $$\n660: \n661: This implies that for $\\sigma$-almost every $x \\in \\mathcal{X}$:\n662: \n663: $$\n664: K(x, E_x) = 0\n665: \n666: $$\n667: \n668: where $E_x := \\{y \\in \\mathcal{Y} : (x, y) \\in E\\}$. Since $\\rho \\ll \\sigma$, this property holds for $\\rho$-almost every $x$ as well. Therefore:\n669: \n670: $$\n671: P(E) = \\int_{\\mathcal{X}} \\rho(dx) \\, K(x, E_x) = 0\n672: \n673: $$\n674: \n675: Thus $P \\ll Q$. ∎\n676: \n677: **Step 4: Radon-Nikodym Derivative Factorization**\n678: \n679: The Radon-Nikodym derivative of $P$ with respect to $Q$ satisfies:\n680: \n681: $$\n682: \\frac{dP}{dQ}(x, y) = \\frac{d\\rho}{d\\sigma}(x) \\quad Q\\text{-a.e.}\n683: \n684: $$\n685: \n686: **Proof**: From Step 3, $P \\ll Q$, so $\\frac{dP}{dQ}$ exists. By the disintegration theorem (Kallenberg, Theorem 6.3), we can write:\n687: \n688: $$\n689: \\begin{aligned}\n690: P(dx, dy) &= \\rho(dx) \\, K(x, dy) \\\\\n691: Q(dx, dy) &= \\sigma(dx) \\, K(x, dy)\n692: \\end{aligned}\n693: \n694: $$\n695: \n696: Since the conditional distributions $P_{Y|X=x} = K(x, \\cdot) = Q_{Y|X=x}$ coincide, the Radon-Nikodym derivative factorizes as:\n697: \n698: $$\n699: \\frac{dP}{dQ}(x, y) = \\frac{d\\rho}{d\\sigma}(x) \\cdot 1 = \\frac{d\\rho}{d\\sigma}(x)\n700: \n701: $$\n702: \n703: **Consequence**: The joint divergence is:\n704: \n705: $$\n706: \\begin{aligned}\n707: D(P \\| Q) &= \\int_{\\mathcal{X} \\times \\mathcal{Y}} \\log\\left(\\frac{d\\rho}{d\\sigma}(x)\\right) \\rho(dx) \\, K(x, dy) \\\\\n708: &= \\int_{\\mathcal{X}} \\log\\left(\\frac{d\\rho}{d\\sigma}(x)\\right) \\rho(dx) = D_{\\text{KL}}(\\rho \\| \\sigma)\n709: \\end{aligned}\n710: \n711: $$\n712: \n713: where we used $\\int_{\\mathcal{Y}} K(x, dy) = 1$ (normalization). ∎\n714: \n715: **Step 5: Chain Rule for Relative Entropy**\n716: \n717: The **chain rule for KL-divergence** (Cover & Thomas 2006, Theorem 2.5.3) states: For probability measures $P, Q$ on $\\mathcal{X} \\times \\mathcal{Y}$ with $P \\ll Q$:\n718: \n719: $$\n720: D(P \\| Q) = D(P_Y \\| Q_Y) + \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, P_Y(dy)\n721: \n722: $$\n723: \n724: where $P_Y, Q_Y$ are the marginals on $\\mathcal{Y}$, and $P_{X|Y=y}, Q_{X|Y=y}$ are the regular conditional probabilities (which exist on standard Borel spaces).\n725: \n726: **Step 6: Derivation of the Data Processing Inequality**\n727: \n728: Applying the chain rule to $P$ and $Q$:\n729: \n730: $$\n731: D(P \\| Q) = D(P_Y \\| Q_Y) + \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, P_Y(dy)\n732: \n733: $$\n734: \n735: From Step 4, $D(P \\| Q) = D_{\\text{KL}}(\\rho \\| \\sigma)$. From Step 2, $P_Y = K\\rho$ and $Q_Y = K\\sigma$. Therefore:\n736: \n737: $$\n738: D_{\\text{KL}}(\\rho \\| \\sigma) = D_{\\text{KL}}(K\\rho \\| K\\sigma) + \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, (K\\rho)(dy)\n739: \n740: $$\n741: \n742: **Key Observation**: KL-divergence is always nonnegative:\n743: \n744: $$\n745: D(P_{X|Y=y} \\| Q_{X|Y=y}) \\ge 0 \\quad \\text{for all } y \\in \\mathcal{Y}\n746: \n747: $$\n748: \n749: Therefore:\n750: \n751: $$\n752: \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, (K\\rho)(dy) \\ge 0\n753: \n754: $$\n755: \n756: Dropping this nonnegative term yields:\n757: \n758: $$\n759: D_{\\text{KL}}(K\\rho \\| K\\sigma) \\le D_{\\text{KL}}(\\rho \\| \\sigma)\n760: \n761: $$\n762: ",
      "metadata": {
        "label": "proof-thm-data-processing"
      },
      "section": "## 2. Analysis of the Finite-N to Mean-Field Transition",
      "references": [],
      "raw_directive": "603: :::\n604: \n605: :::{prf:proof}\n606: :label: proof-thm-data-processing\n607: \n608: **Historical Context**: This theorem is a fundamental result in information theory, first stated by Kullback and formalized by Shannon. The proof follows the classical approach via the chain rule for relative entropy. Primary references: Cover & Thomas (2006, Theorem 2.8.1), Csiszár & Körner (2011, Section 1.2).\n609: \n610: **Step 0: Reduction to Finite KL-Divergence**\n611: \n612: If $\\rho \\not\\ll \\sigma$ (i.e., $\\rho$ is not absolutely continuous with respect to $\\sigma$), then $D_{\\text{KL}}(\\rho \\| \\sigma) = +\\infty$ by definition, and the inequality holds trivially. Therefore, we assume **$\\rho \\ll \\sigma$** and $D_{\\text{KL}}(\\rho \\| \\sigma) < \\infty$.\n613: \n614: **Step 1: Construction of Joint Distributions**\n615: \n616: Define probability measures $P$ and $Q$ on the product space $\\mathcal{X} \\times \\mathcal{Y}$ by:\n617: \n618: $$\n619: \\begin{aligned}\n620: P(A \\times B) &:= \\int_A \\rho(dx) \\, K(x, B), \\\\\n621: Q(A \\times B) &:= \\int_A \\sigma(dx) \\, K(x, B)\n622: \\end{aligned}\n623: \n624: $$\n625: \n626: for all measurable rectangles $A \\subseteq \\mathcal{X}$, $B \\subseteq \\mathcal{Y}$. By Carathéodory's extension theorem, these uniquely extend to probability measures on $\\mathcal{X} \\times \\mathcal{Y}$.\n627: \n628: **Interpretation**: The measure $P$ represents the joint distribution of a Markov chain $(X, Y)$ where $X \\sim \\rho$ and $Y \\sim K(X, \\cdot)$. Similarly, $Q$ corresponds to $X \\sim \\sigma$ and $Y \\sim K(X, \\cdot)$. Both chains use the **same kernel** $K$, differing only in the initial distribution.\n629: \n630: **Step 2: Identification of Marginals**\n631: \n632: The $\\mathcal{Y}$-marginals of $P$ and $Q$ are precisely the push-forward measures:\n633: \n634: $$\n635: P_Y = K\\rho, \\quad Q_Y = K\\sigma\n636: \n637: $$\n638: \n639: **Proof**: For any measurable $B \\subseteq \\mathcal{Y}$:\n640: \n641: $$\n642: \\begin{aligned}\n643: P_Y(B) &= P(\\mathcal{X} \\times B) = \\int_{\\mathcal{X}} \\rho(dx) \\, K(x, B) = (K\\rho)(B)\n644: \\end{aligned}\n645: \n646: $$\n647: \n648: Similarly, $Q_Y(B) = (K\\sigma)(B)$. ∎\n649: \n650: **Step 3: Absolute Continuity of Joint Measures**\n651: \n652: **Lemma**: If $\\rho \\ll \\sigma$, then $P \\ll Q$ on $\\mathcal{X} \\times \\mathcal{Y}$.\n653: \n654: **Proof**: Let $E \\subseteq \\mathcal{X} \\times \\mathcal{Y}$ be measurable with $Q(E) = 0$. By Fubini's theorem:\n655: \n656: $$\n657: 0 = Q(E) = \\int_{\\mathcal{X}} \\sigma(dx) \\int_{\\mathcal{Y}} \\mathbb{1}_E(x, y) \\, K(x, dy)\n658: \n659: $$\n660: \n661: This implies that for $\\sigma$-almost every $x \\in \\mathcal{X}$:\n662: \n663: $$\n664: K(x, E_x) = 0\n665: \n666: $$\n667: \n668: where $E_x := \\{y \\in \\mathcal{Y} : (x, y) \\in E\\}$. Since $\\rho \\ll \\sigma$, this property holds for $\\rho$-almost every $x$ as well. Therefore:\n669: \n670: $$\n671: P(E) = \\int_{\\mathcal{X}} \\rho(dx) \\, K(x, E_x) = 0\n672: \n673: $$\n674: \n675: Thus $P \\ll Q$. ∎\n676: \n677: **Step 4: Radon-Nikodym Derivative Factorization**\n678: \n679: The Radon-Nikodym derivative of $P$ with respect to $Q$ satisfies:\n680: \n681: $$\n682: \\frac{dP}{dQ}(x, y) = \\frac{d\\rho}{d\\sigma}(x) \\quad Q\\text{-a.e.}\n683: \n684: $$\n685: \n686: **Proof**: From Step 3, $P \\ll Q$, so $\\frac{dP}{dQ}$ exists. By the disintegration theorem (Kallenberg, Theorem 6.3), we can write:\n687: \n688: $$\n689: \\begin{aligned}\n690: P(dx, dy) &= \\rho(dx) \\, K(x, dy) \\\\\n691: Q(dx, dy) &= \\sigma(dx) \\, K(x, dy)\n692: \\end{aligned}\n693: \n694: $$\n695: \n696: Since the conditional distributions $P_{Y|X=x} = K(x, \\cdot) = Q_{Y|X=x}$ coincide, the Radon-Nikodym derivative factorizes as:\n697: \n698: $$\n699: \\frac{dP}{dQ}(x, y) = \\frac{d\\rho}{d\\sigma}(x) \\cdot 1 = \\frac{d\\rho}{d\\sigma}(x)\n700: \n701: $$\n702: \n703: **Consequence**: The joint divergence is:\n704: \n705: $$\n706: \\begin{aligned}\n707: D(P \\| Q) &= \\int_{\\mathcal{X} \\times \\mathcal{Y}} \\log\\left(\\frac{d\\rho}{d\\sigma}(x)\\right) \\rho(dx) \\, K(x, dy) \\\\\n708: &= \\int_{\\mathcal{X}} \\log\\left(\\frac{d\\rho}{d\\sigma}(x)\\right) \\rho(dx) = D_{\\text{KL}}(\\rho \\| \\sigma)\n709: \\end{aligned}\n710: \n711: $$\n712: \n713: where we used $\\int_{\\mathcal{Y}} K(x, dy) = 1$ (normalization). ∎\n714: \n715: **Step 5: Chain Rule for Relative Entropy**\n716: \n717: The **chain rule for KL-divergence** (Cover & Thomas 2006, Theorem 2.5.3) states: For probability measures $P, Q$ on $\\mathcal{X} \\times \\mathcal{Y}$ with $P \\ll Q$:\n718: \n719: $$\n720: D(P \\| Q) = D(P_Y \\| Q_Y) + \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, P_Y(dy)\n721: \n722: $$\n723: \n724: where $P_Y, Q_Y$ are the marginals on $\\mathcal{Y}$, and $P_{X|Y=y}, Q_{X|Y=y}$ are the regular conditional probabilities (which exist on standard Borel spaces).\n725: \n726: **Step 6: Derivation of the Data Processing Inequality**\n727: \n728: Applying the chain rule to $P$ and $Q$:\n729: \n730: $$\n731: D(P \\| Q) = D(P_Y \\| Q_Y) + \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, P_Y(dy)\n732: \n733: $$\n734: \n735: From Step 4, $D(P \\| Q) = D_{\\text{KL}}(\\rho \\| \\sigma)$. From Step 2, $P_Y = K\\rho$ and $Q_Y = K\\sigma$. Therefore:\n736: \n737: $$\n738: D_{\\text{KL}}(\\rho \\| \\sigma) = D_{\\text{KL}}(K\\rho \\| K\\sigma) + \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, (K\\rho)(dy)\n739: \n740: $$\n741: \n742: **Key Observation**: KL-divergence is always nonnegative:\n743: \n744: $$\n745: D(P_{X|Y=y} \\| Q_{X|Y=y}) \\ge 0 \\quad \\text{for all } y \\in \\mathcal{Y}\n746: \n747: $$\n748: \n749: Therefore:\n750: \n751: $$\n752: \\int_{\\mathcal{Y}} D(P_{X|Y=y} \\| Q_{X|Y=y}) \\, (K\\rho)(dy) \\ge 0\n753: \n754: $$\n755: \n756: Dropping this nonnegative term yields:\n757: \n758: $$\n759: D_{\\text{KL}}(K\\rho \\| K\\sigma) \\le D_{\\text{KL}}(\\rho \\| \\sigma)\n760: \n761: $$\n762: \n763: which is the Data Processing Inequality. ∎",
      "_registry_context": {
        "stage": "directives",
        "document_id": "16_convergence_mean_field",
        "chapter_index": 7,
        "chapter_file": "chapter_7.json",
        "section_id": "## 2. Analysis of the Finite-N to Mean-Field Transition"
      }
    },
    {
      "directive_type": "proof",
      "label": "proof-thm-stage0-complete",
      "title": null,
      "start_line": 1270,
      "end_line": 1352,
      "header_lines": [
        1271
      ],
      "content_start": 1273,
      "content_end": 1351,
      "content": "1273: :label: proof-thm-stage0-complete\n1274: \n1275: We establish the three statements through direct KL entropy production analysis.\n1276: \n1277: **Framework Setup**: Let $\\rho \\in L^1_+(\\Omega)$ be the unnormalized density with $\\|\\rho\\| = \\int_\\Omega \\rho \\, dxdv \\le 1$. The KL-divergence for unnormalized densities is $D_{\\text{KL}}(\\rho \\| \\pi) = \\int_\\Omega \\rho \\log(\\rho/\\pi) \\, dxdv$, which decomposes as:\n1278: \n1279: $$\n1280: D_{\\text{KL}}(\\rho \\| \\pi) = \\|\\rho\\| D_{\\text{KL}}(\\tilde{\\rho} \\| \\pi) + \\|\\rho\\| \\log \\|\\rho\\|\n1281: \n1282: $$\n1283: \n1284: where $\\tilde{\\rho} = \\rho/\\|\\rho\\|$ is the normalized density.\n1285: \n1286: **Statement 1: Revival is KL-expansive**\n1287: \n1288: The revival operator acts as $\\mathcal{L}_{\\text{revival}}[\\rho] = \\lambda_{\\text{revive}} m_d \\rho/\\|\\rho\\|$ where $m_d = 1 - \\|\\rho\\|$ is the dead mass. Using the Gateaux derivative formula for KL-divergence:\n1289: \n1290: $$\n1291: \\frac{d}{dt} D_{\\text{KL}}(\\rho \\| \\pi)\\bigg|_{\\text{revival}} = \\int_\\Omega \\mathcal{L}_{\\text{revival}}[\\rho] \\left(\\log \\frac{\\rho}{\\pi} + 1\\right) dx dv\n1292: \n1293: $$\n1294: \n1295: Substituting the revival operator:\n1296: \n1297: $$\n1298: = \\int_\\Omega \\lambda_{\\text{revive}} m_d \\frac{\\rho}{\\|\\rho\\|} \\left(\\log \\frac{\\rho}{\\pi} + 1\\right) dx dv = \\lambda_{\\text{revive}} m_d \\int_\\Omega \\tilde{\\rho} \\left(\\log \\frac{\\rho}{\\pi} + 1\\right) dx dv\n1299: \n1300: $$\n1301: \n1302: Since $\\int_\\Omega \\tilde{\\rho} \\, dxdv = 1$ (normalization), we have:\n1303: \n1304: $$\n1305: \\int_\\Omega \\tilde{\\rho} \\cdot 1 \\, dxdv = 1\n1306: \n1307: $$\n1308: \n1309: And using $\\log(\\rho/\\pi) = \\log \\tilde{\\rho} + \\log \\|\\rho\\| - \\log \\pi$:\n1310: \n1311: $$\n1312: \\int_\\Omega \\tilde{\\rho} \\log \\frac{\\rho}{\\pi} \\, dxdv = D_{\\text{KL}}(\\tilde{\\rho} \\| \\pi) + \\log \\|\\rho\\|\n1313: \n1314: $$\n1315: \n1316: Combining:\n1317: \n1318: $$\n1319: \\frac{d}{dt} D_{\\text{KL}}\\bigg|_{\\text{revival}} = \\lambda_{\\text{revive}} m_d \\left(D_{\\text{KL}}(\\tilde{\\rho} \\| \\pi) + \\log \\|\\rho\\| + 1\\right) = \\lambda_{\\text{revive}} m_d \\left(1 + \\frac{D_{\\text{KL}}(\\rho \\| \\pi)}{\\|\\rho\\|}\\right)\n1320: \n1321: $$\n1322: \n1323: Since $D_{\\text{KL}}(\\rho \\| \\pi) \\ge 0$ with equality iff $\\rho = \\|\\rho\\| \\pi$, we have the entropy production is strictly positive for all $\\rho \\not\\propto \\pi$ when $m_d > 0$.\n1324: \n1325: **Statement 2: Joint jump not contractive**\n1326: \n1327: The joint jump operator combines killing and revival: $\\mathcal{L}_{\\text{jump}} = -\\kappa_{\\text{kill}}(x)\\rho + \\lambda_{\\text{revive}} m_d \\rho/\\|\\rho\\|$. The killing contribution is:\n1328: \n1329: $$\n1330: \\frac{d}{dt} D_{\\text{KL}}\\bigg|_{\\text{killing}} = -\\int_\\Omega \\kappa_{\\text{kill}}(x) \\rho \\left(\\log \\frac{\\rho}{\\pi} + 1\\right) dx dv\n1331: \n1332: $$\n1333: \n1334: This is negative (contractive) when $\\log(\\rho/\\pi) > -1$, but can be positive otherwise. The joint entropy production $\\frac{d}{dt} D_{\\text{KL}}|_{\\text{jump}}$ combines killing (potentially contractive) and revival (always expansive), with sign depending on $\\|\\rho\\|$. Therefore, it is not unconditionally contractive.\n1335: \n1336: **Statement 3: Kinetic dominance necessity**\n1337: \n1338: From the generator decomposition $\\mathcal{L} = \\mathcal{L}_{\\text{kin}} + \\mathcal{L}_{\\text{jump}}$ and Statement 1, the jump operator contributes positive entropy production. For exponential KL-convergence $D_{\\text{KL}}(\\rho(t) \\| \\pi) \\to 0$, we require:\n1339: \n1340: $$\n1341: \\frac{d}{dt} D_{\\text{KL}}(\\rho \\| \\pi) = \\frac{d}{dt} D_{\\text{KL}}\\bigg|_{\\text{kin}} + \\frac{d}{dt} D_{\\text{KL}}\\bigg|_{\\text{jump}} < 0\n1342: \n1343: $$\n1344: \n1345: Since the jump term is positive (expansive), this necessitates:\n1346: \n1347: $$\n1348: \\left|\\frac{d}{dt} D_{\\text{KL}}\\bigg|_{\\text{kin}}\\right| > \\frac{d}{dt} D_{\\text{KL}}\\bigg|_{\\text{jump}}\n1349: \n1350: $$\n1351: ",
      "metadata": {
        "label": "proof-thm-stage0-complete"
      },
      "section": "## 8. Stage 0 Conclusion",
      "references": [],
      "raw_directive": "1270: :::\n1271: \n1272: :::{prf:proof}\n1273: :label: proof-thm-stage0-complete\n1274: \n1275: We establish the three statements through direct KL entropy production analysis.\n1276: \n1277: **Framework Setup**: Let $\\rho \\in L^1_+(\\Omega)$ be the unnormalized density with $\\|\\rho\\| = \\int_\\Omega \\rho \\, dxdv \\le 1$. The KL-divergence for unnormalized densities is $D_{\\text{KL}}(\\rho \\| \\pi) = \\int_\\Omega \\rho \\log(\\rho/\\pi) \\, dxdv$, which decomposes as:\n1278: \n1279: $$\n1280: D_{\\text{KL}}(\\rho \\| \\pi) = \\|\\rho\\| D_{\\text{KL}}(\\tilde{\\rho} \\| \\pi) + \\|\\rho\\| \\log \\|\\rho\\|\n1281: \n1282: $$\n1283: \n1284: where $\\tilde{\\rho} = \\rho/\\|\\rho\\|$ is the normalized density.\n1285: \n1286: **Statement 1: Revival is KL-expansive**\n1287: \n1288: The revival operator acts as $\\mathcal{L}_{\\text{revival}}[\\rho] = \\lambda_{\\text{revive}} m_d \\rho/\\|\\rho\\|$ where $m_d = 1 - \\|\\rho\\|$ is the dead mass. Using the Gateaux derivative formula for KL-divergence:\n1289: \n1290: $$\n1291: \\frac{d}{dt} D_{\\text{KL}}(\\rho \\| \\pi)\\bigg|_{\\text{revival}} = \\int_\\Omega \\mathcal{L}_{\\text{revival}}[\\rho] \\left(\\log \\frac{\\rho}{\\pi} + 1\\right) dx dv\n1292: \n1293: $$\n1294: \n1295: Substituting the revival operator:\n1296: \n1297: $$\n1298: = \\int_\\Omega \\lambda_{\\text{revive}} m_d \\frac{\\rho}{\\|\\rho\\|} \\left(\\log \\frac{\\rho}{\\pi} + 1\\right) dx dv = \\lambda_{\\text{revive}} m_d \\int_\\Omega \\tilde{\\rho} \\left(\\log \\frac{\\rho}{\\pi} + 1\\right) dx dv\n1299: \n1300: $$\n1301: \n1302: Since $\\int_\\Omega \\tilde{\\rho} \\, dxdv = 1$ (normalization), we have:\n1303: \n1304: $$\n1305: \\int_\\Omega \\tilde{\\rho} \\cdot 1 \\, dxdv = 1\n1306: \n1307: $$\n1308: \n1309: And using $\\log(\\rho/\\pi) = \\log \\tilde{\\rho} + \\log \\|\\rho\\| - \\log \\pi$:\n1310: \n1311: $$\n1312: \\int_\\Omega \\tilde{\\rho} \\log \\frac{\\rho}{\\pi} \\, dxdv = D_{\\text{KL}}(\\tilde{\\rho} \\| \\pi) + \\log \\|\\rho\\|\n1313: \n1314: $$\n1315: \n1316: Combining:\n1317: \n1318: $$\n1319: \\frac{d}{dt} D_{\\text{KL}}\\bigg|_{\\text{revival}} = \\lambda_{\\text{revive}} m_d \\left(D_{\\text{KL}}(\\tilde{\\rho} \\| \\pi) + \\log \\|\\rho\\| + 1\\right) = \\lambda_{\\text{revive}} m_d \\left(1 + \\frac{D_{\\text{KL}}(\\rho \\| \\pi)}{\\|\\rho\\|}\\right)\n1320: \n1321: $$\n1322: \n1323: Since $D_{\\text{KL}}(\\rho \\| \\pi) \\ge 0$ with equality iff $\\rho = \\|\\rho\\| \\pi$, we have the entropy production is strictly positive for all $\\rho \\not\\propto \\pi$ when $m_d > 0$.\n1324: \n1325: **Statement 2: Joint jump not contractive**\n1326: \n1327: The joint jump operator combines killing and revival: $\\mathcal{L}_{\\text{jump}} = -\\kappa_{\\text{kill}}(x)\\rho + \\lambda_{\\text{revive}} m_d \\rho/\\|\\rho\\|$. The killing contribution is:\n1328: \n1329: $$\n1330: \\frac{d}{dt} D_{\\text{KL}}\\bigg|_{\\text{killing}} = -\\int_\\Omega \\kappa_{\\text{kill}}(x) \\rho \\left(\\log \\frac{\\rho}{\\pi} + 1\\right) dx dv\n1331: \n1332: $$\n1333: \n1334: This is negative (contractive) when $\\log(\\rho/\\pi) > -1$, but can be positive otherwise. The joint entropy production $\\frac{d}{dt} D_{\\text{KL}}|_{\\text{jump}}$ combines killing (potentially contractive) and revival (always expansive), with sign depending on $\\|\\rho\\|$. Therefore, it is not unconditionally contractive.\n1335: \n1336: **Statement 3: Kinetic dominance necessity**\n1337: \n1338: From the generator decomposition $\\mathcal{L} = \\mathcal{L}_{\\text{kin}} + \\mathcal{L}_{\\text{jump}}$ and Statement 1, the jump operator contributes positive entropy production. For exponential KL-convergence $D_{\\text{KL}}(\\rho(t) \\| \\pi) \\to 0$, we require:\n1339: \n1340: $$\n1341: \\frac{d}{dt} D_{\\text{KL}}(\\rho \\| \\pi) = \\frac{d}{dt} D_{\\text{KL}}\\bigg|_{\\text{kin}} + \\frac{d}{dt} D_{\\text{KL}}\\bigg|_{\\text{jump}} < 0\n1342: \n1343: $$\n1344: \n1345: Since the jump term is positive (expansive), this necessitates:\n1346: \n1347: $$\n1348: \\left|\\frac{d}{dt} D_{\\text{KL}}\\bigg|_{\\text{kin}}\\right| > \\frac{d}{dt} D_{\\text{KL}}\\bigg|_{\\text{jump}}\n1349: \n1350: $$\n1351: \n1352: The kinetic dissipation must dominate the jump expansion. This completes the proof.",
      "_registry_context": {
        "stage": "directives",
        "document_id": "16_convergence_mean_field",
        "chapter_index": 13,
        "chapter_file": "chapter_13.json",
        "section_id": "## 8. Stage 0 Conclusion"
      }
    },
    {
      "directive_type": "proof",
      "label": "proof-thm-lsi-constant-explicit",
      "title": null,
      "start_line": 4076,
      "end_line": 4108,
      "header_lines": [
        4077
      ],
      "content_start": 4078,
      "content_end": 4107,
      "content": "4078: :::{prf:proof}\n4079: :label: proof-thm-lsi-constant-explicit\n4080: The proof follows from the Holley-Stroock perturbation theorem. The reference Gaussian measure $\\mu(v) = (2\\pi/\\alpha_{\\exp})^{-d/2} e^{-\\alpha_{\\exp}|v|^2/2}$ has LSI constant $\\lambda_0 = \\alpha_{\\exp}$.\n4081: \n4082: The log-ratio $\\log(\\rho_\\infty^x / \\mu)$ satisfies:\n4083: \n4084: $$\n4085: \\left|\\Delta_v \\log \\frac{\\rho_\\infty^x}{\\mu}\\right| = |\\Delta_v \\log \\rho_\\infty^x - \\Delta_v \\log \\mu| = |\\Delta_v \\log \\rho_\\infty^x + \\alpha_{\\exp} d|\n4086: \n4087: $$\n4088: \n4089: Using $\\|\\Delta_v \\log \\rho_\\infty\\|_{L^\\infty} \\le C_{\\Delta v}$:\n4090: \n4091: $$\n4092: \\left|\\Delta_v \\log \\frac{\\rho_\\infty^x}{\\mu}\\right| \\le C_{\\Delta v} + \\alpha_{\\exp} d\n4093: \n4094: $$\n4095: \n4096: The Holley-Stroock theorem gives:\n4097: \n4098: $$\n4099: \\lambda_{\\text{LSI}} \\ge \\frac{\\lambda_0}{1 + C_{\\text{perturb}}/\\lambda_0}\n4100: \n4101: $$\n4102: \n4103: where $C_{\\text{perturb}} = C_{\\Delta v}$. Substituting $\\lambda_0 = \\alpha_{\\exp}$:\n4104: \n4105: $$\n4106: \\lambda_{\\text{LSI}} \\ge \\frac{\\alpha_{\\exp}}{1 + C_{\\Delta v}/\\alpha_{\\exp}}\n4107: ",
      "metadata": {
        "label": "proof-thm-lsi-constant-explicit"
      },
      "section": "## 2. Log-Sobolev Inequality for the QSD",
      "references": [],
      "raw_directive": "4076: :::\n4077: \n4078: :::{prf:proof}\n4079: :label: proof-thm-lsi-constant-explicit\n4080: The proof follows from the Holley-Stroock perturbation theorem. The reference Gaussian measure $\\mu(v) = (2\\pi/\\alpha_{\\exp})^{-d/2} e^{-\\alpha_{\\exp}|v|^2/2}$ has LSI constant $\\lambda_0 = \\alpha_{\\exp}$.\n4081: \n4082: The log-ratio $\\log(\\rho_\\infty^x / \\mu)$ satisfies:\n4083: \n4084: $$\n4085: \\left|\\Delta_v \\log \\frac{\\rho_\\infty^x}{\\mu}\\right| = |\\Delta_v \\log \\rho_\\infty^x - \\Delta_v \\log \\mu| = |\\Delta_v \\log \\rho_\\infty^x + \\alpha_{\\exp} d|\n4086: \n4087: $$\n4088: \n4089: Using $\\|\\Delta_v \\log \\rho_\\infty\\|_{L^\\infty} \\le C_{\\Delta v}$:\n4090: \n4091: $$\n4092: \\left|\\Delta_v \\log \\frac{\\rho_\\infty^x}{\\mu}\\right| \\le C_{\\Delta v} + \\alpha_{\\exp} d\n4093: \n4094: $$\n4095: \n4096: The Holley-Stroock theorem gives:\n4097: \n4098: $$\n4099: \\lambda_{\\text{LSI}} \\ge \\frac{\\lambda_0}{1 + C_{\\text{perturb}}/\\lambda_0}\n4100: \n4101: $$\n4102: \n4103: where $C_{\\text{perturb}} = C_{\\Delta v}$. Substituting $\\lambda_0 = \\alpha_{\\exp}$:\n4104: \n4105: $$\n4106: \\lambda_{\\text{LSI}} \\ge \\frac{\\alpha_{\\exp}}{1 + C_{\\Delta v}/\\alpha_{\\exp}}\n4107: \n4108: $$",
      "_registry_context": {
        "stage": "directives",
        "document_id": "16_convergence_mean_field",
        "chapter_index": 28,
        "chapter_file": "chapter_28.json",
        "section_id": "## 2. Log-Sobolev Inequality for the QSD"
      }
    },
    {
      "directive_type": "proof",
      "label": "proof-thm-mean-field-lsi-main",
      "title": null,
      "start_line": 6157,
      "end_line": 6348,
      "header_lines": [
        6158
      ],
      "content_start": 6159,
      "content_end": 6347,
      "content": "6159: :::{prf:proof}\n6160: :label: proof-thm-mean-field-lsi-main\n6161: We assemble the proof from the established results in Stages 0-2.\n6162: \n6163: **Step 1: Full Generator Entropy Production Equation**\n6164: \n6165: From Stage 1, Section 7.1 (Final Equation), the time derivative of KL-divergence is:\n6166: \n6167: $$\n6168: \\frac{d}{dt} D_{\\text{KL}}(\\rho_t \\| \\rho_\\infty) = -\\frac{\\sigma^2}{2} \\mathcal{I}_v(\\rho_t \\| \\rho_\\infty) + R_{\\text{coupling}}[\\rho_t] + \\mathcal{I}_{\\text{jump}}[\\rho_t]\n6169: \n6170: $$\n6171: \n6172: where:\n6173: - $\\mathcal{I}_v(\\rho \\| \\rho_\\infty)$ is the relative Fisher information in velocity\n6174: - $R_{\\text{coupling}}[\\rho]$ collects all mean-field coupling terms\n6175: - $\\mathcal{I}_{\\text{jump}}[\\rho]$ is the entropy production from the jump operator\n6176: \n6177: **Step 2: Bound the Coupling Terms**\n6178: \n6179: From Stage 2, Section 3.3 (Final Coupling Bound), we have:\n6180: \n6181: $$\n6182: |R_{\\text{coupling}}[\\rho]| \\le C_{\\text{KL}}^{\\text{coup}} \\cdot D_{\\text{KL}}(\\rho \\| \\rho_\\infty) + C_{\\text{Fisher}}^{\\text{coup}} \\cdot \\mathcal{I}_v(\\rho \\| \\rho_\\infty) + C_0^{\\text{coup}}\n6183: \n6184: $$\n6185: \n6186: where the constants $C_{\\text{KL}}^{\\text{coup}}$, $C_{\\text{Fisher}}^{\\text{coup}}$, $C_0^{\\text{coup}}$ are explicit formulas from Stage 2, Section 1.3.\n6187: \n6188: **Step 3: Bound the Jump Operator Expansion**\n6189: \n6190: From Stage 2, Section 4.4 (Jump Operator Bound), we have:\n6191: \n6192: $$\n6193: \\mathcal{I}_{\\text{jump}}[\\rho] \\le A_{\\text{jump}} \\cdot D_{\\text{KL}}(\\rho \\| \\rho_\\infty) + B_{\\text{jump}}\n6194: \n6195: $$\n6196: \n6197: where $A_{\\text{jump}}$ and $B_{\\text{jump}}$ are given in Stage 2, Section 1.4.\n6198: \n6199: **Step 4: Apply the Log-Sobolev Inequality**\n6200: \n6201: The QSD $\\rho_\\infty$ satisfies regularity properties R1-R6 (proven in Stage 0.5, Section 3). Therefore, by Stage 2, Theorem `thm-lsi-qsd`, it admits a Log-Sobolev inequality:\n6202: \n6203: $$\n6204: \\mathcal{I}_v(\\rho \\| \\rho_\\infty) \\ge 2\\lambda_{\\text{LSI}} D_{\\text{KL}}(\\rho \\| \\rho_\\infty)\n6205: \n6206: $$\n6207: \n6208: However, we need to relate the standard Fisher information $\\mathcal{I}_v(\\rho)$ to the relative one. By Stage 2, Lemma `lem-fisher-bound`:\n6209: \n6210: $$\n6211: \\mathcal{I}_v(\\rho) \\ge 2\\lambda_{\\text{LSI}} D_{\\text{KL}}(\\rho \\| \\rho_\\infty) - C_{\\text{LSI}}\n6212: \n6213: $$\n6214: \n6215: where $C_{\\text{LSI}}$ is the constant from the LSI remainder.\n6216: \n6217: **Step 5: Assemble the Grönwall Inequality**\n6218: \n6219: Substitute Steps 2, 3, 4 into Step 1. Using the notation $D := D_{\\text{KL}}(\\rho_t \\| \\rho_\\infty)$ and $I := \\mathcal{I}_v(\\rho_t \\| \\rho_\\infty)$:\n6220: \n6221: $$\n6222: \\begin{align*}\n6223: \\frac{d D}{dt} &= -\\frac{\\sigma^2}{2} I + R_{\\text{coupling}} + \\mathcal{I}_{\\text{jump}} \\\\\n6224: &\\le -\\frac{\\sigma^2}{2} I + C_{\\text{KL}}^{\\text{coup}} D + C_{\\text{Fisher}}^{\\text{coup}} I + C_0^{\\text{coup}} + A_{\\text{jump}} D + B_{\\text{jump}} \\\\\n6225: &= -\\frac{\\sigma^2}{2} I + C_{\\text{Fisher}}^{\\text{coup}} I + (C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}) D + (C_0^{\\text{coup}} + B_{\\text{jump}})\n6226: \\end{align*}\n6227: \n6228: $$\n6229: \n6230: Factor the Fisher information term:\n6231: \n6232: $$\n6233: \\frac{d D}{dt} \\le -\\left(\\frac{\\sigma^2}{2} - C_{\\text{Fisher}}^{\\text{coup}}\\right) I + (C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}) D + (C_0^{\\text{coup}} + B_{\\text{jump}})\n6234: \n6235: $$\n6236: \n6237: Now apply the LSI bound from Step 4:\n6238: \n6239: $$\n6240: I \\ge 2\\lambda_{\\text{LSI}} D - C_{\\text{LSI}}\n6241: \n6242: $$\n6243: \n6244: Substitute:\n6245: \n6246: $$\n6247: \\begin{align*}\n6248: \\frac{d D}{dt} &\\le -\\left(\\frac{\\sigma^2}{2} - C_{\\text{Fisher}}^{\\text{coup}}\\right) \\left(2\\lambda_{\\text{LSI}} D - C_{\\text{LSI}}\\right) + (C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}) D + (C_0^{\\text{coup}} + B_{\\text{jump}}) \\\\\n6249: &= -\\left(\\frac{\\sigma^2}{2} - C_{\\text{Fisher}}^{\\text{coup}}\\right) 2\\lambda_{\\text{LSI}} D + \\left(\\frac{\\sigma^2}{2} - C_{\\text{Fisher}}^{\\text{coup}}\\right) C_{\\text{LSI}} \\\\\n6250: &\\quad + (C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}) D + (C_0^{\\text{coup}} + B_{\\text{jump}})\n6251: \\end{align*}\n6252: \n6253: $$\n6254: \n6255: Collect terms proportional to $D$:\n6256: \n6257: $$\n6258: \\begin{align*}\n6259: \\frac{d D}{dt} &\\le \\left[-({\\sigma^2} - 2C_{\\text{Fisher}}^{\\text{coup}}) \\lambda_{\\text{LSI}} + C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}\\right] D \\\\\n6260: &\\quad + \\frac{\\sigma^2}{2} C_{\\text{LSI}} + C_0^{\\text{coup}} + B_{\\text{jump}}\n6261: \\end{align*}\n6262: \n6263: $$\n6264: \n6265: Factoring out the negative sign from the coefficient of $D$:\n6266: \n6267: $$\n6268: \\frac{d D}{dt} \\le -\\left[\\lambda_{\\text{LSI}} \\sigma^2 - 2\\lambda_{\\text{LSI}}C_{\\text{Fisher}}^{\\text{coup}} - C_{\\text{KL}}^{\\text{coup}} - A_{\\text{jump}}\\right] D + C_{\\text{offset}}\n6269: \n6270: $$\n6271: \n6272: Define:\n6273: \n6274: $$\n6275: \\delta := \\lambda_{\\text{LSI}} \\sigma^2 - 2\\lambda_{\\text{LSI}}C_{\\text{Fisher}}^{\\text{coup}} - C_{\\text{KL}}^{\\text{coup}} - A_{\\text{jump}}\n6276: \n6277: $$\n6278: \n6279: $$\n6280: C_{\\text{offset}} := \\frac{\\sigma^2}{2} C_{\\text{LSI}} + C_0^{\\text{coup}} + B_{\\text{jump}}\n6281: \n6282: $$\n6283: \n6284: Then:\n6285: \n6286: $$\n6287: \\frac{d D}{dt} \\le -\\delta \\cdot D + C_{\\text{offset}}\n6288: \n6289: $$\n6290: \n6291: This is the **Grönwall differential inequality**.\n6292: \n6293: **Step 6: State the Kinetic Dominance Condition**\n6294: \n6295: For exponential convergence, we require $\\delta > 0$. This is the **Kinetic Dominance Condition**:\n6296: \n6297: $$\n6298: \\lambda_{\\text{LSI}} \\sigma^2 > 2\\lambda_{\\text{LSI}}C_{\\text{Fisher}}^{\\text{coup}} + C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}\n6299: \n6300: $$\n6301: \n6302: Equivalently, rearranging for $\\sigma^2$:\n6303: \n6304: $$\n6305: \\sigma^2 > \\sigma_{\\text{crit}}^2 := \\frac{2C_{\\text{Fisher}}^{\\text{coup}}}{\\lambda_{\\text{LSI}}} + \\frac{C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}}{\\lambda_{\\text{LSI}}}\n6306: \n6307: $$\n6308: \n6309: **Physical meaning**: The velocity diffusion must be strong enough for the hypocoercive dissipation to overcome the destabilizing effects from mean-field coupling and the jump operator.\n6310: \n6311: **Step 7: Solve the Grönwall Inequality**\n6312: \n6313: Assuming $\\delta > 0$, the differential inequality:\n6314: \n6315: $$\n6316: \\frac{d D}{dt} \\le -\\delta \\cdot D + C_{\\text{offset}}\n6317: \n6318: $$\n6319: \n6320: has the solution (by Grönwall's lemma):\n6321: \n6322: $$\n6323: D(t) \\le e^{-\\delta t} D(0) + \\frac{C_{\\text{offset}}}{\\delta} (1 - e^{-\\delta t})\n6324: \n6325: $$\n6326: \n6327: Substituting $D(t) = D_{\\text{KL}}(\\rho_t \\| \\rho_\\infty)$ and $D(0) = D_{\\text{KL}}(\\rho_0 \\| \\rho_\\infty)$:\n6328: \n6329: $$\n6330: D_{\\text{KL}}(\\rho_t \\| \\rho_\\infty) \\le e^{-\\delta t} D_{\\text{KL}}(\\rho_0 \\| \\rho_\\infty) + \\frac{C_{\\text{offset}}}{\\delta}(1 - e^{-\\delta t})\n6331: \n6332: $$\n6333: \n6334: This matches the theorem statement with convergence rate $\\alpha_{\\text{net}} = \\delta$.\n6335: \n6336: **Step 8: Asymptotic Behavior**\n6337: \n6338: As $t \\to \\infty$, $e^{-\\delta t} \\to 0$, so:\n6339: \n6340: $$\n6341: D_{\\text{KL}}(\\rho_t \\| \\rho_\\infty) \\to \\frac{C_{\\text{offset}}}{\\delta}\n6342: \n6343: $$\n6344: \n6345: This is the **steady-state residual entropy**, arising from the constant forcing terms in the Grönwall inequality.\n6346: \n6347: **Conclusion**: Under the Kinetic Dominance Condition $\\delta > 0$, the mean-field density $\\rho_t$ converges exponentially to the QSD $\\rho_\\infty$ in KL-divergence at rate $\\alpha_{\\text{net}} = \\delta$.",
      "metadata": {
        "label": "proof-thm-mean-field-lsi-main"
      },
      "section": "## 2. Proof of Main Theorem",
      "references": [],
      "raw_directive": "6157: ## 2. Proof of Main Theorem\n6158: \n6159: :::{prf:proof}\n6160: :label: proof-thm-mean-field-lsi-main\n6161: We assemble the proof from the established results in Stages 0-2.\n6162: \n6163: **Step 1: Full Generator Entropy Production Equation**\n6164: \n6165: From Stage 1, Section 7.1 (Final Equation), the time derivative of KL-divergence is:\n6166: \n6167: $$\n6168: \\frac{d}{dt} D_{\\text{KL}}(\\rho_t \\| \\rho_\\infty) = -\\frac{\\sigma^2}{2} \\mathcal{I}_v(\\rho_t \\| \\rho_\\infty) + R_{\\text{coupling}}[\\rho_t] + \\mathcal{I}_{\\text{jump}}[\\rho_t]\n6169: \n6170: $$\n6171: \n6172: where:\n6173: - $\\mathcal{I}_v(\\rho \\| \\rho_\\infty)$ is the relative Fisher information in velocity\n6174: - $R_{\\text{coupling}}[\\rho]$ collects all mean-field coupling terms\n6175: - $\\mathcal{I}_{\\text{jump}}[\\rho]$ is the entropy production from the jump operator\n6176: \n6177: **Step 2: Bound the Coupling Terms**\n6178: \n6179: From Stage 2, Section 3.3 (Final Coupling Bound), we have:\n6180: \n6181: $$\n6182: |R_{\\text{coupling}}[\\rho]| \\le C_{\\text{KL}}^{\\text{coup}} \\cdot D_{\\text{KL}}(\\rho \\| \\rho_\\infty) + C_{\\text{Fisher}}^{\\text{coup}} \\cdot \\mathcal{I}_v(\\rho \\| \\rho_\\infty) + C_0^{\\text{coup}}\n6183: \n6184: $$\n6185: \n6186: where the constants $C_{\\text{KL}}^{\\text{coup}}$, $C_{\\text{Fisher}}^{\\text{coup}}$, $C_0^{\\text{coup}}$ are explicit formulas from Stage 2, Section 1.3.\n6187: \n6188: **Step 3: Bound the Jump Operator Expansion**\n6189: \n6190: From Stage 2, Section 4.4 (Jump Operator Bound), we have:\n6191: \n6192: $$\n6193: \\mathcal{I}_{\\text{jump}}[\\rho] \\le A_{\\text{jump}} \\cdot D_{\\text{KL}}(\\rho \\| \\rho_\\infty) + B_{\\text{jump}}\n6194: \n6195: $$\n6196: \n6197: where $A_{\\text{jump}}$ and $B_{\\text{jump}}$ are given in Stage 2, Section 1.4.\n6198: \n6199: **Step 4: Apply the Log-Sobolev Inequality**\n6200: \n6201: The QSD $\\rho_\\infty$ satisfies regularity properties R1-R6 (proven in Stage 0.5, Section 3). Therefore, by Stage 2, Theorem `thm-lsi-qsd`, it admits a Log-Sobolev inequality:\n6202: \n6203: $$\n6204: \\mathcal{I}_v(\\rho \\| \\rho_\\infty) \\ge 2\\lambda_{\\text{LSI}} D_{\\text{KL}}(\\rho \\| \\rho_\\infty)\n6205: \n6206: $$\n6207: \n6208: However, we need to relate the standard Fisher information $\\mathcal{I}_v(\\rho)$ to the relative one. By Stage 2, Lemma `lem-fisher-bound`:\n6209: \n6210: $$\n6211: \\mathcal{I}_v(\\rho) \\ge 2\\lambda_{\\text{LSI}} D_{\\text{KL}}(\\rho \\| \\rho_\\infty) - C_{\\text{LSI}}\n6212: \n6213: $$\n6214: \n6215: where $C_{\\text{LSI}}$ is the constant from the LSI remainder.\n6216: \n6217: **Step 5: Assemble the Grönwall Inequality**\n6218: \n6219: Substitute Steps 2, 3, 4 into Step 1. Using the notation $D := D_{\\text{KL}}(\\rho_t \\| \\rho_\\infty)$ and $I := \\mathcal{I}_v(\\rho_t \\| \\rho_\\infty)$:\n6220: \n6221: $$\n6222: \\begin{align*}\n6223: \\frac{d D}{dt} &= -\\frac{\\sigma^2}{2} I + R_{\\text{coupling}} + \\mathcal{I}_{\\text{jump}} \\\\\n6224: &\\le -\\frac{\\sigma^2}{2} I + C_{\\text{KL}}^{\\text{coup}} D + C_{\\text{Fisher}}^{\\text{coup}} I + C_0^{\\text{coup}} + A_{\\text{jump}} D + B_{\\text{jump}} \\\\\n6225: &= -\\frac{\\sigma^2}{2} I + C_{\\text{Fisher}}^{\\text{coup}} I + (C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}) D + (C_0^{\\text{coup}} + B_{\\text{jump}})\n6226: \\end{align*}\n6227: \n6228: $$\n6229: \n6230: Factor the Fisher information term:\n6231: \n6232: $$\n6233: \\frac{d D}{dt} \\le -\\left(\\frac{\\sigma^2}{2} - C_{\\text{Fisher}}^{\\text{coup}}\\right) I + (C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}) D + (C_0^{\\text{coup}} + B_{\\text{jump}})\n6234: \n6235: $$\n6236: \n6237: Now apply the LSI bound from Step 4:\n6238: \n6239: $$\n6240: I \\ge 2\\lambda_{\\text{LSI}} D - C_{\\text{LSI}}\n6241: \n6242: $$\n6243: \n6244: Substitute:\n6245: \n6246: $$\n6247: \\begin{align*}\n6248: \\frac{d D}{dt} &\\le -\\left(\\frac{\\sigma^2}{2} - C_{\\text{Fisher}}^{\\text{coup}}\\right) \\left(2\\lambda_{\\text{LSI}} D - C_{\\text{LSI}}\\right) + (C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}) D + (C_0^{\\text{coup}} + B_{\\text{jump}}) \\\\\n6249: &= -\\left(\\frac{\\sigma^2}{2} - C_{\\text{Fisher}}^{\\text{coup}}\\right) 2\\lambda_{\\text{LSI}} D + \\left(\\frac{\\sigma^2}{2} - C_{\\text{Fisher}}^{\\text{coup}}\\right) C_{\\text{LSI}} \\\\\n6250: &\\quad + (C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}) D + (C_0^{\\text{coup}} + B_{\\text{jump}})\n6251: \\end{align*}\n6252: \n6253: $$\n6254: \n6255: Collect terms proportional to $D$:\n6256: \n6257: $$\n6258: \\begin{align*}\n6259: \\frac{d D}{dt} &\\le \\left[-({\\sigma^2} - 2C_{\\text{Fisher}}^{\\text{coup}}) \\lambda_{\\text{LSI}} + C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}\\right] D \\\\\n6260: &\\quad + \\frac{\\sigma^2}{2} C_{\\text{LSI}} + C_0^{\\text{coup}} + B_{\\text{jump}}\n6261: \\end{align*}\n6262: \n6263: $$\n6264: \n6265: Factoring out the negative sign from the coefficient of $D$:\n6266: \n6267: $$\n6268: \\frac{d D}{dt} \\le -\\left[\\lambda_{\\text{LSI}} \\sigma^2 - 2\\lambda_{\\text{LSI}}C_{\\text{Fisher}}^{\\text{coup}} - C_{\\text{KL}}^{\\text{coup}} - A_{\\text{jump}}\\right] D + C_{\\text{offset}}\n6269: \n6270: $$\n6271: \n6272: Define:\n6273: \n6274: $$\n6275: \\delta := \\lambda_{\\text{LSI}} \\sigma^2 - 2\\lambda_{\\text{LSI}}C_{\\text{Fisher}}^{\\text{coup}} - C_{\\text{KL}}^{\\text{coup}} - A_{\\text{jump}}\n6276: \n6277: $$\n6278: \n6279: $$\n6280: C_{\\text{offset}} := \\frac{\\sigma^2}{2} C_{\\text{LSI}} + C_0^{\\text{coup}} + B_{\\text{jump}}\n6281: \n6282: $$\n6283: \n6284: Then:\n6285: \n6286: $$\n6287: \\frac{d D}{dt} \\le -\\delta \\cdot D + C_{\\text{offset}}\n6288: \n6289: $$\n6290: \n6291: This is the **Grönwall differential inequality**.\n6292: \n6293: **Step 6: State the Kinetic Dominance Condition**\n6294: \n6295: For exponential convergence, we require $\\delta > 0$. This is the **Kinetic Dominance Condition**:\n6296: \n6297: $$\n6298: \\lambda_{\\text{LSI}} \\sigma^2 > 2\\lambda_{\\text{LSI}}C_{\\text{Fisher}}^{\\text{coup}} + C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}\n6299: \n6300: $$\n6301: \n6302: Equivalently, rearranging for $\\sigma^2$:\n6303: \n6304: $$\n6305: \\sigma^2 > \\sigma_{\\text{crit}}^2 := \\frac{2C_{\\text{Fisher}}^{\\text{coup}}}{\\lambda_{\\text{LSI}}} + \\frac{C_{\\text{KL}}^{\\text{coup}} + A_{\\text{jump}}}{\\lambda_{\\text{LSI}}}\n6306: \n6307: $$\n6308: \n6309: **Physical meaning**: The velocity diffusion must be strong enough for the hypocoercive dissipation to overcome the destabilizing effects from mean-field coupling and the jump operator.\n6310: \n6311: **Step 7: Solve the Grönwall Inequality**\n6312: \n6313: Assuming $\\delta > 0$, the differential inequality:\n6314: \n6315: $$\n6316: \\frac{d D}{dt} \\le -\\delta \\cdot D + C_{\\text{offset}}\n6317: \n6318: $$\n6319: \n6320: has the solution (by Grönwall's lemma):\n6321: \n6322: $$\n6323: D(t) \\le e^{-\\delta t} D(0) + \\frac{C_{\\text{offset}}}{\\delta} (1 - e^{-\\delta t})\n6324: \n6325: $$\n6326: \n6327: Substituting $D(t) = D_{\\text{KL}}(\\rho_t \\| \\rho_\\infty)$ and $D(0) = D_{\\text{KL}}(\\rho_0 \\| \\rho_\\infty)$:\n6328: \n6329: $$\n6330: D_{\\text{KL}}(\\rho_t \\| \\rho_\\infty) \\le e^{-\\delta t} D_{\\text{KL}}(\\rho_0 \\| \\rho_\\infty) + \\frac{C_{\\text{offset}}}{\\delta}(1 - e^{-\\delta t})\n6331: \n6332: $$\n6333: \n6334: This matches the theorem statement with convergence rate $\\alpha_{\\text{net}} = \\delta$.\n6335: \n6336: **Step 8: Asymptotic Behavior**\n6337: \n6338: As $t \\to \\infty$, $e^{-\\delta t} \\to 0$, so:\n6339: \n6340: $$\n6341: D_{\\text{KL}}(\\rho_t \\| \\rho_\\infty) \\to \\frac{C_{\\text{offset}}}{\\delta}\n6342: \n6343: $$\n6344: \n6345: This is the **steady-state residual entropy**, arising from the constant forcing terms in the Grönwall inequality.\n6346: \n6347: **Conclusion**: Under the Kinetic Dominance Condition $\\delta > 0$, the mean-field density $\\rho_t$ converges exponentially to the QSD $\\rho_\\infty$ in KL-divergence at rate $\\alpha_{\\text{net}} = \\delta$.\n6348: ",
      "_registry_context": {
        "stage": "directives",
        "document_id": "16_convergence_mean_field",
        "chapter_index": 50,
        "chapter_file": "chapter_50.json",
        "section_id": "## 2. Proof of Main Theorem"
      }
    }
  ]
}