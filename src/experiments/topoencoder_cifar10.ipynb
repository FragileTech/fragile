{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TopoEncoder Benchmark: CIFAR-10 Image Classification\n",
    "\n",
    "This notebook benchmarks the **Attentive Atlas** (TopoEncoder) architecture on CIFAR-10 images, comparing against a standard VQ-VAE and Vanilla AE baseline.\n",
    "\n",
    "**CIFAR-10 contains 10 classes:** airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "\n",
    "**Key features:**\n",
    "- Input: Flattened 32x32x3 images (3072 dimensions)\n",
    "- Charts: 10 (one per class)\n",
    "- Visualization: Image grids and 2D latent space\n",
    "- Supervised topology loss for class-aware routing\n",
    "\n",
    "**Reference:** fragile-index.md Sections 7.8, 7.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies not available in Colab by default\n",
    "!pip install einops -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone fragile repository and setup path\n",
    "!git clone https://github.com/FragileTech/fragile.git /content/fragile 2>/dev/null || echo \"Repository already cloned\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/fragile/src')\n",
    "\n",
    "# Change to experiments directory for relative imports\n",
    "%cd /content/fragile/src/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Fragile imports\n",
    "from dataviz import visualize_latent_images, visualize_results_images\n",
    "from fragile.datasets import CIFAR10_CLASSES, get_cifar10_data\n",
    "from fragile.core.layers import FactorizedJumpOperator, StandardVQ, TopoEncoder, VanillaAE\n",
    "from fragile.core.losses import (\n",
    "    compute_code_entropy_loss,\n",
    "    compute_disentangle_loss,\n",
    "    compute_diversity_loss,\n",
    "    compute_jump_consistency_loss,\n",
    "    compute_kl_prior_loss,\n",
    "    compute_orthogonality_loss,\n",
    "    compute_orbit_loss,\n",
    "    compute_per_chart_code_entropy_loss,\n",
    "    compute_routing_entropy,\n",
    "    compute_separation_loss,\n",
    "    compute_variance_loss,\n",
    "    compute_vicreg_invariance_loss,\n",
    "    compute_window_loss,\n",
    "    get_jump_weight_schedule,\n",
    "    SupervisedTopologyLoss,\n",
    ")\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The configuration below uses **Colab-friendly defaults**:\n",
    "- 100 epochs (instead of 500) for faster iteration\n",
    "- 10,000 samples (instead of 50,000) for quicker experimentation\n",
    "\n",
    "Adjust these values based on your needs and available GPU time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TopoEncoderCIFAR10Config:\n",
    "    \"\"\"Configuration for the TopoEncoder CIFAR-10 benchmark.\"\"\"\n",
    "\n",
    "    # Data (CIFAR-10: 32x32x3 = 3072 dimensions)\n",
    "    n_samples: int = 10000  # Reduced for Colab (full: 50000)\n",
    "    input_dim: int = 3072  # 32*32*3 flattened\n",
    "    image_shape: tuple = (32, 32, 3)  # For visualization\n",
    "\n",
    "    # Model architecture\n",
    "    hidden_dim: int = 256\n",
    "    latent_dim: int = 2  # For 2D visualization\n",
    "    num_charts: int = 10  # One per CIFAR-10 class\n",
    "    codes_per_chart: int = 64\n",
    "    num_codes_standard: int = 256\n",
    "\n",
    "    # Training\n",
    "    epochs: int = 100  # Reduced for Colab (full: 500)\n",
    "    batch_size: int = 256\n",
    "    lr: float = 1e-3\n",
    "    vq_commitment_cost: float = 0.25\n",
    "    entropy_weight: float = 0.1\n",
    "    consistency_weight: float = 0.1\n",
    "\n",
    "    # Tier 1 losses (low overhead)\n",
    "    variance_weight: float = 0.1\n",
    "    diversity_weight: float = 0.1\n",
    "    separation_weight: float = 0.1\n",
    "    separation_margin: float = 2.0\n",
    "\n",
    "    # Tier 2 losses (medium overhead)\n",
    "    window_weight: float = 0.5\n",
    "    window_eps_ground: float = 0.1\n",
    "    disentangle_weight: float = 0.1\n",
    "\n",
    "    # Tier 3 losses (geometry/codebook health)\n",
    "    orthogonality_weight: float = 0.01\n",
    "    code_entropy_weight: float = 0.0\n",
    "    per_chart_code_entropy_weight: float = 0.1\n",
    "\n",
    "    # Tier 4 losses (invariance)\n",
    "    kl_prior_weight: float = 0.01\n",
    "    orbit_weight: float = 0.0\n",
    "    vicreg_inv_weight: float = 0.0\n",
    "    augment_noise_std: float = 0.1\n",
    "\n",
    "    # Tier 5: Jump Operator\n",
    "    jump_weight: float = 0.1\n",
    "    jump_warmup: int = 50\n",
    "    jump_ramp_end: int = 100\n",
    "    jump_global_rank: int = 0\n",
    "\n",
    "    # Supervised topology loss\n",
    "    enable_supervised: bool = True\n",
    "    num_classes: int = 10\n",
    "    sup_weight: float = 1.0\n",
    "    sup_purity_weight: float = 0.1\n",
    "    sup_balance_weight: float = 0.01\n",
    "    sup_metric_weight: float = 0.01\n",
    "    sup_metric_margin: float = 1.0\n",
    "    sup_temperature: float = 1.0\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    use_scheduler: bool = True\n",
    "    min_lr: float = 1e-5\n",
    "\n",
    "    # Gradient clipping\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # Benchmark control\n",
    "    disable_ae: bool = False\n",
    "    disable_vq: bool = False\n",
    "\n",
    "    # Train/test split\n",
    "    test_split: float = 0.2\n",
    "\n",
    "    # Logging and output\n",
    "    log_every: int = 10  # More frequent for Colab\n",
    "    save_every: int = 25  # More frequent for Colab\n",
    "    output_dir: str = \"/content/outputs/topoencoder_cifar10\"\n",
    "\n",
    "    # Device\n",
    "    device: str = field(\n",
    "        default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Configuration class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"Count total trainable parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def compute_matching_hidden_dim(\n",
    "    target_params: int,\n",
    "    input_dim: int = 3072,\n",
    "    latent_dim: int = 2,\n",
    "    num_codes: int = 256,\n",
    ") -> int:\n",
    "    \"\"\"Compute hidden_dim for StandardVQ to match target parameter count.\"\"\"\n",
    "    offset = 5 + num_codes * latent_dim\n",
    "    coef_h = 2 * input_dim + 8\n",
    "    discriminant = coef_h**2 + 8 * (target_params - offset)\n",
    "    if discriminant < 0:\n",
    "        return 128\n",
    "    h = (-coef_h + math.sqrt(discriminant)) / 4\n",
    "    return max(64, int(h))\n",
    "\n",
    "\n",
    "def compute_ami(labels_true: np.ndarray, labels_pred: np.ndarray) -> float:\n",
    "    \"\"\"Compute Adjusted Mutual Information score.\"\"\"\n",
    "    return float(adjusted_mutual_info_score(labels_true, labels_pred))\n",
    "\n",
    "\n",
    "def augment_cifar10(\n",
    "    x: torch.Tensor,\n",
    "    noise_std: float = 0.1,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Apply noise augmentation to flattened CIFAR-10 images.\"\"\"\n",
    "    return x + torch.randn_like(x) * noise_std\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_benchmark(config: TopoEncoderCIFAR10Config) -> dict:\n    \"\"\"Train models and return results.\"\"\"\n    # Create output directory\n    if config.save_every > 0:\n        os.makedirs(config.output_dir, exist_ok=True)\n        print(f\"Saving training progress to: {config.output_dir}/\")\n\n    # Load CIFAR-10 data\n    X, labels, colors = get_cifar10_data(config.n_samples)\n    labels = labels.astype(np.int64)\n    print(f\"Loaded {len(X)} CIFAR-10 images (shape: {X.shape})\")\n    print(f\"Classes: {CIFAR10_CLASSES}\")\n\n    if not (0.0 <= config.test_split < 1.0):\n        raise ValueError(\"test_split must be in [0.0, 1.0).\")\n\n    n_total = X.shape[0]\n    test_size = max(1, int(n_total * config.test_split)) if config.test_split > 0 else 0\n    if test_size >= n_total:\n        test_size = max(1, n_total - 1)\n    train_size = n_total - test_size\n    perm = torch.randperm(n_total)\n    train_idx = perm[:train_size]\n    test_idx = perm[train_size:]\n    train_idx_np = train_idx.numpy()\n    test_idx_np = test_idx.numpy()\n\n    X_train = X[train_idx]\n    X_test = X[test_idx] if test_size > 0 else X\n    labels_train = labels[train_idx_np]\n    labels_test = labels[test_idx_np] if test_size > 0 else labels\n    colors_train = colors[train_idx_np]\n    colors_test = colors[test_idx_np] if test_size > 0 else colors\n\n    print(\n        f\"Train/test split: {len(X_train)}/{len(X_test)} \"\n        f\"(test={config.test_split:.2f})\"\n    )\n\n    # Create TopoEncoder\n    model_atlas = TopoEncoder(\n        input_dim=config.input_dim,\n        hidden_dim=config.hidden_dim,\n        latent_dim=config.latent_dim,\n        num_charts=config.num_charts,\n        codes_per_chart=config.codes_per_chart,\n    )\n    topo_params = count_parameters(model_atlas)\n\n    # Create StandardVQ with matching parameter count\n    model_std = None\n    opt_std = None\n    std_params = 0\n    std_hidden_dim = 0\n    if not config.disable_vq:\n        std_hidden_dim = compute_matching_hidden_dim(\n            target_params=topo_params,\n            input_dim=config.input_dim,\n            latent_dim=config.latent_dim,\n            num_codes=config.num_codes_standard,\n        )\n        model_std = StandardVQ(\n            input_dim=config.input_dim,\n            hidden_dim=std_hidden_dim,\n            latent_dim=config.latent_dim,\n            num_codes=config.num_codes_standard,\n        )\n        std_params = count_parameters(model_std)\n\n    # Create VanillaAE\n    model_ae = None\n    opt_ae = None\n    ae_params = 0\n    ae_hidden_dim = 0\n    if not config.disable_ae:\n        ae_hidden_dim = compute_matching_hidden_dim(\n            target_params=topo_params,\n            input_dim=config.input_dim,\n            latent_dim=config.latent_dim,\n            num_codes=0,\n        )\n        model_ae = VanillaAE(\n            input_dim=config.input_dim,\n            hidden_dim=ae_hidden_dim,\n            latent_dim=config.latent_dim,\n        )\n        ae_params = count_parameters(model_ae)\n\n    print(f\"\\nModel Parameters (fair comparison):\")\n    print(f\"  TopoEncoder: {topo_params:,} params (hidden_dim={config.hidden_dim})\")\n    if not config.disable_vq:\n        print(f\"  StandardVQ:  {std_params:,} params (hidden_dim={std_hidden_dim})\")\n    else:\n        print(f\"  StandardVQ:  DISABLED\")\n    if not config.disable_ae:\n        print(f\"  VanillaAE:   {ae_params:,} params (hidden_dim={ae_hidden_dim})\")\n    else:\n        print(f\"  VanillaAE:   DISABLED\")\n\n    # Move to device\n    device = torch.device(config.device)\n    model_atlas = model_atlas.to(device)\n    if model_std is not None:\n        model_std = model_std.to(device)\n    if model_ae is not None:\n        model_ae = model_ae.to(device)\n    X_train = X_train.to(device)\n    X_test = X_test.to(device)\n    print(f\"  Device: {device}\")\n\n    # Initialize Jump Operator\n    jump_op = FactorizedJumpOperator(\n        num_charts=config.num_charts,\n        latent_dim=config.latent_dim,\n        global_rank=config.jump_global_rank,\n    ).to(device)\n    print(f\"  Jump Operator: {count_parameters(jump_op):,} params\")\n\n    # Supervised topology loss\n    supervised_loss = None\n    num_classes = int(labels.max()) + 1 if labels.size else config.num_classes\n    if config.enable_supervised:\n        supervised_loss = SupervisedTopologyLoss(\n            num_charts=config.num_charts,\n            num_classes=num_classes,\n            lambda_purity=config.sup_purity_weight,\n            lambda_balance=config.sup_balance_weight,\n            lambda_metric=config.sup_metric_weight,\n            margin=config.sup_metric_margin,\n            temperature=config.sup_temperature,\n        ).to(device)\n        print(\n            f\"  Supervised Topology: \"\n            f\"classes={num_classes}, \"\n            f\"lambda_purity={config.sup_purity_weight}, \"\n            f\"lambda_balance={config.sup_balance_weight}, \"\n            f\"lambda_metric={config.sup_metric_weight}\"\n        )\n\n    # Optimizers\n    if model_std is not None:\n        opt_std = optim.Adam(model_std.parameters(), lr=config.lr)\n    atlas_params = list(model_atlas.parameters()) + list(jump_op.parameters())\n    if supervised_loss is not None:\n        atlas_params.extend(list(supervised_loss.parameters()))\n    opt_atlas = optim.Adam(atlas_params, lr=config.lr)\n    if model_ae is not None:\n        opt_ae = optim.Adam(model_ae.parameters(), lr=config.lr)\n\n    # Learning rate scheduler\n    scheduler = None\n    if config.use_scheduler:\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n            opt_atlas, T_max=config.epochs, eta_min=config.min_lr\n        )\n\n    # Create data loader\n    labels_train_t = torch.from_numpy(labels_train).long().to(device)\n    labels_test_t = torch.from_numpy(labels_test).long().to(device)\n    dataset = TensorDataset(X_train, labels_train_t)\n    batch_size = config.batch_size if config.batch_size > 0 else len(X_train)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    # Training history\n    std_losses = []\n    atlas_losses = []\n    ae_losses = []\n    loss_components: dict[str, list[float]] = {\n        \"recon\": [],\n        \"vq\": [],\n        \"entropy\": [],\n        \"consistency\": [],\n        \"variance\": [],\n        \"diversity\": [],\n        \"separation\": [],\n        \"window\": [],\n        \"disentangle\": [],\n        \"orthogonality\": [],\n        \"code_entropy\": [],\n        \"per_chart_code_entropy\": [],\n        \"kl_prior\": [],\n        \"orbit\": [],\n        \"vicreg_inv\": [],\n        \"jump\": [],\n        \"sup_total\": [],\n        \"sup_route\": [],\n        \"sup_purity\": [],\n        \"sup_balance\": [],\n        \"sup_metric\": [],\n        \"sup_acc\": [],\n    }\n    info_metrics: dict[str, list[float]] = {\n        \"I_XK\": [],\n        \"H_K\": [],\n    }\n\n    print(\"=\" * 60)\n    print(\"Training TopoEncoder on CIFAR-10\")\n    print(f\"  Epochs: {config.epochs}, LR: {config.lr}, Batch size: {batch_size}\")\n    print(f\"  Charts: {config.num_charts}, Codes/chart: {config.codes_per_chart}\")\n    print(f\"  lambda: entropy={config.entropy_weight}, consistency={config.consistency_weight}\")\n    print(\"=\" * 60)\n\n    for epoch in tqdm(range(config.epochs + 1), desc=\"Training\", unit=\"epoch\"):\n        epoch_std_loss = 0.0\n        epoch_atlas_loss = 0.0\n        epoch_ae_loss = 0.0\n        epoch_losses = {k: 0.0 for k in loss_components.keys()}\n        epoch_info = {\"I_XK\": 0.0, \"H_K\": 0.0}\n        n_batches = 0\n\n        for batch_X, batch_labels in dataloader:\n            n_batches += 1\n\n            # --- Standard VQ Step ---\n            loss_s = torch.tensor(0.0, device=device)\n            if model_std is not None:\n                recon_s, vq_loss_s, _ = model_std(batch_X)\n                loss_s = F.mse_loss(recon_s, batch_X) + vq_loss_s\n                opt_std.zero_grad()\n                loss_s.backward()\n                opt_std.step()\n\n            # --- Vanilla AE Step ---\n            loss_ae = torch.tensor(0.0, device=device)\n            if model_ae is not None:\n                recon_ae, _ = model_ae(batch_X)\n                loss_ae = F.mse_loss(recon_ae, batch_X)\n                opt_ae.zero_grad()\n                loss_ae.backward()\n                opt_ae.step()\n\n            # --- Atlas Step ---\n            K_chart, _, z_n, z_tex, enc_w, z_geo, vq_loss_a, indices_stack, z_n_all_charts = model_atlas.encoder(batch_X)\n            recon_a, dec_w = model_atlas.decoder(z_geo, z_tex, chart_index=None)\n\n            # Core losses\n            recon_loss_a = F.mse_loss(recon_a, batch_X)\n            entropy = compute_routing_entropy(enc_w)\n            consistency = model_atlas.compute_consistency_loss(enc_w, dec_w)\n\n            # Tier 1 losses\n            var_loss = compute_variance_loss(z_geo)\n            div_loss = compute_diversity_loss(enc_w, config.num_charts)\n            sep_loss = compute_separation_loss(\n                z_geo, enc_w, config.num_charts, config.separation_margin\n            )\n\n            # Tier 2 losses\n            window_loss, window_info = compute_window_loss(\n                enc_w, config.num_charts, config.window_eps_ground\n            )\n            dis_loss = compute_disentangle_loss(z_geo, enc_w)\n\n            # Tier 3 losses\n            orth_loss = compute_orthogonality_loss(model_atlas)\n            code_ent_loss = compute_code_entropy_loss(indices_stack, config.codes_per_chart)\n            per_chart_code_ent_loss = compute_per_chart_code_entropy_loss(\n                indices_stack, K_chart, config.num_charts, config.codes_per_chart\n            )\n\n            # Tier 4 losses\n            if config.kl_prior_weight > 0:\n                kl_loss = compute_kl_prior_loss(z_n, z_tex)\n            else:\n                kl_loss = torch.tensor(0.0, device=device)\n\n            orbit_loss = torch.tensor(0.0, device=device)\n            vicreg_loss = torch.tensor(0.0, device=device)\n\n            if config.orbit_weight > 0 or config.vicreg_inv_weight > 0:\n                x_aug = augment_cifar10(batch_X, config.augment_noise_std)\n                _, _, _, _, enc_w_aug, z_geo_aug, _, _, _ = model_atlas.encoder(x_aug)\n                del x_aug  # Free memory immediately\n\n                if config.orbit_weight > 0:\n                    orbit_loss = compute_orbit_loss(enc_w, enc_w_aug)\n                if config.vicreg_inv_weight > 0:\n                    vicreg_loss = compute_vicreg_invariance_loss(z_geo, z_geo_aug)\n\n            # Tier 5: Jump Operator\n            current_jump_weight = get_jump_weight_schedule(\n                epoch, config.jump_warmup, config.jump_ramp_end, config.jump_weight\n            )\n            if current_jump_weight > 0:\n                jump_loss = compute_jump_consistency_loss(jump_op, z_n_all_charts, enc_w)\n            else:\n                jump_loss = torch.tensor(0.0, device=device)\n\n            # Supervised topology losses\n            sup_total = torch.tensor(0.0, device=device)\n            sup_route = torch.tensor(0.0, device=device)\n            sup_purity = torch.tensor(0.0, device=device)\n            sup_balance = torch.tensor(0.0, device=device)\n            sup_metric = torch.tensor(0.0, device=device)\n            sup_acc = torch.tensor(0.0, device=device)\n\n            if supervised_loss is not None:\n                sup_out = supervised_loss(enc_w, batch_labels, z_geo)\n                sup_total = sup_out[\"loss_total\"]\n                sup_route = sup_out[\"loss_route\"]\n                sup_purity = sup_out[\"loss_purity\"]\n                sup_balance = sup_out[\"loss_balance\"]\n                sup_metric = sup_out[\"loss_metric\"]\n\n                p_y_x = torch.matmul(enc_w, supervised_loss.p_y_given_k)\n                sup_acc = (p_y_x.argmax(dim=1) == batch_labels).float().mean()\n\n            # Total loss\n            loss_a = (\n                recon_loss_a\n                + vq_loss_a\n                + config.entropy_weight * entropy\n                + config.consistency_weight * consistency\n                + config.variance_weight * var_loss\n                + config.diversity_weight * div_loss\n                + config.separation_weight * sep_loss\n                + config.window_weight * window_loss\n                + config.disentangle_weight * dis_loss\n                + config.orthogonality_weight * orth_loss\n                + config.code_entropy_weight * code_ent_loss\n                + config.per_chart_code_entropy_weight * per_chart_code_ent_loss\n                + config.kl_prior_weight * kl_loss\n                + config.orbit_weight * orbit_loss\n                + config.vicreg_inv_weight * vicreg_loss\n                + current_jump_weight * jump_loss\n                + config.sup_weight * sup_total\n            )\n\n            opt_atlas.zero_grad()\n            loss_a.backward()\n            if config.grad_clip > 0:\n                all_params = list(model_atlas.parameters()) + list(jump_op.parameters())\n                torch.nn.utils.clip_grad_norm_(all_params, config.grad_clip)\n            opt_atlas.step()\n\n            # Accumulate batch losses\n            epoch_std_loss += loss_s.item()\n            epoch_atlas_loss += loss_a.item()\n            epoch_ae_loss += loss_ae.item()\n            epoch_losses[\"recon\"] += recon_loss_a.item()\n            epoch_losses[\"vq\"] += vq_loss_a.item()\n            epoch_losses[\"entropy\"] += entropy\n            epoch_losses[\"consistency\"] += consistency.item()\n            epoch_losses[\"variance\"] += var_loss.item()\n            epoch_losses[\"diversity\"] += div_loss.item()\n            epoch_losses[\"separation\"] += sep_loss.item()\n            epoch_losses[\"window\"] += window_loss.item()\n            epoch_losses[\"disentangle\"] += dis_loss.item()\n            epoch_losses[\"orthogonality\"] += orth_loss.item()\n            epoch_losses[\"code_entropy\"] += code_ent_loss.item()\n            epoch_losses[\"per_chart_code_entropy\"] += per_chart_code_ent_loss.item()\n            epoch_losses[\"kl_prior\"] += kl_loss.item()\n            epoch_losses[\"orbit\"] += orbit_loss.item()\n            epoch_losses[\"vicreg_inv\"] += vicreg_loss.item()\n            epoch_losses[\"jump\"] += jump_loss.item()\n            epoch_losses[\"sup_total\"] += sup_total.item()\n            epoch_losses[\"sup_route\"] += sup_route.item()\n            epoch_losses[\"sup_purity\"] += sup_purity.item()\n            epoch_losses[\"sup_balance\"] += sup_balance.item()\n            epoch_losses[\"sup_metric\"] += sup_metric.item()\n            epoch_losses[\"sup_acc\"] += sup_acc.item()\n            epoch_info[\"I_XK\"] += window_info[\"I_XK\"]\n            epoch_info[\"H_K\"] += window_info[\"H_K\"]\n\n        # Average over batches\n        std_losses.append(epoch_std_loss / n_batches)\n        atlas_losses.append(epoch_atlas_loss / n_batches)\n        ae_losses.append(epoch_ae_loss / n_batches)\n        for k in loss_components.keys():\n            loss_components[k].append(epoch_losses[k] / n_batches)\n        info_metrics[\"I_XK\"].append(epoch_info[\"I_XK\"] / n_batches)\n        info_metrics[\"H_K\"].append(epoch_info[\"H_K\"] / n_batches)\n\n        # Step LR scheduler\n        if scheduler is not None:\n            scheduler.step()\n\n        # Logging and visualization\n        should_log = epoch % config.log_every == 0 or epoch == config.epochs\n        should_save = config.save_every > 0 and (\n            epoch % config.save_every == 0 or epoch == config.epochs\n        )\n        if should_log or should_save:\n            with torch.no_grad():\n                K_chart_full, _, _, _, enc_w_full, _, _, _, _ = model_atlas.encoder(X_test)\n                usage = enc_w_full.mean(dim=0).cpu().numpy()\n                chart_assignments = K_chart_full.cpu().numpy()\n                ami = compute_ami(labels_test, chart_assignments)\n                perplexity = model_atlas.compute_perplexity(K_chart_full)\n\n            # Clear GPU cache after heavy test inference\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n            avg_loss = atlas_losses[-1]\n            avg_recon = loss_components[\"recon\"][-1]\n            avg_vq = loss_components[\"vq\"][-1]\n            avg_entropy = loss_components[\"entropy\"][-1]\n            avg_consistency = loss_components[\"consistency\"][-1]\n            avg_sup_acc = loss_components[\"sup_acc\"][-1]\n            avg_sup_route = loss_components[\"sup_route\"][-1]\n            avg_ixk = info_metrics[\"I_XK\"][-1]\n            avg_hk = info_metrics[\"H_K\"][-1]\n\n            log_jump_weight = get_jump_weight_schedule(\n                epoch, config.jump_warmup, config.jump_ramp_end, config.jump_weight\n            )\n\n            current_lr = scheduler.get_last_lr()[0] if scheduler else config.lr\n            print(f\"Epoch {epoch:5d} | Loss: {avg_loss:.4f} | LR: {current_lr:.2e}\")\n            print(f\"  Usage: {np.array2string(usage, precision=2, separator=', ')}\")\n            print(\n                f\"  Core: recon={avg_recon:.4f} \"\n                f\"vq={avg_vq:.4f} \"\n                f\"entropy={avg_entropy:.4f} \"\n                f\"consistency={avg_consistency:.4f}\"\n            )\n            if supervised_loss is not None:\n                with torch.no_grad():\n                    _, _, _, _, enc_w_test, z_geo_test, _, _, _ = model_atlas.encoder(X_test)\n                    sup_test = supervised_loss(enc_w_test, labels_test_t, z_geo_test)\n                    p_y_x_test = torch.matmul(enc_w_test, supervised_loss.p_y_given_k)\n                    test_sup_acc = (\n                        p_y_x_test.argmax(dim=1) == labels_test_t\n                    ).float().mean().item()\n                    test_sup_route = sup_test[\"loss_route\"].item()\n\n                # Clear GPU cache after supervised test inference\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n                print(\n                    f\"  Sup: train_acc={avg_sup_acc:.4f} \"\n                    f\"test_acc={test_sup_acc:.4f} \"\n                    f\"route={avg_sup_route:.4f}\"\n                )\n            print(\n                f\"  Info: I(X;K)={avg_ixk:.3f} H(K)={avg_hk:.3f} \"\n                f\"jump_w={log_jump_weight:.3f}\"\n            )\n            print(f\"  Metrics: AMI={ami:.4f} perplexity={perplexity:.2f}/{config.num_charts}\")\n            print(\"-\" * 60)\n\n            # Save visualization\n            if should_save:\n                save_path = f\"{config.output_dir}/cifar10_epoch_{epoch:05d}.png\"\n                visualize_latent_images(\n                    model_atlas,\n                    X_test,\n                    labels_test,\n                    CIFAR10_CLASSES,\n                    save_path,\n                    epoch,\n                    jump_op=jump_op,\n                    image_shape=config.image_shape,\n                )\n\n    # Final evaluation\n    print(\"\\n\" + \"=\" * 50)\n    print(\"FINAL RESULTS\")\n    print(\"=\" * 50)\n\n    with torch.no_grad():\n        # VanillaAE metrics\n        mse_ae = 0.0\n        ami_ae = 0.0\n        recon_ae_final = None\n        if model_ae is not None:\n            recon_ae_final, z_ae = model_ae(X_test)\n            mse_ae = F.mse_loss(recon_ae_final, X_test).item()\n            z_ae_np = z_ae.cpu().numpy()\n            kmeans = KMeans(n_clusters=config.num_charts, random_state=42, n_init=10)\n            ae_clusters = kmeans.fit_predict(z_ae_np)\n            ami_ae = compute_ami(labels_test, ae_clusters)\n\n        # Standard VQ metrics\n        mse_std = 0.0\n        ami_std = 0.0\n        std_perplexity = 0.0\n        recon_std_final = None\n        if model_std is not None:\n            recon_std_final, _, indices_s = model_std(X_test)\n            std_perplexity = model_std.compute_perplexity(indices_s)\n            mse_std = F.mse_loss(recon_std_final, X_test).item()\n            vq_clusters = indices_s.cpu().numpy() % config.num_charts\n            ami_std = compute_ami(labels_test, vq_clusters)\n\n        # Atlas metrics\n        recon_atlas_final, _, enc_w, dec_w, K_chart = model_atlas(\n            X_test, use_hard_routing=False\n        )\n        chart_assignments = K_chart.cpu().numpy()\n        atlas_perplexity = model_atlas.compute_perplexity(K_chart)\n        ami_atlas = compute_ami(labels_test, chart_assignments)\n        mse_atlas = F.mse_loss(recon_atlas_final, X_test).item()\n        final_consistency = model_atlas.compute_consistency_loss(enc_w, dec_w).item()\n        sup_acc = 0.0\n        if supervised_loss is not None:\n            p_y_x = torch.matmul(enc_w, supervised_loss.p_y_given_k)\n            sup_acc = (p_y_x.argmax(dim=1) == labels_test_t).float().mean().item()\n\n    # Results table\n    print(\"\\n\" + \"-\" * 70)\n    print(f\"{'Model':<20} {'MSE':>10} {'AMI':>10} {'Perplexity':>15}\")\n    print(\"-\" * 70)\n    if model_ae is not None:\n        print(f\"{'Vanilla AE':<20} {mse_ae:>10.5f} {ami_ae:>10.4f} {'N/A (K-Means)':<15}\")\n    if model_std is not None:\n        print(f\"{'Standard VQ':<20} {mse_std:>10.5f} {ami_std:>10.4f} {std_perplexity:>6.1f}/{config.num_codes_standard:<8}\")\n    print(f\"{'TopoEncoder':<20} {mse_atlas:>10.5f} {ami_atlas:>10.4f} {atlas_perplexity:>6.1f}/{config.num_charts:<8}\")\n    print(\"-\" * 70)\n\n    print(f\"\\nRouting Consistency (KL): {final_consistency:.4f}\")\n    if supervised_loss is not None:\n        print(f\"Supervised Accuracy: {sup_acc:.4f}\")\n\n    # Save final visualization\n    if config.save_every > 0:\n        final_path = f\"{config.output_dir}/cifar10_final.png\"\n        visualize_latent_images(\n            model_atlas,\n            X_test,\n            labels_test,\n            CIFAR10_CLASSES,\n            final_path,\n            epoch=None,\n            jump_op=jump_op,\n            image_shape=config.image_shape,\n        )\n        print(f\"\\nFinal visualization saved to: {final_path}\")\n\n    return {\n        \"std_losses\": std_losses,\n        \"atlas_losses\": atlas_losses,\n        \"ae_losses\": ae_losses,\n        \"loss_components\": loss_components,\n        \"ami_ae\": ami_ae,\n        \"ami_std\": ami_std,\n        \"ami_atlas\": ami_atlas,\n        \"mse_ae\": mse_ae,\n        \"mse_std\": mse_std,\n        \"mse_atlas\": mse_atlas,\n        \"std_perplexity\": std_perplexity,\n        \"atlas_perplexity\": atlas_perplexity,\n        \"sup_acc\": sup_acc,\n        \"X\": X_test,\n        \"labels\": labels_test,\n        \"colors\": colors_test,\n        \"X_train\": X_train,\n        \"X_test\": X_test,\n        \"labels_train\": labels_train,\n        \"labels_test\": labels_test,\n        \"colors_train\": colors_train,\n        \"colors_test\": colors_test,\n        \"chart_assignments\": chart_assignments,\n        \"recon_ae\": recon_ae_final,\n        \"recon_std\": recon_std_final,\n        \"recon_atlas\": recon_atlas_final,\n        \"model_ae\": model_ae,\n        \"model_std\": model_std,\n        \"model_atlas\": model_atlas,\n        \"config\": config,\n    }\n\n\nprint(\"Training function defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "Create configuration and start training. Adjust the parameters below as needed:\n",
    "\n",
    "- `epochs`: Number of training epochs (default: 100 for quick runs, use 500 for full training)\n",
    "- `n_samples`: Number of CIFAR-10 samples (default: 10000, use 50000 for full dataset)\n",
    "- `batch_size`: Batch size (256 works well on T4 GPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create configuration with Colab-friendly defaults\n",
    "config = TopoEncoderCIFAR10Config(\n",
    "    epochs=100,           # Increase to 500 for full training\n",
    "    n_samples=10000,      # Increase to 50000 for full dataset\n",
    "    batch_size=256,\n",
    "    log_every=10,\n",
    "    save_every=25,\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Epochs: {config.epochs}\")\n",
    "print(f\"  Samples: {config.n_samples}\")\n",
    "print(f\"  Device: {config.device}\")\n",
    "print(f\"  Output dir: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "results = train_benchmark(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Display the final benchmark comparison showing:\n",
    "- Sample images and latent space\n",
    "- Loss curves for all models\n",
    "- Reconstruction quality comparison\n",
    "- AMI and accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display final comparison visualization\n",
    "visualize_results_images(results, CIFAR10_CLASSES, save_path=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"=\" * 50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "ami_atlas = results[\"ami_atlas\"]\n",
    "sup_acc = results[\"sup_acc\"]\n",
    "\n",
    "if ami_atlas > 0.5:\n",
    "    print(f\"TopoEncoder AMI = {ami_atlas:.4f} - Good chart-class alignment!\")\n",
    "else:\n",
    "    print(f\"TopoEncoder AMI = {ami_atlas:.4f} - Charts don't align well with classes.\")\n",
    "\n",
    "if sup_acc > 0.7:\n",
    "    print(f\"Supervised Accuracy = {sup_acc:.4f} - Good classification performance!\")\n",
    "else:\n",
    "    print(f\"Supervised Accuracy = {sup_acc:.4f} - Classification needs improvement.\")\n",
    "\n",
    "print(f\"\\nMSE Comparison:\")\n",
    "print(f\"  Vanilla AE:   {results['mse_ae']:.5f}\")\n",
    "print(f\"  Standard VQ:  {results['mse_std']:.5f}\")\n",
    "print(f\"  TopoEncoder:  {results['mse_atlas']:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Google Drive (Optional)\n",
    "\n",
    "Mount Google Drive and copy outputs to persist them beyond the Colab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save results to Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import shutil\n",
    "# drive_output_dir = '/content/drive/MyDrive/topoencoder_cifar10_results'\n",
    "# shutil.copytree(config.output_dir, drive_output_dir, dirs_exist_ok=True)\n",
    "# print(f\"Results saved to: {drive_output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}