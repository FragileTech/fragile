import copy
from typing import Any, Dict

import numpy as np
from plangym.env import Environment

from fragile.core.base_classes import BaseEnvironment
from fragile.core.states import States


class DiscreteEnv(BaseEnvironment):
    """The DiscreteEnv acts as an interface with `plangym` discrete actions.

    It can interact with any environment that accepts discrete actions and \
    follows the interface of `plangym`.
    """

    STATE_CLASS = States

    def __init__(self, env: Environment):
        """
        Initialize a :class:`DiscreteEnv`.

        Args:
           env: Instance of :class:`plangym.Environment`.
        """
        self._env = env
        self._n_actions = self._env.action_space.n

    @property
    def n_actions(self) -> int:
        """Return the number of different discrete actions that can be taken in the environment."""
        return self._n_actions

    def get_params_dict(self) -> Dict[str, Dict[str, Any]]:
        """Return a dictionary containing the param_dict to build an instance \
        of States that can handle all the data generated by the environment.
        """
        params = {
            "states": {"size": self._env.get_state().shape, "dtype": np.int64},
            "observs": {"size": self._env.observation_space.shape, "dtype": np.float32},
            "rewards": {"dtype": np.float32},
            "ends": {"dtype": np.bool_},
        }
        return params

    # @profile
    def step(self, model_states: States, env_states: States) -> States:
        """
        Set the environment to the target states by applying the specified \
        actions an arbitrary number of time steps.

        Args:
            model_states: States representing the data to be used to act on the environment..
            env_states: States representing the data to be set in the environment.

        Returns:
            States containing the information that describes the new state of the Environment.

        """
        actions = model_states.actions.astype(np.int32)
        n_repeat_actions = model_states.dt if hasattr(model_states, "dt") else 1
        new_states, observs, rewards, ends, infos = self._env.step_batch(
            actions=actions, states=env_states.states, n_repeat_action=n_repeat_actions
        )

        new_state = self._get_new_states(new_states, observs, rewards, ends, len(actions))
        return new_state

    # @profile
    def reset(self, batch_size: int = 1, **kwargs) -> States:
        """
        Reset the environment to the start of a new episode and returns a new \
        States instance describing the state of the Environment.

        Args:
            batch_size: Number of walkers that the returned state will have.
            **kwargs: Ignored. This environment resets without using any external data.

        Returns:
            States instance describing the state of the Environment. The first \
            dimension of the data tensors (number of walkers) will be equal to \
            batch_size.

        """
        state, obs = self._env.reset()
        states = np.array([copy.deepcopy(state) for _ in range(batch_size)])
        observs = np.array([copy.deepcopy(obs) for _ in range(batch_size)])
        rewards = np.zeros(batch_size, dtype=np.float32)
        ends = np.zeros(batch_size, dtype=np.uint8)
        new_states = self._get_new_states(states, observs, rewards, ends, batch_size)
        return new_states

    # @profile
    def _get_new_states(self, states, observs, rewards, ends, batch_size) -> States:
        """Return a new :class:`States` object containing the data generated by the environment."""
        ends = np.array(ends, dtype=np.bool_)
        rewards = np.array(rewards, dtype=np.float32)
        observs = np.array(observs)
        states = np.array(states)
        state = self.create_new_states(batch_size=batch_size)
        state.update(states=states, observs=observs, rewards=rewards, ends=ends)
        return state
