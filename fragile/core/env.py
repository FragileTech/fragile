import numpy as np
from plangym.env import Environment
import torch

from fragile.core.base_classes import BaseEnvironment
from fragile.core.states import BaseStates, States
from fragile.core.utils import to_numpy


class DiscreteEnv(BaseEnvironment):
    """The DiscreteEnv acts as an interface with `plangym`. It can interact with
     any environment that accepts discrete actions and follows the interface of
    `plangym`.

    Args:
            env: Instance of plangym.Environment
            device: Device where the state tensors will be placed.
    """

    def __init__(self, env: Environment, device: str = "cpu"):
        self._env = env
        self._n_actions = self._env.action_space.n
        self.device = device

    @property
    def n_actions(self) -> int:
        """Number of discrete actions that can be taken in the environmetn."""
        return self._n_actions

    def get_params_dict(self) -> dict:
        """Return a dictionary containing the param_dict to build an instance
        of States that can handle all the data generated by the environment.
        """
        params = {
            "states": {
                "sizes": self._env.get_state().shape,
                "dtype": torch.int64,
                "device": self.device,
            },
            "observs": {
                "sizes": self._env.observation_space.shape,
                "dtype": torch.float,
                "device": self.device,
            },
            "rewards": {"sizes": tuple([1]), "dtype": torch.float, "device": self.device},
            "ends": {"sizes": tuple([1]), "dtype": torch.uint8, "device": self.device},
        }
        return params

    # @profile
    def step(
        self,
        actions: [torch.Tensor, np.ndarray],
        env_states: BaseStates,
        n_repeat_action: [int, np.ndarray] = 1,
        *args,
        **kwargs
    ) -> BaseStates:
        """
        Sets the environment to the target states by applying the specified actions an arbitrary
        number of time steps.

        Args:
            actions: Vector containing the actions that will be applied to the target states.
            env_states: BaseStates class containing the state data to be set on the Environment.
            n_repeat_action: Number of times that an action will be applied. If it is an array
                it corresponds to the different dts of each walker.
            *args: Ignored.
            **kwargs: Ignored.

        Returns:
            States containing the information that describes the new state of the Environment.
        """
        states = to_numpy(env_states.states)
        actions = to_numpy(actions).astype(np.int32)
        new_states, observs, rewards, ends, infos = self._env.step_batch(
            actions=actions, states=states, n_repeat_action=n_repeat_action
        )

        new_state = self._get_new_states(new_states, observs, rewards, ends, len(actions))
        return new_state

    # @profile
    def reset(self, batch_size: int = 1, states=None) -> BaseStates:
        """
        Resets the environment to the start of a new episode and returns an
        States instance describing the state of the Environment.
        Args:
            batch_size: Number of walkers that the returned state will have.
            states: Ignored.

        Returns:
            States instance describing the state of the Environment. The first
            dimension of the data tensors (number of walkers) will be equal to
            batch_size.
        """
        state, obs = self._env.reset()
        states = np.array([state.copy() for _ in range(batch_size)])
        observs = np.array([obs.copy() for _ in range(batch_size)])
        rewards = np.zeros(batch_size, dtype=np.float32)
        ends = np.zeros(batch_size, dtype=np.uint8)
        new_states = self._get_new_states(states, observs, rewards, ends, batch_size)
        return new_states

    # @profile
    def _get_new_states(self, states, observs, rewards, ends, batch_size) -> BaseStates:
        ends = np.array(ends, dtype=np.uint8).reshape(-1, 1)
        rewards = np.array(rewards, dtype=np.float32).reshape(-1, 1)
        observs = np.array(observs)
        states = np.array(states)
        state = States(state_dict=self.get_params_dict(), n_walkers=batch_size)
        state.update(states=states, observs=observs, rewards=rewards, ends=ends)
        return state
