from typing import Tuple, Set

import numpy as np

from fragile.core.base_classes import BaseWalkers
from fragile.core.states import States
from fragile.core.utils import relativize, statistics_from_array

float_type = np.float32


class StatesWalkers(States):
    """Keeps track of the data structures used by the `Walkers` class."""

    def __init__(self, batch_size: int):
        self.will_clone = None
        self.compas_ix = None
        self.processed_rewards = None
        self.cum_rewards = None
        self.virtual_rewards = None
        self.distances = None
        self.clone_probs = None
        self.alive_mask = None
        self.id_walkers = None
        self.end_condition = None
        super(StatesWalkers, self).__init__(
            state_dict=self.get_params_dict(), batch_size=batch_size
        )
        self.reset()

    def get_params_dict(self) -> dict:
        """Return a dictionary containing the param_dict to build an instance \
        of States that can handle all the data generated by the environment.
        """
        params = {
            "id_walkers": {"dtype": np.int64},
            "compas_ix": {"dtype": np.int64},
            "processed_rewards": {"dtype": float_type},
            "virtual_rewards": {"dtype": float_type},
            "cum_rewards": {"dtype": float_type},
            "distances": {"dtype": float_type},
            "clone_probs": {"dtype": float_type},
            "will_clone": {"dtype": np.bool_},
            "alive_mask": {"dtype": np.bool_},
            "end_condition": {"dtype": np.bool_},
        }
        return params

    def clone(self) -> Tuple[np.ndarray, np.ndarray]:
        """Perform the clone only on cum_rewards and id_walkers and reset the other arrays."""
        clone, compas = self.will_clone, self.compas_ix
        cum_rewards, id_walkers = self.cum_rewards.copy(), self.id_walkers.copy()
        self.reset()
        self.cum_rewards = cum_rewards
        self.id_walkers = id_walkers
        self.cum_rewards[clone] = cum_rewards[compas][clone]
        self.id_walkers[clone] = id_walkers[compas][clone]
        return clone, compas

    def reset(self):
        """Clear the internal data of the class."""
        self.id_walkers[:] = np.zeros(self.n, dtype=np.int64)
        self.compas_ix[:] = np.arange(self.n)
        self.processed_rewards[:] = np.zeros(self.n, dtype=float_type)
        self.cum_rewards[:] = np.zeros(self.n, dtype=float_type)
        self.virtual_rewards[:] = np.ones(self.n, dtype=float_type)
        self.distances[:] = np.zeros(self.n, dtype=float_type)
        self.clone_probs[:] = np.zeros(self.n, dtype=float_type)
        self.will_clone[:] = np.zeros(self.n, dtype=np.bool_)
        self.alive_mask[:] = np.ones(self.n, dtype=np.bool_)
        self.end_condition[:] = np.zeros(self.n, dtype=np.bool_)


class Walkers(BaseWalkers):
    """
    This class is in charge of performing all the mathematical operations involved in evolving a \
    cloud of walkers.

    """

    def __init__(
        self,
        n_walkers: int,
        env_state_params: dict,
        model_state_params: dict,
        reward_scale: float = 1.0,
        dist_scale: float = 1.0,
        max_iters: int = 1000,
        accumulate_rewards: bool = True,
    ):
        """
        Initialize a new `Walkers` instance.

        Args:
            n_walkers: Number of walkers of the instance.
            env_state_params: Dictionary to instantiate the States of an Environment.
            model_state_params: Dictionary to instantiate the States of an Model.
            reward_scale: Regulates the importance of the reward. Recommended to \
                          keep in the [0, 5] range. Higher values correspond to \
                          higher importance.
            dist_scale: Regulates the importance of the distance. Recommended to \
                          keep in the [0, 5] range. Higher values correspond to \
                          higher importance.
            max_iters: Maximum number of iterations that the walkers are allowed \
                       to perform.
            accumulate_rewards: If True the rewards obtained after transitioning \
                                to a new state will accumulate. If False only the last \
                                reward will be taken into account.
        """

        super(Walkers, self).__init__(
            n_walkers=n_walkers,
            env_state_params=env_state_params,
            model_state_params=model_state_params,
            accumulate_rewards=accumulate_rewards,
        )

        self._model_states = States(state_dict=model_state_params, batch_size=n_walkers)
        self._env_states = States(state_dict=env_state_params, batch_size=n_walkers)
        self._states = StatesWalkers(batch_size=n_walkers)
        self.reward_scale = reward_scale
        self.dist_scale = dist_scale
        self.n_iters = 0
        self.max_iters = max_iters

    def __getattr__(self, item):
        """The Walker can directly access all the data involved in the algorithm as attributes."""
        if hasattr(super(Walkers, self), item):
            return super(Walkers, self).__getattribute__(item)
        elif item in self.states.keys():
            return self._states.__getattribute__(item)
        elif item in self._env_states.keys():
            return self._env_states.__getattribute__(item)
        elif item in self._model_states.keys():
            return self._model_states.__getattribute__(item)
        try:
            return super(Walkers, self).__getattribute__(item)
        except Exception as e:
            import sys

            msg = "\nAttribute {} is not in the class nor in its internal states".format(item)
            raise type(e)(str(e) + " Error at Walkers.__getattr__: %s\n" % msg).with_traceback(
                sys.exc_info()[2]
            )

    def __repr__(self) -> str:
        """Print all the data involved in the current run of the algorithm."""
        text = self.print_stats()
        text += "Env: {}\n".format(self.__repr_state(self._env_states))
        text += "Model {}\n".format(self.__repr_state(self._model_states))
        return text

    def print_stats(self) -> str:
        """Print several statistics of the current state of the swarm."""
        text = "{} iteration {}\n".format(self.__class__.__name__, self.n_iters)
        stats = statistics_from_array(self.states.cum_rewards)
        text += "Total Reward: Mean: {:.3f}, Std: {:.3f}, Max: {:.3f} Min: {:.3f}\n".format(*stats)
        stats = statistics_from_array(self.states.virtual_rewards)
        text += "Virtual Rewards: Mean: {:.3f}, Std: {:.3f}, Max: {:.3f} Min: {:.3f}\n".format(
            *stats
        )
        stats = statistics_from_array(self.states.distances)
        text += "Distances: Mean: {:.3f}, Std: {:.3f}, Max: {:.3f} Min: {:.3f}\n".format(*stats)

        text += "Dead walkers: {:.2f}% Cloned: {:.2f}%\n".format(
            100 * self.states.end_condition.sum() / self.n,
            100 * self.states.will_clone.sum() / self.n,
        )
        return text

    @staticmethod
    def __repr_state(state):
        string = "\n"
        for k, v in state.items():
            if k in ["observs", "states"]:
                continue
            shape = v.shape if hasattr(v, "shape") else None
            new_str = "{} shape {} Mean: {:.3f}, Std: {:.3f}, Max: {:.3f} Min: {:.3f}\n".format(
                k, shape, *statistics_from_array(v)
            )
            string += new_str
        return string

    @property
    def observs(self) -> np.ndarray:
        """Alias for the current observation of the environment."""
        try:
            return self._env_states.observs
        except Exception as e:
            if not hasattr(self._env_states, "observs"):
                raise AttributeError(
                    "observs is not a valid attribute of env_states, please make "
                    "sure it exists before calling self.obs and make sure it is "
                    "an instance of np.ndarray"
                )
            raise e

    @property
    def states(self) -> StatesWalkers:
        """The `StatesWalkers` class that contains the data used by the instance."""
        return self._states

    @property
    def env_states(self) -> States:
        """The `States` class that contains the data used by an environment."""
        return self._env_states

    @property
    def model_states(self) -> States:
        """The `States` class that contains the data used by a Model."""
        return self._model_states

    def calculate_end_condition(self) -> bool:
        """
        Process data from the current state to decide if the iteration process should stop.

        Returns:
            Boolean indicating if the iteration process should be finished. True means \
            it should be stopped, and False means it should continue.
        """
        all_dead = self.states.end_condition.sum() == self.n
        max_iters = self.n_iters > self.max_iters
        self.n_iters += 1
        return all_dead or max_iters

    def calculate_distances(self):
        """Calculate the corresponding distance function for each state with \
        respect to another state chosen at random.

        The internal state is update with the relativized distance values.
        """
        self.states.compas_ix = self.get_alive_compas()
        distances = np.linalg.norm(
            self.observs.reshape(self.n, -1)
            - self.observs[self.states.compas_ix].reshape(self.n, -1),
            axis=1,
        ).flatten()
        self.states.update(distances=relativize(distances))

    def calculate_virtual_reward(self):
        """
        Calculate the virtual reward and update the internal state.

        The cumulative_reward is transformed with the relativize function. The distances stored
        in the internal state are already assumed to be transformed.
        """
        processed_rewards = relativize(self.states.cum_rewards)
        virt_rw = processed_rewards ** self.reward_scale * self.states.distances ** self.dist_scale
        self.states.update(virtual_rewards=virt_rw)
        self.states.update(processed_rewards=processed_rewards)

    def get_alive_compas(self) -> np.ndarray:
        """
        Return the indexes of alive companions chosen at random.
        Returns:
            Numpy array containing the int indexes of alive walkers chosen at random with
            repetition.
        """
        self.states.alive_mask = np.logical_not(self.states.end_condition)
        if not self.states.alive_mask.any():  # No need to sample if all walkers are dead.
            return np.arange(self.n)
        compas_ix = np.arange(self.n)[self.states.alive_mask]
        compas = np.random.choice(compas_ix, self.n, replace=True)
        compas[: len(compas_ix)] = compas_ix
        return compas

    def update_clone_probs(self):
        """
        Calculate the new probability of cloning for each walker.

        Updates the internal state with both the probability of cloning and the index of the
        randomly chosen companions that were selected to compare the virtual rewards.
        """
        if (self.states.virtual_rewards.flatten() == self.states.virtual_rewards[0]).all():
            clone_probs = np.zeros(self.n, dtype=float_type) / float(self.n)
            compas_ix = np.arange(self.n)
        else:
            compas_ix = self.get_alive_compas()
            # This value can be negative!!
            companions = self.virtual_rewards[compas_ix]
            clone_probs = (companions - self.states.virtual_rewards) / self.states.virtual_rewards
        self.states.update(clone_probs=clone_probs)
        self.states.update(compas_ix=compas_ix)

    # @profile
    def balance(self) -> Tuple[Set[int], Set[int]]:
        """
        Perform an iteration of the FractalAI algorithm for balancing distributions.

        It performs the necessary calculations to determine which walkers will clone, \
        and performs the cloning process.

        Returns:
            A tuple containing two sets: The first one represent the unique ids \
            of the states for each walker at the start of the iteration. The second \
            one contains the ids of the states after the cloning process.
        """
        old_ids = set(self.states.id_walkers.astype(int).tolist())
        self.calculate_distances()
        self.calculate_virtual_reward()
        self.update_clone_probs()
        self.clone_walkers()
        new_ids = set(self.states.id_walkers.astype(int).tolist())
        return old_ids, new_ids

    def clone_walkers(self):
        """Sample the clone probability distribution and clone the walkers accordingly."""
        rands = np.random.random(self.n)
        will_clone = self.states.clone_probs > rands
        # Dead walkers always clone
        dead_ix = np.arange(self.n)[self.states.end_condition]
        will_clone[dead_ix] = 1
        self.states.update(will_clone=will_clone)

        clone, compas = self.states.clone()
        self._env_states.clone(will_clone=clone, compas_ix=compas)
        self._model_states.clone(will_clone=clone, compas_ix=compas)

    def reset(self, env_states: States = None, model_states: States = None):
        """Restart all the internal states involved in the algorithm iteration.

        After reset a new run of the algorithm will be ready to be launched.
        """
        self.update_states(env_states=env_states, model_states=model_states)
        self.states.reset()
        self.n_iters = 0

    def update_states(self, env_states: States = None, model_states: States = None, **kwargs):
        """
        Update the States variables that do not contain internal data and \
        accumulate the rewards in the internal states if applicable.

        Args:
            env_states: States containing the data associated with the Environment.
            model_states: States containing data associated with the Environment.
            **kwargs: Internal states will be updated via keyword arguments.
        """
        if isinstance(env_states, States):
            self._env_states.update(env_states)
        if hasattr(env_states, "rewards"):
            self._accumulate_and_update_rewards(env_states.rewards)
        if isinstance(model_states, States):
            self._model_states.update(model_states)
        if kwargs:
            self.states.update(**kwargs)

    def _accumulate_and_update_rewards(self, rewards: np.ndarray):
        """
        Use as reward either the sum of all the rewards received during the \
        current run, or use the last reward value received as reward.

        Args:
            rewards: Array containing the last rewards received by every walker.
        """
        if self._accumulate_rewards:
            cum_rewards = self.states.cum_rewards + rewards
        else:
            cum_rewards = rewards
        self.states.update(cum_rewards=cum_rewards)
