from typing import Callable, Union

import numpy as np
import pandas as pd
from pyearth import Earth as EarthModel
from scipy.optimize import minimize
from scipy.optimize import Bounds as ScipyBounds
from statsmodels.tsa.tsatools import lagmat

from fragile.core.base_classes import BaseEnvironment
from fragile.core.states import States
from fragile.core.models import Bounds


class Function(BaseEnvironment):
    """
    Environment that represents an arbitrary mathematical function.
    """

    STATE_CLASS = States

    def __init__(
        self,
        function: Callable,
        shape,
        high: Union[int, float, np.ndarray] = None,
        low: Union[int, float, np.ndarray] = None,
        bounds: Bounds = None,
        *args,
        **kwargs
    ):
        if bounds is not None and not isinstance(bounds, Bounds):
            raise TypeError("Bounds needs to be an instance of Bounds, found {}".format(bounds))
        elif high is None and low is None and bounds is None:
            raise TypeError("Need to specify either bounds or high and low. All three were None")
        super(Function, self).__init__()
        self.function = function
        self.bounds = bounds if bounds is not None else Bounds(high=high, low=low, shape=shape)
        self.shape = shape

    @property
    def func(self):
        return self.function

    def __repr__(self):
        text = "{} with function {}, obs shape {}, and bounds: {}".format(
            self.__class__.__name__, self.func.__name__, self.shape, self.bounds
        )
        return text

    def get_params_dict(self) -> dict:
        """Return a dictionary containing the param_dict to build an instance
        of States that can handle all the data generated by the environment.
        """
        params = {
            "observs": {"size": self.shape, "dtype": np.float},
            "rewards": {"dtype": np.float},
            "ends": {"dtype": np.bool_},
        }
        return params

    def step(self, model_states: States, env_states: States) -> States:
        """
        Sets the environment to the target states by applying the specified actions an arbitrary
        number of time steps.

        Args:
            model_states: States corresponding to the model data.
            env_states: States class containing the state data to be set on the Environment.

        Returns:
            States containing the information that describes the new state of the Environment.
        """
        new_points = (
            # model_states.actions * model_states.dt.reshape(env_states.n, -1) + env_states.observs
            model_states.actions + env_states.observs
        )
        rewards = self.function(new_points).flatten()
        ends = self.calculate_end(points=new_points)

        last_states = self._get_new_states(new_points, rewards, ends, model_states.n)
        return last_states

    def reset(self, batch_size: int = 1, **kwargs) -> States:
        """
        Resets the environment to the start of a new episode and returns an
        States instance describing the state of the Environment.
        Args:
            batch_size: Number of walkers that the returned state will have.
            **kwargs: Ignored. This environment resets without using any external data.

        Returns:
            States instance describing the state of the Environment. The first
            dimension of the data tensors (number of walkers) will be equal to
            batch_size.
        """
        ends = np.zeros(batch_size, dtype=np.bool_)
        new_points = self._sample_init_points(batch_size=batch_size)
        rewards = self.function(new_points).flatten()
        new_states = self._get_new_states(new_points, rewards, ends, batch_size=batch_size)
        return new_states

    def calculate_end(self, points):
        return np.logical_not(self.bounds.points_in_bounds(points)).flatten()

    def _sample_init_points(self, batch_size: int):
        new_points = np.zeros(tuple([batch_size]) + self.shape, dtype=np.float32)
        for i in range(batch_size):
            new_points[i, :] = self.random_state.uniform(low=self.bounds.low,
                                                         high=self.bounds.high,
                                                         size=self.shape)
        return new_points

    def _get_new_states(self, states, rewards, ends, batch_size) -> States:
        state = self.create_new_states(batch_size=batch_size)
        state.update(states=states, observs=states, rewards=rewards, ends=ends)
        return state


class Minimizer:
    def __init__(self, function: Function, bounds=None, *args, **kwargs):
        self.env = function
        self.function = function.function
        self.bounds = self.env.bounds if bounds is None else bounds
        self.args = args
        self.kwargs = kwargs

    def minimize(self, x):
        def _optimize(x):
            try:
                x = x.reshape((1,) + x.shape)
                y = self.function(x)
            except (ZeroDivisionError, RuntimeError) as e:
                y = np.inf
            return y
        bounds = ScipyBounds(ub=self.bounds.high if self.bounds is not None else None,
                             lb=self.bounds.low if self.bounds is not None else None)
        return minimize(_optimize, x, bounds=bounds, *self.args, **self.kwargs)

    def minimize_point(self, x):
        optim_result = self.minimize(x)
        point = optim_result["x"]
        reward = float(optim_result["fun"])
        return point, reward

    def minimize_batch(self, x: np.ndarray):
        result = np.zeros_like(x)
        rewards = np.zeros((x.shape[0], 1))
        for i in range(x.shape[0]):
            new_x, reward = self.minimize_point(x[i, :])
            result[i, :] = new_x
            rewards[i, :] = float(reward)

        return result, rewards


class MinimizerWrapper(Function):

    def __init__(self, function: Function, *args, **kwargs):
        self.env = function
        self.minimizer = Minimizer(function=self.env, *args, **kwargs)

    def __getattr__(self, item):
        return getattr(self.env, item)

    def __repr__(self):
        return self.env.__repr__()

    def step(
        self,
        model_states: States,
        env_states: States,
    ) -> States:
        """
        Sets the environment to the target states by applying the specified actions an arbitrary
        number of time steps.

        Args:

            env_states: States class containing the state data to be set on the Environment.
            *args: Ignored.
            **kwargs: Ignored.

        Returns:
            States containing the information that describes the new state of the Environment.
        """
        env_states = super(MinimizerWrapper, self).step(model_states=model_states,
                                                        env_states=env_states)

        new_points, rewards = self.minimizer.minimize_batch(env_states.observs)
        ends = np.logical_not(self.bounds.points_in_bounds(new_points)).flatten()
        optim_states = self._get_new_states(new_points, rewards.flatten(), ends, model_states.n)
        return optim_states


from functools import partial
def evaluate_one(point, y, data, columns):
    cols = [c for (c, p) in zip(columns, point) if p]
    if len(cols) == 0:
        return 0, 0
    X = data[cols].values
    max_entropy = (2 - (1 / len(X)) ** (1 / len(X))) ** len(X)

    earth = EarthModel()
    model = earth.fit(X, y)
    pred = model.predict(X)
    error = np.abs(pred.flatten() - y.flatten())
    norm = error / error.sum()
    entropy = np.prod(2 - norm ** norm)
    entropy = entropy / max_entropy
    return entropy, model.rsq_


class Earth(Function):

    def __init__(self, data_path, target="ACINETO", *args, **kwargs):

        self.X, self.y = None, None
        self.n_cols = None
        self.load_data(data_path, target=target)
        self.earth = EarthModel()
        super(Earth, self).__init__(function=self._evaluate_model,
                                    shape=(len(self.X.columns),),
                                    low=0,
                                    high=len(self.X.columns) - 1,
                                    *args,
                                    **kwargs)
        self.entropy = 0
        self.rsq = 0
        from multiprocessing import Pool

        self.pool = Pool()

    def __repr__(self):

        msg = "Model R2: {:.3f}, entropy: {:.3f}\n".format(self.rsq, self.entropy)
        return msg + super(Earth, self).__repr__()

    def load_data(self, data_path="data.xls", target="ACINETO"):
        raw_data = pd.read_excel(data_path, index_col="numero")
        df = raw_data.drop("KLEBCRE", axis=1).set_index("year_month").copy()

        def get_lags(df, col, lags, max_lags: int = 7):
            data = pd.concat([df[[col]], lagmat(df[[col]], max_lags, use_pandas=True)],
                             axis=1).iloc[max_lags:].copy()
            cols = ["{}.L.{}".format(col, i) if i != 0 else str(col) for i in lags]

            return data.loc[:, cols].copy()

        lags = pd.concat([get_lags(df, c, range(6) if c == target else range(1, 3))
                          for c in df.columns], axis=1)
        self.X = lags.drop(target, axis=1)
        self.y = lags[target].values
        self.n_cols = len(self.X.columns)

    def _evaluate_model_slow(self, points: np.ndarray) -> np.ndarray:
        vals = [self._evaluate_one(p) for p in points.tolist()]
        return np.array(vals)

    def _evaluate_model(self, points):

        func = partial(evaluate_one, y=np.array(self.y), data=pd.DataFrame(self.X),
                       columns=list(self.X.columns))
        result = self.pool.map(func, points)
        entropy, r_squared = tuple(zip(*result))
        entropy = np.array(entropy)
        r_squared = np.array(r_squared)
        self.entropy = max(self.entropy, entropy.max())
        self.rsq = max(self.rsq, r_squared.max())
        return entropy * r_squared

    def _evaluate_one(self, point):
        cols = [c for (c, p) in zip(self.X.columns, point) if p]
        if len(cols) == 0:
            return 100
        X = self.X[cols].values
        max_entropy = (2 - (1 / len(X)) ** (1 / len(X))) ** len(X)

        self.earth = EarthModel()
        model = self.earth.fit(X, self.y)
        pred = model.predict(X)
        error = np.abs(pred.flatten() - self.y.flatten())
        norm = error / error.sum()
        entropy = np.prod(2 - norm ** norm)
        entropy = entropy / max_entropy
        self.entropy = max(self.entropy, entropy)
        self.rsq = max(self.rsq, model.rsq_)
        return entropy * model.rsq_

    def _sample_init_points(self, batch_size: int):
        new_points = (np.random.random(tuple([batch_size]) + self.shape) > 0.5).astype(int)

        return new_points

    def calculate_end(self, points):
        return np.zeros(len(points), dtype=bool)
