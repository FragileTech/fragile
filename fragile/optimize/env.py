from typing import Callable, Union

import numpy as np

from fragile.core.base_classes import BaseEnvironment
from fragile.core.states import States
from fragile.core.models import Bounds


class Function(BaseEnvironment):
    """
    Environment that represents an arbitrary mathematical function.
    """

    STATE_CLASS = States

    def __init__(
        self,
        function: Callable,
        shape,
        high: Union[int, float, np.ndarray] = None,
        low: Union[int, float, np.ndarray] = None,
        bounds: Bounds = None,
        *args,
        **kwargs
    ):
        if bounds is not None and not isinstance(bounds, Bounds):
            raise TypeError("Bounds needs to be an instance of Bounds, found {}".format(bounds))
        elif high is None and low is None and bounds is None:
            raise TypeError("Need to specify either bounds or high and low. All three were None")
        super(Function, self).__init__()
        self.function = function
        self.bounds = bounds if bounds is not None else Bounds(high=high, low=low, shape=shape)
        self.shape = shape

    def __repr__(self):
        text = "{} with function {}, obs shape {}, and bounds: {}".format(
            self.__class__.__name__, self.function.__name__, self.shape, self.bounds
        )
        return text

    def get_params_dict(self) -> dict:
        """Return a dictionary containing the param_dict to build an instance
        of States that can handle all the data generated by the environment.
        """
        params = {
            "observs": {"size": self.shape, "dtype": np.float},
            "rewards": {"dtype": np.float},
            "ends": {"dtype": np.bool_},
        }
        return params

    def step(self, model_states: States, env_states: States) -> States:
        """
        Sets the environment to the target states by applying the specified actions an arbitrary
        number of time steps.

        Args:
            model_states: States corresponding to the model data.
            env_states: States class containing the state data to be set on the Environment.

        Returns:
            States containing the information that describes the new state of the Environment.
        """
        new_points = (
            # model_states.actions * model_states.dt.reshape(env_states.n, -1) + env_states.observs
            model_states.actions + env_states.observs
        )

        rewards = self.function(new_points).flatten()
        ends = np.logical_not(self.bounds.points_in_bounds(new_points)).flatten()

        last_states = self._get_new_states(new_points, rewards, ends, model_states.n)
        return last_states

    def reset(self, batch_size: int = 1, **kwargs) -> States:
        """
        Resets the environment to the start of a new episode and returns an
        States instance describing the state of the Environment.
        Args:
            batch_size: Number of walkers that the returned state will have.
            **kwargs: Ignored. This environment resets without using any external data.

        Returns:
            States instance describing the state of the Environment. The first
            dimension of the data tensors (number of walkers) will be equal to
            batch_size.
        """
        ends = np.zeros(batch_size, dtype=np.bool_)
        new_points = self._sample_init_points(batch_size=batch_size)
        rewards = self.function(new_points).flatten()
        new_states = self._get_new_states(new_points, rewards, ends, batch_size=batch_size)
        return new_states

    def _sample_init_points(self, batch_size: int):
        new_points = np.zeros(tuple([batch_size]) + self.shape, dtype=np.float32)
        for i in range(batch_size):
            new_points[i, :] = self.random_state.uniform(low=self.bounds.low,
                                                         high=self.bounds.high,
                                                         size=self.shape)
        return new_points

    def _get_new_states(self, states, rewards, ends, batch_size) -> States:
        state = self.create_new_states(batch_size=batch_size)
        state.update(states=states, observs=states, rewards=rewards, ends=ends)
        return state
