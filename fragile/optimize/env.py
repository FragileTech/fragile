from typing import Callable, Union

import numpy as np

from fragile.core.base_classes import BaseEnvironment
from fragile.core.states import States


class Bounds:
    def __init__(
        self,
        high: Union[np.ndarray, float, int] = np.inf,
        low: Union[np.ndarray, float, int] = -np.inf,
    ):
        self.high = high
        self.low = low

    def clip(self, points):
        np.clip(points, self.low, self.high)

    def points_in_bounds(self, points: np.ndarray) -> np.ndarray:
        return (self.clip(points) == points).all(axis=1).flatten()


class Function(BaseEnvironment):
    """
    Environment that represents an arbitrary mathematical function.
    """

    STATE_CLASS = States

    def __init__(self, function: Callable, shape, high: Union[int, float, np.ndarray],
                 low: Union[int, float, np.ndarray],
                 *args, **kwargs):
        super(Function, self).__init__()
        self.function = function
        self.bounds = Bounds(high=high, low=low)
        self.shape = shape

    def __repr__(self):
        text = "{} with function {}, obs shape {}, and bounds: {}".format(
            self.__class__.__name__, self.function.__name__, self.shape, self.bounds
        )
        return text

    def get_params_dict(self) -> dict:
        """Return a dictionary containing the param_dict to build an instance
        of States that can handle all the data generated by the environment.
        """
        params = {
            "observs": {"size": self.shape, "dtype": np.float},
            "rewards": {"dtype": np.float},
            "ends": {"dtype": np.bool_},
        }
        return params

    def step(self, model_states: States, env_states: States) -> States:
        """
        Sets the environment to the target states by applying the specified actions an arbitrary
        number of time steps.

        Args:
            model_states: States corresponding to the model data.
            env_states: States class containing the state data to be set on the Environment.

        Returns:
            States containing the information that describes the new state of the Environment.
        """
        new_points = (
            model_states.actions * model_states.dt.reshape(env_states.n, -1) + env_states.observs
        )

        rewards = self.function(new_points).flatten()
        ends = self.out_of_domain(env_states)

        last_states = self._get_new_states(new_points, rewards, ends, model_states.n)
        return last_states

    def reset(self, batch_size: int = 1, **kwargs) -> States:
        """
        Resets the environment to the start of a new episode and returns an
        States instance describing the state of the Environment.
        Args:
            batch_size: Number of walkers that the returned state will have.
            **kwargs: Ignored. This environment resets without using any external data.

        Returns:
            States instance describing the state of the Environment. The first
            dimension of the data tensors (number of walkers) will be equal to
            batch_size.
        """
        ends = np.zeros(batch_size, dtype=np.bool_)
        new_points = self._sample_init_points(batch_size=batch_size)
        rewards = self.function(new_points).flatten()
        new_states = self._get_new_states(new_points, rewards, ends, batch_size=batch_size)
        return new_states

    def out_of_domain(self, states: States) -> np.ndarray:
        """
        Return a boolean array indicating if the states are in the function domain.

        Args:
            states: States containing all the information about the current state \
                    of the simulation.

        Returns:
            np.ndarray of booleans. The returned array will contain False on the \
            indexes of the walkers that are inside the domain function, and True \
            if a walkers is out of domain.
        """
        return np.zeros(states.n, dtype=np.bool_)

    def _sample_init_points(self, batch_size: int):
        new_points = np.zeros(tuple([batch_size]) + self.shape, dtype=np.float32)
        for i in range(batch_size):
            new_points[i, :] = self.random_state.uniform(0, 1, self.shape)
        return new_points

    def _get_new_states(self, states, rewards, ends, batch_size) -> States:
        state = self.create_new_states(batch_size=batch_size)
        state.update(states=states, observs=states, rewards=rewards, ends=ends)
        return state
