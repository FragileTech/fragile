from typing import Callable

import numpy as np

from fragile.core.base_classes import BaseEnvironment, BaseStates
from fragile.core.states import States


class Function(BaseEnvironment):
    def __init__(self, function: Callable, shape, bounds=None, *args, **kwargs):
        super(Function, self).__init__(*args, **kwargs)
        self.function = function
        self.bounds = bounds
        self.shape = shape
        self._last_states = None

    def __repr__(self):
        text = "{} with function {}, obs shape {}, and bounds: {}".format(
            self.__class__.__name__, self.function.__name__, self.shape, self.bounds
        )
        return text

    def get_params_dict(self) -> dict:
        """Return a dictionary containing the param_dict to build an instance
        of States that can handle all the data generated by the environment.
        """
        params = {
            "states": {"size": self.shape, "dtype": np.int64},
            "observs": {"size": self.shape, "dtype": np.float},
            "rewards": {"dtype": np.float},
            "ends": {"dtype": np.bool_},
        }
        return params

    def step(
        self,
        actions: np.ndarray,
        env_states: BaseStates,
        n_repeat_action: [int, np.ndarray] = 1,
        *args,
        **kwargs
    ) -> BaseStates:
        """
        Sets the environment to the target states by applying the specified actions an arbitrary
        number of time steps.

        Args:
            actions: Vector containing the actions that will be applied to the target states.
            env_states: BaseStates class containing the state data to be set on the Environment.
            n_repeat_action: Number of times that an action will be applied. If it is an array
                it corresponds to the different dts of each walker.
            *args: Ignored.
            **kwargs: Ignored.

        Returns:
            States containing the information that describes the new state of the Environment.
        """
        states = env_states.states
        new_points = actions * n_repeat_action + states

        rewards = self.function(new_points).flatten()
        ends = self.boundary_condition(new_points, rewards)

        last_states = self._get_new_states(new_points, rewards, ends, len(actions))
        return last_states

    def boundary_condition(self, points, rewards):
        ends = np.ones(rewards.shape, dtype=np.bool_)
        ends[self.are_in_bounds(points)] = 0
        return ends

    def _sample_init_points(self, batch_size: int):
        new_points = np.zeros(tuple([batch_size]) + self.shape, dtype=np.float32)
        for i in range(batch_size):
            new_points[i, :] = [np.random.uniform(l, h) for l, h in self.bounds]
        return new_points

    def reset(self, batch_size: int = 1, states=None) -> BaseStates:
        """
        Resets the environment to the start of a new episode and returns an
        States instance describing the state of the Environment.
        Args:
            batch_size: Number of walkers that the returned state will have.
            states: Ignored.

        Returns:
            States instance describing the state of the Environment. The first
            dimension of the data tensors (number of walkers) will be equal to
            batch_size.
        """
        ends = np.zeros(batch_size, dtype=np.bool_)
        new_points = self._sample_init_points(batch_size=batch_size)
        rewards = self.function(new_points).flatten()
        new_states = self._get_new_states(new_points, rewards, ends, batch_size=batch_size)
        return new_states

    def are_in_bounds(self, states):
        in_bounds = np.ones(len(states), dtype=np.bool_)
        if self.bounds is None:
            return in_bounds
        for i, (min_val, max_val) in enumerate(self.bounds):
            valid_min = states[:, i] > min_val
            valid_max = states[:, i] < max_val
            valid_vector = valid_max & valid_min
            in_bounds = valid_vector & in_bounds
        return in_bounds

    def _get_new_states(self, states, rewards, ends, batch_size) -> BaseStates:
        state = States(state_dict=self.get_params_dict(), batch_size=batch_size)
        state.update(states=states, observs=states, rewards=rewards, ends=ends)
        return state
