from typing import Callable, Union

import numpy as np

from scipy.optimize import minimize
from scipy.optimize import Bounds as ScipyBounds

from fragile.core.base_classes import BaseEnvironment
from fragile.core.states import States
from fragile.core.models import Bounds


class Function(BaseEnvironment):
    """
    Environment that represents an arbitrary mathematical function.
    """

    STATE_CLASS = States

    def __init__(
        self,
        function: Callable,
        shape,
        high: Union[int, float, np.ndarray] = None,
        low: Union[int, float, np.ndarray] = None,
        bounds: Bounds = None,
        *args,
        **kwargs
    ):
        if bounds is not None and not isinstance(bounds, Bounds):
            raise TypeError("Bounds needs to be an instance of Bounds, found {}".format(bounds))
        elif high is None and low is None and bounds is None:
            raise TypeError("Need to specify either bounds or high and low. All three were None")
        super(Function, self).__init__()
        self.function = function
        self.bounds = bounds if bounds is not None else Bounds(high=high, low=low, shape=shape)
        self.shape = shape

    @property
    def func(self):
        return self.function

    def __repr__(self):
        text = "{} with function {}, obs shape {}, and bounds: {}".format(
            self.__class__.__name__, self.func.__name__, self.shape, self.bounds
        )
        return text

    def get_params_dict(self) -> dict:
        """Return a dictionary containing the param_dict to build an instance
        of States that can handle all the data generated by the environment.
        """
        params = {
            "states": {"size": self.shape, "dtype": np.float},
            "observs": {"size": self.shape, "dtype": np.float},
            "rewards": {"dtype": np.float},
            "ends": {"dtype": np.bool_},
        }
        return params

    def step(self, model_states: States, env_states: States) -> States:
        """
        Sets the environment to the target states by applying the specified actions an arbitrary
        number of time steps.

        Args:
            model_states: States corresponding to the model data.
            env_states: States class containing the state data to be set on the Environment.

        Returns:
            States containing the information that describes the new state of the Environment.
        """
        new_points = (
            # model_states.actions * model_states.dt.reshape(env_states.n, -1) + env_states.observs
            model_states.actions
            + env_states.observs
        )
        ends = self.calculate_end(points=new_points)
        rewards = self.function(new_points).flatten()

        last_states = self._get_new_states(new_points, rewards, ends, model_states.n)
        return last_states

    def reset(self, batch_size: int = 1, **kwargs) -> States:
        """
        Resets the environment to the start of a new episode and returns an
        States instance describing the state of the Environment.
        Args:
            batch_size: Number of walkers that the returned state will have.
            **kwargs: Ignored. This environment resets without using any external data.

        Returns:
            States instance describing the state of the Environment. The first
            dimension of the data tensors (number of walkers) will be equal to
            batch_size.
        """
        ends = np.zeros(batch_size, dtype=np.bool_)
        new_points = self._sample_init_points(batch_size=batch_size)
        rewards = self.function(new_points).flatten()
        new_states = self._get_new_states(new_points, rewards, ends, batch_size=batch_size)
        return new_states

    def calculate_end(self, points):
        return np.logical_not(self.bounds.points_in_bounds(points)).flatten()

    def _sample_init_points(self, batch_size: int):
        new_points = np.zeros(tuple([batch_size]) + self.shape, dtype=np.float32)
        for i in range(batch_size):
            new_points[i, :] = self.random_state.uniform(
                low=self.bounds.low, high=self.bounds.high, size=self.shape
            )
        return new_points

    def _get_new_states(self, states, rewards, ends, batch_size) -> States:
        state = self.create_new_states(batch_size=batch_size)
        state.update(states=states, observs=states, rewards=rewards, ends=ends)
        return state


class Minimizer:
    def __init__(self, function: Function, bounds=None, *args, **kwargs):
        self.env = function
        self.function = function.function
        self.bounds = self.env.bounds if bounds is None else bounds
        self.args = args
        self.kwargs = kwargs

    def minimize(self, x):
        def _optimize(x):
            try:
                x = x.reshape((1,) + x.shape)
                y = self.function(x)
            except (ZeroDivisionError, RuntimeError) as e:
                y = np.inf
            return y

        bounds = ScipyBounds(
            ub=self.bounds.high if self.bounds is not None else None,
            lb=self.bounds.low if self.bounds is not None else None,
        )
        return minimize(_optimize, x, bounds=bounds, *self.args, **self.kwargs)

    def minimize_point(self, x):
        optim_result = self.minimize(x)
        point = optim_result["x"]
        reward = float(optim_result["fun"])
        return point, reward

    def minimize_batch(self, x: np.ndarray):
        result = np.zeros_like(x)
        rewards = np.zeros((x.shape[0], 1))
        for i in range(x.shape[0]):
            new_x, reward = self.minimize_point(x[i, :])
            result[i, :] = new_x
            rewards[i, :] = float(reward)
        return result, rewards


class MinimizerWrapper(Function):
    def __init__(self, function: Function, *args, **kwargs):
        self.env = function
        self.minimizer = Minimizer(function=self.env, *args, **kwargs)

    def __getattr__(self, item):
        return getattr(self.env, item)

    def __repr__(self):
        return self.env.__repr__()

    def step(self, model_states: States, env_states: States) -> States:
        """
        Sets the environment to the target states by applying the specified actions an arbitrary
        number of time steps.

        Args:

            env_states: States class containing the state data to be set on the Environment.
            *args: Ignored.
            **kwargs: Ignored.

        Returns:
            States containing the information that describes the new state of the Environment.
        """
        env_states = super(MinimizerWrapper, self).step(
            model_states=model_states, env_states=env_states
        )

        new_points, rewards = self.minimizer.minimize_batch(env_states.observs)
        ends = np.logical_not(self.bounds.points_in_bounds(new_points)).flatten()
        optim_states = self._get_new_states(new_points, rewards.flatten(), ends, model_states.n)
        return optim_states
